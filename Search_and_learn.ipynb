{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/Search_and_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xctusmJ6BZ6_"
      },
      "source": [
        "# Scaling Test-Time Compute for Longer Thinking in LLMs\n",
        "\n",
        "_Authored by: [Sergio Paniego](https://github.com/sergiopaniego)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmgoppItAO7B"
      },
      "source": [
        "🚨 **WARNING**: This notebook is **resource-intensive** and requires substantial computational power. If you’re running this in **Colab**, it will utilize an **A100 GPU**.\n",
        "\n",
        "---\n",
        "\n",
        "In this recipe, we'll guide you through extending the inference time for an **Instruct LLM system** using **test-time compute** to solve more challenging problems, such as **complex math problems**. This approach, inspired by [**OpenAI o1-o3 models**](https://openai.com/index/learning-to-reason-with-llms/), demonstrates that **longer reasoning time** during inference can enhance model performance.\n",
        "\n",
        "This technique builds on experiments shared in [this **blog post**](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute), which show that smaller models, like the **1B** and **3B Llama Instruct models**, can outperform much larger ones on the **MATH-500 benchmark** when given enough **\"time to think\"**. Recent research from [DeepMind](https://arxiv.org/abs/2408.03314) suggests that **test-time compute** can be scaled optimally through strategies like iterative self-refinement or using a reward model.\n",
        "\n",
        "The blog introduces a [**new repository**](https://github.com/huggingface/search-and-learn) for running these experiments. In this recipe, we'll focus on building a **small chatbot** that engages in **longer reasoning** to tackle **harder problems** using small open models.\n",
        "\n",
        "![Instruct LLM Methodology](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/methods-thumbnail.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twKCzVIg71Xa"
      },
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "Let’s start by installing the [search-and-learn](https://github.com/huggingface/search-and-learn) repository! 🚀  \n",
        "This repo is designed to replicate the experimental results and is not a Python pip package. However, we can still use it to generate our system. To do so, we’ll need to install it from source with the following steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0YDC2_7XTm8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/huggingface/search-and-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT3jH_d_XcEb"
      },
      "outputs": [],
      "source": [
        "%cd search-and-learn\n",
        "!pip install -e '.[dev]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAQHu9T176zh"
      },
      "source": [
        "Log in to Hugging Face to access [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), as it is a gated model! 🗝️  \n",
        "If you haven't previously requested access, you'll need to submit a request before proceeding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnEaTlFYZF_H"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX07zCTA8MWL"
      },
      "source": [
        "## 2. Setup the Large Language Model (LLM) and the Process Reward Model (PRM) 💬\n",
        "\n",
        "As illustrated in the diagram, the system consists of an LLM that generates intermediate answers based on user input, a [PRM model](https://huggingface.co/papers/2211.14275) that evaluates and scores these answers, and a search strategy that uses the PRM feedback to guide the subsequent steps in the search process until reaching the final answer.\n",
        "\n",
        "Let’s begin by initializing each model. For the LLM, we’ll use the [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) model, and for the PRM, we’ll use the [RLHFlow/Llama3.1-8B-PRM-Deepseek-Data](https://huggingface.co/RLHFlow/Llama3.1-8B-PRM-Deepseek-Data) model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkJw0x7gDJEY"
      },
      "source": [
        "![system](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/system.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG1MolfxmZ7M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from vllm import LLM\n",
        "from sal.models.reward_models import RLHFFlow\n",
        "\n",
        "model_path=\"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "prm_path=\"RLHFlow/Llama3.1-8B-PRM-Deepseek-Data\"\n",
        "\n",
        "llm = LLM(\n",
        "    model=model_path,\n",
        "    gpu_memory_utilization=0.5,  # Utilize 50% of GPU memory\n",
        "    enable_prefix_caching=True,  # Optimize repeated prefix computations\n",
        "    seed=42,                     # Set seed for reproducibility\n",
        ")\n",
        "\n",
        "prm = RLHFFlow(prm_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYtPn0_V_YRx"
      },
      "source": [
        "### 2.1 Instantiate the Question, Search Strategy, and Call the Pipeline\n",
        "\n",
        "Now that we've set up the LLM and PRM, let's proceed by defining the question, selecting a search strategy to retrieve relevant information, and calling the pipeline to process the question through the models.\n",
        "\n",
        "1. **Instantiate the Question**: In this step, we define the input question that the system will answer, considering the given context.\n",
        "\n",
        "2. **Search Strategy**: The system currently supports the following search strategies: `best_of_n`, `beam_search`, and `dvts` (see diagram). For this example, we'll use `best_of_n`, but you can easily switch to any of the other strategies based on your needs. We need to define some configuration parameters for the configuration of the search strategy. You can check the full list [here](https://github.com/huggingface/search-and-learn/blob/main/src/sal/config.py).\n",
        "\n",
        "3. **Call the Pipeline**: With the question and search strategy in place, we’ll call the inference pipeline, processing the inputs through both the LLM and PRM to generate the final answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSWINPerJrhm"
      },
      "source": [
        "![](https://huggingface.co/datasets/HuggingFaceH4/blogpost-images/resolve/main/search-strategies.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z69xD6i2L5a6"
      },
      "source": [
        "The first step is to clearly define the question that the system will answer. This ensures that we have a precise task for the model to tackle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83puLxhzsOM0"
      },
      "outputs": [],
      "source": [
        "question_text = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
        "input_batch = {\"problem\": [question_text]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGpyzMNkAO7H"
      },
      "source": [
        "Next, we define the configuration, including parameters like the number of candidate answers `(N)`, and choose the search strategy that will be used. The search strategy dictates how we explore the potential answers. In this case, we'll use `best_of_n`.\n",
        "\n",
        "With the question and configuration in place, we use the selected search strategy to generate multiple candidate answers. These candidates are evaluated based on their relevance and quality and the final answer is returned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6s6GS16QZLV"
      },
      "outputs": [],
      "source": [
        "from sal.config import Config\n",
        "from sal.search import beam_search, best_of_n, dvts\n",
        "\n",
        "config = Config()\n",
        "config.n=32 # Number of answers to generate during the search\n",
        "\n",
        "search_result = best_of_n(x=input_batch, config=config, llm=llm, prm=prm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsLHD_6C_15p"
      },
      "source": [
        "### 2.2 Display the Final Result\n",
        "\n",
        "Once the pipeline has processed the question through the LLM and PRM, we can display the final result. This result will be the model's output after considering the intermediate answers and scoring them using the PRM.\n",
        "\n",
        "Here's how to display the final answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "v8medbURbgdI",
        "outputId": "3620f3e6-a25d-4bec-f41c-c4f03a6ed770"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## Step 1: Recall the conversion formulas\\nTo convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\\\theta)$, we use the following formulas:\\n- $r = \\\\sqrt{x^2 + y^2}$\\n- $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{y}{x}\\\\right)$\\n\\n## Step 2: Substitute the given values into the formulas\\nGiven $(x, y) = (0, 3)$, we substitute these values into the formulas:\\n- $r = \\\\sqrt{0^2 + 3^2} = \\\\sqrt{0 + 9} = \\\\sqrt{9} = 3$\\n- $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{3}{0}\\\\right)$. However, since division by zero is undefined, we recognize that the point $(0, 3)$ is on the positive y-axis, meaning $\\\\theta = \\\\frac{\\\\pi}{2}$.\\n\\n## Step 3: Combine the results for the polar coordinates\\nTherefore, the polar coordinates of the point $(0, 3)$ are $\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)$.\\n\\nThe final answer is: $\\\\boxed{\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)}$'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search_result['pred'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-8hIu05AO7J"
      },
      "source": [
        "The model’s output might include special tokens, such as `<|start_header_id|>` or `<|end_header_id|>`. To make the answer more readable, we can safely remove them before displaying it to the end user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "flbIu6-rDapM",
        "outputId": "fcb197d5-0f21-4953-8a21-869c92a1f957"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'## Step 1: Recall the conversion formulas\\nTo convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\\\theta)$, we use the following formulas:\\n- $r = \\\\sqrt{x^2 + y^2}$\\n- $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{y}{x}\\\\right)$\\n\\n## Step 2: Substitute the given values into the formulas\\nGiven $(x, y) = (0, 3)$, we substitute these values into the formulas:\\n- $r = \\\\sqrt{0^2 + 3^2} = \\\\sqrt{0 + 9} = \\\\sqrt{9} = 3$\\n- $\\\\theta = \\\\tan^{-1}\\\\left(\\\\frac{3}{0}\\\\right)$. However, since division by zero is undefined, we recognize that the point $(0, 3)$ is on the positive y-axis, meaning $\\\\theta = \\\\frac{\\\\pi}{2}$.\\n\\n## Step 3: Combine the results for the polar coordinates\\nTherefore, the polar coordinates of the point $(0, 3)$ are $\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)$.\\n\\nThe final answer is: $\\\\boxed{\\\\left(3, \\\\frac{\\\\pi}{2}\\\\right)}$'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "formatted_output = search_result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
        "formatted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZuLZNirAO7J"
      },
      "source": [
        "After removing any special tokens, we can display the final answer to the user. Since the answer is based on markdown, it can be rendered properly by displaying it as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "P4En0qJRD0cl",
        "outputId": "56400fea-e304-4f16-d255-909f42f636e0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: Recall the conversion formulas\n",
              "To convert from rectangular coordinates $(x, y)$ to polar coordinates $(r, \\theta)$, we use the following formulas:\n",
              "- $r = \\sqrt{x^2 + y^2}$\n",
              "- $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$\n",
              "\n",
              "## Step 2: Substitute the given values into the formulas\n",
              "Given $(x, y) = (0, 3)$, we substitute these values into the formulas:\n",
              "- $r = \\sqrt{0^2 + 3^2} = \\sqrt{0 + 9} = \\sqrt{9} = 3$\n",
              "- $\\theta = \\tan^{-1}\\left(\\frac{3}{0}\\right)$. However, since division by zero is undefined, we recognize that the point $(0, 3)$ is on the positive y-axis, meaning $\\theta = \\frac{\\pi}{2}$.\n",
              "\n",
              "## Step 3: Combine the results for the polar coordinates\n",
              "Therefore, the polar coordinates of the point $(0, 3)$ are $\\left(3, \\frac{\\pi}{2}\\right)$.\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uCpYzAw_4o9"
      },
      "source": [
        "## 3. Assembling It All! 🧑‍🏭️\n",
        "\n",
        "Now, let's create a method that encapsulates the entire pipeline. This will allow us to easily reuse the process in future applications, making it efficient and modular.\n",
        "\n",
        "By combining the LLM, PRM, search strategy, and result display, we can simplify the workflow and ensure that it’s reusable for other tasks or questions.\n",
        "\n",
        "We simplify the workflow, ensuring that it’s reusable for different tasks or questions. Additionally, we’ll track the time spent on each method so that we can **understand the practical implications** of using each strategy and configuration.\n",
        "\n",
        "Here’s how we can structure the method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpswbcVi37KR"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def generate_with_search_and_learn(question, config, llm, prm, method='best_of_n'):\n",
        "    \"\"\"\n",
        "    Generate an answer for a given question using the search-and-learn pipeline.\n",
        "\n",
        "    Args:\n",
        "    - question (str): The input question to generate an answer for.\n",
        "    - config (Config): Configuration object containing parameters for search strategy.\n",
        "    - llm (LLM): Pretrained large language model used for generating answers.\n",
        "    - prm (RLHFFlow): Process reward model used for evaluating answers.\n",
        "    - method (str): Search strategy to use. Options are 'best_of_n', 'beam_search', 'dvts'. Default is 'best_of_n'.\n",
        "\n",
        "    Returns:\n",
        "    - str: The formatted output after processing the question.\n",
        "    \"\"\"\n",
        "    batch = {\"problem\": [question]}\n",
        "\n",
        "    start_time = time.time()\n",
        "    if method == 'best_of_n':\n",
        "      result = best_of_n(x=batch, config=config, llm=llm, prm=prm)\n",
        "    elif method == 'beam_search':\n",
        "      result = beam_search(examples=batch, config=config, llm=llm, prm=prm)\n",
        "    elif method == 'dvts':\n",
        "      result = dvts(examples=batch, config=config, llm=llm, prm=prm)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"\\nFinished in {elapsed_time:.2f} seconds\\n\")\n",
        "\n",
        "    tokenizer = llm.get_tokenizer()\n",
        "    total_tokens = 0\n",
        "    for completion in result['completions']:\n",
        "        for comp in  completion:\n",
        "            output_tokens = tokenizer.encode(comp)\n",
        "            total_tokens += len(output_tokens)\n",
        "\n",
        "    print(f\"Total tokens in all completions: {total_tokens}\")\n",
        "\n",
        "    formatted_output = result['pred'][0].replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\", \"\").strip()\n",
        "    return formatted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWbOqkiKPVd2"
      },
      "source": [
        "### ⏳  3.1 Comparing Thinking Time for Each Strategy\n",
        "\n",
        "Let’s compare the **thinking time** of three methods: `best_of_n`, `beam_search`, and `dvts`. Each method is evaluated using the same number of answers during the search process, measuring the time spent thinking in seconds and the number of generated tokens.\n",
        "\n",
        "In the results below, the `best_of_n` method shows the least thinking time, while the `dvts` method takes the most time. However, `best_of_n` generates more tokens due to its simpler search strategy.\n",
        "\n",
        "| **Method**      | **Number of Answers During Search** | **Thinking Time (Seconds)** | **Generated Tokens** |\n",
        "|------------------|-------------------------------------|-----------------------------|-----------------------|\n",
        "| **best_of_n**    | 8                                   | 3.54                        | 3087                  |\n",
        "| **beam_search**  | 8                                   | 10.06                       | 2049                  |\n",
        "| **dvts**         | 8                                   | 8.46                        | 2544                  |\n",
        "\n",
        "This comparison illustrates the trade-offs between the strategies, balancing time spent thinking and the complexity of the search process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ROJwROGX8q-"
      },
      "source": [
        "#### 1. **Best of n**\n",
        "\n",
        "We’ll begin by using the `best_of_n` strategy. Here’s how to track the thinking time for this method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_fWKy5CCTLV",
        "outputId": "8d77eea3-b23e-4eba-cfe3-5935fae1405d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 3.54 seconds\n",
            "\n",
            "Total tokens in all completions: 3087\n"
          ]
        }
      ],
      "source": [
        "question = 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$'\n",
        "\n",
        "config.n=8\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "uzKfFoKG9ejC",
        "outputId": "38326907-685e-4a9c-ca8b-32a7c40f1d3e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: Recall the conversion formulas from rectangular to polar coordinates\n",
              "The conversion formulas are $r = \\sqrt{x^2 + y^2}$ for the radial coordinate and $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$ for the angular coordinate.\n",
              "\n",
              "## Step 2: Substitute the given rectangular coordinates into the formulas\n",
              "Given the point $(0, 3)$, we substitute $x = 0$ and $y = 3$ into the formulas.\n",
              "\n",
              "## Step 3: Calculate the radial coordinate\n",
              "$r = \\sqrt{0^2 + 3^2} = \\sqrt{0 + 9} = \\sqrt{9} = 3$\n",
              "\n",
              "## Step 4: Calculate the angular coordinate\n",
              "$\\theta = \\tan^{-1}\\left(\\frac{3}{0}\\right) = \\tan^{-1}(\\infty) = \\frac{\\pi}{2}$\n",
              "\n",
              "## Step 5: Combine the results\n",
              "The polar coordinates of the point $(0, 3)$ are $\\left(3, \\frac{\\pi}{2}\\right)$.\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S9AwP5lQvUN"
      },
      "source": [
        "#### 2. **Beam Search**\n",
        "\n",
        "Now, let's try using the `beam_search` strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7CH6KN8Izp9",
        "outputId": "adef4782-3278-4994-9520-43e23ea047a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beam search iterations:  20%|██        | 8/40 [00:10<00:40,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 10.06 seconds\n",
            "\n",
            "Total tokens in all completions: 2049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config.n=8\n",
        "# beam search specific\n",
        "config.sort_completed=True\n",
        "config.filter_duplicates=True\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='beam_search')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "Hw6tQD_dMwXZ",
        "outputId": "0f66c7ed-2071-45a4-e562-3967deb0bc9d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: To convert the point (0,3) from rectangular coordinates to polar coordinates, we need to find the radius (r) and the angle (heta).\n",
              "\n",
              "The formula to convert from rectangular coordinates (x, y) to polar coordinates (r, heta) is given by:\n",
              "r = sqrt(x^2 + y^2)\n",
              "heta = atan2(y, x)\n",
              "\n",
              "## Step 2: Plug in the values (0,3) into the formula to find the radius (r).\n",
              "\n",
              "r = sqrt(0^2 + 3^2)\n",
              "r = sqrt(0 + 9)\n",
              "r = sqrt(9)\n",
              "r = 3\n",
              "\n",
              "## Step 3: Plug in the values (0,3) into the formula to find the angle (heta).\n",
              "\n",
              "heta = atan2(3, 0)\n",
              "Since the point (0,3) is in the first quadrant and lies on the positive y-axis, heta = pi/2 (or 90 degrees).\n",
              "\n",
              "## Step 4: Combine r and heta to get the polar coordinates.\n",
              "\n",
              "The polar coordinates are (3, pi/2).\n",
              "\n",
              "The final answer is: $\\boxed{(3, \\frac{\\pi}{2})}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxBBUd7HQzhd"
      },
      "source": [
        "#### 3. **Diverse Verifier Tree Search (DVTS)**\n",
        "\n",
        "Finally, let's try the `dvts` strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzXW1g-dI5wN",
        "outputId": "86979d67-7dfa-4346-9adb-c386a52af58c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Beam search iterations:  22%|██▎       | 9/40 [00:08<00:29,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 8.46 seconds\n",
            "\n",
            "Total tokens in all completions: 2544\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config.n=8\n",
        "# dvts specific\n",
        "config.n_beams = config.n // config.beam_width\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='dvts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "RGkG9MPXMvN0",
        "outputId": "18a333ae-7b3a-455e-df2c-bb497b1381a5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Step 1: To convert the point (0,3) from rectangular coordinates to polar coordinates, we need to find the radius r and the angle theta.\n",
              "\n",
              "The radius r can be calculated using the formula $r = \\sqrt{x^2 + y^2}$, where x is the x-coordinate and y is the y-coordinate.\n",
              "\n",
              "## Step 2: Substitute the values of x and y into the formula to find the radius r.\n",
              "\n",
              "$r = \\sqrt{0^2 + 3^2}$\n",
              "$r = \\sqrt{9}$\n",
              "$r = 3$\n",
              "\n",
              "## Step 3: Now that we have the radius r, we can find the angle theta using the formula $\\theta = \\tan^{-1}\\left(\\frac{y}{x}\\right)$.\n",
              "\n",
              "Since x = 0 and y = 3, the angle theta is 90 degrees or $\\frac{\\pi}{2}$ radians.\n",
              "\n",
              "## Step 4: Now that we have the radius r and the angle theta, we can write the polar coordinates as (r, theta).\n",
              "\n",
              "Therefore, the polar coordinates for the point (0, 3) are $\\left(3, \\frac{\\pi}{2}\\right).$\n",
              "\n",
              "The final answer is: $\\boxed{\\left(3, \\frac{\\pi}{2}\\right)}$"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PM9HHwBSYWk"
      },
      "source": [
        "### 🙋 3.2 Testing the System with a Simple Question\n",
        "\n",
        "In this final example, we’ll test the system using a straightforward question to observe how it performs in simpler cases. This allows us to verify that the system works as expected even for basic queries.\n",
        "\n",
        "Let's try the following question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq9vM1uRM7A8",
        "outputId": "65ef318d-2b89-4d46-b660-293195c2b8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Finished in 1.03 seconds\n",
            "\n",
            "Total tokens in all completions: 544\n"
          ]
        }
      ],
      "source": [
        "question = 'What\\'s the capital of Spain?'\n",
        "\n",
        "config.n=32\n",
        "\n",
        "formatted_output = generate_with_search_and_learn(question=question, config=config, llm=llm, prm=prm, method='best_of_n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "ysfR0nPfM-Ub",
        "outputId": "b474aeb6-6cb7-4f15-ba48-fa59022f31ef"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The capital of Spain is Madrid."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(formatted_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgdeSegeANoT"
      },
      "source": [
        "Even though we set a larger number of candidate answers (`N`), the time spent thinking remains relatively small (1.03 seconds and 544 generated tokens). This demonstrates the system’s ability to efficiently handle easier problems, spending less time on them, while leveraging its enhanced capabilities for more complex questions.\n",
        "\n",
        "🏆 **We now have a fully operational pipeline** that leverages test-time compute, enabling the system to \"think longer\" for more complicated queries, while also maintaining fast response times for straightforward questions.\n",
        "\n",
        "This approach ensures the system can scale its thinking time based on the task's complexity, offering an efficient and responsive solution for both simple and challenging problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92znAyJ0AOPY"
      },
      "source": [
        "## 4. Continuing the Journey and Resources 🧑‍🎓️\n",
        "\n",
        "If you're eager to continue exploring, be sure to check out the original experimental [blog](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) and all the references mentioned within it. These resources will deepen your understanding of test-time compute, its benefits, and its applications in LLMs.\n",
        "\n",
        "\n",
        "Happy learning and experimenting! 🚀"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}