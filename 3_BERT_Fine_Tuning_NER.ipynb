{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3- BERT Fine-Tuning NER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49037fb0bb4e4e3d800b58b93175c429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c230de3eee6b4e78a8fb15fd0e41861d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d5cd0aaa63b34ece91d3db93b896dcb8",
              "IPY_MODEL_aa5288295a7c4349a696710fb053ff63"
            ]
          }
        },
        "c230de3eee6b4e78a8fb15fd0e41861d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5cd0aaa63b34ece91d3db93b896dcb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58e434f6f0ee4b50bc5e406c668e5432",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2c166f02d0a43c2b65f24d3c3cc25d0"
          }
        },
        "aa5288295a7c4349a696710fb053ff63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cd567cf19f6d4f989113bfff4d6c9df5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 1.40MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c7732a44ec4471d9ee1677e0af37074"
          }
        },
        "58e434f6f0ee4b50bc5e406c668e5432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2c166f02d0a43c2b65f24d3c3cc25d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd567cf19f6d4f989113bfff4d6c9df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c7732a44ec4471d9ee1677e0af37074": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/3_BERT_Fine_Tuning_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKOTlwcmxmej"
      },
      "source": [
        "# BERT Fine-Tuning NER Tutorial with PyTorch\n",
        "\n",
        "By Chris McCormick and Nick Ryan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdrC7XSgv6Hz"
      },
      "source": [
        "In this tutorial I'll show you how to use BERT with the huggingface PyTorch library to quickly and efficiently fine-tune a model to get near state of the art performance in named entity recognition (NER from here on out.) \n",
        "\n",
        "NER is a widely used NLP task that tries to locate and classify the names and entities contained in a piece of text, commonly things like people, organizations, locations, etc. In this tutorial we'll be training our NER model to look at a dataset of short restaurant-related reviews and queries and identify things like dishes, restaurant names, and prices. If you're following along with your own dataset, as long as your processed inputs resemble ours you'll be able to run the fine-tune the model in the same fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkUGTqixRWo"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP8eXq3IJbZD"
      },
      "source": [
        "Since we'll be focusing on NER fine-tuning and for the sake of legibility, I won't go into great detail about BERT in general or the broader process of language model fine-tuning. We've covered these topics in our free [BERT fine-tuning tutorial](http://mccormickml.com/2019/07/22/BERT-fine-tuning/) in detail here, so if you're curious or some of the code isn't immediately obvious I encourage you to take a look.\n",
        "\n",
        "In general, fine-tuning BERT or another language model with your dataset will yield very good results and requires less data than from-scratch models. However this comes at the cost of having to train it yourself and, more importantly, possibly at the cost of making a large transformer-sized model work quickly in a development setting (buying GPUs). In some cases, using existing NLP libraries with off-the-shelf NER models might be to your benefit, so look for the [related post](http://mccormickml.com/2020/05/19/existing-ner-tools/) providing some resources and tips on using existing NER models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66"
      },
      "source": [
        "## 1.1. Using Colab GPU for Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI0iOY8zvZzL"
      },
      "source": [
        "\n",
        "Google Colab offers free GPUs and TPUs! Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time.\n",
        "\n",
        "A GPU can be added by going to the menu and selecting:\n",
        "\n",
        "`Edit ðŸ¡’ Notebook Settings ðŸ¡’ Hardware accelerator ðŸ¡’ (GPU)`\n",
        "\n",
        "Then run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEfSbAA4QHas",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6017b4ea-9d44-4fca-a572-93d4ae845a46"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "123e92be-81ef-4289-eb63-7b68130f37e5"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI"
      },
      "source": [
        "## 1.2. Installing the Hugging Face Library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "0ca811a8-88be-4e41-9479-353846bd1670"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 778kB 3.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 19.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 19.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=f8137db538e16279e0482454ce6a5271c0bcb08fbc99f179c17374d05dfde2e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "# 2. Loading the MIT Restaurant Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn"
      },
      "source": [
        "## 2.1. Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZNVW6xd0T0X"
      },
      "source": [
        "We'll use the `wget` package to download the dataset to the Colab instance's file system. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6AnuFv0QXQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "outputId": "b71b5c7a-9d51-4321-c676-efb19582e17e"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=d2d155c2bd7d732d9048dd3ec617d935af0962adc5797593458d9959df57df5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08pO03Ff1BjI"
      },
      "source": [
        "The test and train dataset is here: https://groups.csail.mit.edu/sls/downloads/restaurant/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDW8NXhtoqyT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2b237b39-ccd7-4868-bb8a-583609f04f43"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "# The URL for the *training* examples.\n",
        "url = 'https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttrain.bio'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "#if not os.path.exists('./restauranttrain.bio'):\n",
        "wget.download(url, './restauranttrain.bio')\n",
        "\n",
        "# The URL for the *test* examples.\n",
        "url = 'https://groups.csail.mit.edu/sls/downloads/restaurant/restauranttest.bio'\n",
        "\n",
        "# Download the file (if we haven't already)\n",
        "#if not os.path.exists('./restauranttest.bio'):\n",
        "wget.download(url, './restauranttest.bio')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./restauranttest.bio'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyugAznPBNQ2"
      },
      "source": [
        "## 2.2. NER Tags and IOB Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV-LjMrjz_uY"
      },
      "source": [
        "Both the train and test datasets are single files containing restaurant-related text (mostly reviews and queries), where each word has a NER tag specifying it as one of the following restaurant-related entities:\n",
        "\n",
        "- Amenity\n",
        "- Cuisine\n",
        "- Dish\n",
        "- Hours\n",
        "- Location\n",
        "- Price\n",
        "- Rating\n",
        "- Restaurant_Name\n",
        "\n",
        "\n",
        "The NER tag follows a special format used widely in NER literature called the [IOB format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)#:~:text=) (Inside, Outside, Beginning Format). This format gives us a way to not only tag entities, but to indicate which words are part of the same entity:\n",
        "\n",
        "- **O**: this tag means the word is not part of an entity.\n",
        "- **B**: this tag means the word is either a single-word entity name, or else the first word in a multi-word entity name.\n",
        "- **I**: this tag means the word is part of a multi-word entity, but is not the first word in the full entity name.\n",
        "\n",
        "For example:\n",
        "\n",
        "| TAG | WORD  |\n",
        "|---|---|\n",
        "| O  | My  |\n",
        "| O  | first  |\n",
        "| O  |  name |\n",
        "| O  |  is |\n",
        "| B-Person  | John  |\n",
        "|  O | and  |\n",
        "| O  | my  |\n",
        "| O  | full  |\n",
        "| O  |  name |\n",
        "| O  |  is |\n",
        "| B-Person  | John  |\n",
        "| I-Person  | A.  |\n",
        "| I-Person  | Smith  |\n",
        "\n",
        "Furthermore, as you can see above the **B** and **I** tags are used as prefixes before the entity types and can be used before any entity type you like, whether it's people or restaurants or countries. There are some variations of the IOB format that might use special tags for single-word entities or tags for words that are the last word in a multi-word entity. Different formats might confer small advantages on different kinds of datasets, but for now there's no need to worry if you're working on a dataset with a different tag format (as long as you are consistent!) \n",
        "\n",
        "Now, back to our dataset. The words in each review are separated by line, and reviews themselves are separated by blank lines. Each line that is not blank contains a word from the review and its NER tag:\n",
        "\n",
        "| TAG | WORD  |\n",
        "|---|---|\n",
        "|  O | a   |\n",
        "|  B-Rating | four  |\n",
        "| I-Rating | star  |\n",
        "|  O |  restaurant |\n",
        "| B-Location  | with  |\n",
        "| I-Location  | a  |\n",
        "| B-Amenity  | bar  |\n",
        "|    |    |\n",
        "| O  | any  |\n",
        "|  B-Cuisine |  bbq |\n",
        "| O  | places  |\n",
        "|  B-Hours | open  |\n",
        "| I-Hours  | before  |\n",
        "| I-Hours  | 5  |\n",
        "|  B-Location |  nearby |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6A5kw2UBXsQ"
      },
      "source": [
        "## 2.3. Parse Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INvOaDMUBfpo"
      },
      "source": [
        "\n",
        "I couldn't find anything to quickly parse the .bio file format found here, so we'll need to do some quick processing to get our data into the desired format for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkAWuZTalRjy"
      },
      "source": [
        "import csv\n",
        "\n",
        "# List of all sentences in the dataset.\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "# Lists to store the current sentence.\n",
        "tokens = []\n",
        "token_labels = []\n",
        "\n",
        "# Gather the set of unique labels.\n",
        "unique_labels = set()\n",
        "\n",
        "# Read the dataset line by line. Each line of the file\n",
        "# is either empty or has two tokens, separated by a tab.\n",
        "with open(\"./restauranttrain.bio\", newline = '') as lines:                                                                                          \n",
        "    \n",
        "    # Use the `csv` class to split the lines on the tab character.\n",
        "    line_reader = csv.reader(lines, delimiter='\\t')\n",
        "    \n",
        "    # For each line in the file...\n",
        "    for line in line_reader:\n",
        "        \n",
        "        # If we encounter a blank line, it means we've completed the previous \n",
        "        # sentence. \n",
        "        if line == []:\n",
        "\n",
        "            # Add the completed sentence.\n",
        "            sentences.append(tokens)\n",
        "            labels.append(token_labels)           \n",
        "    \n",
        "            # Start a new sentence.\n",
        "            tokens = []\n",
        "            token_labels = []        \n",
        "\n",
        "        else: \n",
        "            # Add the token and its label to the current sentence.\n",
        "            tokens.append(line[1])\n",
        "            token_labels.append(line[0])\n",
        "\n",
        "            # Add the label to the set (no effect if it already exists).\n",
        "            unique_labels.add(line[0])\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHYaEItOcx9-"
      },
      "source": [
        "Here are all of the unique labels in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b00POq7BchEn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "374f8076-39ce-43ca-d6d3-f7a744d20291"
      },
      "source": [
        "unique_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-Amenity',\n",
              " 'B-Cuisine',\n",
              " 'B-Dish',\n",
              " 'B-Hours',\n",
              " 'B-Location',\n",
              " 'B-Price',\n",
              " 'B-Rating',\n",
              " 'B-Restaurant_Name',\n",
              " 'I-Amenity',\n",
              " 'I-Cuisine',\n",
              " 'I-Dish',\n",
              " 'I-Hours',\n",
              " 'I-Location',\n",
              " 'I-Price',\n",
              " 'I-Rating',\n",
              " 'I-Restaurant_Name',\n",
              " 'O'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BbOt-rfdlQT"
      },
      "source": [
        "# Map each unique label to an integer.\n",
        "label_map = {}\n",
        "\n",
        "# For each label...\n",
        "for (i, label) in enumerate(unique_labels):\n",
        "    \n",
        "    # Map it to its integer.\n",
        "    label_map[label] = i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUZVhciWALGN"
      },
      "source": [
        "Now we have our data separated into:\n",
        "- **sentences** containing a list of tokenized sentences\n",
        "- **labels** containing a list of corresponding IOB\n",
        "\n",
        "**If you have you are using this tutorial to fine-tune BERT on your own dataset, make sure to format your input data in the same way.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtZahSM8_f4n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0943d3de-ea48-4375-ce48-f4a28f035085"
      },
      "source": [
        "print(\"Number of training sentences: {:,}\".format(len(sentences)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 7,660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiktKYvPe1KY"
      },
      "source": [
        "Let's look at a random sentence as an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iRs8gqD-8r5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "8e9b7940-a274-4f95-ec49-536249582de1"
      },
      "source": [
        "print(\"Example sentence:\")\n",
        "print (\"    Tokens:\", sentences[4])\n",
        "print (\"    Labels:\", labels[4])\n",
        "\n",
        "print ('\\nSentence Tokens and Labels:')\n",
        "for i in zip(sentences[4], labels[4]):\n",
        "  print (i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example sentence:\n",
            "    Tokens: ['a', 'great', 'lunch', 'spot', 'but', 'open', 'till', '2', 'a', 'm', 'passims', 'kitchen']\n",
            "    Labels: ['O', 'O', 'O', 'O', 'O', 'B-Hours', 'I-Hours', 'I-Hours', 'I-Hours', 'I-Hours', 'B-Restaurant_Name', 'I-Restaurant_Name']\n",
            "\n",
            "Sentence Tokens and Labels:\n",
            "('a', 'O')\n",
            "('great', 'O')\n",
            "('lunch', 'O')\n",
            "('spot', 'O')\n",
            "('but', 'O')\n",
            "('open', 'B-Hours')\n",
            "('till', 'I-Hours')\n",
            "('2', 'I-Hours')\n",
            "('a', 'I-Hours')\n",
            "('m', 'I-Hours')\n",
            "('passims', 'B-Restaurant_Name')\n",
            "('kitchen', 'I-Restaurant_Name')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxxItg7vA5vc"
      },
      "source": [
        "We also have a **label_map** dictionary that will map our tags to integers during the fine-tuning process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKZdvjwPDO9n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "2f66d5cb-e7e6-4728-8bd2-4fb2f8e2c57e"
      },
      "source": [
        "label_map"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'B-Amenity': 15,\n",
              " 'B-Cuisine': 6,\n",
              " 'B-Dish': 5,\n",
              " 'B-Hours': 11,\n",
              " 'B-Location': 13,\n",
              " 'B-Price': 1,\n",
              " 'B-Rating': 16,\n",
              " 'B-Restaurant_Name': 12,\n",
              " 'I-Amenity': 14,\n",
              " 'I-Cuisine': 0,\n",
              " 'I-Dish': 2,\n",
              " 'I-Hours': 3,\n",
              " 'I-Location': 10,\n",
              " 'I-Price': 7,\n",
              " 'I-Rating': 4,\n",
              " 'I-Restaurant_Name': 8,\n",
              " 'O': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIDfAzc9C2wl"
      },
      "source": [
        "# 3. Tokenization & Input Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVA44wu4C5HL"
      },
      "source": [
        "The dataset comes already â€œtokenizedâ€ in the sense that itâ€™s split into lists of words.\n",
        "\n",
        "There is still more processing of the text to be done before we can send these sentences through BERT, though.\n",
        "\n",
        "Here's what needs to happen:\n",
        "\n",
        "1. The words need to be mapped to their IDs in the BERT vocabulary.\n",
        "2. For words that aren't in the BERT vocabulary, the tokenizer will break these down into subwords.\n",
        "3. BERT requires that we prepend the special `[CLS]` token to the beginning of every sentence, and append the `[SEP]` token to the end.\n",
        "4. To support batch processing of the inputs, all of the sentences should be padded out (with the special `[PAD]` token) to the same length. \n",
        "    * This, in turn, requires us to provide \"attention masks\" to prevent BERT from actually looking at the `[PAD]` tokens for meaning.\n",
        "\n",
        "Because all of the above steps are common to many applications of BERT, the BertTokenizer class in the Transformers library has a help function named `encode_plus` which will handle all of the above tasks for us.\n",
        "\n",
        "> *Side Note: Do you see a problem here? The tokenizer is going to screw up our token labels! We'll address this in section 3.3.*\n",
        "\n",
        "First, though, we need to pick a maximum sentence length to pad all of the sentences to.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEyKypVX9-y6"
      },
      "source": [
        "## 3.1. Max Length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M296yz577fV"
      },
      "source": [
        "Let's measure the length of all of the sentences and look at the distribution of lengths in order to choose a sensible \"max_len\" value for padding our sentences to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "49037fb0bb4e4e3d800b58b93175c429",
            "c230de3eee6b4e78a8fb15fd0e41861d",
            "d5cd0aaa63b34ece91d3db93b896dcb8",
            "aa5288295a7c4349a696710fb053ff63",
            "58e434f6f0ee4b50bc5e406c668e5432",
            "d2c166f02d0a43c2b65f24d3c3cc25d0",
            "cd567cf19f6d4f989113bfff4d6c9df5",
            "3c7732a44ec4471d9ee1677e0af37074"
          ]
        },
        "outputId": "7ce0785d-e5d0-424e-a657-f89f9ea16309"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# We'll need the BertTokenizer in order to determine the number of tokens in\n",
        "# the sentences *according to BERT*.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Record the length of each sequence.\n",
        "lengths = []\n",
        "\n",
        "print('Measuring sentence lengths...')\n",
        "\n",
        "# For every sentence...\n",
        "for sen in sentences:\n",
        "\n",
        "    # Reconstruct the sentence to let BERT decide how to tokenize it.\n",
        "    sen = ' '.join(sen)\n",
        "\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sen,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    # Record the length of the sentence after tokenization.\n",
        "    lengths.append(len(encoded_sent))\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49037fb0bb4e4e3d800b58b93175c429",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Measuring sentence lengths...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_erWZU7hDTB"
      },
      "source": [
        "Let's grab some quick statistics--what are the min, max and median comment lenghts?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmrQVSsjg6By",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1f6b6833-71af-41ce-8a2d-127d2e7f4d8b"
      },
      "source": [
        "print('   Min length: {:,} tokens'.format(min(lengths)))\n",
        "print('   Max length: {:,} tokens'.format(max(lengths)))\n",
        "print('Median length: {:,} tokens'.format(int(np.median(lengths))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Min length: 3 tokens\n",
            "   Max length: 39 tokens\n",
            "Median length: 11 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f31emWtu9ntl"
      },
      "source": [
        "To further analyze it, let's plot the distribution. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLhi9A49zmsy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "6f2b3c7f-5f35-4533-e185-d0f108bb3304"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "\n",
        "# Plot the distribution of comment lengths.\n",
        "sns.distplot(lengths, kde=False, rug=False)\n",
        "\n",
        "plt.title('Sentence Lengths')\n",
        "plt.xlabel('Sentence Length')\n",
        "plt.ylabel('# of Sentences')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '# of Sentences')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvs9RvI2dLNE"
      },
      "source": [
        "These are some very short sentences!\n",
        "\n",
        "Since we don't know what the longest sentence in the test set is, let's set the max_len to 50 to leave some room (the maximum training sentence length was 39 tokens)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91PCHUVoEw3L"
      },
      "source": [
        "## 3.2. Re-Tokenize with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIWAoWL2RK1p"
      },
      "source": [
        "Now we're ready to perform the real tokenization! \n",
        "\n",
        "> *Note that, to allow the BERT tokenizer to split \"unknown\" words into subwords, we're actually going to  reconstruct each sentence (by combining the tokens with white space) before feeding it to the tokenizer.*\n",
        "\n",
        "The `tokenizer.encode_plus` function combines multiple steps for us:\n",
        "\n",
        "1. Split unknown words into subwords.\n",
        "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
        "3. Map the tokens to their IDs.\n",
        "4. Pad or truncate all sentences to the same length (we chose 50).\n",
        "5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens (so that the self-attention mechanism in BERT ignores the `[PAD]` tokens).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rZLGEABFPez",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "7bccca82-2115-4405-83ef-e1621f4dff10"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Reconstruct the sentence--otherwise `tokenizer` will interpret the list\n",
        "    # of string tokens as having already been tokenized by BERT.\n",
        "    sent_str = ' '.join(sent)\n",
        "\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent_str,                  # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        truncation = True,\n",
        "                        max_length = 50,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'][0])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'][0])\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Masks:', attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ['2', 'start', 'restaurants', 'with', 'inside', 'dining']\n",
            "Token IDs: tensor([ 101, 1016, 2707, 7884, 2007, 2503, 7759,  102,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0])\n",
            "Masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze5ljJQuFkKL"
      },
      "source": [
        "## 3.3. Add Null Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRKfNEOuHQbM"
      },
      "source": [
        "We've prepared all of our sentences to feed into BERT, but there's a problem--we've added a bunch of tokens to the input sequences (special tokens, padding, and splitting \"unknown\" words) and now *the original token labels are no longer aligned with the tokens!*\n",
        "\n",
        "To fix this, we'll define an additional token label that we'll give to any tokens that were added. You might call this the \"null\" label, or the \"padding\" label. \n",
        "\n",
        "So, for example if our sentence is \"5 star resturants nearby with chorizo tacos,\" the words \"resturants,\" \"chorizo,\" and \"tacos\" all get tokenized into smaller subwords and need new labels:\n",
        "![](https://drive.google.com/uc?id=1TnGVa13ufIq3Yu0v-eNuYH1BZaqcym-z)\n",
        "\n",
        "\n",
        "We add null labels to each of the subsequent subword tokens and the result is the newly tokenized sentence that looks like this:\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=1YIMz23PbdzhYrBmzsICvLE-deuzgFRdK)\n",
        "\n",
        "Our `label_map` dictionary, which we created while parsing the dataset, maps the token *label* strings to unique integer IDs. We'll add to this the integer ID `-100` to label any of these \"added\" tokens.\n",
        "\n",
        "The following cell will update the list of token labels for each sentence by adding in the null label where needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDCfJPLjFn0r"
      },
      "source": [
        "# New labels for all of the input sentences.\n",
        "new_labels = []\n",
        "\n",
        "# The special label ID we'll give to \"extra\" tokens.\n",
        "null_label_id = -100\n",
        "\n",
        "# For each sentence...\n",
        "for (sen, orig_labels) in zip(input_ids, labels):\n",
        "    \n",
        "    # Create a new list to hold the adjusted labels for this sentence.\n",
        "    padded_labels = []\n",
        "\n",
        "    # This will be our index into the original label list.\n",
        "    orig_labels_i = 0 \n",
        "\n",
        "    # For each token in the padded sentence...\n",
        "    for token_id in sen:\n",
        "\n",
        "        # Pull the value out of the tensor.\n",
        "        token_id = token_id.numpy().item()\n",
        "\n",
        "        # If `[PAD]`, `[CLS]`, or `[SEP]`...\n",
        "        if (token_id == tokenizer.pad_token_id) or \\\n",
        "            (token_id == tokenizer.cls_token_id) or \\\n",
        "            (token_id == tokenizer.sep_token_id):\n",
        "            \n",
        "            # Assign it the null label.\n",
        "            padded_labels.append(null_label_id)\n",
        "\n",
        "        # If the token string starts with \"##\"...\n",
        "        elif tokenizer.ids_to_tokens[token_id][0:2] == '##':\n",
        "\n",
        "            # It's a subword token, and not part of the original dataset, so\n",
        "            # assign it the null label.\n",
        "            padded_labels.append(null_label_id)\n",
        "\n",
        "        # If it's not any of the above...\n",
        "        else:\n",
        "            \n",
        "            # This token corresponds to one of the original ones, so assign it\n",
        "            # it's original label.\n",
        "\n",
        "            # Look up the label for this token.\n",
        "            label_str = orig_labels[orig_labels_i]\n",
        "\n",
        "            # Map the label to its ID, and assign it.\n",
        "            padded_labels.append(label_map[label_str])\n",
        "\n",
        "            # Increment our index into the original labels list.\n",
        "            orig_labels_i += 1\n",
        "\n",
        "    # If we did this right, then the new `padded_labels` list should match\n",
        "    # the length of the tokenized sentence.\n",
        "    assert(len(sen) == len(padded_labels))    \n",
        "\n",
        "    # Store the updated labels list for this sentence.\n",
        "    new_labels.append(padded_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uGVpkelQ4Hm"
      },
      "source": [
        "Now, when we look at a real example sentence from our dataset we can see the result of these changes:\n",
        "\n",
        "\"5 star resturants in my town\"\n",
        "\n",
        "This sentence, like the one above, includes a misspelling of the word 'restaurant', so the BERT tokenizer splits this word into multiple subwords. \n",
        "\n",
        "The first subword `rest` is given the original label, while `##ura` and `##nts` are labeled with ID `-100`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1xQC8xtRC_-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "ee3fcf6a-86c6-49b1-9583-c83d15551f84"
      },
      "source": [
        "print('\\nSentence:    ', sentences[2])\n",
        "print('\\nLabels:      ', labels[2])\n",
        "print('\\nBERT Tokens: ', tokenizer.tokenize(' '.join(sentences[2])))\n",
        "print('\\nToken IDs:   ', input_ids[2])\n",
        "print('\\nNew Labels:  ', new_labels[2])\n",
        "print('\\nMask:        ', attention_masks[2])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence:     ['5', 'star', 'resturants', 'in', 'my', 'town']\n",
            "\n",
            "Labels:       ['B-Rating', 'I-Rating', 'O', 'B-Location', 'I-Location', 'I-Location']\n",
            "\n",
            "BERT Tokens:  ['5', 'star', 'rest', '##ura', '##nts', 'in', 'my', 'town']\n",
            "\n",
            "Token IDs:    tensor([ 101, 1019, 2732, 2717, 4648, 7666, 1999, 2026, 2237,  102,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0])\n",
            "\n",
            "New Labels:   [-100, 9, 5, 13, -100, -100, 3, 10, 10, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
            "\n",
            "Mask:         tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_npmX0BHRzk"
      },
      "source": [
        "Finally, this cell turns our Python lists into Pytorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWOEAJQvPC6o"
      },
      "source": [
        "# Convert the lists into PyTorch tensors.\n",
        "\n",
        "# `input_ids` is a list of tensor arrays--stack them into a matrix with size\n",
        "# [7,660  x  50].\n",
        "pt_input_ids = torch.stack(input_ids, dim=0)\n",
        "\n",
        "# `attention_masks` is a list of tensor arrays--stack them into a matrix with\n",
        "# size [7,660  x  50].\n",
        "pt_attention_masks = torch.stack(attention_masks, dim=0)\n",
        "\n",
        "# Labels is a list of lists. Convert it into a tensor matrix with size \n",
        "# [7,660  x  50].\n",
        "pt_labels = torch.tensor(new_labels, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yKfkY_ZUxxn"
      },
      "source": [
        "## 3.4. Training & Validation Split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeO07RZUUxxo"
      },
      "source": [
        "Divide up our training set to use 90% for training and 10% for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk0MqIi7Uxxp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2fb8e61b-b77b-4fec-aede-e13f1d6b5a9c"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(pt_input_ids, pt_attention_masks, pt_labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6,894 training samples\n",
            "  766 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYSos3lQUxxq"
      },
      "source": [
        "We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGUqOCtgqGhP"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "# size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-"
      },
      "source": [
        "# 4. Train Our Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xYQ3iLO08SX"
      },
      "source": [
        "Now that our input data is properly formatted, it's time to fine tune the BERT model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKgyUzPIQc"
      },
      "source": [
        "## 4.1. BertForTokenClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAuiUrhCrNCB"
      },
      "source": [
        "We'll be using BertForTokenClassification. This is the normal BERT model with an added single linear layer that is used for classification of each token's entity class. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqgicdWMEPf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74343165-f7eb-43e4-d7d3-e8e9f5ccaa85"
      },
      "source": [
        "# This is the number of labels in our dataset, so we will be doing 18-way classification (all labels plus our padding label)\n",
        "len(label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23276116-0858-4721-f477-ff31a76db917"
      },
      "source": [
        "from transformers import BertForTokenClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForTokenClassification \n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = len(label_map) + 1, # The number of output labels--18 for our NER dataset\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx"
      },
      "source": [
        "## 4.2. Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO"
      },
      "source": [
        "# Load the AdamW optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate \n",
        "                  eps = 1e-8 # args.adam_epsilon \n",
        "                )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs \n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox"
      },
      "source": [
        "## 4.3. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNhRtWPXH9C3"
      },
      "source": [
        "Helper function for formatting elapsed times.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfNIhN19te3N"
      },
      "source": [
        "We're ready to kick off the training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "8ac15a05-a5c0-4162-c6ae-40b80d73f556"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    216.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    216.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    216.    Elapsed: 0:00:42.\n",
            "  Batch   160  of    216.    Elapsed: 0:00:56.\n",
            "  Batch   200  of    216.    Elapsed: 0:01:10.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:01:15\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    216.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    216.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    216.    Elapsed: 0:00:42.\n",
            "  Batch   160  of    216.    Elapsed: 0:00:57.\n",
            "  Batch   200  of    216.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epcoh took: 0:01:16\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    216.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    216.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    216.    Elapsed: 0:00:43.\n",
            "  Batch   160  of    216.    Elapsed: 0:00:57.\n",
            "  Batch   200  of    216.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epcoh took: 0:01:16\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    216.    Elapsed: 0:00:14.\n",
            "  Batch    80  of    216.    Elapsed: 0:00:28.\n",
            "  Batch   120  of    216.    Elapsed: 0:00:43.\n",
            "  Batch   160  of    216.    Elapsed: 0:00:57.\n",
            "  Batch   200  of    216.    Elapsed: 0:01:11.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:01:17\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-G03mmwH3aI"
      },
      "source": [
        "Let's take a look at our training loss over all batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBhywMlXVmUJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "7f711e7d-dcdc-4dd7-870e-c4d1d20f17c7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "# 5. Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosV94BYIYxg"
      },
      "source": [
        "Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using F1 score because this is the metric used for CONLL2003 and other NER datasets, and gives an intuitive understanding of model performance.\n",
        "\n",
        "When we evaluate performance we only pay attention to predictions for the non-pad tokens, so first remove all pad tokens before calculating the F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30w5nU1aY4Pp"
      },
      "source": [
        "### 5.1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYvL5EzvY4Pq"
      },
      "source": [
        "\n",
        "We'll need to apply all of the same steps that we did for the training data to prepare our test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyZg3CsfaiUq"
      },
      "source": [
        "**Step 1: Parse Dataset File**\n",
        "\n",
        "Read in the test set file \"./restauranttest.bio\" and extract the test samples from it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF3N0K9SY-OJ"
      },
      "source": [
        "import csv\n",
        "\n",
        "# List of all sentences in the dataset.\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "# Lists to store the current sentence.\n",
        "tokens = []\n",
        "token_labels = []\n",
        "\n",
        "# Gather the set of unique labels.\n",
        "unique_labels = set()\n",
        "\n",
        "# Read the dataset line by line. Each line of the file\n",
        "# is either empty or has two tokens, separated by a tab.\n",
        "with open(\"./restauranttest.bio\", newline = '') as lines:                                                                                          \n",
        "    \n",
        "    # Use the `csv` class to split the lines on the tab character.\n",
        "    line_reader = csv.reader(lines, delimiter='\\t')\n",
        "    \n",
        "    # For each line in the file...\n",
        "    for line in line_reader:\n",
        "        \n",
        "        # If we encounter a blank line, it means we've completed the previous \n",
        "        # sentence. \n",
        "        if line == []:\n",
        "\n",
        "            # Add the completed sentence.\n",
        "            sentences.append(tokens)\n",
        "            labels.append(token_labels)           \n",
        "    \n",
        "            # Start a new sentence.\n",
        "            tokens = []\n",
        "            token_labels = []        \n",
        "\n",
        "        else: \n",
        "            # Add the token and its label to the current sentence.\n",
        "            tokens.append(line[1])\n",
        "            token_labels.append(line[0])\n",
        "\n",
        "            # Add the label to the set (no effect if it already exists).\n",
        "            unique_labels.add(line[0])\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7_Vsn6tZQJq"
      },
      "source": [
        "**Step 2: Re-Tokenize with BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExeXl-RwZQJs"
      },
      "source": [
        "The `tokenizer.encode_plus` function combines multiple steps for us:\n",
        "\n",
        "1. Split unknown words into subwords.\n",
        "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
        "3. Map the tokens to their IDs.\n",
        "4. Pad or truncate all sentences to the same length.\n",
        "5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens (so that the self-attention mechanism in BERT ignores the `[PAD]` tokens).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFQ-vencZQJt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "556a024a-62a9-4ba1-8cbd-7e6834bac1a6"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Reconstruct the sentence--otherwise `tokenizer` will interpret the list\n",
        "    # of string tokens as having already been tokenized by BERT.\n",
        "    sent_str = ' '.join(sent)\n",
        "\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent_str,                  # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 50,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.    \n",
        "    input_ids.append(encoded_dict['input_ids'][0])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'][0])\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "print('Masks:', attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  ['a', 'four', 'star', 'restaurant', 'with', 'a', 'bar']\n",
            "Token IDs: tensor([ 101, 1037, 2176, 2732, 4825, 2007, 1037, 3347,  102,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0])\n",
            "Masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ACse-8LZQJv"
      },
      "source": [
        "**Step 3: Add Null Labels**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sOjmzy8ZQJw"
      },
      "source": [
        "Add in the null label for all tokens added by the BERT tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPPjb6t_ZQJw"
      },
      "source": [
        "# New labels for all of the input sentences.\n",
        "new_labels = []\n",
        "\n",
        "# The special label ID we'll give to \"extra\" tokens.\n",
        "null_label_id = -100\n",
        "\n",
        "# For each sentence...\n",
        "for (sen, orig_labels) in zip(input_ids, labels):\n",
        "    \n",
        "    # Create a new list to hold the adjusted labels for this sentence.\n",
        "    padded_labels = []\n",
        "\n",
        "    # This will be our index into the original label list.\n",
        "    orig_labels_i = 0 \n",
        "\n",
        "    # For each token in the padded sentence...\n",
        "    for token_id in sen:\n",
        "        \n",
        "        # Pull the value out of the tensor.\n",
        "        token_id = token_id.numpy().item()\n",
        "\n",
        "        # If `[PAD]`, `[CLS]`, or `[SEP]`...\n",
        "        if (token_id == tokenizer.pad_token_id) or \\\n",
        "            (token_id == tokenizer.cls_token_id) or \\\n",
        "            (token_id == tokenizer.sep_token_id):\n",
        "            \n",
        "            # Assign it the null label.\n",
        "            padded_labels.append(null_label_id)\n",
        "\n",
        "        # If the token string starts with \"##\"...\n",
        "        elif tokenizer.ids_to_tokens[token_id][0:2] == '##':\n",
        "\n",
        "            # It's a subword token, and not part of the original dataset, so\n",
        "            # assign it the null label.\n",
        "            padded_labels.append(null_label_id)\n",
        "\n",
        "        # If it's not any of the above...\n",
        "        else:\n",
        "            \n",
        "            # This token corresponds to one of the original ones, so assign it\n",
        "            # it's original label.\n",
        "\n",
        "            # Look up the label for this token.\n",
        "            label_str = orig_labels[orig_labels_i]\n",
        "\n",
        "            # Map the label to its ID, and assign it.\n",
        "            padded_labels.append(label_map[label_str])\n",
        "\n",
        "            # Increment our index into the original labels list.\n",
        "            orig_labels_i += 1\n",
        "\n",
        "    # If we did this right, then the new `padded_labels` list should match\n",
        "    # the length of the tokenized sentence.\n",
        "    assert(len(sen) == len(padded_labels))    \n",
        "\n",
        "    # Store the updated labels list for this sentence.\n",
        "    new_labels.append(padded_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E56Oluj0ZQJy"
      },
      "source": [
        "Let's look at an example sentence to see the results of all of our changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R8-GMHXZQJz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "5a8b4bb8-ab0d-4e4c-869e-d0767592934b"
      },
      "source": [
        "print('\\nSentence:    ', sentences[2])\n",
        "print('\\nLabels:      ', labels[2])\n",
        "print('\\nBERT Tokens: ', tokenizer.tokenize(' '.join(sentences[2])))\n",
        "print('\\nToken IDs:   ', input_ids[2])\n",
        "print('\\nMask:        ', attention_masks[2])\n",
        "print('\\nNew Labels:  ', new_labels[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence:     ['any', 'bbq', 'places', 'open', 'before', '5', 'nearby']\n",
            "\n",
            "Labels:       ['O', 'B-Cuisine', 'O', 'B-Hours', 'I-Hours', 'I-Hours', 'B-Location']\n",
            "\n",
            "BERT Tokens:  ['any', 'bb', '##q', 'places', 'open', 'before', '5', 'nearby']\n",
            "\n",
            "Token IDs:    tensor([  101,  2151, 22861,  4160,  3182,  2330,  2077,  1019,  3518,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "\n",
            "Mask:         tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n",
            "\n",
            "New Labels:   [-100, 13, 12, -100, 13, 2, 4, 4, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYs1JgJfbzqr"
      },
      "source": [
        "**Step 4: Setup DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV_qRH48ZQJ1"
      },
      "source": [
        "# Convert the lists into PyTorch tensors.\n",
        "\n",
        "# `input_ids` is a list of tensor arrays--stack them into a matrix.\n",
        "pt_input_ids = torch.stack(input_ids, dim=0)\n",
        "\n",
        "# `attention_masks` is a list of tensor arrays--stack them into a matrix.\n",
        "pt_attention_masks = torch.stack(attention_masks, dim=0)\n",
        "\n",
        "# Labels is a list of lists. Convert it into a tensor matrix.\n",
        "pt_labels = torch.tensor(new_labels, dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEEOBcMKY4Ps"
      },
      "source": [
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(pt_input_ids, pt_attention_masks, pt_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "## 5.2. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhR99IISNMg9"
      },
      "source": [
        "\n",
        "With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f63d01a-cfbd-4df4-b8ce-a4a4314f804d"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(pt_input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 1,521 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdkfN3GFDz2b"
      },
      "source": [
        "Now we can score the predictions. \n",
        "\n",
        "First, we need to do some reshaping of our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM_KID9aIZj6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "68004899-af8b-4b60-e136-71270dd8855a"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# First, combine the results across the batches.\n",
        "all_predictions = np.concatenate(predictions, axis=0)\n",
        "all_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "print(\"After flattening the batches, the predictions have shape:\")\n",
        "print(\"    \", all_predictions.shape)\n",
        "\n",
        "# Next, let's remove the third dimension (axis 2), which has the scores\n",
        "# for all 18 labels. \n",
        "\n",
        "# For each token, pick the label with the highest score.\n",
        "predicted_label_ids = np.argmax(all_predictions, axis=2)\n",
        "\n",
        "print(\"\\nAfter choosing the highest scoring label for each token:\")\n",
        "print(\"    \", predicted_label_ids.shape) \n",
        "\n",
        "# Finally, for the sake of scoring, we don't actually care about the different\n",
        "# sentences--we just look at whether the model made correct predictions for the\n",
        "# individual tokens.\n",
        "\n",
        "# Eliminate axis 0, which corresponds to the sentences.\n",
        "predicted_label_ids = np.concatenate(predicted_label_ids, axis=0)\n",
        "all_true_labels = np.concatenate(all_true_labels, axis=0)\n",
        "\n",
        "print(\"\\nAfter flattening the sentences, we have predictions:\")\n",
        "print(\"    \", predicted_label_ids.shape)\n",
        "print(\"and ground truth:\")\n",
        "print(\"    \", all_true_labels.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After flattening the batches, the predictions have shape:\n",
            "     (1521, 50, 18)\n",
            "\n",
            "After choosing the highest scoring label for each token:\n",
            "     (1521, 50)\n",
            "\n",
            "After flattening the sentences, we have predictions:\n",
            "     (76050,)\n",
            "and ground truth:\n",
            "     (76050,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oCLbJXOQFhE"
      },
      "source": [
        "Before we score the predictions, we want to remove all of the tokens labeled `null`, since we don't care about these.\n",
        "\n",
        "This is ok because, in a real NER system, you would:\n",
        "1. Ignore the predictions for the special tokens (`[CLS]`, `[SEP]`, and `[PAD]`)\n",
        "2. Use the prediction for the first subword for any broken-apart words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8ve17VPQE4z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f66beb2-8ebb-430b-c733-dd08d155b2ff"
      },
      "source": [
        "# Construct new lists of predictions which don't include any null tokens.\n",
        "real_token_predictions = []\n",
        "real_token_labels = []\n",
        "\n",
        "# For each of the input tokens in the dataset...\n",
        "for i in range(len(all_true_labels)):\n",
        "\n",
        "    # If it's not a token with a null label...\n",
        "    if not all_true_labels[i] == -100:\n",
        "        \n",
        "        # Add the prediction and the ground truth to their lists.\n",
        "        real_token_predictions.append(predicted_label_ids[i])\n",
        "        real_token_labels.append(all_true_labels[i])\n",
        "\n",
        "print(\"Before filtering out `null` tokens, length = {:,}\".format(len(all_true_labels)))\n",
        "print(\" After filtering out `null` tokens, length = {:,}\".format(len(real_token_labels)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before filtering out `null` tokens, length = 76,050\n",
            " After filtering out `null` tokens, length = 14,256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoD71AZAPkis"
      },
      "source": [
        "The last step is to compute the F1 score!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q7ggFNaTkWu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce4370a1-8b94-42c6-c02b-e5b33060e7ef"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Calculate the F1 score. Because this is a multi-class problem, we have\n",
        "# to set the `average` parameter. TODO - What does `micro` do?\n",
        "f1 = f1_score(real_token_labels, real_token_predictions, average='micro') \n",
        "\n",
        "print (\"F1 score: {:.2%}\".format(f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score: 91.94%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0-epg0Y1-Vw"
      },
      "source": [
        "For reference, [this paper](https://www.aclweb.org/anthology/W18-5711.pdf) from 2018 benchmarked the F1 score at around 79. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfjYoa6WmkN6"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlQG7qgkmf4n"
      },
      "source": [
        "We've seen here that by fine-tuning BERT you can quickly create a high quality NER model with minimal effort and training time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUmsUOIv8EUO"
      },
      "source": [
        "# Appendix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2079Qyn8Mt8"
      },
      "source": [
        "## A1. Saving & Loading Fine-Tuned Model\n",
        "\n",
        "This first cell (taken from `run_glue.py` [here](https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495)) writes the model and tokenizer out to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ulTWaOr8QNY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "08f4127d-74b5-459a-b63a-cd080b59bee2"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-tjHkR7lc1I"
      },
      "source": [
        "Let's check out the file sizes, out of curiosity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqMzI3VTCZo5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "e06ca59e-0491-42ff-e910-aba35471b172"
      },
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 428008K\n",
            "-rw-r--r-- 1 root root      2K Mar 16 17:25 config.json\n",
            "-rw-r--r-- 1 root root 427767K Mar 16 17:25 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1K Mar 16 17:25 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root      1K Mar 16 17:25 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root    227K Mar 16 17:25 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr_bt2rFlgDn"
      },
      "source": [
        "The largest file is the model weights, at around 418 megabytes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUFUIQ8Cu8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af1217f6-5fa7-498f-88aa-a166799a39e2"
      },
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 418M Mar 16 17:25 ./model_save/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzGKvOFAll_e"
      },
      "source": [
        "To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trr-A-POC18_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "1c5440e6-2042-44e4-81a4-5de127c16961"
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-173-3512784ce1b6>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    drive.mount('/content/drive')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxlZsafTC-V5"
      },
      "source": [
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/Shared drives/ChrisMcCormick.AI/Blog Posts/BERT Fine-Tuning/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0vstijw85SZ"
      },
      "source": [
        "The following functions will load the model back from disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskPzUM084zL"
      },
      "source": [
        "# Load a trained model and vocabulary that you have fine-tuned\n",
        "model = model_class.from_pretrained(output_dir)\n",
        "tokenizer = tokenizer_class.from_pretrained(output_dir)\n",
        "\n",
        "# Copy the model to the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}