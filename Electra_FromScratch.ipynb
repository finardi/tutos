{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Electra-FromScratch.ipynb",
      "provenance": [],
      "mount_file_id": "1Y9eOd_cnExzwMNxUmH5EgktsKfMn9svp",
      "authorship_tag": "ABX9TyOKKkxp6DfOcmUInpVVo+1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/Electra_FromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIHC6Pg66zHg"
      },
      "source": [
        "## 1. Introduction\n",
        "\n",
        "At ICLR 2020, [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB), a new method for self-supervised language representation learning, was introduced. ELECTRA is another member of the Transformer pre-training method family, whose previous members such as BERT, GPT-2, RoBERTa have achieved many state-of-the-art results in Natural Language Processing benchmarks.\n",
        "\n",
        "Different from other masked language modeling methods, ELECTRA is a more sample-efficient pre-training task called replaced token detection. At a small scale, ELECTRA-small can be trained on a single GPU for 4 days to outperform [GPT (Radford et al., 2018)](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) (trained using 30x more compute) on the GLUE benchmark. At a large scale, ELECTRA-large outperforms [ALBERT (Lan et al., 2019)]() on GLUE and sets a new state-of-the-art for SQuAD 2.0.\n",
        "\n",
        "![](https://github.com/chriskhanhtran/spanish-bert/blob/master/img/electra-performance.JPG?raw=true)\n",
        "*ELECTRA consistently outperforms masked language model pre-training approaches.*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkVcyxKD6mqE"
      },
      "source": [
        "## 2. Method\n",
        "\n",
        "Masked language modeling pre-training methods such as [BERT (Devlin et al., 2019)](https://arxiv.org/abs/1810.04805) corrupt the input by replacing some tokens (typically 15% of the input) with `[MASK]` and then train a model to re-construct the original tokens.\n",
        "\n",
        "Instead of masking, ELECTRA corrupts the input by replacing some tokens with samples from the outputs of a smalled masked language model. Then, a discriminative model is trained to predict whether each token was an original or a replacement. After pre-training, the generator is thrown out and the discriminator is fine-tuned on downstream tasks.\n",
        "\n",
        "![](https://github.com/chriskhanhtran/spanish-bert/blob/master/img/electra-overview.JPG?raw=true)\n",
        "*An overview of ELECTRA.*\n",
        "\n",
        "Although having a generator and a discriminator like GAN, ELECTRA is not adversarial in that the generator producing corrupted tokens is trained with maximum likelihood rather than being trained to fool the discriminator.\n",
        "\n",
        "**Why is ELECTRA so efficient?**\n",
        "\n",
        "With a new training objective, ELECTRA can achieve comparable performance to strong models such as [RoBERTa (Liu et al., (2019)](https://arxiv.org/abs/1907.11692) which has more parameters and needs 4x more compute for training. In the paper, an analysis was conducted to understand what really contribute to ELECTRA's efficiency. The key findings are:\n",
        "\n",
        "- ELECTRA is greatly benefiting from having a loss defined over all input tokens rather than just a subset. More specifically, in ELECTRA, the discriminator predicts on every token in the input, while in BERT, the generator only predicts 15% masked tokens of the input.\n",
        "- BERT's performance is slightly harmed because in the pre-training phase, the model sees `[MASK]` tokens, while it is not the case in the fine-tuning phase.\n",
        "\n",
        "![](https://github.com/chriskhanhtran/spanish-bert/blob/master/img/electra-vs-bert.JPG?raw=true)\n",
        "*ELECTRA vs. BERT*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYla4vyCMoJl"
      },
      "source": [
        "## 3. Pre-train ELECTRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RedlfwNM2Ii"
      },
      "source": [
        "In this section, we will train ELECTRA from scratch with TensorFlow using scripts provided by ELECTRA's authors in [google-research/electra](https://github.com/google-research/electra). Then we will convert the model to PyTorch's checkpoint, which can be easily fine-tuned on downstream tasks using Hugging Face's `transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIN5qf3dffb0"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNWDG4fac5DJ"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCGeCYwXOJZD"
      },
      "source": [
        "We will pre-train ELECTRA on a Spanish movie subtitle dataset retrieved from OpenSubtitles. This dataset is 5.4 GB in size and we will train on a small subset of ~30 MB for presentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjYMVA0-aDsO",
        "outputId": "eea19b5a-edf9-4f9e-c0e6-0e2be7b1af20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip install -q tensorflow==1.15\n",
        "!pip install -q transformers==2.8.0\n",
        "!git clone https://github.com/google-research/electra.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 38.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 12.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 18.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 36.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.7MB 42.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'electra'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Total 104 (delta 0), reused 0 (delta 0), pack-reused 104\u001b[K\n",
            "Receiving objects: 100% (104/104), 96.57 KiB | 276.00 KiB/s, done.\n",
            "Resolving deltas: 100% (46/46), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZluW8cr5Z7OA"
      },
      "source": [
        "import os\n",
        "import json\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tokenizers.normalizers import Lowercase, NFKC, Sequence"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEb8JPtbjJk3",
        "outputId": "b0d1709f-516c-455e-baf0-c4558f5bfb91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# %%time\n",
        "# tokenizer = BertWordPieceTokenizer(\n",
        "#     clean_text=True,\n",
        "#     handle_chinese_chars=False,\n",
        "#     strip_accents=True,\n",
        "#     lowercase=False,\n",
        "# )\n",
        "\n",
        "# tokenizer.normalizer = Sequence([NFKC()])\n",
        "\n",
        "# path = ['/content/drive/My Drive/Colab Notebooks/from_scratch/artigo2020/Data e Dataprep/wikidump_clean_shuffle_merge.txt']\n",
        "\n",
        "# tokenizer.train(files=path, \n",
        "#                 vocab_size=30000,\n",
        "#                 min_frequency=2, \n",
        "#                 show_progress=True,\n",
        "#                 special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
        "#                 limit_alphabet=2000,\n",
        "#                 wordpieces_prefix=\"##\",\n",
        "#                 )\n",
        "\n",
        "# tokenizer.save('/content/drive/My Drive/Colab Notebooks/from_scratch/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 11min 9s, sys: 8.3 s, total: 11min 17s\n",
            "Wall time: 11min 18s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIJS29L1Pcv4"
      },
      "source": [
        "DATA_DIR = \"/content/drive/My\\ Drive/Colab\\ Notebooks/from_scratch\"\n",
        "MODEL_NAME = \"electra_MALS\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATU2mz_MQGPD"
      },
      "source": [
        "Before building the pre-training dataset, we should make sure the corpus has the following format:\n",
        "- each line is a sentence\n",
        "- a blank line separates two documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O40aslMjfXvW"
      },
      "source": [
        "### Build Pretraining Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHnGR1u2zv8r"
      },
      "source": [
        "We will use the tokenizer of `bert-base-multilingual-cased` to process Spanish texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b8facRPvjv"
      },
      "source": [
        "We use `build_pretraining_dataset.py` to create a pre-training dataset from a dump of raw text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0rKREphcl7",
        "outputId": "5231ceb1-32c9-480c-fbee-0c314b2292ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# !python3 electra/build_pretraining_dataset.py \\\n",
        "#   --corpus-dir $DATA_DIR \\\n",
        "#   --vocab-file $DATA_DIR/vocab.txt \\\n",
        "#   --output-dir $DATA_DIR/pretrain_tfrecords \\\n",
        "#   --max-seq-length 128 \\\n",
        "#   --blanks-separate-docs False \\\n",
        "#   --no-lower-case \\\n",
        "#   --num-processes 5"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Job 0: Creating example writer\n",
            "Job 1: Creating example writer\n",
            "Job 2: Creating example writer\n",
            "Job 3: Creating example writer\n",
            "Job 4: Creating example writer\n",
            "Job 3: Writing tf examples\n",
            "Job 4: Writing tf examples\n",
            "Job 0: Writing tf examples\n",
            "Job 3: Done!\n",
            "Job 0: Done!\n",
            "Job 1: Writing tf examples\n",
            "Job 1: Done!\n",
            "Job 2: Writing tf examples\n",
            "Job 4: Done!\n",
            "Job 2: Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb3PQwU0jFEF"
      },
      "source": [
        "### Start Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1InjwsFRXeN"
      },
      "source": [
        "We use `run_pretraining.py` to pre-train an ELECTRA model.\n",
        "\n",
        "To train a small ELECTRA model for 1 million steps, run:\n",
        "\n",
        "```\n",
        "python3 run_pretraining.py --data-dir $DATA_DIR --model-name electra_small\n",
        "```\n",
        "\n",
        "This takes slightly over 4 days on a Tesla V100 GPU. However, the model should achieve decent results after 200k steps (10 hours of training on the v100 GPU).\n",
        "\n",
        "To customize the training, create a `.json` file containing the hyperparameters. Please refer [`configure_pretraining.py`](https://github.com/google-research/electra/blob/master/configure_pretraining.py) for default values of all hyperparameters.\n",
        "\n",
        "Below, we set the hyperparameters to train the model for only 100 steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LntOzQIopBzX"
      },
      "source": [
        "hparams = {\n",
        "    \"do_train\": \"true\",\n",
        "    \"do_eval\": \"false\",\n",
        "    \"model_size\": \"small\",\n",
        "    \"do_lower_case\": \"false\",\n",
        "    \"vocab_size\": 30000,\n",
        "    \"num_train_steps\": 10_000,\n",
        "    \"save_checkpoints_steps\": 100,\n",
        "    \"train_batch_size\": 32,\n",
        "}\n",
        "           \n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/from_scratch/hparams.json\", \"w\") as f:\n",
        "    json.dump(hparams, f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHskrniLXFQ8"
      },
      "source": [
        "Let's start training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjoPP5rrlAQj",
        "outputId": "7e4130aa-9744-46b7-b991-f1d7c4f8ab2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 electra/run_pretraining.py \\\n",
        "  --data-dir $DATA_DIR \\\n",
        "  --model-name $MODEL_NAME \\\n",
        "  --hparams \"/content/drive/My Drive/Colab Notebooks/from_scratch/hparams.json\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mA saída de streaming foi truncada nas últimas 5000 linhas.\u001b[0m\n",
            "5138/10000 = 51.4%, SPS: 3.8, ELAP: 18:00, ETA: 21:09 - loss: 24.4225\n",
            "5139/10000 = 51.4%, SPS: 3.8, ELAP: 18:01, ETA: 21:09 - loss: 24.6090\n",
            "5140/10000 = 51.4%, SPS: 3.8, ELAP: 18:01, ETA: 21:09 - loss: 24.0571\n",
            "5141/10000 = 51.4%, SPS: 3.8, ELAP: 18:01, ETA: 21:08 - loss: 24.0268\n",
            "5142/10000 = 51.4%, SPS: 3.8, ELAP: 18:01, ETA: 21:08 - loss: 25.5911\n",
            "5143/10000 = 51.4%, SPS: 3.8, ELAP: 18:02, ETA: 21:08 - loss: 24.7074\n",
            "5144/10000 = 51.4%, SPS: 3.8, ELAP: 18:02, ETA: 21:08 - loss: 24.1777\n",
            "5145/10000 = 51.5%, SPS: 3.8, ELAP: 18:02, ETA: 21:07 - loss: 24.9475\n",
            "5146/10000 = 51.5%, SPS: 3.8, ELAP: 18:02, ETA: 21:07 - loss: 23.5475\n",
            "5147/10000 = 51.5%, SPS: 3.8, ELAP: 18:03, ETA: 21:07 - loss: 24.6130\n",
            "5148/10000 = 51.5%, SPS: 3.8, ELAP: 18:03, ETA: 21:07 - loss: 24.6527\n",
            "5149/10000 = 51.5%, SPS: 3.8, ELAP: 18:03, ETA: 21:06 - loss: 24.6164\n",
            "5150/10000 = 51.5%, SPS: 3.8, ELAP: 18:03, ETA: 21:06 - loss: 24.7649\n",
            "5151/10000 = 51.5%, SPS: 3.8, ELAP: 18:03, ETA: 21:06 - loss: 24.1286\n",
            "5152/10000 = 51.5%, SPS: 3.8, ELAP: 18:04, ETA: 21:05 - loss: 23.9267\n",
            "5153/10000 = 51.5%, SPS: 3.8, ELAP: 18:04, ETA: 21:05 - loss: 24.5879\n",
            "5154/10000 = 51.5%, SPS: 3.8, ELAP: 18:04, ETA: 21:05 - loss: 24.1082\n",
            "5155/10000 = 51.5%, SPS: 3.8, ELAP: 18:04, ETA: 21:04 - loss: 24.2616\n",
            "5156/10000 = 51.6%, SPS: 3.8, ELAP: 18:05, ETA: 21:04 - loss: 25.2699\n",
            "5157/10000 = 51.6%, SPS: 3.8, ELAP: 18:05, ETA: 21:04 - loss: 24.9770\n",
            "5158/10000 = 51.6%, SPS: 3.8, ELAP: 18:05, ETA: 21:04 - loss: 24.9176\n",
            "5159/10000 = 51.6%, SPS: 3.8, ELAP: 18:05, ETA: 21:03 - loss: 24.5413\n",
            "5160/10000 = 51.6%, SPS: 3.8, ELAP: 18:06, ETA: 21:03 - loss: 24.9872\n",
            "5161/10000 = 51.6%, SPS: 3.8, ELAP: 18:06, ETA: 21:03 - loss: 24.8321\n",
            "5162/10000 = 51.6%, SPS: 3.8, ELAP: 18:06, ETA: 21:02 - loss: 23.8174\n",
            "5163/10000 = 51.6%, SPS: 3.8, ELAP: 18:06, ETA: 21:02 - loss: 25.0650\n",
            "5164/10000 = 51.6%, SPS: 3.8, ELAP: 18:06, ETA: 21:02 - loss: 25.1858\n",
            "5165/10000 = 51.6%, SPS: 3.8, ELAP: 18:07, ETA: 21:02 - loss: 24.9710\n",
            "5166/10000 = 51.7%, SPS: 3.8, ELAP: 18:07, ETA: 21:01 - loss: 24.6134\n",
            "5167/10000 = 51.7%, SPS: 3.8, ELAP: 18:07, ETA: 21:01 - loss: 24.2514\n",
            "5168/10000 = 51.7%, SPS: 3.8, ELAP: 18:07, ETA: 21:01 - loss: 24.3641\n",
            "5169/10000 = 51.7%, SPS: 3.8, ELAP: 18:08, ETA: 21:00 - loss: 24.5870\n",
            "5170/10000 = 51.7%, SPS: 3.8, ELAP: 18:08, ETA: 21:00 - loss: 24.4460\n",
            "5171/10000 = 51.7%, SPS: 3.8, ELAP: 18:08, ETA: 21:00 - loss: 24.0234\n",
            "5172/10000 = 51.7%, SPS: 3.8, ELAP: 18:08, ETA: 20:59 - loss: 25.1791\n",
            "5173/10000 = 51.7%, SPS: 3.8, ELAP: 18:09, ETA: 20:59 - loss: 23.8923\n",
            "5174/10000 = 51.7%, SPS: 3.8, ELAP: 18:09, ETA: 20:59 - loss: 24.5980\n",
            "5175/10000 = 51.8%, SPS: 3.8, ELAP: 18:09, ETA: 20:59 - loss: 24.3914\n",
            "5176/10000 = 51.8%, SPS: 3.8, ELAP: 18:09, ETA: 20:58 - loss: 24.4589\n",
            "5177/10000 = 51.8%, SPS: 3.8, ELAP: 18:09, ETA: 20:58 - loss: 24.0826\n",
            "5178/10000 = 51.8%, SPS: 3.8, ELAP: 18:10, ETA: 20:58 - loss: 24.8575\n",
            "5179/10000 = 51.8%, SPS: 3.8, ELAP: 18:10, ETA: 20:57 - loss: 24.3121\n",
            "5180/10000 = 51.8%, SPS: 3.8, ELAP: 18:10, ETA: 20:57 - loss: 24.4629\n",
            "5181/10000 = 51.8%, SPS: 3.8, ELAP: 18:10, ETA: 20:57 - loss: 24.4353\n",
            "5182/10000 = 51.8%, SPS: 3.8, ELAP: 18:11, ETA: 20:56 - loss: 24.8608\n",
            "5183/10000 = 51.8%, SPS: 3.8, ELAP: 18:11, ETA: 20:56 - loss: 24.7334\n",
            "5184/10000 = 51.8%, SPS: 3.8, ELAP: 18:11, ETA: 20:56 - loss: 24.5160\n",
            "5185/10000 = 51.9%, SPS: 3.8, ELAP: 18:11, ETA: 20:56 - loss: 24.2005\n",
            "5186/10000 = 51.9%, SPS: 3.8, ELAP: 18:12, ETA: 20:55 - loss: 24.5502\n",
            "5187/10000 = 51.9%, SPS: 3.8, ELAP: 18:12, ETA: 20:55 - loss: 24.4099\n",
            "5188/10000 = 51.9%, SPS: 3.8, ELAP: 18:12, ETA: 20:55 - loss: 24.6847\n",
            "5189/10000 = 51.9%, SPS: 3.8, ELAP: 18:12, ETA: 20:54 - loss: 24.7629\n",
            "5190/10000 = 51.9%, SPS: 3.8, ELAP: 18:12, ETA: 20:54 - loss: 24.3968\n",
            "5191/10000 = 51.9%, SPS: 3.8, ELAP: 18:13, ETA: 20:54 - loss: 24.5074\n",
            "5192/10000 = 51.9%, SPS: 3.8, ELAP: 18:13, ETA: 20:54 - loss: 24.0433\n",
            "5193/10000 = 51.9%, SPS: 3.8, ELAP: 18:13, ETA: 20:53 - loss: 24.6350\n",
            "5194/10000 = 51.9%, SPS: 3.8, ELAP: 18:13, ETA: 20:53 - loss: 24.8358\n",
            "5195/10000 = 52.0%, SPS: 3.8, ELAP: 18:14, ETA: 20:53 - loss: 24.6606\n",
            "5196/10000 = 52.0%, SPS: 3.8, ELAP: 18:14, ETA: 20:52 - loss: 24.5076\n",
            "5197/10000 = 52.0%, SPS: 3.8, ELAP: 18:14, ETA: 20:52 - loss: 24.3755\n",
            "5198/10000 = 52.0%, SPS: 3.8, ELAP: 18:14, ETA: 20:52 - loss: 24.7259\n",
            "5199/10000 = 52.0%, SPS: 3.8, ELAP: 18:15, ETA: 20:51 - loss: 24.4515\n",
            "5200/10000 = 52.0%, SPS: 3.8, ELAP: 18:17, ETA: 20:54 - loss: 24.2612\n",
            "5201/10000 = 52.0%, SPS: 3.8, ELAP: 18:17, ETA: 20:53 - loss: 24.4986\n",
            "5202/10000 = 52.0%, SPS: 3.8, ELAP: 18:17, ETA: 20:53 - loss: 24.4974\n",
            "5203/10000 = 52.0%, SPS: 3.8, ELAP: 18:18, ETA: 20:53 - loss: 24.8409\n",
            "5204/10000 = 52.0%, SPS: 3.8, ELAP: 18:18, ETA: 20:52 - loss: 23.4431\n",
            "5205/10000 = 52.0%, SPS: 3.8, ELAP: 18:18, ETA: 20:52 - loss: 25.1046\n",
            "5206/10000 = 52.1%, SPS: 3.8, ELAP: 18:18, ETA: 20:52 - loss: 24.4870\n",
            "5207/10000 = 52.1%, SPS: 3.8, ELAP: 18:19, ETA: 20:52 - loss: 24.1501\n",
            "5208/10000 = 52.1%, SPS: 3.8, ELAP: 18:19, ETA: 20:51 - loss: 24.5207\n",
            "5209/10000 = 52.1%, SPS: 3.8, ELAP: 18:19, ETA: 20:51 - loss: 24.2561\n",
            "5210/10000 = 52.1%, SPS: 3.8, ELAP: 18:19, ETA: 20:51 - loss: 24.7611\n",
            "5211/10000 = 52.1%, SPS: 3.8, ELAP: 18:19, ETA: 20:50 - loss: 24.4294\n",
            "5212/10000 = 52.1%, SPS: 3.8, ELAP: 18:20, ETA: 20:50 - loss: 24.2994\n",
            "5213/10000 = 52.1%, SPS: 3.8, ELAP: 18:20, ETA: 20:50 - loss: 24.0701\n",
            "5214/10000 = 52.1%, SPS: 3.8, ELAP: 18:20, ETA: 20:50 - loss: 24.7404\n",
            "5215/10000 = 52.1%, SPS: 3.8, ELAP: 18:20, ETA: 20:49 - loss: 24.1395\n",
            "5216/10000 = 52.2%, SPS: 3.8, ELAP: 18:21, ETA: 20:49 - loss: 24.1269\n",
            "5217/10000 = 52.2%, SPS: 3.8, ELAP: 18:21, ETA: 20:49 - loss: 24.6565\n",
            "5218/10000 = 52.2%, SPS: 3.8, ELAP: 18:21, ETA: 20:48 - loss: 24.0537\n",
            "5219/10000 = 52.2%, SPS: 3.8, ELAP: 18:21, ETA: 20:48 - loss: 24.5786\n",
            "5220/10000 = 52.2%, SPS: 3.8, ELAP: 18:22, ETA: 20:48 - loss: 24.7660\n",
            "5221/10000 = 52.2%, SPS: 3.8, ELAP: 18:22, ETA: 20:47 - loss: 24.7021\n",
            "5222/10000 = 52.2%, SPS: 3.8, ELAP: 18:22, ETA: 20:47 - loss: 24.4236\n",
            "5223/10000 = 52.2%, SPS: 3.8, ELAP: 18:22, ETA: 20:47 - loss: 24.1045\n",
            "5224/10000 = 52.2%, SPS: 3.8, ELAP: 18:22, ETA: 20:47 - loss: 24.7312\n",
            "5225/10000 = 52.2%, SPS: 3.8, ELAP: 18:23, ETA: 20:46 - loss: 24.0888\n",
            "5226/10000 = 52.3%, SPS: 3.8, ELAP: 18:23, ETA: 20:46 - loss: 24.3437\n",
            "5227/10000 = 52.3%, SPS: 3.8, ELAP: 18:23, ETA: 20:46 - loss: 24.6559\n",
            "5228/10000 = 52.3%, SPS: 3.8, ELAP: 18:23, ETA: 20:45 - loss: 24.9022\n",
            "5229/10000 = 52.3%, SPS: 3.8, ELAP: 18:24, ETA: 20:45 - loss: 24.7935\n",
            "5230/10000 = 52.3%, SPS: 3.8, ELAP: 18:24, ETA: 20:45 - loss: 24.4366\n",
            "5231/10000 = 52.3%, SPS: 3.8, ELAP: 18:24, ETA: 20:45 - loss: 24.5525\n",
            "5232/10000 = 52.3%, SPS: 3.8, ELAP: 18:24, ETA: 20:44 - loss: 24.5260\n",
            "5233/10000 = 52.3%, SPS: 3.8, ELAP: 18:25, ETA: 20:44 - loss: 24.2931\n",
            "5234/10000 = 52.3%, SPS: 3.8, ELAP: 18:25, ETA: 20:44 - loss: 24.2724\n",
            "5235/10000 = 52.4%, SPS: 3.8, ELAP: 18:25, ETA: 20:43 - loss: 24.2577\n",
            "5236/10000 = 52.4%, SPS: 3.8, ELAP: 18:25, ETA: 20:43 - loss: 24.5469\n",
            "5237/10000 = 52.4%, SPS: 3.8, ELAP: 18:26, ETA: 20:43 - loss: 24.1718\n",
            "5238/10000 = 52.4%, SPS: 3.8, ELAP: 18:26, ETA: 20:43 - loss: 24.4335\n",
            "5239/10000 = 52.4%, SPS: 3.8, ELAP: 18:26, ETA: 20:42 - loss: 24.2642\n",
            "5240/10000 = 52.4%, SPS: 3.8, ELAP: 18:26, ETA: 20:42 - loss: 24.2598\n",
            "5241/10000 = 52.4%, SPS: 3.8, ELAP: 18:27, ETA: 20:42 - loss: 24.5388\n",
            "5242/10000 = 52.4%, SPS: 3.8, ELAP: 18:27, ETA: 20:42 - loss: 24.4111\n",
            "5243/10000 = 52.4%, SPS: 3.8, ELAP: 18:27, ETA: 20:41 - loss: 24.2008\n",
            "5244/10000 = 52.4%, SPS: 3.8, ELAP: 18:27, ETA: 20:41 - loss: 23.9809\n",
            "5245/10000 = 52.5%, SPS: 3.8, ELAP: 18:28, ETA: 20:41 - loss: 24.1517\n",
            "5246/10000 = 52.5%, SPS: 3.8, ELAP: 18:28, ETA: 20:40 - loss: 24.8312\n",
            "5247/10000 = 52.5%, SPS: 3.8, ELAP: 18:28, ETA: 20:40 - loss: 24.3844\n",
            "5248/10000 = 52.5%, SPS: 3.8, ELAP: 18:28, ETA: 20:40 - loss: 24.8676\n",
            "5249/10000 = 52.5%, SPS: 3.8, ELAP: 18:29, ETA: 20:40 - loss: 23.7593\n",
            "5250/10000 = 52.5%, SPS: 3.8, ELAP: 18:29, ETA: 20:39 - loss: 24.8561\n",
            "5251/10000 = 52.5%, SPS: 3.8, ELAP: 18:29, ETA: 20:39 - loss: 24.0282\n",
            "5252/10000 = 52.5%, SPS: 3.8, ELAP: 18:29, ETA: 20:39 - loss: 24.7262\n",
            "5253/10000 = 52.5%, SPS: 3.8, ELAP: 18:29, ETA: 20:38 - loss: 24.7563\n",
            "5254/10000 = 52.5%, SPS: 3.8, ELAP: 18:30, ETA: 20:38 - loss: 24.4895\n",
            "5255/10000 = 52.5%, SPS: 3.8, ELAP: 18:30, ETA: 20:38 - loss: 24.0666\n",
            "5256/10000 = 52.6%, SPS: 3.8, ELAP: 18:30, ETA: 20:37 - loss: 24.4390\n",
            "5257/10000 = 52.6%, SPS: 3.8, ELAP: 18:30, ETA: 20:37 - loss: 24.3807\n",
            "5258/10000 = 52.6%, SPS: 3.8, ELAP: 18:31, ETA: 20:37 - loss: 24.6469\n",
            "5259/10000 = 52.6%, SPS: 3.8, ELAP: 18:31, ETA: 20:37 - loss: 24.9371\n",
            "5260/10000 = 52.6%, SPS: 3.8, ELAP: 18:31, ETA: 20:36 - loss: 24.5288\n",
            "5261/10000 = 52.6%, SPS: 3.8, ELAP: 18:31, ETA: 20:36 - loss: 24.2881\n",
            "5262/10000 = 52.6%, SPS: 3.8, ELAP: 18:32, ETA: 20:36 - loss: 25.1950\n",
            "5263/10000 = 52.6%, SPS: 3.8, ELAP: 18:32, ETA: 20:35 - loss: 23.8743\n",
            "5264/10000 = 52.6%, SPS: 3.8, ELAP: 18:32, ETA: 20:35 - loss: 24.3943\n",
            "5265/10000 = 52.6%, SPS: 3.8, ELAP: 18:32, ETA: 20:35 - loss: 24.3947\n",
            "5266/10000 = 52.7%, SPS: 3.8, ELAP: 18:33, ETA: 20:35 - loss: 24.5662\n",
            "5267/10000 = 52.7%, SPS: 3.8, ELAP: 18:33, ETA: 20:34 - loss: 24.1814\n",
            "5268/10000 = 52.7%, SPS: 3.8, ELAP: 18:33, ETA: 20:34 - loss: 25.0867\n",
            "5269/10000 = 52.7%, SPS: 3.8, ELAP: 18:33, ETA: 20:34 - loss: 24.2831\n",
            "5270/10000 = 52.7%, SPS: 3.8, ELAP: 18:33, ETA: 20:33 - loss: 24.1472\n",
            "5271/10000 = 52.7%, SPS: 3.8, ELAP: 18:34, ETA: 20:33 - loss: 24.3390\n",
            "5272/10000 = 52.7%, SPS: 3.8, ELAP: 18:34, ETA: 20:33 - loss: 24.3392\n",
            "5273/10000 = 52.7%, SPS: 3.8, ELAP: 18:34, ETA: 20:33 - loss: 24.6510\n",
            "5274/10000 = 52.7%, SPS: 3.8, ELAP: 18:34, ETA: 20:32 - loss: 24.4224\n",
            "5275/10000 = 52.8%, SPS: 3.8, ELAP: 18:35, ETA: 20:32 - loss: 24.8906\n",
            "5276/10000 = 52.8%, SPS: 3.8, ELAP: 18:35, ETA: 20:32 - loss: 24.3181\n",
            "5277/10000 = 52.8%, SPS: 3.8, ELAP: 18:35, ETA: 20:31 - loss: 24.4706\n",
            "5278/10000 = 52.8%, SPS: 3.8, ELAP: 18:35, ETA: 20:31 - loss: 24.7500\n",
            "5279/10000 = 52.8%, SPS: 3.8, ELAP: 18:36, ETA: 20:31 - loss: 24.7939\n",
            "5280/10000 = 52.8%, SPS: 3.8, ELAP: 18:36, ETA: 20:30 - loss: 23.9217\n",
            "5281/10000 = 52.8%, SPS: 3.8, ELAP: 18:36, ETA: 20:30 - loss: 24.6152\n",
            "5282/10000 = 52.8%, SPS: 3.8, ELAP: 18:36, ETA: 20:30 - loss: 24.5380\n",
            "5283/10000 = 52.8%, SPS: 3.8, ELAP: 18:36, ETA: 20:30 - loss: 24.6488\n",
            "5284/10000 = 52.8%, SPS: 3.8, ELAP: 18:37, ETA: 20:29 - loss: 24.4665\n",
            "5285/10000 = 52.9%, SPS: 3.8, ELAP: 18:37, ETA: 20:29 - loss: 25.1880\n",
            "5286/10000 = 52.9%, SPS: 3.8, ELAP: 18:37, ETA: 20:29 - loss: 24.9064\n",
            "5287/10000 = 52.9%, SPS: 3.8, ELAP: 18:37, ETA: 20:28 - loss: 24.7169\n",
            "5288/10000 = 52.9%, SPS: 3.8, ELAP: 18:38, ETA: 20:28 - loss: 24.9839\n",
            "5289/10000 = 52.9%, SPS: 3.8, ELAP: 18:38, ETA: 20:28 - loss: 24.7563\n",
            "5290/10000 = 52.9%, SPS: 3.8, ELAP: 18:38, ETA: 20:28 - loss: 24.2226\n",
            "5291/10000 = 52.9%, SPS: 3.8, ELAP: 18:38, ETA: 20:27 - loss: 24.0124\n",
            "5292/10000 = 52.9%, SPS: 3.8, ELAP: 18:39, ETA: 20:27 - loss: 24.3832\n",
            "5293/10000 = 52.9%, SPS: 3.8, ELAP: 18:39, ETA: 20:27 - loss: 24.0969\n",
            "5294/10000 = 52.9%, SPS: 3.8, ELAP: 18:39, ETA: 20:26 - loss: 24.7292\n",
            "5295/10000 = 53.0%, SPS: 3.8, ELAP: 18:39, ETA: 20:26 - loss: 24.6441\n",
            "5296/10000 = 53.0%, SPS: 3.8, ELAP: 18:39, ETA: 20:26 - loss: 24.7134\n",
            "5297/10000 = 53.0%, SPS: 3.8, ELAP: 18:40, ETA: 20:25 - loss: 24.2314\n",
            "5298/10000 = 53.0%, SPS: 3.8, ELAP: 18:40, ETA: 20:25 - loss: 24.2014\n",
            "5299/10000 = 53.0%, SPS: 3.8, ELAP: 18:40, ETA: 20:25 - loss: 23.1085\n",
            "5300/10000 = 53.0%, SPS: 3.8, ELAP: 18:43, ETA: 20:27 - loss: 24.1921\n",
            "5301/10000 = 53.0%, SPS: 3.8, ELAP: 18:43, ETA: 20:27 - loss: 24.7623\n",
            "5302/10000 = 53.0%, SPS: 3.8, ELAP: 18:43, ETA: 20:26 - loss: 24.7522\n",
            "5303/10000 = 53.0%, SPS: 3.8, ELAP: 18:43, ETA: 20:26 - loss: 23.7338\n",
            "5304/10000 = 53.0%, SPS: 3.8, ELAP: 18:43, ETA: 20:26 - loss: 24.5522\n",
            "5305/10000 = 53.0%, SPS: 3.8, ELAP: 18:44, ETA: 20:26 - loss: 24.7259\n",
            "5306/10000 = 53.1%, SPS: 3.8, ELAP: 18:44, ETA: 20:25 - loss: 25.0156\n",
            "5307/10000 = 53.1%, SPS: 3.8, ELAP: 18:44, ETA: 20:25 - loss: 24.3732\n",
            "5308/10000 = 53.1%, SPS: 3.8, ELAP: 18:44, ETA: 20:25 - loss: 24.4928\n",
            "5309/10000 = 53.1%, SPS: 3.8, ELAP: 18:45, ETA: 20:24 - loss: 24.2892\n",
            "5310/10000 = 53.1%, SPS: 3.8, ELAP: 18:45, ETA: 20:24 - loss: 24.7477\n",
            "5311/10000 = 53.1%, SPS: 3.8, ELAP: 18:45, ETA: 20:24 - loss: 24.7398\n",
            "5312/10000 = 53.1%, SPS: 3.8, ELAP: 18:45, ETA: 20:23 - loss: 24.1328\n",
            "5313/10000 = 53.1%, SPS: 3.8, ELAP: 18:46, ETA: 20:23 - loss: 23.6997\n",
            "5314/10000 = 53.1%, SPS: 3.8, ELAP: 18:46, ETA: 20:23 - loss: 24.8890\n",
            "5315/10000 = 53.1%, SPS: 3.8, ELAP: 18:46, ETA: 20:23 - loss: 24.8595\n",
            "5316/10000 = 53.2%, SPS: 3.8, ELAP: 18:46, ETA: 20:22 - loss: 25.0147\n",
            "5317/10000 = 53.2%, SPS: 3.8, ELAP: 18:46, ETA: 20:22 - loss: 24.8537\n",
            "5318/10000 = 53.2%, SPS: 3.8, ELAP: 18:47, ETA: 20:22 - loss: 25.1111\n",
            "5319/10000 = 53.2%, SPS: 3.8, ELAP: 18:47, ETA: 20:21 - loss: 24.6388\n",
            "5320/10000 = 53.2%, SPS: 3.8, ELAP: 18:47, ETA: 20:21 - loss: 24.8199\n",
            "5321/10000 = 53.2%, SPS: 3.8, ELAP: 18:47, ETA: 20:21 - loss: 24.7343\n",
            "5322/10000 = 53.2%, SPS: 3.8, ELAP: 18:48, ETA: 20:20 - loss: 24.7574\n",
            "5323/10000 = 53.2%, SPS: 3.8, ELAP: 18:48, ETA: 20:20 - loss: 24.3996\n",
            "5324/10000 = 53.2%, SPS: 3.8, ELAP: 18:48, ETA: 20:20 - loss: 24.5987\n",
            "5325/10000 = 53.2%, SPS: 3.8, ELAP: 18:48, ETA: 20:20 - loss: 24.6524\n",
            "5326/10000 = 53.3%, SPS: 3.8, ELAP: 18:49, ETA: 20:19 - loss: 23.6850\n",
            "5327/10000 = 53.3%, SPS: 3.8, ELAP: 18:49, ETA: 20:19 - loss: 24.3609\n",
            "5328/10000 = 53.3%, SPS: 3.8, ELAP: 18:49, ETA: 20:19 - loss: 24.8435\n",
            "5329/10000 = 53.3%, SPS: 3.8, ELAP: 18:49, ETA: 20:19 - loss: 25.0187\n",
            "5330/10000 = 53.3%, SPS: 3.8, ELAP: 18:50, ETA: 20:18 - loss: 24.0278\n",
            "5331/10000 = 53.3%, SPS: 3.8, ELAP: 18:50, ETA: 20:18 - loss: 24.4025\n",
            "5332/10000 = 53.3%, SPS: 3.8, ELAP: 18:50, ETA: 20:18 - loss: 24.7336\n",
            "5333/10000 = 53.3%, SPS: 3.8, ELAP: 18:50, ETA: 20:17 - loss: 24.4992\n",
            "5334/10000 = 53.3%, SPS: 3.8, ELAP: 18:51, ETA: 20:17 - loss: 24.7866\n",
            "5335/10000 = 53.4%, SPS: 3.8, ELAP: 18:51, ETA: 20:17 - loss: 24.2738\n",
            "5336/10000 = 53.4%, SPS: 3.8, ELAP: 18:51, ETA: 20:17 - loss: 24.7641\n",
            "5337/10000 = 53.4%, SPS: 3.8, ELAP: 18:51, ETA: 20:16 - loss: 24.8815\n",
            "5338/10000 = 53.4%, SPS: 3.8, ELAP: 18:52, ETA: 20:16 - loss: 24.1850\n",
            "5339/10000 = 53.4%, SPS: 3.8, ELAP: 18:52, ETA: 20:16 - loss: 24.7869\n",
            "5340/10000 = 53.4%, SPS: 3.8, ELAP: 18:52, ETA: 20:16 - loss: 25.0980\n",
            "5341/10000 = 53.4%, SPS: 3.8, ELAP: 18:52, ETA: 20:15 - loss: 24.9349\n",
            "5342/10000 = 53.4%, SPS: 3.8, ELAP: 18:53, ETA: 20:15 - loss: 24.2730\n",
            "5343/10000 = 53.4%, SPS: 3.8, ELAP: 18:53, ETA: 20:15 - loss: 24.5815\n",
            "5344/10000 = 53.4%, SPS: 3.8, ELAP: 18:53, ETA: 20:14 - loss: 24.5313\n",
            "5345/10000 = 53.5%, SPS: 3.8, ELAP: 18:53, ETA: 20:14 - loss: 24.7700\n",
            "5346/10000 = 53.5%, SPS: 3.8, ELAP: 18:53, ETA: 20:14 - loss: 24.4750\n",
            "5347/10000 = 53.5%, SPS: 3.8, ELAP: 18:54, ETA: 20:13 - loss: 24.2139\n",
            "5348/10000 = 53.5%, SPS: 3.8, ELAP: 18:54, ETA: 20:13 - loss: 24.8035\n",
            "5349/10000 = 53.5%, SPS: 3.8, ELAP: 18:54, ETA: 20:13 - loss: 23.7102\n",
            "5350/10000 = 53.5%, SPS: 3.8, ELAP: 18:54, ETA: 20:13 - loss: 24.2069\n",
            "5351/10000 = 53.5%, SPS: 3.8, ELAP: 18:55, ETA: 20:12 - loss: 24.4918\n",
            "5352/10000 = 53.5%, SPS: 3.8, ELAP: 18:55, ETA: 20:12 - loss: 24.6169\n",
            "5353/10000 = 53.5%, SPS: 3.8, ELAP: 18:55, ETA: 20:12 - loss: 24.8104\n",
            "5354/10000 = 53.5%, SPS: 3.8, ELAP: 18:55, ETA: 20:11 - loss: 24.6056\n",
            "5355/10000 = 53.5%, SPS: 3.8, ELAP: 18:55, ETA: 20:11 - loss: 24.0727\n",
            "5356/10000 = 53.6%, SPS: 3.8, ELAP: 18:56, ETA: 20:11 - loss: 24.1253\n",
            "5357/10000 = 53.6%, SPS: 3.8, ELAP: 18:56, ETA: 20:11 - loss: 24.7474\n",
            "5358/10000 = 53.6%, SPS: 3.8, ELAP: 18:56, ETA: 20:10 - loss: 24.3105\n",
            "5359/10000 = 53.6%, SPS: 3.8, ELAP: 18:56, ETA: 20:10 - loss: 24.8039\n",
            "5360/10000 = 53.6%, SPS: 3.8, ELAP: 18:57, ETA: 20:10 - loss: 23.7363\n",
            "5361/10000 = 53.6%, SPS: 3.8, ELAP: 18:57, ETA: 20:09 - loss: 24.4899\n",
            "5362/10000 = 53.6%, SPS: 3.8, ELAP: 18:57, ETA: 20:09 - loss: 24.7688\n",
            "5363/10000 = 53.6%, SPS: 3.8, ELAP: 18:57, ETA: 20:09 - loss: 24.6592\n",
            "5364/10000 = 53.6%, SPS: 3.8, ELAP: 18:58, ETA: 20:08 - loss: 24.3655\n",
            "5365/10000 = 53.6%, SPS: 3.8, ELAP: 18:58, ETA: 20:08 - loss: 24.0576\n",
            "5366/10000 = 53.7%, SPS: 3.8, ELAP: 18:58, ETA: 20:08 - loss: 24.5865\n",
            "5367/10000 = 53.7%, SPS: 3.8, ELAP: 18:58, ETA: 20:08 - loss: 23.8490\n",
            "5368/10000 = 53.7%, SPS: 3.8, ELAP: 18:58, ETA: 20:07 - loss: 24.1374\n",
            "5369/10000 = 53.7%, SPS: 3.8, ELAP: 18:59, ETA: 20:07 - loss: 24.2264\n",
            "5370/10000 = 53.7%, SPS: 3.8, ELAP: 18:59, ETA: 20:07 - loss: 23.7807\n",
            "5371/10000 = 53.7%, SPS: 3.8, ELAP: 18:59, ETA: 20:06 - loss: 24.1980\n",
            "5372/10000 = 53.7%, SPS: 3.8, ELAP: 18:59, ETA: 20:06 - loss: 24.3751\n",
            "5373/10000 = 53.7%, SPS: 3.8, ELAP: 19:00, ETA: 20:06 - loss: 24.5113\n",
            "5374/10000 = 53.7%, SPS: 3.8, ELAP: 19:00, ETA: 20:06 - loss: 24.1478\n",
            "5375/10000 = 53.8%, SPS: 3.8, ELAP: 19:00, ETA: 20:05 - loss: 24.4339\n",
            "5376/10000 = 53.8%, SPS: 3.8, ELAP: 19:00, ETA: 20:05 - loss: 24.6183\n",
            "5377/10000 = 53.8%, SPS: 3.8, ELAP: 19:01, ETA: 20:05 - loss: 24.3941\n",
            "5378/10000 = 53.8%, SPS: 3.8, ELAP: 19:01, ETA: 20:04 - loss: 24.2211\n",
            "5379/10000 = 53.8%, SPS: 3.8, ELAP: 19:01, ETA: 20:04 - loss: 24.7429\n",
            "5380/10000 = 53.8%, SPS: 3.8, ELAP: 19:01, ETA: 20:04 - loss: 24.3663\n",
            "5381/10000 = 53.8%, SPS: 3.8, ELAP: 19:01, ETA: 20:03 - loss: 24.6598\n",
            "5382/10000 = 53.8%, SPS: 3.8, ELAP: 19:02, ETA: 20:03 - loss: 23.7370\n",
            "5383/10000 = 53.8%, SPS: 3.8, ELAP: 19:02, ETA: 20:03 - loss: 24.3271\n",
            "5384/10000 = 53.8%, SPS: 3.8, ELAP: 19:02, ETA: 20:03 - loss: 24.8774\n",
            "5385/10000 = 53.9%, SPS: 3.8, ELAP: 19:02, ETA: 20:02 - loss: 24.3320\n",
            "5386/10000 = 53.9%, SPS: 3.8, ELAP: 19:03, ETA: 20:02 - loss: 24.4064\n",
            "5387/10000 = 53.9%, SPS: 3.8, ELAP: 19:03, ETA: 20:02 - loss: 24.1503\n",
            "5388/10000 = 53.9%, SPS: 3.8, ELAP: 19:03, ETA: 20:01 - loss: 25.2322\n",
            "5389/10000 = 53.9%, SPS: 3.8, ELAP: 19:03, ETA: 20:01 - loss: 24.9138\n",
            "5390/10000 = 53.9%, SPS: 3.8, ELAP: 19:04, ETA: 20:01 - loss: 24.4275\n",
            "5391/10000 = 53.9%, SPS: 3.8, ELAP: 19:04, ETA: 20:01 - loss: 24.5774\n",
            "5392/10000 = 53.9%, SPS: 3.8, ELAP: 19:04, ETA: 20:00 - loss: 24.3481\n",
            "5393/10000 = 53.9%, SPS: 3.8, ELAP: 19:04, ETA: 20:00 - loss: 23.8817\n",
            "5394/10000 = 53.9%, SPS: 3.8, ELAP: 19:04, ETA: 20:00 - loss: 24.1184\n",
            "5395/10000 = 54.0%, SPS: 3.8, ELAP: 19:05, ETA: 19:59 - loss: 24.1503\n",
            "5396/10000 = 54.0%, SPS: 3.8, ELAP: 19:05, ETA: 19:59 - loss: 24.3423\n",
            "5397/10000 = 54.0%, SPS: 3.8, ELAP: 19:05, ETA: 19:59 - loss: 24.5059\n",
            "5398/10000 = 54.0%, SPS: 3.8, ELAP: 19:05, ETA: 19:58 - loss: 24.4896\n",
            "5399/10000 = 54.0%, SPS: 3.8, ELAP: 19:06, ETA: 19:58 - loss: 24.2407\n",
            "5400/10000 = 54.0%, SPS: 3.8, ELAP: 19:08, ETA: 20:00 - loss: 24.8312\n",
            "5401/10000 = 54.0%, SPS: 3.8, ELAP: 19:08, ETA: 20:00 - loss: 23.9080\n",
            "5402/10000 = 54.0%, SPS: 3.8, ELAP: 19:08, ETA: 20:00 - loss: 24.0784\n",
            "5403/10000 = 54.0%, SPS: 3.8, ELAP: 19:09, ETA: 19:59 - loss: 23.8525\n",
            "5404/10000 = 54.0%, SPS: 3.8, ELAP: 19:09, ETA: 19:59 - loss: 24.7551\n",
            "5405/10000 = 54.0%, SPS: 3.8, ELAP: 19:09, ETA: 19:59 - loss: 24.6518\n",
            "5406/10000 = 54.1%, SPS: 3.8, ELAP: 19:09, ETA: 19:58 - loss: 24.9572\n",
            "5407/10000 = 54.1%, SPS: 3.8, ELAP: 19:10, ETA: 19:58 - loss: 24.8242\n",
            "5408/10000 = 54.1%, SPS: 3.8, ELAP: 19:10, ETA: 19:58 - loss: 24.8456\n",
            "5409/10000 = 54.1%, SPS: 3.8, ELAP: 19:10, ETA: 19:58 - loss: 24.1711\n",
            "5410/10000 = 54.1%, SPS: 3.8, ELAP: 19:10, ETA: 19:57 - loss: 24.6277\n",
            "5411/10000 = 54.1%, SPS: 3.8, ELAP: 19:11, ETA: 19:57 - loss: 24.8594\n",
            "5412/10000 = 54.1%, SPS: 3.8, ELAP: 19:11, ETA: 19:57 - loss: 24.8440\n",
            "5413/10000 = 54.1%, SPS: 3.8, ELAP: 19:11, ETA: 19:56 - loss: 24.4406\n",
            "5414/10000 = 54.1%, SPS: 3.8, ELAP: 19:11, ETA: 19:56 - loss: 23.9825\n",
            "5415/10000 = 54.1%, SPS: 3.8, ELAP: 19:11, ETA: 19:56 - loss: 24.4551\n",
            "5416/10000 = 54.2%, SPS: 3.8, ELAP: 19:12, ETA: 19:55 - loss: 24.9852\n",
            "5417/10000 = 54.2%, SPS: 3.8, ELAP: 19:12, ETA: 19:55 - loss: 24.4462\n",
            "5418/10000 = 54.2%, SPS: 3.8, ELAP: 19:12, ETA: 19:55 - loss: 24.2536\n",
            "5419/10000 = 54.2%, SPS: 3.8, ELAP: 19:12, ETA: 19:55 - loss: 23.8838\n",
            "5420/10000 = 54.2%, SPS: 3.8, ELAP: 19:13, ETA: 19:54 - loss: 23.9498\n",
            "5421/10000 = 54.2%, SPS: 3.8, ELAP: 19:13, ETA: 19:54 - loss: 24.8882\n",
            "5422/10000 = 54.2%, SPS: 3.8, ELAP: 19:13, ETA: 19:54 - loss: 23.8760\n",
            "5423/10000 = 54.2%, SPS: 3.8, ELAP: 19:13, ETA: 19:53 - loss: 24.2020\n",
            "5424/10000 = 54.2%, SPS: 3.8, ELAP: 19:13, ETA: 19:53 - loss: 25.2475\n",
            "5425/10000 = 54.2%, SPS: 3.8, ELAP: 19:14, ETA: 19:53 - loss: 24.3333\n",
            "5426/10000 = 54.3%, SPS: 3.8, ELAP: 19:14, ETA: 19:53 - loss: 24.5112\n",
            "5427/10000 = 54.3%, SPS: 3.8, ELAP: 19:14, ETA: 19:52 - loss: 24.9294\n",
            "5428/10000 = 54.3%, SPS: 3.8, ELAP: 19:14, ETA: 19:52 - loss: 24.1862\n",
            "5429/10000 = 54.3%, SPS: 3.8, ELAP: 19:15, ETA: 19:52 - loss: 24.5068\n",
            "5430/10000 = 54.3%, SPS: 3.8, ELAP: 19:15, ETA: 19:52 - loss: 24.6751\n",
            "5431/10000 = 54.3%, SPS: 3.8, ELAP: 19:15, ETA: 19:51 - loss: 24.4911\n",
            "5432/10000 = 54.3%, SPS: 3.8, ELAP: 19:15, ETA: 19:51 - loss: 24.7056\n",
            "5433/10000 = 54.3%, SPS: 3.8, ELAP: 19:16, ETA: 19:51 - loss: 24.7702\n",
            "5434/10000 = 54.3%, SPS: 3.8, ELAP: 19:16, ETA: 19:50 - loss: 24.2876\n",
            "5435/10000 = 54.4%, SPS: 3.8, ELAP: 19:16, ETA: 19:50 - loss: 24.3822\n",
            "5436/10000 = 54.4%, SPS: 3.8, ELAP: 19:16, ETA: 19:50 - loss: 23.9093\n",
            "5437/10000 = 54.4%, SPS: 3.8, ELAP: 19:17, ETA: 19:50 - loss: 24.2092\n",
            "5438/10000 = 54.4%, SPS: 3.8, ELAP: 19:17, ETA: 19:49 - loss: 24.6004\n",
            "5439/10000 = 54.4%, SPS: 3.8, ELAP: 19:17, ETA: 19:49 - loss: 24.4171\n",
            "5440/10000 = 54.4%, SPS: 3.8, ELAP: 19:17, ETA: 19:49 - loss: 24.0176\n",
            "5441/10000 = 54.4%, SPS: 3.8, ELAP: 19:18, ETA: 19:48 - loss: 24.1347\n",
            "5442/10000 = 54.4%, SPS: 3.8, ELAP: 19:18, ETA: 19:48 - loss: 25.1671\n",
            "5443/10000 = 54.4%, SPS: 3.8, ELAP: 19:18, ETA: 19:48 - loss: 24.6865\n",
            "5444/10000 = 54.4%, SPS: 3.8, ELAP: 19:18, ETA: 19:47 - loss: 24.0673\n",
            "5445/10000 = 54.5%, SPS: 3.8, ELAP: 19:19, ETA: 19:47 - loss: 24.3663\n",
            "5446/10000 = 54.5%, SPS: 3.8, ELAP: 19:19, ETA: 19:47 - loss: 24.4745\n",
            "5447/10000 = 54.5%, SPS: 3.8, ELAP: 19:19, ETA: 19:47 - loss: 25.0316\n",
            "5448/10000 = 54.5%, SPS: 3.8, ELAP: 19:19, ETA: 19:46 - loss: 24.5928\n",
            "5449/10000 = 54.5%, SPS: 3.8, ELAP: 19:19, ETA: 19:46 - loss: 24.7376\n",
            "5450/10000 = 54.5%, SPS: 3.8, ELAP: 19:20, ETA: 19:46 - loss: 24.1670\n",
            "5451/10000 = 54.5%, SPS: 3.8, ELAP: 19:20, ETA: 19:45 - loss: 24.0951\n",
            "5452/10000 = 54.5%, SPS: 3.8, ELAP: 19:20, ETA: 19:45 - loss: 24.7300\n",
            "5453/10000 = 54.5%, SPS: 3.8, ELAP: 19:20, ETA: 19:45 - loss: 24.8724\n",
            "5454/10000 = 54.5%, SPS: 3.8, ELAP: 19:21, ETA: 19:45 - loss: 24.7990\n",
            "5455/10000 = 54.5%, SPS: 3.8, ELAP: 19:21, ETA: 19:44 - loss: 24.8698\n",
            "5456/10000 = 54.6%, SPS: 3.8, ELAP: 19:21, ETA: 19:44 - loss: 23.7366\n",
            "5457/10000 = 54.6%, SPS: 3.8, ELAP: 19:21, ETA: 19:44 - loss: 24.1944\n",
            "5458/10000 = 54.6%, SPS: 3.8, ELAP: 19:22, ETA: 19:43 - loss: 24.4694\n",
            "5459/10000 = 54.6%, SPS: 3.8, ELAP: 19:22, ETA: 19:43 - loss: 23.6466\n",
            "5460/10000 = 54.6%, SPS: 3.8, ELAP: 19:22, ETA: 19:43 - loss: 24.4916\n",
            "5461/10000 = 54.6%, SPS: 3.8, ELAP: 19:22, ETA: 19:43 - loss: 24.2640\n",
            "5462/10000 = 54.6%, SPS: 3.8, ELAP: 19:22, ETA: 19:42 - loss: 24.2615\n",
            "5463/10000 = 54.6%, SPS: 3.8, ELAP: 19:23, ETA: 19:42 - loss: 24.2841\n",
            "5464/10000 = 54.6%, SPS: 3.8, ELAP: 19:23, ETA: 19:42 - loss: 24.5314\n",
            "5465/10000 = 54.6%, SPS: 3.8, ELAP: 19:23, ETA: 19:41 - loss: 24.9150\n",
            "5466/10000 = 54.7%, SPS: 3.8, ELAP: 19:23, ETA: 19:41 - loss: 24.4802\n",
            "5467/10000 = 54.7%, SPS: 3.8, ELAP: 19:24, ETA: 19:41 - loss: 24.8521\n",
            "5468/10000 = 54.7%, SPS: 3.8, ELAP: 19:24, ETA: 19:40 - loss: 24.9242\n",
            "5469/10000 = 54.7%, SPS: 3.8, ELAP: 19:24, ETA: 19:40 - loss: 25.2761\n",
            "5470/10000 = 54.7%, SPS: 3.8, ELAP: 19:24, ETA: 19:40 - loss: 24.0229\n",
            "5471/10000 = 54.7%, SPS: 3.8, ELAP: 19:24, ETA: 19:40 - loss: 24.5928\n",
            "5472/10000 = 54.7%, SPS: 3.8, ELAP: 19:25, ETA: 19:39 - loss: 24.1207\n",
            "5473/10000 = 54.7%, SPS: 3.8, ELAP: 19:25, ETA: 19:39 - loss: 24.8419\n",
            "5474/10000 = 54.7%, SPS: 3.8, ELAP: 19:25, ETA: 19:39 - loss: 24.4155\n",
            "5475/10000 = 54.8%, SPS: 3.8, ELAP: 19:25, ETA: 19:38 - loss: 25.0216\n",
            "5476/10000 = 54.8%, SPS: 3.8, ELAP: 19:26, ETA: 19:38 - loss: 24.2599\n",
            "5477/10000 = 54.8%, SPS: 3.8, ELAP: 19:26, ETA: 19:38 - loss: 24.7236\n",
            "5478/10000 = 54.8%, SPS: 3.8, ELAP: 19:26, ETA: 19:38 - loss: 24.8452\n",
            "5479/10000 = 54.8%, SPS: 3.8, ELAP: 19:26, ETA: 19:37 - loss: 24.8305\n",
            "5480/10000 = 54.8%, SPS: 3.8, ELAP: 19:27, ETA: 19:37 - loss: 24.6951\n",
            "5481/10000 = 54.8%, SPS: 3.8, ELAP: 19:27, ETA: 19:37 - loss: 24.7471\n",
            "5482/10000 = 54.8%, SPS: 3.8, ELAP: 19:27, ETA: 19:36 - loss: 24.4322\n",
            "5483/10000 = 54.8%, SPS: 3.8, ELAP: 19:27, ETA: 19:36 - loss: 24.4985\n",
            "5484/10000 = 54.8%, SPS: 3.8, ELAP: 19:27, ETA: 19:36 - loss: 24.1672\n",
            "5485/10000 = 54.9%, SPS: 3.8, ELAP: 19:28, ETA: 19:36 - loss: 23.9368\n",
            "5486/10000 = 54.9%, SPS: 3.8, ELAP: 19:28, ETA: 19:35 - loss: 24.9016\n",
            "5487/10000 = 54.9%, SPS: 3.8, ELAP: 19:28, ETA: 19:35 - loss: 23.9267\n",
            "5488/10000 = 54.9%, SPS: 3.8, ELAP: 19:28, ETA: 19:35 - loss: 24.7282\n",
            "5489/10000 = 54.9%, SPS: 3.8, ELAP: 19:29, ETA: 19:34 - loss: 24.2320\n",
            "5490/10000 = 54.9%, SPS: 3.8, ELAP: 19:29, ETA: 19:34 - loss: 24.7108\n",
            "5491/10000 = 54.9%, SPS: 3.8, ELAP: 19:29, ETA: 19:34 - loss: 24.4343\n",
            "5492/10000 = 54.9%, SPS: 3.8, ELAP: 19:29, ETA: 19:33 - loss: 24.3611\n",
            "5493/10000 = 54.9%, SPS: 3.8, ELAP: 19:30, ETA: 19:33 - loss: 23.9362\n",
            "5494/10000 = 54.9%, SPS: 3.8, ELAP: 19:30, ETA: 19:33 - loss: 24.1420\n",
            "5495/10000 = 55.0%, SPS: 3.8, ELAP: 19:30, ETA: 19:33 - loss: 24.4917\n",
            "5496/10000 = 55.0%, SPS: 3.8, ELAP: 19:30, ETA: 19:32 - loss: 24.9698\n",
            "5497/10000 = 55.0%, SPS: 3.8, ELAP: 19:30, ETA: 19:32 - loss: 24.1038\n",
            "5498/10000 = 55.0%, SPS: 3.8, ELAP: 19:31, ETA: 19:32 - loss: 24.6539\n",
            "5499/10000 = 55.0%, SPS: 3.8, ELAP: 19:31, ETA: 19:31 - loss: 24.4949\n",
            "5500/10000 = 55.0%, SPS: 3.8, ELAP: 19:33, ETA: 19:33 - loss: 24.9580\n",
            "5501/10000 = 55.0%, SPS: 3.8, ELAP: 19:34, ETA: 19:33 - loss: 23.6180\n",
            "5502/10000 = 55.0%, SPS: 3.8, ELAP: 19:34, ETA: 19:33 - loss: 25.4897\n",
            "5503/10000 = 55.0%, SPS: 3.8, ELAP: 19:34, ETA: 19:32 - loss: 24.8406\n",
            "5504/10000 = 55.0%, SPS: 3.8, ELAP: 19:34, ETA: 19:32 - loss: 24.7609\n",
            "5505/10000 = 55.0%, SPS: 3.8, ELAP: 19:34, ETA: 19:32 - loss: 24.7695\n",
            "5506/10000 = 55.1%, SPS: 3.8, ELAP: 19:35, ETA: 19:32 - loss: 23.5177\n",
            "5507/10000 = 55.1%, SPS: 3.8, ELAP: 19:35, ETA: 19:31 - loss: 24.2544\n",
            "5508/10000 = 55.1%, SPS: 3.8, ELAP: 19:35, ETA: 19:31 - loss: 24.4121\n",
            "5509/10000 = 55.1%, SPS: 3.8, ELAP: 19:35, ETA: 19:31 - loss: 24.3723\n",
            "5510/10000 = 55.1%, SPS: 3.8, ELAP: 19:36, ETA: 19:30 - loss: 24.2294\n",
            "5511/10000 = 55.1%, SPS: 3.8, ELAP: 19:36, ETA: 19:30 - loss: 24.3078\n",
            "5512/10000 = 55.1%, SPS: 3.8, ELAP: 19:36, ETA: 19:30 - loss: 24.9302\n",
            "5513/10000 = 55.1%, SPS: 3.8, ELAP: 19:36, ETA: 19:30 - loss: 24.7947\n",
            "5514/10000 = 55.1%, SPS: 3.8, ELAP: 19:37, ETA: 19:29 - loss: 24.2090\n",
            "5515/10000 = 55.1%, SPS: 3.8, ELAP: 19:37, ETA: 19:29 - loss: 23.3078\n",
            "5516/10000 = 55.2%, SPS: 3.8, ELAP: 19:37, ETA: 19:29 - loss: 24.9042\n",
            "5517/10000 = 55.2%, SPS: 3.8, ELAP: 19:37, ETA: 19:28 - loss: 24.6489\n",
            "5518/10000 = 55.2%, SPS: 3.8, ELAP: 19:37, ETA: 19:28 - loss: 24.6840\n",
            "5519/10000 = 55.2%, SPS: 3.8, ELAP: 19:38, ETA: 19:28 - loss: 24.8830\n",
            "5520/10000 = 55.2%, SPS: 3.8, ELAP: 19:38, ETA: 19:27 - loss: 24.5750\n",
            "5521/10000 = 55.2%, SPS: 3.8, ELAP: 19:38, ETA: 19:27 - loss: 24.5437\n",
            "5522/10000 = 55.2%, SPS: 3.8, ELAP: 19:38, ETA: 19:27 - loss: 24.6495\n",
            "5523/10000 = 55.2%, SPS: 3.8, ELAP: 19:39, ETA: 19:27 - loss: 24.4326\n",
            "5524/10000 = 55.2%, SPS: 3.8, ELAP: 19:39, ETA: 19:26 - loss: 24.9109\n",
            "5525/10000 = 55.2%, SPS: 3.8, ELAP: 19:39, ETA: 19:26 - loss: 24.2924\n",
            "5526/10000 = 55.3%, SPS: 3.8, ELAP: 19:39, ETA: 19:26 - loss: 24.4071\n",
            "5527/10000 = 55.3%, SPS: 3.8, ELAP: 19:40, ETA: 19:26 - loss: 24.1188\n",
            "5528/10000 = 55.3%, SPS: 3.8, ELAP: 19:40, ETA: 19:25 - loss: 24.8361\n",
            "5529/10000 = 55.3%, SPS: 3.8, ELAP: 19:40, ETA: 19:25 - loss: 24.0171\n",
            "5530/10000 = 55.3%, SPS: 3.8, ELAP: 19:40, ETA: 19:25 - loss: 25.0383\n",
            "5531/10000 = 55.3%, SPS: 3.8, ELAP: 19:41, ETA: 19:25 - loss: 24.6990\n",
            "5532/10000 = 55.3%, SPS: 3.8, ELAP: 19:41, ETA: 19:24 - loss: 24.6656\n",
            "5533/10000 = 55.3%, SPS: 3.8, ELAP: 19:41, ETA: 19:24 - loss: 24.5364\n",
            "5534/10000 = 55.3%, SPS: 3.8, ELAP: 19:41, ETA: 19:24 - loss: 24.3507\n",
            "5535/10000 = 55.4%, SPS: 3.8, ELAP: 19:42, ETA: 19:23 - loss: 24.4544\n",
            "5536/10000 = 55.4%, SPS: 3.8, ELAP: 19:42, ETA: 19:23 - loss: 24.6626\n",
            "5537/10000 = 55.4%, SPS: 3.8, ELAP: 19:42, ETA: 19:23 - loss: 24.4416\n",
            "5538/10000 = 55.4%, SPS: 3.8, ELAP: 19:42, ETA: 19:23 - loss: 24.3907\n",
            "5539/10000 = 55.4%, SPS: 3.8, ELAP: 19:43, ETA: 19:22 - loss: 24.4836\n",
            "5540/10000 = 55.4%, SPS: 3.8, ELAP: 19:43, ETA: 19:22 - loss: 24.9284\n",
            "5541/10000 = 55.4%, SPS: 3.8, ELAP: 19:43, ETA: 19:22 - loss: 24.6890\n",
            "5542/10000 = 55.4%, SPS: 3.8, ELAP: 19:43, ETA: 19:21 - loss: 24.6601\n",
            "5543/10000 = 55.4%, SPS: 3.8, ELAP: 19:43, ETA: 19:21 - loss: 24.3251\n",
            "5544/10000 = 55.4%, SPS: 3.8, ELAP: 19:44, ETA: 19:21 - loss: 24.9568\n",
            "5545/10000 = 55.5%, SPS: 3.8, ELAP: 19:44, ETA: 19:21 - loss: 24.2931\n",
            "5546/10000 = 55.5%, SPS: 3.8, ELAP: 19:44, ETA: 19:20 - loss: 24.4583\n",
            "5547/10000 = 55.5%, SPS: 3.8, ELAP: 19:44, ETA: 19:20 - loss: 25.0973\n",
            "5548/10000 = 55.5%, SPS: 3.8, ELAP: 19:45, ETA: 19:20 - loss: 24.1984\n",
            "5549/10000 = 55.5%, SPS: 3.8, ELAP: 19:45, ETA: 19:19 - loss: 24.5038\n",
            "5550/10000 = 55.5%, SPS: 3.8, ELAP: 19:45, ETA: 19:19 - loss: 24.4888\n",
            "5551/10000 = 55.5%, SPS: 3.8, ELAP: 19:45, ETA: 19:19 - loss: 24.4050\n",
            "5552/10000 = 55.5%, SPS: 3.8, ELAP: 19:46, ETA: 19:18 - loss: 24.5448\n",
            "5553/10000 = 55.5%, SPS: 3.8, ELAP: 19:46, ETA: 19:18 - loss: 24.3666\n",
            "5554/10000 = 55.5%, SPS: 3.8, ELAP: 19:46, ETA: 19:18 - loss: 24.1098\n",
            "5555/10000 = 55.5%, SPS: 3.8, ELAP: 19:46, ETA: 19:18 - loss: 24.4578\n",
            "5556/10000 = 55.6%, SPS: 3.8, ELAP: 19:46, ETA: 19:17 - loss: 23.9735\n",
            "5557/10000 = 55.6%, SPS: 3.8, ELAP: 19:47, ETA: 19:17 - loss: 24.9911\n",
            "5558/10000 = 55.6%, SPS: 3.8, ELAP: 19:47, ETA: 19:17 - loss: 24.2539\n",
            "5559/10000 = 55.6%, SPS: 3.8, ELAP: 19:47, ETA: 19:16 - loss: 24.1039\n",
            "5560/10000 = 55.6%, SPS: 3.8, ELAP: 19:47, ETA: 19:16 - loss: 24.4067\n",
            "5561/10000 = 55.6%, SPS: 3.8, ELAP: 19:48, ETA: 19:16 - loss: 23.4562\n",
            "5562/10000 = 55.6%, SPS: 3.8, ELAP: 19:48, ETA: 19:16 - loss: 24.8963\n",
            "5563/10000 = 55.6%, SPS: 3.8, ELAP: 19:48, ETA: 19:15 - loss: 24.0766\n",
            "5564/10000 = 55.6%, SPS: 3.8, ELAP: 19:48, ETA: 19:15 - loss: 24.7067\n",
            "5565/10000 = 55.6%, SPS: 3.8, ELAP: 19:49, ETA: 19:15 - loss: 24.7514\n",
            "5566/10000 = 55.7%, SPS: 3.8, ELAP: 19:49, ETA: 19:14 - loss: 24.3052\n",
            "5567/10000 = 55.7%, SPS: 3.8, ELAP: 19:49, ETA: 19:14 - loss: 24.0101\n",
            "5568/10000 = 55.7%, SPS: 3.8, ELAP: 19:49, ETA: 19:14 - loss: 24.6177\n",
            "5569/10000 = 55.7%, SPS: 3.8, ELAP: 19:49, ETA: 19:14 - loss: 24.4644\n",
            "5570/10000 = 55.7%, SPS: 3.8, ELAP: 19:50, ETA: 19:13 - loss: 24.7943\n",
            "5571/10000 = 55.7%, SPS: 3.8, ELAP: 19:50, ETA: 19:13 - loss: 24.4785\n",
            "5572/10000 = 55.7%, SPS: 3.8, ELAP: 19:50, ETA: 19:13 - loss: 25.0288\n",
            "5573/10000 = 55.7%, SPS: 3.8, ELAP: 19:50, ETA: 19:12 - loss: 24.7269\n",
            "5574/10000 = 55.7%, SPS: 3.8, ELAP: 19:51, ETA: 19:12 - loss: 24.6847\n",
            "5575/10000 = 55.8%, SPS: 3.8, ELAP: 19:51, ETA: 19:12 - loss: 24.8554\n",
            "5576/10000 = 55.8%, SPS: 3.8, ELAP: 19:51, ETA: 19:12 - loss: 24.4537\n",
            "5577/10000 = 55.8%, SPS: 3.8, ELAP: 19:51, ETA: 19:11 - loss: 23.7683\n",
            "5578/10000 = 55.8%, SPS: 3.8, ELAP: 19:52, ETA: 19:11 - loss: 24.5709\n",
            "5579/10000 = 55.8%, SPS: 3.8, ELAP: 19:52, ETA: 19:11 - loss: 24.4397\n",
            "5580/10000 = 55.8%, SPS: 3.8, ELAP: 19:52, ETA: 19:10 - loss: 24.3935\n",
            "5581/10000 = 55.8%, SPS: 3.8, ELAP: 19:52, ETA: 19:10 - loss: 24.4256\n",
            "5582/10000 = 55.8%, SPS: 3.8, ELAP: 19:52, ETA: 19:10 - loss: 24.3734\n",
            "5583/10000 = 55.8%, SPS: 3.8, ELAP: 19:53, ETA: 19:09 - loss: 24.1578\n",
            "5584/10000 = 55.8%, SPS: 3.8, ELAP: 19:53, ETA: 19:09 - loss: 24.7181\n",
            "5585/10000 = 55.9%, SPS: 3.8, ELAP: 19:53, ETA: 19:09 - loss: 24.0357\n",
            "5586/10000 = 55.9%, SPS: 3.8, ELAP: 19:53, ETA: 19:09 - loss: 24.2988\n",
            "5587/10000 = 55.9%, SPS: 3.8, ELAP: 19:54, ETA: 19:08 - loss: 25.1522\n",
            "5588/10000 = 55.9%, SPS: 3.8, ELAP: 19:54, ETA: 19:08 - loss: 24.0566\n",
            "5589/10000 = 55.9%, SPS: 3.8, ELAP: 19:54, ETA: 19:08 - loss: 24.6773\n",
            "5590/10000 = 55.9%, SPS: 3.8, ELAP: 19:54, ETA: 19:07 - loss: 24.9368\n",
            "5591/10000 = 55.9%, SPS: 3.8, ELAP: 19:55, ETA: 19:07 - loss: 24.3062\n",
            "5592/10000 = 55.9%, SPS: 3.8, ELAP: 19:55, ETA: 19:07 - loss: 24.8045\n",
            "5593/10000 = 55.9%, SPS: 3.8, ELAP: 19:55, ETA: 19:07 - loss: 24.6157\n",
            "5594/10000 = 55.9%, SPS: 3.8, ELAP: 19:55, ETA: 19:06 - loss: 24.5880\n",
            "5595/10000 = 56.0%, SPS: 3.8, ELAP: 19:55, ETA: 19:06 - loss: 24.0487\n",
            "5596/10000 = 56.0%, SPS: 3.8, ELAP: 19:56, ETA: 19:06 - loss: 24.5248\n",
            "5597/10000 = 56.0%, SPS: 3.8, ELAP: 19:56, ETA: 19:05 - loss: 24.2622\n",
            "5598/10000 = 56.0%, SPS: 3.8, ELAP: 19:56, ETA: 19:05 - loss: 24.6554\n",
            "5599/10000 = 56.0%, SPS: 3.8, ELAP: 19:56, ETA: 19:05 - loss: 24.6018\n",
            "5600/10000 = 56.0%, SPS: 3.8, ELAP: 19:59, ETA: 19:07 - loss: 24.5261\n",
            "5601/10000 = 56.0%, SPS: 3.8, ELAP: 19:59, ETA: 19:06 - loss: 23.8319\n",
            "5602/10000 = 56.0%, SPS: 3.8, ELAP: 19:59, ETA: 19:06 - loss: 24.1403\n",
            "5603/10000 = 56.0%, SPS: 3.8, ELAP: 19:59, ETA: 19:06 - loss: 24.0963\n",
            "5604/10000 = 56.0%, SPS: 3.8, ELAP: 20:00, ETA: 19:06 - loss: 24.7857\n",
            "5605/10000 = 56.0%, SPS: 3.8, ELAP: 20:00, ETA: 19:05 - loss: 24.1805\n",
            "5606/10000 = 56.1%, SPS: 3.8, ELAP: 20:00, ETA: 19:05 - loss: 24.4561\n",
            "5607/10000 = 56.1%, SPS: 3.8, ELAP: 20:00, ETA: 19:05 - loss: 24.7315\n",
            "5608/10000 = 56.1%, SPS: 3.8, ELAP: 20:01, ETA: 19:04 - loss: 24.7008\n",
            "5609/10000 = 56.1%, SPS: 3.8, ELAP: 20:01, ETA: 19:04 - loss: 24.1362\n",
            "5610/10000 = 56.1%, SPS: 3.8, ELAP: 20:01, ETA: 19:04 - loss: 24.0208\n",
            "5611/10000 = 56.1%, SPS: 3.8, ELAP: 20:01, ETA: 19:03 - loss: 24.2343\n",
            "5612/10000 = 56.1%, SPS: 3.8, ELAP: 20:02, ETA: 19:03 - loss: 24.1519\n",
            "5613/10000 = 56.1%, SPS: 3.8, ELAP: 20:02, ETA: 19:03 - loss: 24.3230\n",
            "5614/10000 = 56.1%, SPS: 3.8, ELAP: 20:02, ETA: 19:03 - loss: 24.8780\n",
            "5615/10000 = 56.1%, SPS: 3.8, ELAP: 20:02, ETA: 19:02 - loss: 25.0172\n",
            "5616/10000 = 56.2%, SPS: 3.8, ELAP: 20:02, ETA: 19:02 - loss: 24.2283\n",
            "5617/10000 = 56.2%, SPS: 3.8, ELAP: 20:03, ETA: 19:02 - loss: 24.5200\n",
            "5618/10000 = 56.2%, SPS: 3.8, ELAP: 20:03, ETA: 19:01 - loss: 24.7741\n",
            "5619/10000 = 56.2%, SPS: 3.8, ELAP: 20:03, ETA: 19:01 - loss: 24.7390\n",
            "5620/10000 = 56.2%, SPS: 3.8, ELAP: 20:03, ETA: 19:01 - loss: 24.2019\n",
            "5621/10000 = 56.2%, SPS: 3.8, ELAP: 20:04, ETA: 19:01 - loss: 23.9480\n",
            "5622/10000 = 56.2%, SPS: 3.8, ELAP: 20:04, ETA: 19:00 - loss: 23.8862\n",
            "5623/10000 = 56.2%, SPS: 3.8, ELAP: 20:04, ETA: 19:00 - loss: 23.5964\n",
            "5624/10000 = 56.2%, SPS: 3.8, ELAP: 20:04, ETA: 19:00 - loss: 24.1467\n",
            "5625/10000 = 56.2%, SPS: 3.8, ELAP: 20:05, ETA: 19:00 - loss: 24.4520\n",
            "5626/10000 = 56.3%, SPS: 3.8, ELAP: 20:05, ETA: 18:59 - loss: 24.7857\n",
            "5627/10000 = 56.3%, SPS: 3.8, ELAP: 20:05, ETA: 18:59 - loss: 24.2496\n",
            "5628/10000 = 56.3%, SPS: 3.8, ELAP: 20:05, ETA: 18:59 - loss: 24.6289\n",
            "5629/10000 = 56.3%, SPS: 3.8, ELAP: 20:06, ETA: 18:59 - loss: 24.1945\n",
            "5630/10000 = 56.3%, SPS: 3.8, ELAP: 20:06, ETA: 18:58 - loss: 24.0029\n",
            "5631/10000 = 56.3%, SPS: 3.8, ELAP: 20:06, ETA: 18:58 - loss: 24.3499\n",
            "5632/10000 = 56.3%, SPS: 3.8, ELAP: 20:06, ETA: 18:58 - loss: 24.6276\n",
            "5633/10000 = 56.3%, SPS: 3.8, ELAP: 20:07, ETA: 18:57 - loss: 25.1649\n",
            "5634/10000 = 56.3%, SPS: 3.8, ELAP: 20:07, ETA: 18:57 - loss: 24.1932\n",
            "5635/10000 = 56.4%, SPS: 3.8, ELAP: 20:07, ETA: 18:57 - loss: 24.3581\n",
            "5636/10000 = 56.4%, SPS: 3.8, ELAP: 20:07, ETA: 18:57 - loss: 24.0881\n",
            "5637/10000 = 56.4%, SPS: 3.8, ELAP: 20:08, ETA: 18:56 - loss: 24.6950\n",
            "5638/10000 = 56.4%, SPS: 3.8, ELAP: 20:08, ETA: 18:56 - loss: 23.7187\n",
            "5639/10000 = 56.4%, SPS: 3.8, ELAP: 20:08, ETA: 18:56 - loss: 24.4002\n",
            "5640/10000 = 56.4%, SPS: 3.8, ELAP: 20:08, ETA: 18:55 - loss: 24.0380\n",
            "5641/10000 = 56.4%, SPS: 3.8, ELAP: 20:09, ETA: 18:55 - loss: 24.4429\n",
            "5642/10000 = 56.4%, SPS: 3.8, ELAP: 20:09, ETA: 18:55 - loss: 25.0466\n",
            "5643/10000 = 56.4%, SPS: 3.8, ELAP: 20:09, ETA: 18:55 - loss: 24.5166\n",
            "5644/10000 = 56.4%, SPS: 3.8, ELAP: 20:09, ETA: 18:54 - loss: 24.3858\n",
            "5645/10000 = 56.5%, SPS: 3.8, ELAP: 20:09, ETA: 18:54 - loss: 24.7550\n",
            "5646/10000 = 56.5%, SPS: 3.8, ELAP: 20:10, ETA: 18:54 - loss: 24.4324\n",
            "5647/10000 = 56.5%, SPS: 3.8, ELAP: 20:10, ETA: 18:53 - loss: 24.6658\n",
            "5648/10000 = 56.5%, SPS: 3.8, ELAP: 20:10, ETA: 18:53 - loss: 23.8667\n",
            "5649/10000 = 56.5%, SPS: 3.8, ELAP: 20:10, ETA: 18:53 - loss: 24.3374\n",
            "5650/10000 = 56.5%, SPS: 3.8, ELAP: 20:11, ETA: 18:52 - loss: 24.4094\n",
            "5651/10000 = 56.5%, SPS: 3.8, ELAP: 20:11, ETA: 18:52 - loss: 24.5202\n",
            "5652/10000 = 56.5%, SPS: 3.8, ELAP: 20:11, ETA: 18:52 - loss: 24.4748\n",
            "5653/10000 = 56.5%, SPS: 3.8, ELAP: 20:11, ETA: 18:52 - loss: 24.5518\n",
            "5654/10000 = 56.5%, SPS: 3.8, ELAP: 20:12, ETA: 18:51 - loss: 24.6785\n",
            "5655/10000 = 56.5%, SPS: 3.8, ELAP: 20:12, ETA: 18:51 - loss: 24.6894\n",
            "5656/10000 = 56.6%, SPS: 3.8, ELAP: 20:12, ETA: 18:51 - loss: 24.0547\n",
            "5657/10000 = 56.6%, SPS: 3.8, ELAP: 20:12, ETA: 18:50 - loss: 24.5714\n",
            "5658/10000 = 56.6%, SPS: 3.8, ELAP: 20:12, ETA: 18:50 - loss: 24.3216\n",
            "5659/10000 = 56.6%, SPS: 3.8, ELAP: 20:13, ETA: 18:50 - loss: 24.5907\n",
            "5660/10000 = 56.6%, SPS: 3.8, ELAP: 20:13, ETA: 18:50 - loss: 24.2169\n",
            "5661/10000 = 56.6%, SPS: 3.8, ELAP: 20:13, ETA: 18:49 - loss: 24.1179\n",
            "5662/10000 = 56.6%, SPS: 3.8, ELAP: 20:13, ETA: 18:49 - loss: 25.2309\n",
            "5663/10000 = 56.6%, SPS: 3.8, ELAP: 20:14, ETA: 18:49 - loss: 24.3556\n",
            "5664/10000 = 56.6%, SPS: 3.8, ELAP: 20:14, ETA: 18:48 - loss: 25.0776\n",
            "5665/10000 = 56.6%, SPS: 3.8, ELAP: 20:14, ETA: 18:48 - loss: 24.9923\n",
            "5666/10000 = 56.7%, SPS: 3.8, ELAP: 20:14, ETA: 18:48 - loss: 24.1788\n",
            "5667/10000 = 56.7%, SPS: 3.8, ELAP: 20:15, ETA: 18:48 - loss: 24.0151\n",
            "5668/10000 = 56.7%, SPS: 3.8, ELAP: 20:15, ETA: 18:47 - loss: 24.3407\n",
            "5669/10000 = 56.7%, SPS: 3.8, ELAP: 20:15, ETA: 18:47 - loss: 24.5504\n",
            "5670/10000 = 56.7%, SPS: 3.8, ELAP: 20:15, ETA: 18:47 - loss: 24.2937\n",
            "5671/10000 = 56.7%, SPS: 3.8, ELAP: 20:15, ETA: 18:46 - loss: 24.5410\n",
            "5672/10000 = 56.7%, SPS: 3.8, ELAP: 20:16, ETA: 18:46 - loss: 23.8826\n",
            "5673/10000 = 56.7%, SPS: 3.8, ELAP: 20:16, ETA: 18:46 - loss: 24.9316\n",
            "5674/10000 = 56.7%, SPS: 3.8, ELAP: 20:16, ETA: 18:46 - loss: 24.5339\n",
            "5675/10000 = 56.8%, SPS: 3.8, ELAP: 20:16, ETA: 18:45 - loss: 24.5442\n",
            "5676/10000 = 56.8%, SPS: 3.8, ELAP: 20:17, ETA: 18:45 - loss: 24.3876\n",
            "5677/10000 = 56.8%, SPS: 3.8, ELAP: 20:17, ETA: 18:45 - loss: 24.9448\n",
            "5678/10000 = 56.8%, SPS: 3.8, ELAP: 20:17, ETA: 18:44 - loss: 24.1024\n",
            "5679/10000 = 56.8%, SPS: 3.8, ELAP: 20:17, ETA: 18:44 - loss: 24.1699\n",
            "5680/10000 = 56.8%, SPS: 3.8, ELAP: 20:18, ETA: 18:44 - loss: 23.3962\n",
            "5681/10000 = 56.8%, SPS: 3.8, ELAP: 20:18, ETA: 18:44 - loss: 24.9667\n",
            "5682/10000 = 56.8%, SPS: 3.8, ELAP: 20:18, ETA: 18:43 - loss: 24.3118\n",
            "5683/10000 = 56.8%, SPS: 3.8, ELAP: 20:18, ETA: 18:43 - loss: 23.7027\n",
            "5684/10000 = 56.8%, SPS: 3.8, ELAP: 20:18, ETA: 18:43 - loss: 24.5372\n",
            "5685/10000 = 56.9%, SPS: 3.8, ELAP: 20:19, ETA: 18:42 - loss: 24.2883\n",
            "5686/10000 = 56.9%, SPS: 3.8, ELAP: 20:19, ETA: 18:42 - loss: 24.2791\n",
            "5687/10000 = 56.9%, SPS: 3.8, ELAP: 20:19, ETA: 18:42 - loss: 24.0713\n",
            "5688/10000 = 56.9%, SPS: 3.8, ELAP: 20:19, ETA: 18:42 - loss: 23.8632\n",
            "5689/10000 = 56.9%, SPS: 3.8, ELAP: 20:20, ETA: 18:41 - loss: 25.0771\n",
            "5690/10000 = 56.9%, SPS: 3.8, ELAP: 20:20, ETA: 18:41 - loss: 24.5255\n",
            "5691/10000 = 56.9%, SPS: 3.8, ELAP: 20:20, ETA: 18:41 - loss: 24.0665\n",
            "5692/10000 = 56.9%, SPS: 3.8, ELAP: 20:20, ETA: 18:40 - loss: 24.0535\n",
            "5693/10000 = 56.9%, SPS: 3.8, ELAP: 20:20, ETA: 18:40 - loss: 24.0265\n",
            "5694/10000 = 56.9%, SPS: 3.8, ELAP: 20:21, ETA: 18:40 - loss: 24.7441\n",
            "5695/10000 = 57.0%, SPS: 3.8, ELAP: 20:21, ETA: 18:40 - loss: 24.6061\n",
            "5696/10000 = 57.0%, SPS: 3.8, ELAP: 20:21, ETA: 18:39 - loss: 24.2596\n",
            "5697/10000 = 57.0%, SPS: 3.8, ELAP: 20:21, ETA: 18:39 - loss: 24.5382\n",
            "5698/10000 = 57.0%, SPS: 3.8, ELAP: 20:22, ETA: 18:39 - loss: 24.5178\n",
            "5699/10000 = 57.0%, SPS: 3.8, ELAP: 20:22, ETA: 18:38 - loss: 24.0955\n",
            "5700/10000 = 57.0%, SPS: 3.8, ELAP: 20:24, ETA: 18:40 - loss: 24.0893\n",
            "5701/10000 = 57.0%, SPS: 3.8, ELAP: 20:25, ETA: 18:40 - loss: 24.3333\n",
            "5702/10000 = 57.0%, SPS: 3.8, ELAP: 20:25, ETA: 18:40 - loss: 24.6237\n",
            "5703/10000 = 57.0%, SPS: 3.8, ELAP: 20:25, ETA: 18:39 - loss: 24.3710\n",
            "5704/10000 = 57.0%, SPS: 3.8, ELAP: 20:25, ETA: 18:39 - loss: 24.6700\n",
            "5705/10000 = 57.0%, SPS: 3.8, ELAP: 20:25, ETA: 18:39 - loss: 24.3303\n",
            "5706/10000 = 57.1%, SPS: 3.8, ELAP: 20:26, ETA: 18:38 - loss: 24.5208\n",
            "5707/10000 = 57.1%, SPS: 3.8, ELAP: 20:26, ETA: 18:38 - loss: 24.0781\n",
            "5708/10000 = 57.1%, SPS: 3.8, ELAP: 20:26, ETA: 18:38 - loss: 24.8259\n",
            "5709/10000 = 57.1%, SPS: 3.8, ELAP: 20:26, ETA: 18:37 - loss: 24.7946\n",
            "5710/10000 = 57.1%, SPS: 3.8, ELAP: 20:27, ETA: 18:37 - loss: 24.3452\n",
            "5711/10000 = 57.1%, SPS: 3.8, ELAP: 20:27, ETA: 18:37 - loss: 24.2635\n",
            "5712/10000 = 57.1%, SPS: 3.8, ELAP: 20:27, ETA: 18:37 - loss: 24.7399\n",
            "5713/10000 = 57.1%, SPS: 3.8, ELAP: 20:27, ETA: 18:36 - loss: 23.9751\n",
            "5714/10000 = 57.1%, SPS: 3.8, ELAP: 20:27, ETA: 18:36 - loss: 24.2064\n",
            "5715/10000 = 57.1%, SPS: 3.8, ELAP: 20:28, ETA: 18:36 - loss: 24.9079\n",
            "5716/10000 = 57.2%, SPS: 3.8, ELAP: 20:28, ETA: 18:35 - loss: 23.9920\n",
            "5717/10000 = 57.2%, SPS: 3.8, ELAP: 20:28, ETA: 18:35 - loss: 24.1191\n",
            "5718/10000 = 57.2%, SPS: 3.8, ELAP: 20:28, ETA: 18:35 - loss: 24.6271\n",
            "5719/10000 = 57.2%, SPS: 3.8, ELAP: 20:29, ETA: 18:35 - loss: 24.1770\n",
            "5720/10000 = 57.2%, SPS: 3.8, ELAP: 20:29, ETA: 18:34 - loss: 24.8763\n",
            "5721/10000 = 57.2%, SPS: 3.8, ELAP: 20:29, ETA: 18:34 - loss: 24.1539\n",
            "5722/10000 = 57.2%, SPS: 3.8, ELAP: 20:29, ETA: 18:34 - loss: 24.0271\n",
            "5723/10000 = 57.2%, SPS: 3.8, ELAP: 20:30, ETA: 18:33 - loss: 24.6281\n",
            "5724/10000 = 57.2%, SPS: 3.8, ELAP: 20:30, ETA: 18:33 - loss: 24.2999\n",
            "5725/10000 = 57.2%, SPS: 3.8, ELAP: 20:30, ETA: 18:33 - loss: 24.4650\n",
            "5726/10000 = 57.3%, SPS: 3.8, ELAP: 20:30, ETA: 18:33 - loss: 24.1423\n",
            "5727/10000 = 57.3%, SPS: 3.8, ELAP: 20:31, ETA: 18:32 - loss: 24.8100\n",
            "5728/10000 = 57.3%, SPS: 3.8, ELAP: 20:31, ETA: 18:32 - loss: 24.2867\n",
            "5729/10000 = 57.3%, SPS: 3.8, ELAP: 20:31, ETA: 18:32 - loss: 24.7765\n",
            "5730/10000 = 57.3%, SPS: 3.8, ELAP: 20:31, ETA: 18:32 - loss: 24.7461\n",
            "5731/10000 = 57.3%, SPS: 3.8, ELAP: 20:32, ETA: 18:31 - loss: 24.3192\n",
            "5732/10000 = 57.3%, SPS: 3.8, ELAP: 20:32, ETA: 18:31 - loss: 24.7335\n",
            "5733/10000 = 57.3%, SPS: 3.8, ELAP: 20:32, ETA: 18:31 - loss: 24.8386\n",
            "5734/10000 = 57.3%, SPS: 3.8, ELAP: 20:32, ETA: 18:31 - loss: 24.4859\n",
            "5735/10000 = 57.4%, SPS: 3.8, ELAP: 20:33, ETA: 18:30 - loss: 24.6506\n",
            "5736/10000 = 57.4%, SPS: 3.8, ELAP: 20:33, ETA: 18:30 - loss: 24.3261\n",
            "5737/10000 = 57.4%, SPS: 3.8, ELAP: 20:33, ETA: 18:30 - loss: 24.1530\n",
            "5738/10000 = 57.4%, SPS: 3.8, ELAP: 20:33, ETA: 18:29 - loss: 24.6747\n",
            "5739/10000 = 57.4%, SPS: 3.8, ELAP: 20:34, ETA: 18:29 - loss: 24.2009\n",
            "5740/10000 = 57.4%, SPS: 3.8, ELAP: 20:34, ETA: 18:29 - loss: 24.7867\n",
            "5741/10000 = 57.4%, SPS: 3.8, ELAP: 20:34, ETA: 18:29 - loss: 24.4016\n",
            "5742/10000 = 57.4%, SPS: 3.8, ELAP: 20:34, ETA: 18:28 - loss: 24.2260\n",
            "5743/10000 = 57.4%, SPS: 3.8, ELAP: 20:34, ETA: 18:28 - loss: 23.6706\n",
            "5744/10000 = 57.4%, SPS: 3.8, ELAP: 20:35, ETA: 18:28 - loss: 24.6890\n",
            "5745/10000 = 57.5%, SPS: 3.8, ELAP: 20:35, ETA: 18:27 - loss: 24.1810\n",
            "5746/10000 = 57.5%, SPS: 3.8, ELAP: 20:35, ETA: 18:27 - loss: 24.0713\n",
            "5747/10000 = 57.5%, SPS: 3.8, ELAP: 20:35, ETA: 18:27 - loss: 24.1015\n",
            "5748/10000 = 57.5%, SPS: 3.8, ELAP: 20:36, ETA: 18:26 - loss: 24.3288\n",
            "5749/10000 = 57.5%, SPS: 3.8, ELAP: 20:36, ETA: 18:26 - loss: 24.5073\n",
            "5750/10000 = 57.5%, SPS: 3.8, ELAP: 20:36, ETA: 18:26 - loss: 24.8731\n",
            "5751/10000 = 57.5%, SPS: 3.8, ELAP: 20:36, ETA: 18:26 - loss: 24.4515\n",
            "5752/10000 = 57.5%, SPS: 3.8, ELAP: 20:36, ETA: 18:25 - loss: 24.8901\n",
            "5753/10000 = 57.5%, SPS: 3.8, ELAP: 20:37, ETA: 18:25 - loss: 24.8834\n",
            "5754/10000 = 57.5%, SPS: 3.8, ELAP: 20:37, ETA: 18:25 - loss: 23.6502\n",
            "5755/10000 = 57.5%, SPS: 3.8, ELAP: 20:37, ETA: 18:24 - loss: 24.6981\n",
            "5756/10000 = 57.6%, SPS: 3.8, ELAP: 20:37, ETA: 18:24 - loss: 24.5166\n",
            "5757/10000 = 57.6%, SPS: 3.8, ELAP: 20:38, ETA: 18:24 - loss: 24.4530\n",
            "5758/10000 = 57.6%, SPS: 3.8, ELAP: 20:38, ETA: 18:24 - loss: 24.1360\n",
            "5759/10000 = 57.6%, SPS: 3.8, ELAP: 20:38, ETA: 18:23 - loss: 24.0584\n",
            "5760/10000 = 57.6%, SPS: 3.8, ELAP: 20:38, ETA: 18:23 - loss: 24.2198\n",
            "5761/10000 = 57.6%, SPS: 3.8, ELAP: 20:39, ETA: 18:23 - loss: 24.5602\n",
            "5762/10000 = 57.6%, SPS: 3.8, ELAP: 20:39, ETA: 18:22 - loss: 24.4744\n",
            "5763/10000 = 57.6%, SPS: 3.8, ELAP: 20:39, ETA: 18:22 - loss: 23.7032\n",
            "5764/10000 = 57.6%, SPS: 3.8, ELAP: 20:39, ETA: 18:22 - loss: 24.5861\n",
            "5765/10000 = 57.6%, SPS: 3.8, ELAP: 20:39, ETA: 18:22 - loss: 24.2980\n",
            "5766/10000 = 57.7%, SPS: 3.8, ELAP: 20:40, ETA: 18:21 - loss: 24.8511\n",
            "5767/10000 = 57.7%, SPS: 3.8, ELAP: 20:40, ETA: 18:21 - loss: 25.0360\n",
            "5768/10000 = 57.7%, SPS: 3.8, ELAP: 20:40, ETA: 18:21 - loss: 24.0370\n",
            "5769/10000 = 57.7%, SPS: 3.8, ELAP: 20:40, ETA: 18:20 - loss: 23.8277\n",
            "5770/10000 = 57.7%, SPS: 3.8, ELAP: 20:41, ETA: 18:20 - loss: 24.8169\n",
            "5771/10000 = 57.7%, SPS: 3.8, ELAP: 20:41, ETA: 18:20 - loss: 23.9228\n",
            "5772/10000 = 57.7%, SPS: 3.8, ELAP: 20:41, ETA: 18:20 - loss: 23.6277\n",
            "5773/10000 = 57.7%, SPS: 3.8, ELAP: 20:41, ETA: 18:19 - loss: 23.9698\n",
            "5774/10000 = 57.7%, SPS: 3.8, ELAP: 20:42, ETA: 18:19 - loss: 24.2919\n",
            "5775/10000 = 57.8%, SPS: 3.8, ELAP: 20:42, ETA: 18:19 - loss: 24.1164\n",
            "5776/10000 = 57.8%, SPS: 3.8, ELAP: 20:42, ETA: 18:18 - loss: 24.4227\n",
            "5777/10000 = 57.8%, SPS: 3.8, ELAP: 20:42, ETA: 18:18 - loss: 23.8100\n",
            "5778/10000 = 57.8%, SPS: 3.8, ELAP: 20:42, ETA: 18:18 - loss: 25.0650\n",
            "5779/10000 = 57.8%, SPS: 3.8, ELAP: 20:43, ETA: 18:18 - loss: 24.5245\n",
            "5780/10000 = 57.8%, SPS: 3.8, ELAP: 20:43, ETA: 18:17 - loss: 24.2327\n",
            "5781/10000 = 57.8%, SPS: 3.8, ELAP: 20:43, ETA: 18:17 - loss: 24.6001\n",
            "5782/10000 = 57.8%, SPS: 3.8, ELAP: 20:43, ETA: 18:17 - loss: 24.7256\n",
            "5783/10000 = 57.8%, SPS: 3.8, ELAP: 20:44, ETA: 18:16 - loss: 24.7614\n",
            "5784/10000 = 57.8%, SPS: 3.8, ELAP: 20:44, ETA: 18:16 - loss: 24.4976\n",
            "5785/10000 = 57.9%, SPS: 3.8, ELAP: 20:44, ETA: 18:16 - loss: 24.4567\n",
            "5786/10000 = 57.9%, SPS: 3.8, ELAP: 20:44, ETA: 18:16 - loss: 24.5213\n",
            "5787/10000 = 57.9%, SPS: 3.8, ELAP: 20:45, ETA: 18:15 - loss: 24.8199\n",
            "5788/10000 = 57.9%, SPS: 3.8, ELAP: 20:45, ETA: 18:15 - loss: 24.5776\n",
            "5789/10000 = 57.9%, SPS: 3.8, ELAP: 20:45, ETA: 18:15 - loss: 24.4059\n",
            "5790/10000 = 57.9%, SPS: 3.8, ELAP: 20:45, ETA: 18:14 - loss: 24.6393\n",
            "5791/10000 = 57.9%, SPS: 3.8, ELAP: 20:45, ETA: 18:14 - loss: 24.5130\n",
            "5792/10000 = 57.9%, SPS: 3.8, ELAP: 20:46, ETA: 18:14 - loss: 24.8613\n",
            "5793/10000 = 57.9%, SPS: 3.8, ELAP: 20:46, ETA: 18:14 - loss: 24.9871\n",
            "5794/10000 = 57.9%, SPS: 3.8, ELAP: 20:46, ETA: 18:13 - loss: 24.7609\n",
            "5795/10000 = 58.0%, SPS: 3.8, ELAP: 20:46, ETA: 18:13 - loss: 24.3511\n",
            "5796/10000 = 58.0%, SPS: 3.8, ELAP: 20:47, ETA: 18:13 - loss: 24.3152\n",
            "5797/10000 = 58.0%, SPS: 3.8, ELAP: 20:47, ETA: 18:12 - loss: 24.3549\n",
            "5798/10000 = 58.0%, SPS: 3.8, ELAP: 20:47, ETA: 18:12 - loss: 25.1740\n",
            "5799/10000 = 58.0%, SPS: 3.8, ELAP: 20:47, ETA: 18:12 - loss: 24.4987\n",
            "5800/10000 = 58.0%, SPS: 3.8, ELAP: 20:50, ETA: 18:13 - loss: 25.1341\n",
            "5801/10000 = 58.0%, SPS: 3.8, ELAP: 20:50, ETA: 18:13 - loss: 23.8595\n",
            "5802/10000 = 58.0%, SPS: 3.8, ELAP: 20:50, ETA: 18:13 - loss: 24.9349\n",
            "5803/10000 = 58.0%, SPS: 3.8, ELAP: 20:50, ETA: 18:13 - loss: 23.7059\n",
            "5804/10000 = 58.0%, SPS: 3.8, ELAP: 20:51, ETA: 18:12 - loss: 24.2082\n",
            "5805/10000 = 58.0%, SPS: 3.8, ELAP: 20:51, ETA: 18:12 - loss: 25.0158\n",
            "5806/10000 = 58.1%, SPS: 3.8, ELAP: 20:51, ETA: 18:12 - loss: 24.0289\n",
            "5807/10000 = 58.1%, SPS: 3.8, ELAP: 20:51, ETA: 18:11 - loss: 24.2932\n",
            "5808/10000 = 58.1%, SPS: 3.8, ELAP: 20:52, ETA: 18:11 - loss: 24.5422\n",
            "5809/10000 = 58.1%, SPS: 3.8, ELAP: 20:52, ETA: 18:11 - loss: 24.3676\n",
            "5810/10000 = 58.1%, SPS: 3.8, ELAP: 20:52, ETA: 18:11 - loss: 24.9370\n",
            "5811/10000 = 58.1%, SPS: 3.8, ELAP: 20:52, ETA: 18:10 - loss: 24.2270\n",
            "5812/10000 = 58.1%, SPS: 3.8, ELAP: 20:52, ETA: 18:10 - loss: 24.5210\n",
            "5813/10000 = 58.1%, SPS: 3.8, ELAP: 20:53, ETA: 18:10 - loss: 24.6118\n",
            "5814/10000 = 58.1%, SPS: 3.8, ELAP: 20:53, ETA: 18:09 - loss: 24.9584\n",
            "5815/10000 = 58.1%, SPS: 3.8, ELAP: 20:53, ETA: 18:09 - loss: 24.6518\n",
            "5816/10000 = 58.2%, SPS: 3.8, ELAP: 20:53, ETA: 18:09 - loss: 24.2787\n",
            "5817/10000 = 58.2%, SPS: 3.8, ELAP: 20:54, ETA: 18:09 - loss: 24.7076\n",
            "5818/10000 = 58.2%, SPS: 3.8, ELAP: 20:54, ETA: 18:08 - loss: 24.3901\n",
            "5819/10000 = 58.2%, SPS: 3.8, ELAP: 20:54, ETA: 18:08 - loss: 24.3234\n",
            "5820/10000 = 58.2%, SPS: 3.8, ELAP: 20:54, ETA: 18:08 - loss: 24.3537\n",
            "5821/10000 = 58.2%, SPS: 3.8, ELAP: 20:55, ETA: 18:08 - loss: 24.6323\n",
            "5822/10000 = 58.2%, SPS: 3.8, ELAP: 20:55, ETA: 18:07 - loss: 24.8395\n",
            "5823/10000 = 58.2%, SPS: 3.8, ELAP: 20:55, ETA: 18:07 - loss: 24.8588\n",
            "5824/10000 = 58.2%, SPS: 3.8, ELAP: 20:55, ETA: 18:07 - loss: 24.4277\n",
            "5825/10000 = 58.2%, SPS: 3.8, ELAP: 20:56, ETA: 18:06 - loss: 24.4676\n",
            "5826/10000 = 58.3%, SPS: 3.8, ELAP: 20:56, ETA: 18:06 - loss: 23.6383\n",
            "5827/10000 = 58.3%, SPS: 3.8, ELAP: 20:56, ETA: 18:06 - loss: 24.6367\n",
            "5828/10000 = 58.3%, SPS: 3.8, ELAP: 20:56, ETA: 18:06 - loss: 24.9350\n",
            "5829/10000 = 58.3%, SPS: 3.8, ELAP: 20:57, ETA: 18:05 - loss: 24.4801\n",
            "5830/10000 = 58.3%, SPS: 3.8, ELAP: 20:57, ETA: 18:05 - loss: 24.1594\n",
            "5831/10000 = 58.3%, SPS: 3.8, ELAP: 20:57, ETA: 18:05 - loss: 24.5887\n",
            "5832/10000 = 58.3%, SPS: 3.8, ELAP: 20:57, ETA: 18:05 - loss: 24.2790\n",
            "5833/10000 = 58.3%, SPS: 3.8, ELAP: 20:58, ETA: 18:04 - loss: 24.3709\n",
            "5834/10000 = 58.3%, SPS: 3.8, ELAP: 20:58, ETA: 18:04 - loss: 24.6840\n",
            "5835/10000 = 58.4%, SPS: 3.8, ELAP: 20:58, ETA: 18:04 - loss: 23.6674\n",
            "5836/10000 = 58.4%, SPS: 3.8, ELAP: 20:58, ETA: 18:03 - loss: 24.5564\n",
            "5837/10000 = 58.4%, SPS: 3.8, ELAP: 20:58, ETA: 18:03 - loss: 24.6215\n",
            "5838/10000 = 58.4%, SPS: 3.8, ELAP: 20:59, ETA: 18:03 - loss: 23.9887\n",
            "5839/10000 = 58.4%, SPS: 3.8, ELAP: 20:59, ETA: 18:03 - loss: 24.8383\n",
            "5840/10000 = 58.4%, SPS: 3.8, ELAP: 20:59, ETA: 18:02 - loss: 24.6409\n",
            "5841/10000 = 58.4%, SPS: 3.8, ELAP: 20:59, ETA: 18:02 - loss: 25.0414\n",
            "5842/10000 = 58.4%, SPS: 3.8, ELAP: 21:00, ETA: 18:02 - loss: 24.4302\n",
            "5843/10000 = 58.4%, SPS: 3.8, ELAP: 21:00, ETA: 18:01 - loss: 23.7891\n",
            "5844/10000 = 58.4%, SPS: 3.8, ELAP: 21:00, ETA: 18:01 - loss: 24.0028\n",
            "5845/10000 = 58.5%, SPS: 3.8, ELAP: 21:00, ETA: 18:01 - loss: 24.2810\n",
            "5846/10000 = 58.5%, SPS: 3.8, ELAP: 21:01, ETA: 18:01 - loss: 24.5728\n",
            "5847/10000 = 58.5%, SPS: 3.8, ELAP: 21:01, ETA: 18:00 - loss: 24.3643\n",
            "5848/10000 = 58.5%, SPS: 3.8, ELAP: 21:01, ETA: 18:00 - loss: 23.5996\n",
            "5849/10000 = 58.5%, SPS: 3.8, ELAP: 21:01, ETA: 18:00 - loss: 24.7992\n",
            "5850/10000 = 58.5%, SPS: 3.8, ELAP: 21:01, ETA: 17:59 - loss: 24.3283\n",
            "5851/10000 = 58.5%, SPS: 3.8, ELAP: 21:02, ETA: 17:59 - loss: 24.3014\n",
            "5852/10000 = 58.5%, SPS: 3.8, ELAP: 21:02, ETA: 17:59 - loss: 24.1435\n",
            "5853/10000 = 58.5%, SPS: 3.8, ELAP: 21:02, ETA: 17:59 - loss: 24.1541\n",
            "5854/10000 = 58.5%, SPS: 3.8, ELAP: 21:02, ETA: 17:58 - loss: 24.5197\n",
            "5855/10000 = 58.5%, SPS: 3.8, ELAP: 21:03, ETA: 17:58 - loss: 25.1327\n",
            "5856/10000 = 58.6%, SPS: 3.8, ELAP: 21:03, ETA: 17:58 - loss: 24.9606\n",
            "5857/10000 = 58.6%, SPS: 3.8, ELAP: 21:03, ETA: 17:57 - loss: 24.1881\n",
            "5858/10000 = 58.6%, SPS: 3.8, ELAP: 21:03, ETA: 17:57 - loss: 24.5305\n",
            "5859/10000 = 58.6%, SPS: 3.8, ELAP: 21:04, ETA: 17:57 - loss: 24.3791\n",
            "5860/10000 = 58.6%, SPS: 3.8, ELAP: 21:04, ETA: 17:57 - loss: 24.5884\n",
            "5861/10000 = 58.6%, SPS: 3.8, ELAP: 21:04, ETA: 17:56 - loss: 24.3256\n",
            "5862/10000 = 58.6%, SPS: 3.8, ELAP: 21:04, ETA: 17:56 - loss: 24.5799\n",
            "5863/10000 = 58.6%, SPS: 3.8, ELAP: 21:04, ETA: 17:56 - loss: 23.7201\n",
            "5864/10000 = 58.6%, SPS: 3.8, ELAP: 21:05, ETA: 17:55 - loss: 24.6610\n",
            "5865/10000 = 58.6%, SPS: 3.8, ELAP: 21:05, ETA: 17:55 - loss: 24.8294\n",
            "5866/10000 = 58.7%, SPS: 3.8, ELAP: 21:05, ETA: 17:55 - loss: 24.4484\n",
            "5867/10000 = 58.7%, SPS: 3.8, ELAP: 21:05, ETA: 17:55 - loss: 24.5794\n",
            "5868/10000 = 58.7%, SPS: 3.8, ELAP: 21:06, ETA: 17:54 - loss: 24.4050\n",
            "5869/10000 = 58.7%, SPS: 3.8, ELAP: 21:06, ETA: 17:54 - loss: 24.9865\n",
            "5870/10000 = 58.7%, SPS: 3.8, ELAP: 21:06, ETA: 17:54 - loss: 24.3424\n",
            "5871/10000 = 58.7%, SPS: 3.8, ELAP: 21:06, ETA: 17:53 - loss: 24.4571\n",
            "5872/10000 = 58.7%, SPS: 3.8, ELAP: 21:07, ETA: 17:53 - loss: 24.9681\n",
            "5873/10000 = 58.7%, SPS: 3.8, ELAP: 21:07, ETA: 17:53 - loss: 24.3827\n",
            "5874/10000 = 58.7%, SPS: 3.8, ELAP: 21:07, ETA: 17:53 - loss: 24.2921\n",
            "5875/10000 = 58.8%, SPS: 3.8, ELAP: 21:07, ETA: 17:52 - loss: 24.5936\n",
            "5876/10000 = 58.8%, SPS: 3.8, ELAP: 21:07, ETA: 17:52 - loss: 23.8666\n",
            "5877/10000 = 58.8%, SPS: 3.8, ELAP: 21:08, ETA: 17:52 - loss: 24.7587\n",
            "5878/10000 = 58.8%, SPS: 3.8, ELAP: 21:08, ETA: 17:51 - loss: 24.7505\n",
            "5879/10000 = 58.8%, SPS: 3.8, ELAP: 21:08, ETA: 17:51 - loss: 24.2993\n",
            "5880/10000 = 58.8%, SPS: 3.8, ELAP: 21:08, ETA: 17:51 - loss: 24.3171\n",
            "5881/10000 = 58.8%, SPS: 3.8, ELAP: 21:09, ETA: 17:51 - loss: 24.2774\n",
            "5882/10000 = 58.8%, SPS: 3.8, ELAP: 21:09, ETA: 17:50 - loss: 24.5896\n",
            "5883/10000 = 58.8%, SPS: 3.8, ELAP: 21:09, ETA: 17:50 - loss: 24.8503\n",
            "5884/10000 = 58.8%, SPS: 3.8, ELAP: 21:09, ETA: 17:50 - loss: 24.0353\n",
            "5885/10000 = 58.9%, SPS: 3.8, ELAP: 21:10, ETA: 17:49 - loss: 23.9066\n",
            "5886/10000 = 58.9%, SPS: 3.8, ELAP: 21:10, ETA: 17:49 - loss: 24.2240\n",
            "5887/10000 = 58.9%, SPS: 3.8, ELAP: 21:10, ETA: 17:49 - loss: 24.6008\n",
            "5888/10000 = 58.9%, SPS: 3.8, ELAP: 21:10, ETA: 17:49 - loss: 24.3318\n",
            "5889/10000 = 58.9%, SPS: 3.8, ELAP: 21:11, ETA: 17:48 - loss: 24.4239\n",
            "5890/10000 = 58.9%, SPS: 3.8, ELAP: 21:11, ETA: 17:48 - loss: 24.2026\n",
            "5891/10000 = 58.9%, SPS: 3.8, ELAP: 21:11, ETA: 17:48 - loss: 24.6078\n",
            "5892/10000 = 58.9%, SPS: 3.8, ELAP: 21:11, ETA: 17:47 - loss: 24.9643\n",
            "5893/10000 = 58.9%, SPS: 3.8, ELAP: 21:11, ETA: 17:47 - loss: 24.8355\n",
            "5894/10000 = 58.9%, SPS: 3.8, ELAP: 21:12, ETA: 17:47 - loss: 24.2957\n",
            "5895/10000 = 59.0%, SPS: 3.8, ELAP: 21:12, ETA: 17:47 - loss: 24.7620\n",
            "5896/10000 = 59.0%, SPS: 3.8, ELAP: 21:12, ETA: 17:46 - loss: 24.2296\n",
            "5897/10000 = 59.0%, SPS: 3.8, ELAP: 21:12, ETA: 17:46 - loss: 24.7962\n",
            "5898/10000 = 59.0%, SPS: 3.8, ELAP: 21:13, ETA: 17:46 - loss: 23.6684\n",
            "5899/10000 = 59.0%, SPS: 3.8, ELAP: 21:13, ETA: 17:45 - loss: 24.4917\n",
            "5900/10000 = 59.0%, SPS: 3.8, ELAP: 21:15, ETA: 17:47 - loss: 23.6660\n",
            "5901/10000 = 59.0%, SPS: 3.8, ELAP: 21:15, ETA: 17:47 - loss: 24.7971\n",
            "5902/10000 = 59.0%, SPS: 3.8, ELAP: 21:16, ETA: 17:46 - loss: 24.1179\n",
            "5903/10000 = 59.0%, SPS: 3.8, ELAP: 21:16, ETA: 17:46 - loss: 25.0130\n",
            "5904/10000 = 59.0%, SPS: 3.8, ELAP: 21:16, ETA: 17:46 - loss: 24.4206\n",
            "5905/10000 = 59.0%, SPS: 3.8, ELAP: 21:16, ETA: 17:46 - loss: 24.4927\n",
            "5906/10000 = 59.1%, SPS: 3.8, ELAP: 21:17, ETA: 17:45 - loss: 24.7000\n",
            "5907/10000 = 59.1%, SPS: 3.8, ELAP: 21:17, ETA: 17:45 - loss: 24.3961\n",
            "5908/10000 = 59.1%, SPS: 3.8, ELAP: 21:17, ETA: 17:45 - loss: 24.8421\n",
            "5909/10000 = 59.1%, SPS: 3.8, ELAP: 21:17, ETA: 17:44 - loss: 24.5730\n",
            "5910/10000 = 59.1%, SPS: 3.8, ELAP: 21:17, ETA: 17:44 - loss: 24.8888\n",
            "5911/10000 = 59.1%, SPS: 3.8, ELAP: 21:18, ETA: 17:44 - loss: 23.9236\n",
            "5912/10000 = 59.1%, SPS: 3.8, ELAP: 21:18, ETA: 17:44 - loss: 24.3072\n",
            "5913/10000 = 59.1%, SPS: 3.8, ELAP: 21:18, ETA: 17:43 - loss: 24.9768\n",
            "5914/10000 = 59.1%, SPS: 3.8, ELAP: 21:18, ETA: 17:43 - loss: 24.6095\n",
            "5915/10000 = 59.1%, SPS: 3.8, ELAP: 21:19, ETA: 17:43 - loss: 24.9248\n",
            "5916/10000 = 59.2%, SPS: 3.8, ELAP: 21:19, ETA: 17:42 - loss: 24.4449\n",
            "5917/10000 = 59.2%, SPS: 3.8, ELAP: 21:19, ETA: 17:42 - loss: 24.7948\n",
            "5918/10000 = 59.2%, SPS: 3.8, ELAP: 21:19, ETA: 17:42 - loss: 24.0243\n",
            "5919/10000 = 59.2%, SPS: 3.8, ELAP: 21:20, ETA: 17:42 - loss: 23.7538\n",
            "5920/10000 = 59.2%, SPS: 3.8, ELAP: 21:20, ETA: 17:41 - loss: 25.0740\n",
            "5921/10000 = 59.2%, SPS: 3.8, ELAP: 21:20, ETA: 17:41 - loss: 25.0202\n",
            "5922/10000 = 59.2%, SPS: 3.8, ELAP: 21:20, ETA: 17:41 - loss: 24.2826\n",
            "5923/10000 = 59.2%, SPS: 3.8, ELAP: 21:21, ETA: 17:40 - loss: 24.0231\n",
            "5924/10000 = 59.2%, SPS: 3.8, ELAP: 21:21, ETA: 17:40 - loss: 24.2424\n",
            "5925/10000 = 59.2%, SPS: 3.8, ELAP: 21:21, ETA: 17:40 - loss: 24.5431\n",
            "5926/10000 = 59.3%, SPS: 3.8, ELAP: 21:21, ETA: 17:40 - loss: 23.9566\n",
            "5927/10000 = 59.3%, SPS: 3.8, ELAP: 21:21, ETA: 17:39 - loss: 24.3131\n",
            "5928/10000 = 59.3%, SPS: 3.8, ELAP: 21:22, ETA: 17:39 - loss: 24.4472\n",
            "5929/10000 = 59.3%, SPS: 3.8, ELAP: 21:22, ETA: 17:39 - loss: 24.5093\n",
            "5930/10000 = 59.3%, SPS: 3.8, ELAP: 21:22, ETA: 17:39 - loss: 23.9305\n",
            "5931/10000 = 59.3%, SPS: 3.8, ELAP: 21:22, ETA: 17:38 - loss: 23.9566\n",
            "5932/10000 = 59.3%, SPS: 3.8, ELAP: 21:23, ETA: 17:38 - loss: 24.3020\n",
            "5933/10000 = 59.3%, SPS: 3.8, ELAP: 21:23, ETA: 17:38 - loss: 24.9163\n",
            "5934/10000 = 59.3%, SPS: 3.8, ELAP: 21:23, ETA: 17:37 - loss: 24.8333\n",
            "5935/10000 = 59.4%, SPS: 3.8, ELAP: 21:23, ETA: 17:37 - loss: 24.1938\n",
            "5936/10000 = 59.4%, SPS: 3.8, ELAP: 21:24, ETA: 17:37 - loss: 25.2868\n",
            "5937/10000 = 59.4%, SPS: 3.8, ELAP: 21:24, ETA: 17:37 - loss: 24.4359\n",
            "5938/10000 = 59.4%, SPS: 3.8, ELAP: 21:24, ETA: 17:36 - loss: 24.5426\n",
            "5939/10000 = 59.4%, SPS: 3.8, ELAP: 21:24, ETA: 17:36 - loss: 24.3885\n",
            "5940/10000 = 59.4%, SPS: 3.8, ELAP: 21:25, ETA: 17:36 - loss: 24.6061\n",
            "5941/10000 = 59.4%, SPS: 3.8, ELAP: 21:25, ETA: 17:35 - loss: 24.8366\n",
            "5942/10000 = 59.4%, SPS: 3.8, ELAP: 21:25, ETA: 17:35 - loss: 24.3483\n",
            "5943/10000 = 59.4%, SPS: 3.8, ELAP: 21:25, ETA: 17:35 - loss: 24.6113\n",
            "5944/10000 = 59.4%, SPS: 3.8, ELAP: 21:25, ETA: 17:35 - loss: 24.7376\n",
            "5945/10000 = 59.5%, SPS: 3.8, ELAP: 21:26, ETA: 17:34 - loss: 23.6960\n",
            "5946/10000 = 59.5%, SPS: 3.8, ELAP: 21:26, ETA: 17:34 - loss: 24.5123\n",
            "5947/10000 = 59.5%, SPS: 3.8, ELAP: 21:26, ETA: 17:34 - loss: 24.5717\n",
            "5948/10000 = 59.5%, SPS: 3.8, ELAP: 21:26, ETA: 17:33 - loss: 24.0366\n",
            "5949/10000 = 59.5%, SPS: 3.8, ELAP: 21:27, ETA: 17:33 - loss: 24.7332\n",
            "5950/10000 = 59.5%, SPS: 3.8, ELAP: 21:27, ETA: 17:33 - loss: 24.2613\n",
            "5951/10000 = 59.5%, SPS: 3.8, ELAP: 21:27, ETA: 17:33 - loss: 24.6033\n",
            "5952/10000 = 59.5%, SPS: 3.8, ELAP: 21:27, ETA: 17:32 - loss: 24.0014\n",
            "5953/10000 = 59.5%, SPS: 3.8, ELAP: 21:27, ETA: 17:32 - loss: 24.5733\n",
            "5954/10000 = 59.5%, SPS: 3.8, ELAP: 21:28, ETA: 17:32 - loss: 24.7091\n",
            "5955/10000 = 59.5%, SPS: 3.8, ELAP: 21:28, ETA: 17:31 - loss: 24.3351\n",
            "5956/10000 = 59.6%, SPS: 3.8, ELAP: 21:28, ETA: 17:31 - loss: 24.4658\n",
            "5957/10000 = 59.6%, SPS: 3.8, ELAP: 21:28, ETA: 17:31 - loss: 24.3155\n",
            "5958/10000 = 59.6%, SPS: 3.8, ELAP: 21:29, ETA: 17:31 - loss: 24.6005\n",
            "5959/10000 = 59.6%, SPS: 3.8, ELAP: 21:29, ETA: 17:30 - loss: 24.0661\n",
            "5960/10000 = 59.6%, SPS: 3.8, ELAP: 21:29, ETA: 17:30 - loss: 24.9089\n",
            "5961/10000 = 59.6%, SPS: 3.8, ELAP: 21:29, ETA: 17:30 - loss: 24.6113\n",
            "5962/10000 = 59.6%, SPS: 3.8, ELAP: 21:30, ETA: 17:29 - loss: 24.5690\n",
            "5963/10000 = 59.6%, SPS: 3.8, ELAP: 21:30, ETA: 17:29 - loss: 24.1797\n",
            "5964/10000 = 59.6%, SPS: 3.8, ELAP: 21:30, ETA: 17:29 - loss: 24.2228\n",
            "5965/10000 = 59.6%, SPS: 3.8, ELAP: 21:30, ETA: 17:29 - loss: 24.2085\n",
            "5966/10000 = 59.7%, SPS: 3.8, ELAP: 21:30, ETA: 17:28 - loss: 24.1709\n",
            "5967/10000 = 59.7%, SPS: 3.8, ELAP: 21:31, ETA: 17:28 - loss: 24.0622\n",
            "5968/10000 = 59.7%, SPS: 3.8, ELAP: 21:31, ETA: 17:28 - loss: 24.4217\n",
            "5969/10000 = 59.7%, SPS: 3.8, ELAP: 21:31, ETA: 17:27 - loss: 24.5849\n",
            "5970/10000 = 59.7%, SPS: 3.8, ELAP: 21:31, ETA: 17:27 - loss: 24.4289\n",
            "5971/10000 = 59.7%, SPS: 3.8, ELAP: 21:32, ETA: 17:27 - loss: 24.4307\n",
            "5972/10000 = 59.7%, SPS: 3.8, ELAP: 21:32, ETA: 17:27 - loss: 24.8688\n",
            "5973/10000 = 59.7%, SPS: 3.8, ELAP: 21:32, ETA: 17:26 - loss: 24.4544\n",
            "5974/10000 = 59.7%, SPS: 3.8, ELAP: 21:32, ETA: 17:26 - loss: 24.1582\n",
            "5975/10000 = 59.8%, SPS: 3.8, ELAP: 21:33, ETA: 17:26 - loss: 24.5847\n",
            "5976/10000 = 59.8%, SPS: 3.8, ELAP: 21:33, ETA: 17:25 - loss: 24.3777\n",
            "5977/10000 = 59.8%, SPS: 3.8, ELAP: 21:33, ETA: 17:25 - loss: 24.0426\n",
            "5978/10000 = 59.8%, SPS: 3.8, ELAP: 21:33, ETA: 17:25 - loss: 24.1152\n",
            "5979/10000 = 59.8%, SPS: 3.8, ELAP: 21:33, ETA: 17:25 - loss: 24.9200\n",
            "5980/10000 = 59.8%, SPS: 3.8, ELAP: 21:34, ETA: 17:24 - loss: 24.6933\n",
            "5981/10000 = 59.8%, SPS: 3.8, ELAP: 21:34, ETA: 17:24 - loss: 24.4213\n",
            "5982/10000 = 59.8%, SPS: 3.8, ELAP: 21:34, ETA: 17:24 - loss: 24.6693\n",
            "5983/10000 = 59.8%, SPS: 3.8, ELAP: 21:34, ETA: 17:23 - loss: 24.1566\n",
            "5984/10000 = 59.8%, SPS: 3.8, ELAP: 21:35, ETA: 17:23 - loss: 24.5097\n",
            "5985/10000 = 59.9%, SPS: 3.8, ELAP: 21:35, ETA: 17:23 - loss: 24.4670\n",
            "5986/10000 = 59.9%, SPS: 3.8, ELAP: 21:35, ETA: 17:23 - loss: 24.4204\n",
            "5987/10000 = 59.9%, SPS: 3.9, ELAP: 21:35, ETA: 17:22 - loss: 24.8784\n",
            "5988/10000 = 59.9%, SPS: 3.9, ELAP: 21:36, ETA: 17:22 - loss: 24.7675\n",
            "5989/10000 = 59.9%, SPS: 3.9, ELAP: 21:36, ETA: 17:22 - loss: 24.1085\n",
            "5990/10000 = 59.9%, SPS: 3.9, ELAP: 21:36, ETA: 17:21 - loss: 24.3924\n",
            "5991/10000 = 59.9%, SPS: 3.9, ELAP: 21:36, ETA: 17:21 - loss: 24.0946\n",
            "5992/10000 = 59.9%, SPS: 3.9, ELAP: 21:36, ETA: 17:21 - loss: 24.6149\n",
            "5993/10000 = 59.9%, SPS: 3.9, ELAP: 21:37, ETA: 17:21 - loss: 24.3711\n",
            "5994/10000 = 59.9%, SPS: 3.9, ELAP: 21:37, ETA: 17:20 - loss: 24.2319\n",
            "5995/10000 = 60.0%, SPS: 3.9, ELAP: 21:37, ETA: 17:20 - loss: 24.1257\n",
            "5996/10000 = 60.0%, SPS: 3.9, ELAP: 21:37, ETA: 17:20 - loss: 24.6906\n",
            "5997/10000 = 60.0%, SPS: 3.9, ELAP: 21:38, ETA: 17:19 - loss: 24.5606\n",
            "5998/10000 = 60.0%, SPS: 3.9, ELAP: 21:38, ETA: 17:19 - loss: 24.4409\n",
            "5999/10000 = 60.0%, SPS: 3.9, ELAP: 21:38, ETA: 17:19 - loss: 23.8988\n",
            "6000/10000 = 60.0%, SPS: 3.8, ELAP: 21:41, ETA: 17:20 - loss: 25.0233\n",
            "6001/10000 = 60.0%, SPS: 3.8, ELAP: 21:41, ETA: 17:20 - loss: 25.5241\n",
            "6002/10000 = 60.0%, SPS: 3.8, ELAP: 21:41, ETA: 17:20 - loss: 24.4742\n",
            "6003/10000 = 60.0%, SPS: 3.8, ELAP: 21:41, ETA: 17:20 - loss: 24.3558\n",
            "6004/10000 = 60.0%, SPS: 3.8, ELAP: 21:41, ETA: 17:19 - loss: 24.0542\n",
            "6005/10000 = 60.0%, SPS: 3.8, ELAP: 21:42, ETA: 17:19 - loss: 23.7551\n",
            "6006/10000 = 60.1%, SPS: 3.8, ELAP: 21:42, ETA: 17:19 - loss: 23.9837\n",
            "6007/10000 = 60.1%, SPS: 3.8, ELAP: 21:42, ETA: 17:18 - loss: 23.9562\n",
            "6008/10000 = 60.1%, SPS: 3.8, ELAP: 21:42, ETA: 17:18 - loss: 24.6260\n",
            "6009/10000 = 60.1%, SPS: 3.8, ELAP: 21:43, ETA: 17:18 - loss: 24.5147\n",
            "6010/10000 = 60.1%, SPS: 3.8, ELAP: 21:43, ETA: 17:18 - loss: 24.2954\n",
            "6011/10000 = 60.1%, SPS: 3.8, ELAP: 21:43, ETA: 17:17 - loss: 24.4727\n",
            "6012/10000 = 60.1%, SPS: 3.8, ELAP: 21:43, ETA: 17:17 - loss: 24.7948\n",
            "6013/10000 = 60.1%, SPS: 3.8, ELAP: 21:44, ETA: 17:17 - loss: 24.6059\n",
            "6014/10000 = 60.1%, SPS: 3.8, ELAP: 21:44, ETA: 17:16 - loss: 24.9014\n",
            "6015/10000 = 60.1%, SPS: 3.8, ELAP: 21:44, ETA: 17:16 - loss: 24.7087\n",
            "6016/10000 = 60.2%, SPS: 3.8, ELAP: 21:44, ETA: 17:16 - loss: 23.9098\n",
            "6017/10000 = 60.2%, SPS: 3.8, ELAP: 21:44, ETA: 17:16 - loss: 24.7085\n",
            "6018/10000 = 60.2%, SPS: 3.8, ELAP: 21:45, ETA: 17:15 - loss: 24.4716\n",
            "6019/10000 = 60.2%, SPS: 3.8, ELAP: 21:45, ETA: 17:15 - loss: 23.8565\n",
            "6020/10000 = 60.2%, SPS: 3.8, ELAP: 21:45, ETA: 17:15 - loss: 24.3850\n",
            "6021/10000 = 60.2%, SPS: 3.8, ELAP: 21:45, ETA: 17:14 - loss: 24.4295\n",
            "6022/10000 = 60.2%, SPS: 3.8, ELAP: 21:46, ETA: 17:14 - loss: 24.1125\n",
            "6023/10000 = 60.2%, SPS: 3.8, ELAP: 21:46, ETA: 17:14 - loss: 24.0786\n",
            "6024/10000 = 60.2%, SPS: 3.8, ELAP: 21:46, ETA: 17:14 - loss: 24.7440\n",
            "6025/10000 = 60.2%, SPS: 3.8, ELAP: 21:46, ETA: 17:13 - loss: 24.6285\n",
            "6026/10000 = 60.3%, SPS: 3.8, ELAP: 21:47, ETA: 17:13 - loss: 23.3648\n",
            "6027/10000 = 60.3%, SPS: 3.8, ELAP: 21:47, ETA: 17:13 - loss: 25.0698\n",
            "6028/10000 = 60.3%, SPS: 3.8, ELAP: 21:47, ETA: 17:12 - loss: 25.1359\n",
            "6029/10000 = 60.3%, SPS: 3.8, ELAP: 21:47, ETA: 17:12 - loss: 24.8707\n",
            "6030/10000 = 60.3%, SPS: 3.8, ELAP: 21:47, ETA: 17:12 - loss: 24.0986\n",
            "6031/10000 = 60.3%, SPS: 3.8, ELAP: 21:48, ETA: 17:12 - loss: 24.0170\n",
            "6032/10000 = 60.3%, SPS: 3.8, ELAP: 21:48, ETA: 17:11 - loss: 24.4182\n",
            "6033/10000 = 60.3%, SPS: 3.8, ELAP: 21:48, ETA: 17:11 - loss: 24.5468\n",
            "6034/10000 = 60.3%, SPS: 3.8, ELAP: 21:48, ETA: 17:11 - loss: 24.4942\n",
            "6035/10000 = 60.4%, SPS: 3.8, ELAP: 21:49, ETA: 17:10 - loss: 24.3020\n",
            "6036/10000 = 60.4%, SPS: 3.8, ELAP: 21:49, ETA: 17:10 - loss: 24.3390\n",
            "6037/10000 = 60.4%, SPS: 3.8, ELAP: 21:49, ETA: 17:10 - loss: 24.9500\n",
            "6038/10000 = 60.4%, SPS: 3.8, ELAP: 21:49, ETA: 17:10 - loss: 24.2751\n",
            "6039/10000 = 60.4%, SPS: 3.8, ELAP: 21:50, ETA: 17:09 - loss: 24.9035\n",
            "6040/10000 = 60.4%, SPS: 3.8, ELAP: 21:50, ETA: 17:09 - loss: 23.7004\n",
            "6041/10000 = 60.4%, SPS: 3.8, ELAP: 21:50, ETA: 17:09 - loss: 24.6823\n",
            "6042/10000 = 60.4%, SPS: 3.8, ELAP: 21:50, ETA: 17:09 - loss: 24.1869\n",
            "6043/10000 = 60.4%, SPS: 3.8, ELAP: 21:51, ETA: 17:08 - loss: 24.5154\n",
            "6044/10000 = 60.4%, SPS: 3.8, ELAP: 21:51, ETA: 17:08 - loss: 24.0692\n",
            "6045/10000 = 60.5%, SPS: 3.8, ELAP: 21:51, ETA: 17:08 - loss: 24.1695\n",
            "6046/10000 = 60.5%, SPS: 3.8, ELAP: 21:51, ETA: 17:08 - loss: 24.3150\n",
            "6047/10000 = 60.5%, SPS: 3.8, ELAP: 21:52, ETA: 17:07 - loss: 24.5422\n",
            "6048/10000 = 60.5%, SPS: 3.8, ELAP: 21:52, ETA: 17:07 - loss: 25.0114\n",
            "6049/10000 = 60.5%, SPS: 3.8, ELAP: 21:52, ETA: 17:07 - loss: 24.4242\n",
            "6050/10000 = 60.5%, SPS: 3.8, ELAP: 21:52, ETA: 17:06 - loss: 24.1731\n",
            "6051/10000 = 60.5%, SPS: 3.8, ELAP: 21:53, ETA: 17:06 - loss: 24.4305\n",
            "6052/10000 = 60.5%, SPS: 3.8, ELAP: 21:53, ETA: 17:06 - loss: 24.5403\n",
            "6053/10000 = 60.5%, SPS: 3.8, ELAP: 21:53, ETA: 17:06 - loss: 23.9254\n",
            "6054/10000 = 60.5%, SPS: 3.8, ELAP: 21:53, ETA: 17:05 - loss: 24.1331\n",
            "6055/10000 = 60.5%, SPS: 3.8, ELAP: 21:53, ETA: 17:05 - loss: 24.4331\n",
            "6056/10000 = 60.6%, SPS: 3.8, ELAP: 21:54, ETA: 17:05 - loss: 25.1095\n",
            "6057/10000 = 60.6%, SPS: 3.8, ELAP: 21:54, ETA: 17:04 - loss: 24.2439\n",
            "6058/10000 = 60.6%, SPS: 3.8, ELAP: 21:54, ETA: 17:04 - loss: 24.8030\n",
            "6059/10000 = 60.6%, SPS: 3.8, ELAP: 21:54, ETA: 17:04 - loss: 24.8354\n",
            "6060/10000 = 60.6%, SPS: 3.8, ELAP: 21:55, ETA: 17:04 - loss: 24.3141\n",
            "6061/10000 = 60.6%, SPS: 3.8, ELAP: 21:55, ETA: 17:03 - loss: 24.3791\n",
            "6062/10000 = 60.6%, SPS: 3.8, ELAP: 21:55, ETA: 17:03 - loss: 24.2468\n",
            "6063/10000 = 60.6%, SPS: 3.8, ELAP: 21:55, ETA: 17:03 - loss: 24.3080\n",
            "6064/10000 = 60.6%, SPS: 3.8, ELAP: 21:56, ETA: 17:03 - loss: 23.9001\n",
            "6065/10000 = 60.6%, SPS: 3.8, ELAP: 21:56, ETA: 17:02 - loss: 24.3330\n",
            "6066/10000 = 60.7%, SPS: 3.8, ELAP: 21:56, ETA: 17:02 - loss: 24.1287\n",
            "6067/10000 = 60.7%, SPS: 3.8, ELAP: 21:56, ETA: 17:02 - loss: 24.2751\n",
            "6068/10000 = 60.7%, SPS: 3.8, ELAP: 21:56, ETA: 17:01 - loss: 24.2617\n",
            "6069/10000 = 60.7%, SPS: 3.8, ELAP: 21:57, ETA: 17:01 - loss: 24.3220\n",
            "6070/10000 = 60.7%, SPS: 3.8, ELAP: 21:57, ETA: 17:01 - loss: 24.0402\n",
            "6071/10000 = 60.7%, SPS: 3.8, ELAP: 21:57, ETA: 17:01 - loss: 24.2244\n",
            "6072/10000 = 60.7%, SPS: 3.9, ELAP: 21:57, ETA: 17:00 - loss: 24.2435\n",
            "6073/10000 = 60.7%, SPS: 3.9, ELAP: 21:58, ETA: 17:00 - loss: 24.7480\n",
            "6074/10000 = 60.7%, SPS: 3.9, ELAP: 21:58, ETA: 17:00 - loss: 24.2671\n",
            "6075/10000 = 60.8%, SPS: 3.9, ELAP: 21:58, ETA: 16:59 - loss: 24.1544\n",
            "6076/10000 = 60.8%, SPS: 3.9, ELAP: 21:58, ETA: 16:59 - loss: 23.8565\n",
            "6077/10000 = 60.8%, SPS: 3.9, ELAP: 21:59, ETA: 16:59 - loss: 23.5110\n",
            "6078/10000 = 60.8%, SPS: 3.9, ELAP: 21:59, ETA: 16:59 - loss: 23.5527\n",
            "6079/10000 = 60.8%, SPS: 3.9, ELAP: 21:59, ETA: 16:58 - loss: 24.5554\n",
            "6080/10000 = 60.8%, SPS: 3.9, ELAP: 21:59, ETA: 16:58 - loss: 24.3865\n",
            "6081/10000 = 60.8%, SPS: 3.9, ELAP: 21:59, ETA: 16:58 - loss: 24.2349\n",
            "6082/10000 = 60.8%, SPS: 3.9, ELAP: 22:00, ETA: 16:57 - loss: 24.3214\n",
            "6083/10000 = 60.8%, SPS: 3.9, ELAP: 22:00, ETA: 16:57 - loss: 24.4227\n",
            "6084/10000 = 60.8%, SPS: 3.9, ELAP: 22:00, ETA: 16:57 - loss: 24.1745\n",
            "6085/10000 = 60.9%, SPS: 3.9, ELAP: 22:00, ETA: 16:57 - loss: 24.6952\n",
            "6086/10000 = 60.9%, SPS: 3.9, ELAP: 22:01, ETA: 16:56 - loss: 24.4777\n",
            "6087/10000 = 60.9%, SPS: 3.9, ELAP: 22:01, ETA: 16:56 - loss: 24.3461\n",
            "6088/10000 = 60.9%, SPS: 3.9, ELAP: 22:01, ETA: 16:56 - loss: 24.3204\n",
            "6089/10000 = 60.9%, SPS: 3.9, ELAP: 22:01, ETA: 16:55 - loss: 24.5461\n",
            "6090/10000 = 60.9%, SPS: 3.9, ELAP: 22:02, ETA: 16:55 - loss: 24.4832\n",
            "6091/10000 = 60.9%, SPS: 3.9, ELAP: 22:02, ETA: 16:55 - loss: 24.3183\n",
            "6092/10000 = 60.9%, SPS: 3.9, ELAP: 22:02, ETA: 16:55 - loss: 24.6332\n",
            "6093/10000 = 60.9%, SPS: 3.9, ELAP: 22:02, ETA: 16:54 - loss: 24.5341\n",
            "6094/10000 = 60.9%, SPS: 3.9, ELAP: 22:02, ETA: 16:54 - loss: 24.4739\n",
            "6095/10000 = 61.0%, SPS: 3.9, ELAP: 22:03, ETA: 16:54 - loss: 23.8412\n",
            "6096/10000 = 61.0%, SPS: 3.9, ELAP: 22:03, ETA: 16:53 - loss: 23.7537\n",
            "6097/10000 = 61.0%, SPS: 3.9, ELAP: 22:03, ETA: 16:53 - loss: 24.4837\n",
            "6098/10000 = 61.0%, SPS: 3.9, ELAP: 22:03, ETA: 16:53 - loss: 24.2996\n",
            "6099/10000 = 61.0%, SPS: 3.9, ELAP: 22:04, ETA: 16:53 - loss: 24.0183\n",
            "6100/10000 = 61.0%, SPS: 3.8, ELAP: 22:06, ETA: 16:54 - loss: 24.0009\n",
            "6101/10000 = 61.0%, SPS: 3.8, ELAP: 22:06, ETA: 16:54 - loss: 24.2150\n",
            "6102/10000 = 61.0%, SPS: 3.8, ELAP: 22:06, ETA: 16:53 - loss: 24.3273\n",
            "6103/10000 = 61.0%, SPS: 3.8, ELAP: 22:07, ETA: 16:53 - loss: 24.4397\n",
            "6104/10000 = 61.0%, SPS: 3.8, ELAP: 22:07, ETA: 16:53 - loss: 24.5274\n",
            "6105/10000 = 61.0%, SPS: 3.8, ELAP: 22:07, ETA: 16:53 - loss: 24.6391\n",
            "6106/10000 = 61.1%, SPS: 3.8, ELAP: 22:07, ETA: 16:52 - loss: 23.8931\n",
            "6107/10000 = 61.1%, SPS: 3.8, ELAP: 22:08, ETA: 16:52 - loss: 24.1895\n",
            "6108/10000 = 61.1%, SPS: 3.8, ELAP: 22:08, ETA: 16:52 - loss: 24.4638\n",
            "6109/10000 = 61.1%, SPS: 3.8, ELAP: 22:08, ETA: 16:51 - loss: 23.5465\n",
            "6110/10000 = 61.1%, SPS: 3.8, ELAP: 22:08, ETA: 16:51 - loss: 24.4141\n",
            "6111/10000 = 61.1%, SPS: 3.8, ELAP: 22:09, ETA: 16:51 - loss: 24.3542\n",
            "6112/10000 = 61.1%, SPS: 3.8, ELAP: 22:09, ETA: 16:51 - loss: 24.9666\n",
            "6113/10000 = 61.1%, SPS: 3.8, ELAP: 22:09, ETA: 16:50 - loss: 24.4258\n",
            "6114/10000 = 61.1%, SPS: 3.8, ELAP: 22:09, ETA: 16:50 - loss: 24.9020\n",
            "6115/10000 = 61.1%, SPS: 3.8, ELAP: 22:10, ETA: 16:50 - loss: 24.2280\n",
            "6116/10000 = 61.2%, SPS: 3.8, ELAP: 22:10, ETA: 16:50 - loss: 24.5046\n",
            "6117/10000 = 61.2%, SPS: 3.8, ELAP: 22:10, ETA: 16:49 - loss: 24.4634\n",
            "6118/10000 = 61.2%, SPS: 3.8, ELAP: 22:10, ETA: 16:49 - loss: 24.4104\n",
            "6119/10000 = 61.2%, SPS: 3.8, ELAP: 22:10, ETA: 16:49 - loss: 24.7420\n",
            "6120/10000 = 61.2%, SPS: 3.8, ELAP: 22:11, ETA: 16:48 - loss: 24.6154\n",
            "6121/10000 = 61.2%, SPS: 3.8, ELAP: 22:11, ETA: 16:48 - loss: 24.3811\n",
            "6122/10000 = 61.2%, SPS: 3.8, ELAP: 22:11, ETA: 16:48 - loss: 23.7574\n",
            "6123/10000 = 61.2%, SPS: 3.8, ELAP: 22:11, ETA: 16:48 - loss: 24.7916\n",
            "6124/10000 = 61.2%, SPS: 3.8, ELAP: 22:12, ETA: 16:47 - loss: 24.3945\n",
            "6125/10000 = 61.2%, SPS: 3.8, ELAP: 22:12, ETA: 16:47 - loss: 24.2153\n",
            "6126/10000 = 61.3%, SPS: 3.8, ELAP: 22:12, ETA: 16:47 - loss: 25.0111\n",
            "6127/10000 = 61.3%, SPS: 3.8, ELAP: 22:12, ETA: 16:46 - loss: 24.3066\n",
            "6128/10000 = 61.3%, SPS: 3.8, ELAP: 22:13, ETA: 16:46 - loss: 24.9132\n",
            "6129/10000 = 61.3%, SPS: 3.8, ELAP: 22:13, ETA: 16:46 - loss: 24.9510\n",
            "6130/10000 = 61.3%, SPS: 3.8, ELAP: 22:13, ETA: 16:46 - loss: 24.7698\n",
            "6131/10000 = 61.3%, SPS: 3.8, ELAP: 22:13, ETA: 16:45 - loss: 24.3690\n",
            "6132/10000 = 61.3%, SPS: 3.8, ELAP: 22:13, ETA: 16:45 - loss: 24.7970\n",
            "6133/10000 = 61.3%, SPS: 3.8, ELAP: 22:14, ETA: 16:45 - loss: 24.4806\n",
            "6134/10000 = 61.3%, SPS: 3.8, ELAP: 22:14, ETA: 16:44 - loss: 24.2080\n",
            "6135/10000 = 61.4%, SPS: 3.8, ELAP: 22:14, ETA: 16:44 - loss: 24.9181\n",
            "6136/10000 = 61.4%, SPS: 3.8, ELAP: 22:14, ETA: 16:44 - loss: 24.6918\n",
            "6137/10000 = 61.4%, SPS: 3.8, ELAP: 22:15, ETA: 16:44 - loss: 24.5052\n",
            "6138/10000 = 61.4%, SPS: 3.8, ELAP: 22:15, ETA: 16:43 - loss: 24.1521\n",
            "6139/10000 = 61.4%, SPS: 3.8, ELAP: 22:15, ETA: 16:43 - loss: 25.0271\n",
            "6140/10000 = 61.4%, SPS: 3.8, ELAP: 22:15, ETA: 16:43 - loss: 24.2403\n",
            "6141/10000 = 61.4%, SPS: 3.8, ELAP: 22:16, ETA: 16:43 - loss: 24.1633\n",
            "6142/10000 = 61.4%, SPS: 3.8, ELAP: 22:16, ETA: 16:42 - loss: 24.4161\n",
            "6143/10000 = 61.4%, SPS: 3.8, ELAP: 22:16, ETA: 16:42 - loss: 24.5259\n",
            "6144/10000 = 61.4%, SPS: 3.8, ELAP: 22:16, ETA: 16:42 - loss: 24.7235\n",
            "6145/10000 = 61.5%, SPS: 3.8, ELAP: 22:17, ETA: 16:41 - loss: 25.3973\n",
            "6146/10000 = 61.5%, SPS: 3.8, ELAP: 22:17, ETA: 16:41 - loss: 24.3672\n",
            "6147/10000 = 61.5%, SPS: 3.8, ELAP: 22:17, ETA: 16:41 - loss: 24.7539\n",
            "6148/10000 = 61.5%, SPS: 3.8, ELAP: 22:17, ETA: 16:41 - loss: 24.2259\n",
            "6149/10000 = 61.5%, SPS: 3.8, ELAP: 22:18, ETA: 16:40 - loss: 23.7053\n",
            "6150/10000 = 61.5%, SPS: 3.8, ELAP: 22:18, ETA: 16:40 - loss: 24.2668\n",
            "6151/10000 = 61.5%, SPS: 3.8, ELAP: 22:18, ETA: 16:40 - loss: 24.4302\n",
            "6152/10000 = 61.5%, SPS: 3.8, ELAP: 22:18, ETA: 16:40 - loss: 24.3475\n",
            "6153/10000 = 61.5%, SPS: 3.8, ELAP: 22:18, ETA: 16:39 - loss: 24.1212\n",
            "6154/10000 = 61.5%, SPS: 3.9, ELAP: 22:19, ETA: 16:39 - loss: 24.2836\n",
            "6155/10000 = 61.5%, SPS: 3.9, ELAP: 22:19, ETA: 16:39 - loss: 24.4875\n",
            "6156/10000 = 61.6%, SPS: 3.9, ELAP: 22:19, ETA: 16:38 - loss: 23.9233\n",
            "6157/10000 = 61.6%, SPS: 3.9, ELAP: 22:19, ETA: 16:38 - loss: 24.6101\n",
            "6158/10000 = 61.6%, SPS: 3.9, ELAP: 22:20, ETA: 16:38 - loss: 24.3017\n",
            "6159/10000 = 61.6%, SPS: 3.9, ELAP: 22:20, ETA: 16:38 - loss: 24.6827\n",
            "6160/10000 = 61.6%, SPS: 3.9, ELAP: 22:20, ETA: 16:37 - loss: 23.9492\n",
            "6161/10000 = 61.6%, SPS: 3.9, ELAP: 22:20, ETA: 16:37 - loss: 25.1353\n",
            "6162/10000 = 61.6%, SPS: 3.9, ELAP: 22:21, ETA: 16:37 - loss: 24.2243\n",
            "6163/10000 = 61.6%, SPS: 3.9, ELAP: 22:21, ETA: 16:36 - loss: 24.6090\n",
            "6164/10000 = 61.6%, SPS: 3.9, ELAP: 22:21, ETA: 16:36 - loss: 24.2172\n",
            "6165/10000 = 61.6%, SPS: 3.9, ELAP: 22:21, ETA: 16:36 - loss: 24.4623\n",
            "6166/10000 = 61.7%, SPS: 3.9, ELAP: 22:21, ETA: 16:36 - loss: 24.4904\n",
            "6167/10000 = 61.7%, SPS: 3.9, ELAP: 22:22, ETA: 16:35 - loss: 24.2987\n",
            "6168/10000 = 61.7%, SPS: 3.9, ELAP: 22:22, ETA: 16:35 - loss: 24.3904\n",
            "6169/10000 = 61.7%, SPS: 3.9, ELAP: 22:22, ETA: 16:35 - loss: 24.2617\n",
            "6170/10000 = 61.7%, SPS: 3.9, ELAP: 22:22, ETA: 16:34 - loss: 24.8971\n",
            "6171/10000 = 61.7%, SPS: 3.9, ELAP: 22:23, ETA: 16:34 - loss: 24.4085\n",
            "6172/10000 = 61.7%, SPS: 3.9, ELAP: 22:23, ETA: 16:34 - loss: 24.4612\n",
            "6173/10000 = 61.7%, SPS: 3.9, ELAP: 22:23, ETA: 16:34 - loss: 24.7069\n",
            "6174/10000 = 61.7%, SPS: 3.9, ELAP: 22:23, ETA: 16:33 - loss: 24.3004\n",
            "6175/10000 = 61.8%, SPS: 3.9, ELAP: 22:23, ETA: 16:33 - loss: 24.5573\n",
            "6176/10000 = 61.8%, SPS: 3.9, ELAP: 22:24, ETA: 16:33 - loss: 24.5932\n",
            "6177/10000 = 61.8%, SPS: 3.9, ELAP: 22:24, ETA: 16:32 - loss: 24.0485\n",
            "6178/10000 = 61.8%, SPS: 3.9, ELAP: 22:24, ETA: 16:32 - loss: 24.4988\n",
            "6179/10000 = 61.8%, SPS: 3.9, ELAP: 22:24, ETA: 16:32 - loss: 24.1956\n",
            "6180/10000 = 61.8%, SPS: 3.9, ELAP: 22:25, ETA: 16:32 - loss: 24.2149\n",
            "6181/10000 = 61.8%, SPS: 3.9, ELAP: 22:25, ETA: 16:31 - loss: 23.9817\n",
            "6182/10000 = 61.8%, SPS: 3.9, ELAP: 22:25, ETA: 16:31 - loss: 24.7112\n",
            "6183/10000 = 61.8%, SPS: 3.9, ELAP: 22:25, ETA: 16:31 - loss: 24.3050\n",
            "6184/10000 = 61.8%, SPS: 3.9, ELAP: 22:26, ETA: 16:30 - loss: 23.9963\n",
            "6185/10000 = 61.9%, SPS: 3.9, ELAP: 22:26, ETA: 16:30 - loss: 23.9485\n",
            "6186/10000 = 61.9%, SPS: 3.9, ELAP: 22:26, ETA: 16:30 - loss: 24.0318\n",
            "6187/10000 = 61.9%, SPS: 3.9, ELAP: 22:26, ETA: 16:30 - loss: 24.9209\n",
            "6188/10000 = 61.9%, SPS: 3.9, ELAP: 22:26, ETA: 16:29 - loss: 24.1497\n",
            "6189/10000 = 61.9%, SPS: 3.9, ELAP: 22:27, ETA: 16:29 - loss: 24.6914\n",
            "6190/10000 = 61.9%, SPS: 3.9, ELAP: 22:27, ETA: 16:29 - loss: 24.4520\n",
            "6191/10000 = 61.9%, SPS: 3.9, ELAP: 22:27, ETA: 16:28 - loss: 23.7687\n",
            "6192/10000 = 61.9%, SPS: 3.9, ELAP: 22:27, ETA: 16:28 - loss: 24.5017\n",
            "6193/10000 = 61.9%, SPS: 3.9, ELAP: 22:28, ETA: 16:28 - loss: 24.1447\n",
            "6194/10000 = 61.9%, SPS: 3.9, ELAP: 22:28, ETA: 16:28 - loss: 24.0534\n",
            "6195/10000 = 62.0%, SPS: 3.9, ELAP: 22:28, ETA: 16:27 - loss: 24.0278\n",
            "6196/10000 = 62.0%, SPS: 3.9, ELAP: 22:28, ETA: 16:27 - loss: 24.8973\n",
            "6197/10000 = 62.0%, SPS: 3.9, ELAP: 22:28, ETA: 16:27 - loss: 24.1866\n",
            "6198/10000 = 62.0%, SPS: 3.9, ELAP: 22:29, ETA: 16:26 - loss: 24.6002\n",
            "6199/10000 = 62.0%, SPS: 3.9, ELAP: 22:29, ETA: 16:26 - loss: 24.5632\n",
            "6200/10000 = 62.0%, SPS: 3.8, ELAP: 22:31, ETA: 16:27 - loss: 24.4679\n",
            "6201/10000 = 62.0%, SPS: 3.8, ELAP: 22:32, ETA: 16:27 - loss: 24.7051\n",
            "6202/10000 = 62.0%, SPS: 3.8, ELAP: 22:32, ETA: 16:27 - loss: 24.2543\n",
            "6203/10000 = 62.0%, SPS: 3.8, ELAP: 22:32, ETA: 16:27 - loss: 23.7946\n",
            "6204/10000 = 62.0%, SPS: 3.8, ELAP: 22:32, ETA: 16:26 - loss: 24.0133\n",
            "6205/10000 = 62.0%, SPS: 3.8, ELAP: 22:32, ETA: 16:26 - loss: 24.0591\n",
            "6206/10000 = 62.1%, SPS: 3.8, ELAP: 22:33, ETA: 16:26 - loss: 24.0562\n",
            "6207/10000 = 62.1%, SPS: 3.8, ELAP: 22:33, ETA: 16:25 - loss: 24.5935\n",
            "6208/10000 = 62.1%, SPS: 3.8, ELAP: 22:33, ETA: 16:25 - loss: 24.2057\n",
            "6209/10000 = 62.1%, SPS: 3.8, ELAP: 22:33, ETA: 16:25 - loss: 24.5813\n",
            "6210/10000 = 62.1%, SPS: 3.8, ELAP: 22:34, ETA: 16:25 - loss: 24.0370\n",
            "6211/10000 = 62.1%, SPS: 3.8, ELAP: 22:34, ETA: 16:24 - loss: 24.3698\n",
            "6212/10000 = 62.1%, SPS: 3.8, ELAP: 22:34, ETA: 16:24 - loss: 24.5186\n",
            "6213/10000 = 62.1%, SPS: 3.8, ELAP: 22:34, ETA: 16:24 - loss: 24.5957\n",
            "6214/10000 = 62.1%, SPS: 3.8, ELAP: 22:34, ETA: 16:24 - loss: 24.4096\n",
            "6215/10000 = 62.1%, SPS: 3.8, ELAP: 22:35, ETA: 16:23 - loss: 24.6208\n",
            "6216/10000 = 62.2%, SPS: 3.8, ELAP: 22:35, ETA: 16:23 - loss: 24.3009\n",
            "6217/10000 = 62.2%, SPS: 3.8, ELAP: 22:35, ETA: 16:23 - loss: 24.3454\n",
            "6218/10000 = 62.2%, SPS: 3.8, ELAP: 22:35, ETA: 16:22 - loss: 24.2660\n",
            "6219/10000 = 62.2%, SPS: 3.8, ELAP: 22:36, ETA: 16:22 - loss: 24.0632\n",
            "6220/10000 = 62.2%, SPS: 3.9, ELAP: 22:36, ETA: 16:22 - loss: 23.9033\n",
            "6221/10000 = 62.2%, SPS: 3.9, ELAP: 22:36, ETA: 16:22 - loss: 24.1483\n",
            "6222/10000 = 62.2%, SPS: 3.9, ELAP: 22:36, ETA: 16:21 - loss: 24.1943\n",
            "6223/10000 = 62.2%, SPS: 3.9, ELAP: 22:37, ETA: 16:21 - loss: 23.7981\n",
            "6224/10000 = 62.2%, SPS: 3.9, ELAP: 22:37, ETA: 16:21 - loss: 24.2422\n",
            "6225/10000 = 62.2%, SPS: 3.9, ELAP: 22:37, ETA: 16:20 - loss: 24.4238\n",
            "6226/10000 = 62.3%, SPS: 3.9, ELAP: 22:37, ETA: 16:20 - loss: 23.9155\n",
            "6227/10000 = 62.3%, SPS: 3.9, ELAP: 22:37, ETA: 16:20 - loss: 24.4056\n",
            "6228/10000 = 62.3%, SPS: 3.9, ELAP: 22:38, ETA: 16:20 - loss: 24.6533\n",
            "6229/10000 = 62.3%, SPS: 3.9, ELAP: 22:38, ETA: 16:19 - loss: 24.3319\n",
            "6230/10000 = 62.3%, SPS: 3.9, ELAP: 22:38, ETA: 16:19 - loss: 24.3340\n",
            "6231/10000 = 62.3%, SPS: 3.9, ELAP: 22:38, ETA: 16:19 - loss: 24.2117\n",
            "6232/10000 = 62.3%, SPS: 3.9, ELAP: 22:39, ETA: 16:18 - loss: 23.8876\n",
            "6233/10000 = 62.3%, SPS: 3.9, ELAP: 22:39, ETA: 16:18 - loss: 24.5726\n",
            "6234/10000 = 62.3%, SPS: 3.9, ELAP: 22:39, ETA: 16:18 - loss: 24.2503\n",
            "6235/10000 = 62.4%, SPS: 3.9, ELAP: 22:39, ETA: 16:18 - loss: 23.9471\n",
            "6236/10000 = 62.4%, SPS: 3.9, ELAP: 22:40, ETA: 16:17 - loss: 24.5136\n",
            "6237/10000 = 62.4%, SPS: 3.9, ELAP: 22:40, ETA: 16:17 - loss: 24.1267\n",
            "6238/10000 = 62.4%, SPS: 3.9, ELAP: 22:40, ETA: 16:17 - loss: 24.2997\n",
            "6239/10000 = 62.4%, SPS: 3.9, ELAP: 22:40, ETA: 16:17 - loss: 24.7834\n",
            "6240/10000 = 62.4%, SPS: 3.9, ELAP: 22:41, ETA: 16:16 - loss: 24.6540\n",
            "6241/10000 = 62.4%, SPS: 3.9, ELAP: 22:41, ETA: 16:16 - loss: 24.5600\n",
            "6242/10000 = 62.4%, SPS: 3.9, ELAP: 22:41, ETA: 16:16 - loss: 24.4546\n",
            "6243/10000 = 62.4%, SPS: 3.9, ELAP: 22:41, ETA: 16:15 - loss: 24.4507\n",
            "6244/10000 = 62.4%, SPS: 3.9, ELAP: 22:41, ETA: 16:15 - loss: 24.2945\n",
            "6245/10000 = 62.5%, SPS: 3.9, ELAP: 22:42, ETA: 16:15 - loss: 23.9237\n",
            "6246/10000 = 62.5%, SPS: 3.9, ELAP: 22:42, ETA: 16:15 - loss: 24.1105\n",
            "6247/10000 = 62.5%, SPS: 3.9, ELAP: 22:42, ETA: 16:14 - loss: 25.2250\n",
            "6248/10000 = 62.5%, SPS: 3.9, ELAP: 22:42, ETA: 16:14 - loss: 24.6737\n",
            "6249/10000 = 62.5%, SPS: 3.9, ELAP: 22:43, ETA: 16:14 - loss: 24.4953\n",
            "6250/10000 = 62.5%, SPS: 3.9, ELAP: 22:43, ETA: 16:13 - loss: 24.3032\n",
            "6251/10000 = 62.5%, SPS: 3.9, ELAP: 22:43, ETA: 16:13 - loss: 24.6444\n",
            "6252/10000 = 62.5%, SPS: 3.9, ELAP: 22:43, ETA: 16:13 - loss: 23.9092\n",
            "6253/10000 = 62.5%, SPS: 3.9, ELAP: 22:44, ETA: 16:13 - loss: 24.5891\n",
            "6254/10000 = 62.5%, SPS: 3.9, ELAP: 22:44, ETA: 16:12 - loss: 24.2737\n",
            "6255/10000 = 62.5%, SPS: 3.9, ELAP: 22:44, ETA: 16:12 - loss: 24.5335\n",
            "6256/10000 = 62.6%, SPS: 3.9, ELAP: 22:44, ETA: 16:12 - loss: 24.3307\n",
            "6257/10000 = 62.6%, SPS: 3.9, ELAP: 22:45, ETA: 16:12 - loss: 24.3308\n",
            "6258/10000 = 62.6%, SPS: 3.9, ELAP: 22:45, ETA: 16:11 - loss: 24.6726\n",
            "6259/10000 = 62.6%, SPS: 3.9, ELAP: 22:45, ETA: 16:11 - loss: 23.7897\n",
            "6260/10000 = 62.6%, SPS: 3.9, ELAP: 22:45, ETA: 16:11 - loss: 24.5727\n",
            "6261/10000 = 62.6%, SPS: 3.9, ELAP: 22:45, ETA: 16:10 - loss: 24.8457\n",
            "6262/10000 = 62.6%, SPS: 3.9, ELAP: 22:46, ETA: 16:10 - loss: 24.4368\n",
            "6263/10000 = 62.6%, SPS: 3.9, ELAP: 22:46, ETA: 16:10 - loss: 23.6327\n",
            "6264/10000 = 62.6%, SPS: 3.9, ELAP: 22:46, ETA: 16:10 - loss: 24.0402\n",
            "6265/10000 = 62.6%, SPS: 3.9, ELAP: 22:46, ETA: 16:09 - loss: 24.5603\n",
            "6266/10000 = 62.7%, SPS: 3.9, ELAP: 22:47, ETA: 16:09 - loss: 24.0128\n",
            "6267/10000 = 62.7%, SPS: 3.9, ELAP: 22:47, ETA: 16:09 - loss: 23.8569\n",
            "6268/10000 = 62.7%, SPS: 3.9, ELAP: 22:47, ETA: 16:08 - loss: 24.5613\n",
            "6269/10000 = 62.7%, SPS: 3.9, ELAP: 22:47, ETA: 16:08 - loss: 24.4715\n",
            "6270/10000 = 62.7%, SPS: 3.9, ELAP: 22:47, ETA: 16:08 - loss: 24.4517\n",
            "6271/10000 = 62.7%, SPS: 3.9, ELAP: 22:48, ETA: 16:08 - loss: 24.2299\n",
            "6272/10000 = 62.7%, SPS: 3.9, ELAP: 22:48, ETA: 16:07 - loss: 24.9622\n",
            "6273/10000 = 62.7%, SPS: 3.9, ELAP: 22:48, ETA: 16:07 - loss: 23.8529\n",
            "6274/10000 = 62.7%, SPS: 3.9, ELAP: 22:48, ETA: 16:07 - loss: 24.2487\n",
            "6275/10000 = 62.8%, SPS: 3.9, ELAP: 22:49, ETA: 16:06 - loss: 24.4064\n",
            "6276/10000 = 62.8%, SPS: 3.9, ELAP: 22:49, ETA: 16:06 - loss: 24.7117\n",
            "6277/10000 = 62.8%, SPS: 3.9, ELAP: 22:49, ETA: 16:06 - loss: 24.6325\n",
            "6278/10000 = 62.8%, SPS: 3.9, ELAP: 22:49, ETA: 16:06 - loss: 24.5690\n",
            "6279/10000 = 62.8%, SPS: 3.9, ELAP: 22:50, ETA: 16:05 - loss: 24.9794\n",
            "6280/10000 = 62.8%, SPS: 3.9, ELAP: 22:50, ETA: 16:05 - loss: 24.3125\n",
            "6281/10000 = 62.8%, SPS: 3.9, ELAP: 22:50, ETA: 16:05 - loss: 24.6851\n",
            "6282/10000 = 62.8%, SPS: 3.9, ELAP: 22:50, ETA: 16:05 - loss: 24.6069\n",
            "6283/10000 = 62.8%, SPS: 3.9, ELAP: 22:50, ETA: 16:04 - loss: 23.8160\n",
            "6284/10000 = 62.8%, SPS: 3.9, ELAP: 22:51, ETA: 16:04 - loss: 24.3696\n",
            "6285/10000 = 62.9%, SPS: 3.9, ELAP: 22:51, ETA: 16:04 - loss: 24.7963\n",
            "6286/10000 = 62.9%, SPS: 3.9, ELAP: 22:51, ETA: 16:03 - loss: 24.6788\n",
            "6287/10000 = 62.9%, SPS: 3.9, ELAP: 22:51, ETA: 16:03 - loss: 24.3764\n",
            "6288/10000 = 62.9%, SPS: 3.9, ELAP: 22:52, ETA: 16:03 - loss: 24.1946\n",
            "6289/10000 = 62.9%, SPS: 3.9, ELAP: 22:52, ETA: 16:03 - loss: 24.8476\n",
            "6290/10000 = 62.9%, SPS: 3.9, ELAP: 22:52, ETA: 16:02 - loss: 24.6267\n",
            "6291/10000 = 62.9%, SPS: 3.9, ELAP: 22:52, ETA: 16:02 - loss: 24.4201\n",
            "6292/10000 = 62.9%, SPS: 3.9, ELAP: 22:53, ETA: 16:02 - loss: 24.1227\n",
            "6293/10000 = 62.9%, SPS: 3.9, ELAP: 22:53, ETA: 16:01 - loss: 24.3223\n",
            "6294/10000 = 62.9%, SPS: 3.9, ELAP: 22:53, ETA: 16:01 - loss: 24.6858\n",
            "6295/10000 = 63.0%, SPS: 3.9, ELAP: 22:53, ETA: 16:01 - loss: 23.6985\n",
            "6296/10000 = 63.0%, SPS: 3.9, ELAP: 22:53, ETA: 16:01 - loss: 24.4120\n",
            "6297/10000 = 63.0%, SPS: 3.9, ELAP: 22:54, ETA: 16:00 - loss: 24.3941\n",
            "6298/10000 = 63.0%, SPS: 3.9, ELAP: 22:54, ETA: 16:00 - loss: 23.9638\n",
            "6299/10000 = 63.0%, SPS: 3.9, ELAP: 22:54, ETA: 16:00 - loss: 23.5994\n",
            "6300/10000 = 63.0%, SPS: 3.9, ELAP: 22:57, ETA: 16:01 - loss: 23.6116\n",
            "6301/10000 = 63.0%, SPS: 3.9, ELAP: 22:57, ETA: 16:01 - loss: 24.2620\n",
            "6302/10000 = 63.0%, SPS: 3.9, ELAP: 22:57, ETA: 16:00 - loss: 24.2116\n",
            "6303/10000 = 63.0%, SPS: 3.9, ELAP: 22:57, ETA: 16:00 - loss: 25.1221\n",
            "6304/10000 = 63.0%, SPS: 3.9, ELAP: 22:57, ETA: 16:00 - loss: 24.6082\n",
            "6305/10000 = 63.0%, SPS: 3.9, ELAP: 22:58, ETA: 16:00 - loss: 24.4393\n",
            "6306/10000 = 63.1%, SPS: 3.9, ELAP: 22:58, ETA: 15:59 - loss: 24.0750\n",
            "6307/10000 = 63.1%, SPS: 3.9, ELAP: 22:58, ETA: 15:59 - loss: 24.1883\n",
            "6308/10000 = 63.1%, SPS: 3.9, ELAP: 22:58, ETA: 15:59 - loss: 24.1456\n",
            "6309/10000 = 63.1%, SPS: 3.9, ELAP: 22:59, ETA: 15:58 - loss: 24.1560\n",
            "6310/10000 = 63.1%, SPS: 3.9, ELAP: 22:59, ETA: 15:58 - loss: 24.1578\n",
            "6311/10000 = 63.1%, SPS: 3.9, ELAP: 22:59, ETA: 15:58 - loss: 23.9194\n",
            "6312/10000 = 63.1%, SPS: 3.9, ELAP: 22:59, ETA: 15:58 - loss: 24.2442\n",
            "6313/10000 = 63.1%, SPS: 3.9, ELAP: 23:00, ETA: 15:57 - loss: 24.5706\n",
            "6314/10000 = 63.1%, SPS: 3.9, ELAP: 23:00, ETA: 15:57 - loss: 23.8223\n",
            "6315/10000 = 63.1%, SPS: 3.9, ELAP: 23:00, ETA: 15:57 - loss: 24.3574\n",
            "6316/10000 = 63.2%, SPS: 3.9, ELAP: 23:00, ETA: 15:56 - loss: 24.2107\n",
            "6317/10000 = 63.2%, SPS: 3.9, ELAP: 23:00, ETA: 15:56 - loss: 24.6368\n",
            "6318/10000 = 63.2%, SPS: 3.9, ELAP: 23:01, ETA: 15:56 - loss: 24.1622\n",
            "6319/10000 = 63.2%, SPS: 3.9, ELAP: 23:01, ETA: 15:56 - loss: 23.9540\n",
            "6320/10000 = 63.2%, SPS: 3.9, ELAP: 23:01, ETA: 15:55 - loss: 25.0234\n",
            "6321/10000 = 63.2%, SPS: 3.9, ELAP: 23:01, ETA: 15:55 - loss: 24.7535\n",
            "6322/10000 = 63.2%, SPS: 3.9, ELAP: 23:02, ETA: 15:55 - loss: 24.0627\n",
            "6323/10000 = 63.2%, SPS: 3.9, ELAP: 23:02, ETA: 15:55 - loss: 24.2082\n",
            "6324/10000 = 63.2%, SPS: 3.9, ELAP: 23:02, ETA: 15:54 - loss: 24.2102\n",
            "6325/10000 = 63.2%, SPS: 3.9, ELAP: 23:02, ETA: 15:54 - loss: 24.6532\n",
            "6326/10000 = 63.3%, SPS: 3.9, ELAP: 23:03, ETA: 15:54 - loss: 24.2074\n",
            "6327/10000 = 63.3%, SPS: 3.9, ELAP: 23:03, ETA: 15:53 - loss: 25.1242\n",
            "6328/10000 = 63.3%, SPS: 3.9, ELAP: 23:03, ETA: 15:53 - loss: 24.1956\n",
            "6329/10000 = 63.3%, SPS: 3.9, ELAP: 23:03, ETA: 15:53 - loss: 23.9551\n",
            "6330/10000 = 63.3%, SPS: 3.9, ELAP: 23:03, ETA: 15:53 - loss: 23.8197\n",
            "6331/10000 = 63.3%, SPS: 3.9, ELAP: 23:04, ETA: 15:52 - loss: 24.1405\n",
            "6332/10000 = 63.3%, SPS: 3.9, ELAP: 23:04, ETA: 15:52 - loss: 24.2894\n",
            "6333/10000 = 63.3%, SPS: 3.9, ELAP: 23:04, ETA: 15:52 - loss: 24.1595\n",
            "6334/10000 = 63.3%, SPS: 3.9, ELAP: 23:04, ETA: 15:51 - loss: 24.4244\n",
            "6335/10000 = 63.4%, SPS: 3.9, ELAP: 23:05, ETA: 15:51 - loss: 24.5692\n",
            "6336/10000 = 63.4%, SPS: 3.9, ELAP: 23:05, ETA: 15:51 - loss: 24.0865\n",
            "6337/10000 = 63.4%, SPS: 3.9, ELAP: 23:05, ETA: 15:51 - loss: 24.7903\n",
            "6338/10000 = 63.4%, SPS: 3.9, ELAP: 23:05, ETA: 15:50 - loss: 25.0637\n",
            "6339/10000 = 63.4%, SPS: 3.9, ELAP: 23:06, ETA: 15:50 - loss: 24.4212\n",
            "6340/10000 = 63.4%, SPS: 3.9, ELAP: 23:06, ETA: 15:50 - loss: 24.5532\n",
            "6341/10000 = 63.4%, SPS: 3.9, ELAP: 23:06, ETA: 15:50 - loss: 24.5056\n",
            "6342/10000 = 63.4%, SPS: 3.9, ELAP: 23:06, ETA: 15:49 - loss: 24.5303\n",
            "6343/10000 = 63.4%, SPS: 3.9, ELAP: 23:07, ETA: 15:49 - loss: 24.1404\n",
            "6344/10000 = 63.4%, SPS: 3.9, ELAP: 23:07, ETA: 15:49 - loss: 24.0313\n",
            "6345/10000 = 63.5%, SPS: 3.9, ELAP: 23:07, ETA: 15:48 - loss: 23.8985\n",
            "6346/10000 = 63.5%, SPS: 3.9, ELAP: 23:07, ETA: 15:48 - loss: 24.3130\n",
            "6347/10000 = 63.5%, SPS: 3.9, ELAP: 23:08, ETA: 15:48 - loss: 24.5112\n",
            "6348/10000 = 63.5%, SPS: 3.9, ELAP: 23:08, ETA: 15:48 - loss: 24.3621\n",
            "6349/10000 = 63.5%, SPS: 3.9, ELAP: 23:08, ETA: 15:47 - loss: 24.2889\n",
            "6350/10000 = 63.5%, SPS: 3.9, ELAP: 23:08, ETA: 15:47 - loss: 24.3181\n",
            "6351/10000 = 63.5%, SPS: 3.9, ELAP: 23:08, ETA: 15:47 - loss: 23.8524\n",
            "6352/10000 = 63.5%, SPS: 3.9, ELAP: 23:09, ETA: 15:47 - loss: 24.1051\n",
            "6353/10000 = 63.5%, SPS: 3.9, ELAP: 23:09, ETA: 15:46 - loss: 24.2319\n",
            "6354/10000 = 63.5%, SPS: 3.9, ELAP: 23:09, ETA: 15:46 - loss: 24.4416\n",
            "6355/10000 = 63.5%, SPS: 3.9, ELAP: 23:09, ETA: 15:46 - loss: 24.0537\n",
            "6356/10000 = 63.6%, SPS: 3.9, ELAP: 23:10, ETA: 15:45 - loss: 24.5977\n",
            "6357/10000 = 63.6%, SPS: 3.9, ELAP: 23:10, ETA: 15:45 - loss: 24.5339\n",
            "6358/10000 = 63.6%, SPS: 3.9, ELAP: 23:10, ETA: 15:45 - loss: 23.6947\n",
            "6359/10000 = 63.6%, SPS: 3.9, ELAP: 23:10, ETA: 15:45 - loss: 24.5364\n",
            "6360/10000 = 63.6%, SPS: 3.9, ELAP: 23:11, ETA: 15:44 - loss: 23.8307\n",
            "6361/10000 = 63.6%, SPS: 3.9, ELAP: 23:11, ETA: 15:44 - loss: 24.2931\n",
            "6362/10000 = 63.6%, SPS: 3.9, ELAP: 23:11, ETA: 15:44 - loss: 23.8657\n",
            "6363/10000 = 63.6%, SPS: 3.9, ELAP: 23:11, ETA: 15:43 - loss: 24.0793\n",
            "6364/10000 = 63.6%, SPS: 3.9, ELAP: 23:11, ETA: 15:43 - loss: 23.7249\n",
            "6365/10000 = 63.6%, SPS: 3.9, ELAP: 23:12, ETA: 15:43 - loss: 24.5974\n",
            "6366/10000 = 63.7%, SPS: 3.9, ELAP: 23:12, ETA: 15:43 - loss: 25.2005\n",
            "6367/10000 = 63.7%, SPS: 3.9, ELAP: 23:12, ETA: 15:42 - loss: 24.0562\n",
            "6368/10000 = 63.7%, SPS: 3.9, ELAP: 23:12, ETA: 15:42 - loss: 24.9574\n",
            "6369/10000 = 63.7%, SPS: 3.9, ELAP: 23:13, ETA: 15:42 - loss: 24.7026\n",
            "6370/10000 = 63.7%, SPS: 3.9, ELAP: 23:13, ETA: 15:42 - loss: 24.1227\n",
            "6371/10000 = 63.7%, SPS: 3.9, ELAP: 23:13, ETA: 15:41 - loss: 24.5112\n",
            "6372/10000 = 63.7%, SPS: 3.9, ELAP: 23:13, ETA: 15:41 - loss: 24.4636\n",
            "6373/10000 = 63.7%, SPS: 3.9, ELAP: 23:13, ETA: 15:41 - loss: 25.0406\n",
            "6374/10000 = 63.7%, SPS: 3.9, ELAP: 23:14, ETA: 15:40 - loss: 24.4098\n",
            "6375/10000 = 63.8%, SPS: 3.9, ELAP: 23:14, ETA: 15:40 - loss: 23.9528\n",
            "6376/10000 = 63.8%, SPS: 3.9, ELAP: 23:14, ETA: 15:40 - loss: 24.5212\n",
            "6377/10000 = 63.8%, SPS: 3.9, ELAP: 23:14, ETA: 15:40 - loss: 24.1497\n",
            "6378/10000 = 63.8%, SPS: 3.9, ELAP: 23:15, ETA: 15:39 - loss: 25.1531\n",
            "6379/10000 = 63.8%, SPS: 3.9, ELAP: 23:15, ETA: 15:39 - loss: 24.4186\n",
            "6380/10000 = 63.8%, SPS: 3.9, ELAP: 23:15, ETA: 15:39 - loss: 24.4420\n",
            "6381/10000 = 63.8%, SPS: 3.9, ELAP: 23:15, ETA: 15:38 - loss: 24.5682\n",
            "6382/10000 = 63.8%, SPS: 3.9, ELAP: 23:16, ETA: 15:38 - loss: 24.4408\n",
            "6383/10000 = 63.8%, SPS: 3.9, ELAP: 23:16, ETA: 15:38 - loss: 24.5608\n",
            "6384/10000 = 63.8%, SPS: 3.9, ELAP: 23:16, ETA: 15:38 - loss: 24.1001\n",
            "6385/10000 = 63.9%, SPS: 3.9, ELAP: 23:16, ETA: 15:37 - loss: 24.5634\n",
            "6386/10000 = 63.9%, SPS: 3.9, ELAP: 23:16, ETA: 15:37 - loss: 24.5186\n",
            "6387/10000 = 63.9%, SPS: 3.9, ELAP: 23:17, ETA: 15:37 - loss: 24.1727\n",
            "6388/10000 = 63.9%, SPS: 3.9, ELAP: 23:17, ETA: 15:36 - loss: 24.2026\n",
            "6389/10000 = 63.9%, SPS: 3.9, ELAP: 23:17, ETA: 15:36 - loss: 24.4386\n",
            "6390/10000 = 63.9%, SPS: 3.9, ELAP: 23:17, ETA: 15:36 - loss: 24.8879\n",
            "6391/10000 = 63.9%, SPS: 3.9, ELAP: 23:18, ETA: 15:36 - loss: 24.0259\n",
            "6392/10000 = 63.9%, SPS: 3.9, ELAP: 23:18, ETA: 15:35 - loss: 24.7853\n",
            "6393/10000 = 63.9%, SPS: 3.9, ELAP: 23:18, ETA: 15:35 - loss: 24.4939\n",
            "6394/10000 = 63.9%, SPS: 3.9, ELAP: 23:18, ETA: 15:35 - loss: 24.1490\n",
            "6395/10000 = 64.0%, SPS: 3.9, ELAP: 23:19, ETA: 15:35 - loss: 23.8285\n",
            "6396/10000 = 64.0%, SPS: 3.9, ELAP: 23:19, ETA: 15:34 - loss: 24.0971\n",
            "6397/10000 = 64.0%, SPS: 3.9, ELAP: 23:19, ETA: 15:34 - loss: 23.9966\n",
            "6398/10000 = 64.0%, SPS: 3.9, ELAP: 23:19, ETA: 15:34 - loss: 24.4960\n",
            "6399/10000 = 64.0%, SPS: 3.9, ELAP: 23:19, ETA: 15:33 - loss: 24.5282\n",
            "6400/10000 = 64.0%, SPS: 3.9, ELAP: 23:22, ETA: 15:35 - loss: 23.8330\n",
            "6401/10000 = 64.0%, SPS: 3.9, ELAP: 23:22, ETA: 15:34 - loss: 24.5239\n",
            "6402/10000 = 64.0%, SPS: 3.9, ELAP: 23:22, ETA: 15:34 - loss: 24.2794\n",
            "6403/10000 = 64.0%, SPS: 3.9, ELAP: 23:22, ETA: 15:34 - loss: 24.2895\n",
            "6404/10000 = 64.0%, SPS: 3.9, ELAP: 23:23, ETA: 15:33 - loss: 24.1957\n",
            "6405/10000 = 64.0%, SPS: 3.9, ELAP: 23:23, ETA: 15:33 - loss: 24.4252\n",
            "6406/10000 = 64.1%, SPS: 3.9, ELAP: 23:23, ETA: 15:33 - loss: 24.5392\n",
            "6407/10000 = 64.1%, SPS: 3.9, ELAP: 23:23, ETA: 15:33 - loss: 23.7919\n",
            "6408/10000 = 64.1%, SPS: 3.9, ELAP: 23:24, ETA: 15:32 - loss: 24.3447\n",
            "6409/10000 = 64.1%, SPS: 3.9, ELAP: 23:24, ETA: 15:32 - loss: 24.3346\n",
            "6410/10000 = 64.1%, SPS: 3.9, ELAP: 23:24, ETA: 15:32 - loss: 24.3550\n",
            "6411/10000 = 64.1%, SPS: 3.9, ELAP: 23:24, ETA: 15:31 - loss: 23.6203\n",
            "6412/10000 = 64.1%, SPS: 3.9, ELAP: 23:25, ETA: 15:31 - loss: 24.7801\n",
            "6413/10000 = 64.1%, SPS: 3.9, ELAP: 23:25, ETA: 15:31 - loss: 24.1721\n",
            "6414/10000 = 64.1%, SPS: 3.9, ELAP: 23:25, ETA: 15:31 - loss: 23.7741\n",
            "6415/10000 = 64.2%, SPS: 3.9, ELAP: 23:25, ETA: 15:30 - loss: 24.5783\n",
            "6416/10000 = 64.2%, SPS: 3.9, ELAP: 23:26, ETA: 15:30 - loss: 24.6228\n",
            "6417/10000 = 64.2%, SPS: 3.9, ELAP: 23:26, ETA: 15:30 - loss: 24.5333\n",
            "6418/10000 = 64.2%, SPS: 3.9, ELAP: 23:26, ETA: 15:30 - loss: 23.8315\n",
            "6419/10000 = 64.2%, SPS: 3.9, ELAP: 23:26, ETA: 15:29 - loss: 23.6061\n",
            "6420/10000 = 64.2%, SPS: 3.9, ELAP: 23:26, ETA: 15:29 - loss: 24.4403\n",
            "6421/10000 = 64.2%, SPS: 3.9, ELAP: 23:27, ETA: 15:29 - loss: 24.7372\n",
            "6422/10000 = 64.2%, SPS: 3.9, ELAP: 23:27, ETA: 15:28 - loss: 24.6606\n",
            "6423/10000 = 64.2%, SPS: 3.9, ELAP: 23:27, ETA: 15:28 - loss: 24.5093\n",
            "6424/10000 = 64.2%, SPS: 3.9, ELAP: 23:27, ETA: 15:28 - loss: 24.1007\n",
            "6425/10000 = 64.2%, SPS: 3.9, ELAP: 23:28, ETA: 15:28 - loss: 24.1273\n",
            "6426/10000 = 64.3%, SPS: 3.9, ELAP: 23:28, ETA: 15:27 - loss: 24.4923\n",
            "6427/10000 = 64.3%, SPS: 3.9, ELAP: 23:28, ETA: 15:27 - loss: 24.5809\n",
            "6428/10000 = 64.3%, SPS: 3.9, ELAP: 23:28, ETA: 15:27 - loss: 25.3444\n",
            "6429/10000 = 64.3%, SPS: 3.9, ELAP: 23:28, ETA: 15:26 - loss: 25.4698\n",
            "6430/10000 = 64.3%, SPS: 3.9, ELAP: 23:29, ETA: 15:26 - loss: 23.9026\n",
            "6431/10000 = 64.3%, SPS: 3.9, ELAP: 23:29, ETA: 15:26 - loss: 24.5201\n",
            "6432/10000 = 64.3%, SPS: 3.9, ELAP: 23:29, ETA: 15:26 - loss: 24.4712\n",
            "6433/10000 = 64.3%, SPS: 3.9, ELAP: 23:29, ETA: 15:25 - loss: 24.4493\n",
            "6434/10000 = 64.3%, SPS: 3.9, ELAP: 23:30, ETA: 15:25 - loss: 24.7797\n",
            "6435/10000 = 64.3%, SPS: 3.9, ELAP: 23:30, ETA: 15:25 - loss: 24.4188\n",
            "6436/10000 = 64.4%, SPS: 3.9, ELAP: 23:30, ETA: 15:25 - loss: 24.0070\n",
            "6437/10000 = 64.4%, SPS: 3.9, ELAP: 23:30, ETA: 15:24 - loss: 24.1109\n",
            "6438/10000 = 64.4%, SPS: 3.9, ELAP: 23:31, ETA: 15:24 - loss: 24.3677\n",
            "6439/10000 = 64.4%, SPS: 3.9, ELAP: 23:31, ETA: 15:24 - loss: 24.4596\n",
            "6440/10000 = 64.4%, SPS: 3.9, ELAP: 23:31, ETA: 15:23 - loss: 24.1528\n",
            "6441/10000 = 64.4%, SPS: 3.9, ELAP: 23:31, ETA: 15:23 - loss: 23.9999\n",
            "6442/10000 = 64.4%, SPS: 3.9, ELAP: 23:32, ETA: 15:23 - loss: 24.4347\n",
            "6443/10000 = 64.4%, SPS: 3.9, ELAP: 23:32, ETA: 15:23 - loss: 24.9295\n",
            "6444/10000 = 64.4%, SPS: 3.9, ELAP: 23:32, ETA: 15:22 - loss: 24.3715\n",
            "6445/10000 = 64.5%, SPS: 3.9, ELAP: 23:32, ETA: 15:22 - loss: 23.8228\n",
            "6446/10000 = 64.5%, SPS: 3.9, ELAP: 23:33, ETA: 15:22 - loss: 24.4193\n",
            "6447/10000 = 64.5%, SPS: 3.9, ELAP: 23:33, ETA: 15:22 - loss: 25.1179\n",
            "6448/10000 = 64.5%, SPS: 3.9, ELAP: 23:33, ETA: 15:21 - loss: 24.1921\n",
            "6449/10000 = 64.5%, SPS: 3.9, ELAP: 23:33, ETA: 15:21 - loss: 24.1734\n",
            "6450/10000 = 64.5%, SPS: 3.9, ELAP: 23:33, ETA: 15:21 - loss: 24.1982\n",
            "6451/10000 = 64.5%, SPS: 3.9, ELAP: 23:34, ETA: 15:20 - loss: 24.2067\n",
            "6452/10000 = 64.5%, SPS: 3.9, ELAP: 23:34, ETA: 15:20 - loss: 24.8242\n",
            "6453/10000 = 64.5%, SPS: 3.9, ELAP: 23:34, ETA: 15:20 - loss: 24.1507\n",
            "6454/10000 = 64.5%, SPS: 3.9, ELAP: 23:34, ETA: 15:20 - loss: 24.2108\n",
            "6455/10000 = 64.5%, SPS: 3.9, ELAP: 23:35, ETA: 15:19 - loss: 24.7841\n",
            "6456/10000 = 64.6%, SPS: 3.9, ELAP: 23:35, ETA: 15:19 - loss: 24.6775\n",
            "6457/10000 = 64.6%, SPS: 3.9, ELAP: 23:35, ETA: 15:19 - loss: 23.9458\n",
            "6458/10000 = 64.6%, SPS: 3.9, ELAP: 23:35, ETA: 15:18 - loss: 24.5924\n",
            "6459/10000 = 64.6%, SPS: 3.9, ELAP: 23:36, ETA: 15:18 - loss: 24.8117\n",
            "6460/10000 = 64.6%, SPS: 3.9, ELAP: 23:36, ETA: 15:18 - loss: 24.5959\n",
            "6461/10000 = 64.6%, SPS: 3.9, ELAP: 23:36, ETA: 15:18 - loss: 24.3388\n",
            "6462/10000 = 64.6%, SPS: 3.9, ELAP: 23:36, ETA: 15:17 - loss: 24.3942\n",
            "6463/10000 = 64.6%, SPS: 3.9, ELAP: 23:36, ETA: 15:17 - loss: 24.1699\n",
            "6464/10000 = 64.6%, SPS: 3.9, ELAP: 23:37, ETA: 15:17 - loss: 24.7421\n",
            "6465/10000 = 64.7%, SPS: 3.9, ELAP: 23:37, ETA: 15:17 - loss: 24.3602\n",
            "6466/10000 = 64.7%, SPS: 3.9, ELAP: 23:37, ETA: 15:16 - loss: 24.1928\n",
            "6467/10000 = 64.7%, SPS: 3.9, ELAP: 23:37, ETA: 15:16 - loss: 24.0016\n",
            "6468/10000 = 64.7%, SPS: 3.9, ELAP: 23:38, ETA: 15:16 - loss: 24.2762\n",
            "6469/10000 = 64.7%, SPS: 3.9, ELAP: 23:38, ETA: 15:15 - loss: 24.6182\n",
            "6470/10000 = 64.7%, SPS: 3.9, ELAP: 23:38, ETA: 15:15 - loss: 24.5930\n",
            "6471/10000 = 64.7%, SPS: 3.9, ELAP: 23:38, ETA: 15:15 - loss: 24.5316\n",
            "6472/10000 = 64.7%, SPS: 3.9, ELAP: 23:39, ETA: 15:15 - loss: 25.3036\n",
            "6473/10000 = 64.7%, SPS: 3.9, ELAP: 23:39, ETA: 15:14 - loss: 24.7844\n",
            "6474/10000 = 64.7%, SPS: 3.9, ELAP: 23:39, ETA: 15:14 - loss: 24.9777\n",
            "6475/10000 = 64.8%, SPS: 3.9, ELAP: 23:39, ETA: 15:14 - loss: 23.8334\n",
            "6476/10000 = 64.8%, SPS: 3.9, ELAP: 23:39, ETA: 15:13 - loss: 24.2799\n",
            "6477/10000 = 64.8%, SPS: 3.9, ELAP: 23:40, ETA: 15:13 - loss: 23.7162\n",
            "6478/10000 = 64.8%, SPS: 3.9, ELAP: 23:40, ETA: 15:13 - loss: 24.9617\n",
            "6479/10000 = 64.8%, SPS: 3.9, ELAP: 23:40, ETA: 15:13 - loss: 23.9377\n",
            "6480/10000 = 64.8%, SPS: 3.9, ELAP: 23:40, ETA: 15:12 - loss: 24.7084\n",
            "6481/10000 = 64.8%, SPS: 3.9, ELAP: 23:41, ETA: 15:12 - loss: 24.0562\n",
            "6482/10000 = 64.8%, SPS: 3.9, ELAP: 23:41, ETA: 15:12 - loss: 24.6527\n",
            "6483/10000 = 64.8%, SPS: 3.9, ELAP: 23:41, ETA: 15:12 - loss: 24.7910\n",
            "6484/10000 = 64.8%, SPS: 3.9, ELAP: 23:41, ETA: 15:11 - loss: 24.4128\n",
            "6485/10000 = 64.8%, SPS: 3.9, ELAP: 23:42, ETA: 15:11 - loss: 24.1971\n",
            "6486/10000 = 64.9%, SPS: 3.9, ELAP: 23:42, ETA: 15:11 - loss: 24.7818\n",
            "6487/10000 = 64.9%, SPS: 3.9, ELAP: 23:42, ETA: 15:10 - loss: 23.7425\n",
            "6488/10000 = 64.9%, SPS: 3.9, ELAP: 23:42, ETA: 15:10 - loss: 23.4412\n",
            "6489/10000 = 64.9%, SPS: 3.9, ELAP: 23:42, ETA: 15:10 - loss: 24.3889\n",
            "6490/10000 = 64.9%, SPS: 3.9, ELAP: 23:43, ETA: 15:10 - loss: 24.7889\n",
            "6491/10000 = 64.9%, SPS: 3.9, ELAP: 23:43, ETA: 15:09 - loss: 23.8625\n",
            "6492/10000 = 64.9%, SPS: 3.9, ELAP: 23:43, ETA: 15:09 - loss: 24.8642\n",
            "6493/10000 = 64.9%, SPS: 3.9, ELAP: 23:43, ETA: 15:09 - loss: 24.2217\n",
            "6494/10000 = 64.9%, SPS: 3.9, ELAP: 23:44, ETA: 15:08 - loss: 24.7771\n",
            "6495/10000 = 65.0%, SPS: 3.9, ELAP: 23:44, ETA: 15:08 - loss: 24.0842\n",
            "6496/10000 = 65.0%, SPS: 3.9, ELAP: 23:44, ETA: 15:08 - loss: 24.4103\n",
            "6497/10000 = 65.0%, SPS: 3.9, ELAP: 23:44, ETA: 15:08 - loss: 23.6033\n",
            "6498/10000 = 65.0%, SPS: 3.9, ELAP: 23:44, ETA: 15:07 - loss: 24.4905\n",
            "6499/10000 = 65.0%, SPS: 3.9, ELAP: 23:45, ETA: 15:07 - loss: 23.8229\n",
            "6500/10000 = 65.0%, SPS: 3.9, ELAP: 23:47, ETA: 15:08 - loss: 24.2192\n",
            "6501/10000 = 65.0%, SPS: 3.9, ELAP: 23:47, ETA: 15:08 - loss: 23.8084\n",
            "6502/10000 = 65.0%, SPS: 3.9, ELAP: 23:48, ETA: 15:08 - loss: 24.3078\n",
            "6503/10000 = 65.0%, SPS: 3.9, ELAP: 23:48, ETA: 15:07 - loss: 24.1317\n",
            "6504/10000 = 65.0%, SPS: 3.9, ELAP: 23:48, ETA: 15:07 - loss: 23.6879\n",
            "6505/10000 = 65.0%, SPS: 3.9, ELAP: 23:48, ETA: 15:07 - loss: 23.6197\n",
            "6506/10000 = 65.1%, SPS: 3.9, ELAP: 23:48, ETA: 15:06 - loss: 24.1501\n",
            "6507/10000 = 65.1%, SPS: 3.9, ELAP: 23:49, ETA: 15:06 - loss: 24.3434\n",
            "6508/10000 = 65.1%, SPS: 3.9, ELAP: 23:49, ETA: 15:06 - loss: 24.4022\n",
            "6509/10000 = 65.1%, SPS: 3.9, ELAP: 23:49, ETA: 15:06 - loss: 23.7726\n",
            "6510/10000 = 65.1%, SPS: 3.9, ELAP: 23:49, ETA: 15:05 - loss: 24.7594\n",
            "6511/10000 = 65.1%, SPS: 3.9, ELAP: 23:50, ETA: 15:05 - loss: 24.1166\n",
            "6512/10000 = 65.1%, SPS: 3.9, ELAP: 23:50, ETA: 15:05 - loss: 24.1091\n",
            "6513/10000 = 65.1%, SPS: 3.9, ELAP: 23:50, ETA: 15:05 - loss: 24.5471\n",
            "6514/10000 = 65.1%, SPS: 3.9, ELAP: 23:50, ETA: 15:04 - loss: 24.8873\n",
            "6515/10000 = 65.2%, SPS: 3.9, ELAP: 23:51, ETA: 15:04 - loss: 24.1859\n",
            "6516/10000 = 65.2%, SPS: 3.9, ELAP: 23:51, ETA: 15:04 - loss: 24.5951\n",
            "6517/10000 = 65.2%, SPS: 3.9, ELAP: 23:51, ETA: 15:03 - loss: 24.0853\n",
            "6518/10000 = 65.2%, SPS: 3.9, ELAP: 23:51, ETA: 15:03 - loss: 24.1052\n",
            "6519/10000 = 65.2%, SPS: 3.9, ELAP: 23:51, ETA: 15:03 - loss: 23.9309\n",
            "6520/10000 = 65.2%, SPS: 3.9, ELAP: 23:52, ETA: 15:03 - loss: 24.8556\n",
            "6521/10000 = 65.2%, SPS: 3.9, ELAP: 23:52, ETA: 15:02 - loss: 24.1821\n",
            "6522/10000 = 65.2%, SPS: 3.9, ELAP: 23:52, ETA: 15:02 - loss: 24.0382\n",
            "6523/10000 = 65.2%, SPS: 3.9, ELAP: 23:52, ETA: 15:02 - loss: 24.8055\n",
            "6524/10000 = 65.2%, SPS: 3.9, ELAP: 23:53, ETA: 15:01 - loss: 24.4771\n",
            "6525/10000 = 65.2%, SPS: 3.9, ELAP: 23:53, ETA: 15:01 - loss: 24.0108\n",
            "6526/10000 = 65.3%, SPS: 3.9, ELAP: 23:53, ETA: 15:01 - loss: 24.6652\n",
            "6527/10000 = 65.3%, SPS: 3.9, ELAP: 23:53, ETA: 15:01 - loss: 24.3415\n",
            "6528/10000 = 65.3%, SPS: 3.9, ELAP: 23:54, ETA: 15:00 - loss: 23.8493\n",
            "6529/10000 = 65.3%, SPS: 3.9, ELAP: 23:54, ETA: 15:00 - loss: 23.9944\n",
            "6530/10000 = 65.3%, SPS: 3.9, ELAP: 23:54, ETA: 15:00 - loss: 24.4726\n",
            "6531/10000 = 65.3%, SPS: 3.9, ELAP: 23:54, ETA: 15:00 - loss: 24.4549\n",
            "6532/10000 = 65.3%, SPS: 3.9, ELAP: 23:54, ETA: 14:59 - loss: 24.0075\n",
            "6533/10000 = 65.3%, SPS: 3.9, ELAP: 23:55, ETA: 14:59 - loss: 24.0500\n",
            "6534/10000 = 65.3%, SPS: 3.9, ELAP: 23:55, ETA: 14:59 - loss: 24.9001\n",
            "6535/10000 = 65.3%, SPS: 3.9, ELAP: 23:55, ETA: 14:58 - loss: 24.4416\n",
            "6536/10000 = 65.4%, SPS: 3.9, ELAP: 23:55, ETA: 14:58 - loss: 24.7835\n",
            "6537/10000 = 65.4%, SPS: 3.9, ELAP: 23:56, ETA: 14:58 - loss: 24.3086\n",
            "6538/10000 = 65.4%, SPS: 3.9, ELAP: 23:56, ETA: 14:58 - loss: 24.2867\n",
            "6539/10000 = 65.4%, SPS: 3.9, ELAP: 23:56, ETA: 14:57 - loss: 24.5149\n",
            "6540/10000 = 65.4%, SPS: 3.9, ELAP: 23:56, ETA: 14:57 - loss: 24.2525\n",
            "6541/10000 = 65.4%, SPS: 3.9, ELAP: 23:57, ETA: 14:57 - loss: 24.6925\n",
            "6542/10000 = 65.4%, SPS: 3.9, ELAP: 23:57, ETA: 14:57 - loss: 23.5054\n",
            "6543/10000 = 65.4%, SPS: 3.9, ELAP: 23:57, ETA: 14:56 - loss: 23.7446\n",
            "6544/10000 = 65.4%, SPS: 3.9, ELAP: 23:57, ETA: 14:56 - loss: 23.9870\n",
            "6545/10000 = 65.5%, SPS: 3.9, ELAP: 23:58, ETA: 14:56 - loss: 24.2443\n",
            "6546/10000 = 65.5%, SPS: 3.9, ELAP: 23:58, ETA: 14:55 - loss: 23.7388\n",
            "6547/10000 = 65.5%, SPS: 3.9, ELAP: 23:58, ETA: 14:55 - loss: 24.4505\n",
            "6548/10000 = 65.5%, SPS: 3.9, ELAP: 23:58, ETA: 14:55 - loss: 24.2053\n",
            "6549/10000 = 65.5%, SPS: 3.9, ELAP: 23:59, ETA: 14:55 - loss: 24.2529\n",
            "6550/10000 = 65.5%, SPS: 3.9, ELAP: 23:59, ETA: 14:54 - loss: 24.7313\n",
            "6551/10000 = 65.5%, SPS: 3.9, ELAP: 23:59, ETA: 14:54 - loss: 23.8234\n",
            "6552/10000 = 65.5%, SPS: 3.9, ELAP: 23:59, ETA: 14:54 - loss: 24.6856\n",
            "6553/10000 = 65.5%, SPS: 3.9, ELAP: 23:59, ETA: 14:54 - loss: 23.9562\n",
            "6554/10000 = 65.5%, SPS: 3.9, ELAP: 24:00, ETA: 14:53 - loss: 24.0374\n",
            "6555/10000 = 65.5%, SPS: 3.9, ELAP: 24:00, ETA: 14:53 - loss: 24.6864\n",
            "6556/10000 = 65.6%, SPS: 3.9, ELAP: 24:00, ETA: 14:53 - loss: 24.8603\n",
            "6557/10000 = 65.6%, SPS: 3.9, ELAP: 24:00, ETA: 14:52 - loss: 24.1524\n",
            "6558/10000 = 65.6%, SPS: 3.9, ELAP: 24:01, ETA: 14:52 - loss: 24.2078\n",
            "6559/10000 = 65.6%, SPS: 3.9, ELAP: 24:01, ETA: 14:52 - loss: 23.2558\n",
            "6560/10000 = 65.6%, SPS: 3.9, ELAP: 24:01, ETA: 14:52 - loss: 23.8790\n",
            "6561/10000 = 65.6%, SPS: 3.9, ELAP: 24:01, ETA: 14:51 - loss: 24.5252\n",
            "6562/10000 = 65.6%, SPS: 3.9, ELAP: 24:01, ETA: 14:51 - loss: 24.1292\n",
            "6563/10000 = 65.6%, SPS: 3.9, ELAP: 24:02, ETA: 14:51 - loss: 24.6067\n",
            "6564/10000 = 65.6%, SPS: 3.9, ELAP: 24:02, ETA: 14:50 - loss: 24.4325\n",
            "6565/10000 = 65.7%, SPS: 3.9, ELAP: 24:02, ETA: 14:50 - loss: 24.3476\n",
            "6566/10000 = 65.7%, SPS: 3.9, ELAP: 24:02, ETA: 14:50 - loss: 24.4353\n",
            "6567/10000 = 65.7%, SPS: 3.9, ELAP: 24:03, ETA: 14:50 - loss: 24.3666\n",
            "6568/10000 = 65.7%, SPS: 3.9, ELAP: 24:03, ETA: 14:49 - loss: 24.3490\n",
            "6569/10000 = 65.7%, SPS: 3.9, ELAP: 24:03, ETA: 14:49 - loss: 24.8083\n",
            "6570/10000 = 65.7%, SPS: 3.9, ELAP: 24:03, ETA: 14:49 - loss: 24.3234\n",
            "6571/10000 = 65.7%, SPS: 3.9, ELAP: 24:04, ETA: 14:49 - loss: 23.9088\n",
            "6572/10000 = 65.7%, SPS: 3.9, ELAP: 24:04, ETA: 14:48 - loss: 24.9169\n",
            "6573/10000 = 65.7%, SPS: 3.9, ELAP: 24:04, ETA: 14:48 - loss: 24.4235\n",
            "6574/10000 = 65.7%, SPS: 3.9, ELAP: 24:04, ETA: 14:48 - loss: 24.5817\n",
            "6575/10000 = 65.8%, SPS: 3.9, ELAP: 24:04, ETA: 14:47 - loss: 24.3614\n",
            "6576/10000 = 65.8%, SPS: 3.9, ELAP: 24:05, ETA: 14:47 - loss: 24.7201\n",
            "6577/10000 = 65.8%, SPS: 3.9, ELAP: 24:05, ETA: 14:47 - loss: 23.5423\n",
            "6578/10000 = 65.8%, SPS: 3.9, ELAP: 24:05, ETA: 14:47 - loss: 24.0054\n",
            "6579/10000 = 65.8%, SPS: 3.9, ELAP: 24:05, ETA: 14:46 - loss: 24.1954\n",
            "6580/10000 = 65.8%, SPS: 3.9, ELAP: 24:06, ETA: 14:46 - loss: 24.8563\n",
            "6581/10000 = 65.8%, SPS: 3.9, ELAP: 24:06, ETA: 14:46 - loss: 23.8614\n",
            "6582/10000 = 65.8%, SPS: 3.9, ELAP: 24:06, ETA: 14:45 - loss: 24.1072\n",
            "6583/10000 = 65.8%, SPS: 3.9, ELAP: 24:06, ETA: 14:45 - loss: 23.9158\n",
            "6584/10000 = 65.8%, SPS: 3.9, ELAP: 24:06, ETA: 14:45 - loss: 24.0945\n",
            "6585/10000 = 65.8%, SPS: 3.9, ELAP: 24:07, ETA: 14:45 - loss: 24.2495\n",
            "6586/10000 = 65.9%, SPS: 3.9, ELAP: 24:07, ETA: 14:44 - loss: 24.0351\n",
            "6587/10000 = 65.9%, SPS: 3.9, ELAP: 24:07, ETA: 14:44 - loss: 23.9542\n",
            "6588/10000 = 65.9%, SPS: 3.9, ELAP: 24:07, ETA: 14:44 - loss: 24.5610\n",
            "6589/10000 = 65.9%, SPS: 3.9, ELAP: 24:08, ETA: 14:43 - loss: 23.9492\n",
            "6590/10000 = 65.9%, SPS: 3.9, ELAP: 24:08, ETA: 14:43 - loss: 24.1307\n",
            "6591/10000 = 65.9%, SPS: 3.9, ELAP: 24:08, ETA: 14:43 - loss: 24.1741\n",
            "6592/10000 = 65.9%, SPS: 3.9, ELAP: 24:08, ETA: 14:43 - loss: 23.6160\n",
            "6593/10000 = 65.9%, SPS: 3.9, ELAP: 24:09, ETA: 14:42 - loss: 23.7617\n",
            "6594/10000 = 65.9%, SPS: 3.9, ELAP: 24:09, ETA: 14:42 - loss: 24.3062\n",
            "6595/10000 = 66.0%, SPS: 3.9, ELAP: 24:09, ETA: 14:42 - loss: 24.0585\n",
            "6596/10000 = 66.0%, SPS: 3.9, ELAP: 24:09, ETA: 14:42 - loss: 24.8235\n",
            "6597/10000 = 66.0%, SPS: 3.9, ELAP: 24:09, ETA: 14:41 - loss: 24.0903\n",
            "6598/10000 = 66.0%, SPS: 3.9, ELAP: 24:10, ETA: 14:41 - loss: 24.5808\n",
            "6599/10000 = 66.0%, SPS: 3.9, ELAP: 24:10, ETA: 14:41 - loss: 24.2142\n",
            "6600/10000 = 66.0%, SPS: 3.9, ELAP: 24:12, ETA: 14:42 - loss: 24.3137\n",
            "6601/10000 = 66.0%, SPS: 3.9, ELAP: 24:12, ETA: 14:41 - loss: 25.0292\n",
            "6602/10000 = 66.0%, SPS: 3.9, ELAP: 24:13, ETA: 14:41 - loss: 24.9521\n",
            "6603/10000 = 66.0%, SPS: 3.9, ELAP: 24:13, ETA: 14:41 - loss: 24.7829\n",
            "6604/10000 = 66.0%, SPS: 3.9, ELAP: 24:13, ETA: 14:41 - loss: 24.6533\n",
            "6605/10000 = 66.0%, SPS: 3.9, ELAP: 24:13, ETA: 14:40 - loss: 24.1504\n",
            "6606/10000 = 66.1%, SPS: 3.9, ELAP: 24:14, ETA: 14:40 - loss: 24.0612\n",
            "6607/10000 = 66.1%, SPS: 3.9, ELAP: 24:14, ETA: 14:40 - loss: 24.1694\n",
            "6608/10000 = 66.1%, SPS: 3.9, ELAP: 24:14, ETA: 14:40 - loss: 24.6793\n",
            "6609/10000 = 66.1%, SPS: 3.9, ELAP: 24:14, ETA: 14:39 - loss: 24.6689\n",
            "6610/10000 = 66.1%, SPS: 3.9, ELAP: 24:15, ETA: 14:39 - loss: 24.4925\n",
            "6611/10000 = 66.1%, SPS: 3.9, ELAP: 24:15, ETA: 14:39 - loss: 25.0293\n",
            "6612/10000 = 66.1%, SPS: 3.9, ELAP: 24:15, ETA: 14:38 - loss: 24.3096\n",
            "6613/10000 = 66.1%, SPS: 3.9, ELAP: 24:15, ETA: 14:38 - loss: 24.9459\n",
            "6614/10000 = 66.1%, SPS: 3.9, ELAP: 24:15, ETA: 14:38 - loss: 23.8135\n",
            "6615/10000 = 66.2%, SPS: 3.9, ELAP: 24:16, ETA: 14:38 - loss: 24.1814\n",
            "6616/10000 = 66.2%, SPS: 3.9, ELAP: 24:16, ETA: 14:37 - loss: 24.4724\n",
            "6617/10000 = 66.2%, SPS: 3.9, ELAP: 24:16, ETA: 14:37 - loss: 24.6022\n",
            "6618/10000 = 66.2%, SPS: 3.9, ELAP: 24:16, ETA: 14:37 - loss: 24.0976\n",
            "6619/10000 = 66.2%, SPS: 3.9, ELAP: 24:17, ETA: 14:36 - loss: 24.2108\n",
            "6620/10000 = 66.2%, SPS: 3.9, ELAP: 24:17, ETA: 14:36 - loss: 24.1610\n",
            "6621/10000 = 66.2%, SPS: 3.9, ELAP: 24:17, ETA: 14:36 - loss: 24.6989\n",
            "6622/10000 = 66.2%, SPS: 3.9, ELAP: 24:17, ETA: 14:36 - loss: 24.0720\n",
            "6623/10000 = 66.2%, SPS: 3.9, ELAP: 24:18, ETA: 14:35 - loss: 24.4607\n",
            "6624/10000 = 66.2%, SPS: 3.9, ELAP: 24:18, ETA: 14:35 - loss: 24.3367\n",
            "6625/10000 = 66.2%, SPS: 3.9, ELAP: 24:18, ETA: 14:35 - loss: 24.1205\n",
            "6626/10000 = 66.3%, SPS: 3.9, ELAP: 24:18, ETA: 14:35 - loss: 23.7431\n",
            "6627/10000 = 66.3%, SPS: 3.9, ELAP: 24:18, ETA: 14:34 - loss: 23.9406\n",
            "6628/10000 = 66.3%, SPS: 3.9, ELAP: 24:19, ETA: 14:34 - loss: 25.1046\n",
            "6629/10000 = 66.3%, SPS: 3.9, ELAP: 24:19, ETA: 14:34 - loss: 23.9956\n",
            "6630/10000 = 66.3%, SPS: 3.9, ELAP: 24:19, ETA: 14:33 - loss: 23.9194\n",
            "6631/10000 = 66.3%, SPS: 3.9, ELAP: 24:19, ETA: 14:33 - loss: 24.0324\n",
            "6632/10000 = 66.3%, SPS: 3.9, ELAP: 24:20, ETA: 14:33 - loss: 23.7140\n",
            "6633/10000 = 66.3%, SPS: 3.9, ELAP: 24:20, ETA: 14:33 - loss: 24.4233\n",
            "6634/10000 = 66.3%, SPS: 3.9, ELAP: 24:20, ETA: 14:32 - loss: 23.8705\n",
            "6635/10000 = 66.3%, SPS: 3.9, ELAP: 24:20, ETA: 14:32 - loss: 24.2546\n",
            "6636/10000 = 66.4%, SPS: 3.9, ELAP: 24:21, ETA: 14:32 - loss: 24.1112\n",
            "6637/10000 = 66.4%, SPS: 3.9, ELAP: 24:21, ETA: 14:32 - loss: 24.0390\n",
            "6638/10000 = 66.4%, SPS: 3.9, ELAP: 24:21, ETA: 14:31 - loss: 23.6651\n",
            "6639/10000 = 66.4%, SPS: 3.9, ELAP: 24:21, ETA: 14:31 - loss: 24.1958\n",
            "6640/10000 = 66.4%, SPS: 3.9, ELAP: 24:22, ETA: 14:31 - loss: 24.6585\n",
            "6641/10000 = 66.4%, SPS: 3.9, ELAP: 24:22, ETA: 14:31 - loss: 24.3779\n",
            "6642/10000 = 66.4%, SPS: 3.9, ELAP: 24:22, ETA: 14:30 - loss: 24.1588\n",
            "6643/10000 = 66.4%, SPS: 3.9, ELAP: 24:22, ETA: 14:30 - loss: 24.4136\n",
            "6644/10000 = 66.4%, SPS: 3.9, ELAP: 24:23, ETA: 14:30 - loss: 23.9951\n",
            "6645/10000 = 66.5%, SPS: 3.9, ELAP: 24:23, ETA: 14:29 - loss: 23.9462\n",
            "6646/10000 = 66.5%, SPS: 3.9, ELAP: 24:23, ETA: 14:29 - loss: 23.9847\n",
            "6647/10000 = 66.5%, SPS: 3.9, ELAP: 24:23, ETA: 14:29 - loss: 24.3410\n",
            "6648/10000 = 66.5%, SPS: 3.9, ELAP: 24:24, ETA: 14:29 - loss: 24.1622\n",
            "6649/10000 = 66.5%, SPS: 3.9, ELAP: 24:24, ETA: 14:28 - loss: 23.9263\n",
            "6650/10000 = 66.5%, SPS: 3.9, ELAP: 24:24, ETA: 14:28 - loss: 23.9419\n",
            "6651/10000 = 66.5%, SPS: 3.9, ELAP: 24:24, ETA: 14:28 - loss: 24.6845\n",
            "6652/10000 = 66.5%, SPS: 3.9, ELAP: 24:24, ETA: 14:28 - loss: 24.0347\n",
            "6653/10000 = 66.5%, SPS: 3.9, ELAP: 24:25, ETA: 14:27 - loss: 24.1810\n",
            "6654/10000 = 66.5%, SPS: 3.9, ELAP: 24:25, ETA: 14:27 - loss: 24.3276\n",
            "6655/10000 = 66.5%, SPS: 3.9, ELAP: 24:25, ETA: 14:27 - loss: 24.0159\n",
            "6656/10000 = 66.6%, SPS: 3.9, ELAP: 24:25, ETA: 14:26 - loss: 23.7762\n",
            "6657/10000 = 66.6%, SPS: 3.9, ELAP: 24:26, ETA: 14:26 - loss: 24.1092\n",
            "6658/10000 = 66.6%, SPS: 3.9, ELAP: 24:26, ETA: 14:26 - loss: 24.5430\n",
            "6659/10000 = 66.6%, SPS: 3.9, ELAP: 24:26, ETA: 14:26 - loss: 23.4731\n",
            "6660/10000 = 66.6%, SPS: 3.9, ELAP: 24:26, ETA: 14:25 - loss: 24.3661\n",
            "6661/10000 = 66.6%, SPS: 3.9, ELAP: 24:27, ETA: 14:25 - loss: 23.9376\n",
            "6662/10000 = 66.6%, SPS: 3.9, ELAP: 24:27, ETA: 14:25 - loss: 24.2071\n",
            "6663/10000 = 66.6%, SPS: 3.9, ELAP: 24:27, ETA: 14:24 - loss: 24.7410\n",
            "6664/10000 = 66.6%, SPS: 3.9, ELAP: 24:27, ETA: 14:24 - loss: 24.2206\n",
            "6665/10000 = 66.7%, SPS: 3.9, ELAP: 24:27, ETA: 14:24 - loss: 24.1661\n",
            "6666/10000 = 66.7%, SPS: 3.9, ELAP: 24:28, ETA: 14:24 - loss: 23.2267\n",
            "6667/10000 = 66.7%, SPS: 3.9, ELAP: 24:28, ETA: 14:23 - loss: 24.4373\n",
            "6668/10000 = 66.7%, SPS: 3.9, ELAP: 24:28, ETA: 14:23 - loss: 24.5868\n",
            "6669/10000 = 66.7%, SPS: 3.9, ELAP: 24:28, ETA: 14:23 - loss: 23.9925\n",
            "6670/10000 = 66.7%, SPS: 3.9, ELAP: 24:29, ETA: 14:22 - loss: 24.3101\n",
            "6671/10000 = 66.7%, SPS: 3.9, ELAP: 24:29, ETA: 14:22 - loss: 23.9319\n",
            "6672/10000 = 66.7%, SPS: 3.9, ELAP: 24:29, ETA: 14:22 - loss: 24.5122\n",
            "6673/10000 = 66.7%, SPS: 3.9, ELAP: 24:29, ETA: 14:22 - loss: 24.5486\n",
            "6674/10000 = 66.7%, SPS: 3.9, ELAP: 24:29, ETA: 14:21 - loss: 24.1942\n",
            "6675/10000 = 66.8%, SPS: 3.9, ELAP: 24:30, ETA: 14:21 - loss: 24.3819\n",
            "6676/10000 = 66.8%, SPS: 3.9, ELAP: 24:30, ETA: 14:21 - loss: 24.3156\n",
            "6677/10000 = 66.8%, SPS: 3.9, ELAP: 24:30, ETA: 14:21 - loss: 23.9532\n",
            "6678/10000 = 66.8%, SPS: 3.9, ELAP: 24:30, ETA: 14:20 - loss: 24.0044\n",
            "6679/10000 = 66.8%, SPS: 3.9, ELAP: 24:31, ETA: 14:20 - loss: 23.7767\n",
            "6680/10000 = 66.8%, SPS: 3.9, ELAP: 24:31, ETA: 14:20 - loss: 24.1011\n",
            "6681/10000 = 66.8%, SPS: 3.9, ELAP: 24:31, ETA: 14:19 - loss: 24.7254\n",
            "6682/10000 = 66.8%, SPS: 3.9, ELAP: 24:31, ETA: 14:19 - loss: 24.3982\n",
            "6683/10000 = 66.8%, SPS: 3.9, ELAP: 24:32, ETA: 14:19 - loss: 24.4489\n",
            "6684/10000 = 66.8%, SPS: 3.9, ELAP: 24:32, ETA: 14:19 - loss: 23.9271\n",
            "6685/10000 = 66.8%, SPS: 3.9, ELAP: 24:32, ETA: 14:18 - loss: 24.8577\n",
            "6686/10000 = 66.9%, SPS: 3.9, ELAP: 24:32, ETA: 14:18 - loss: 24.5171\n",
            "6687/10000 = 66.9%, SPS: 3.9, ELAP: 24:32, ETA: 14:18 - loss: 23.7310\n",
            "6688/10000 = 66.9%, SPS: 3.9, ELAP: 24:33, ETA: 14:18 - loss: 24.4010\n",
            "6689/10000 = 66.9%, SPS: 3.9, ELAP: 24:33, ETA: 14:17 - loss: 24.4544\n",
            "6690/10000 = 66.9%, SPS: 3.9, ELAP: 24:33, ETA: 14:17 - loss: 23.4745\n",
            "6691/10000 = 66.9%, SPS: 3.9, ELAP: 24:33, ETA: 14:17 - loss: 24.3786\n",
            "6692/10000 = 66.9%, SPS: 3.9, ELAP: 24:34, ETA: 14:16 - loss: 24.1097\n",
            "6693/10000 = 66.9%, SPS: 3.9, ELAP: 24:34, ETA: 14:16 - loss: 24.3633\n",
            "6694/10000 = 66.9%, SPS: 3.9, ELAP: 24:34, ETA: 14:16 - loss: 24.3475\n",
            "6695/10000 = 67.0%, SPS: 3.9, ELAP: 24:34, ETA: 14:16 - loss: 24.8193\n",
            "6696/10000 = 67.0%, SPS: 3.9, ELAP: 24:35, ETA: 14:15 - loss: 24.1896\n",
            "6697/10000 = 67.0%, SPS: 3.9, ELAP: 24:35, ETA: 14:15 - loss: 23.9124\n",
            "6698/10000 = 67.0%, SPS: 3.9, ELAP: 24:35, ETA: 14:15 - loss: 24.9363\n",
            "6699/10000 = 67.0%, SPS: 3.9, ELAP: 24:35, ETA: 14:14 - loss: 23.7143\n",
            "6700/10000 = 67.0%, SPS: 3.9, ELAP: 24:38, ETA: 14:15 - loss: 24.2453\n",
            "6701/10000 = 67.0%, SPS: 3.9, ELAP: 24:38, ETA: 14:15 - loss: 24.4496\n",
            "6702/10000 = 67.0%, SPS: 3.9, ELAP: 24:38, ETA: 14:15 - loss: 24.0275\n",
            "6703/10000 = 67.0%, SPS: 3.9, ELAP: 24:38, ETA: 14:15 - loss: 24.3511\n",
            "6704/10000 = 67.0%, SPS: 3.9, ELAP: 24:39, ETA: 14:14 - loss: 24.7212\n",
            "6705/10000 = 67.0%, SPS: 3.9, ELAP: 24:39, ETA: 14:14 - loss: 23.9103\n",
            "6706/10000 = 67.1%, SPS: 3.9, ELAP: 24:39, ETA: 14:14 - loss: 24.0679\n",
            "6707/10000 = 67.1%, SPS: 3.9, ELAP: 24:39, ETA: 14:14 - loss: 23.9166\n",
            "6708/10000 = 67.1%, SPS: 3.9, ELAP: 24:39, ETA: 14:13 - loss: 24.4777\n",
            "6709/10000 = 67.1%, SPS: 3.9, ELAP: 24:40, ETA: 14:13 - loss: 24.3995\n",
            "6710/10000 = 67.1%, SPS: 3.9, ELAP: 24:40, ETA: 14:13 - loss: 24.3712\n",
            "6711/10000 = 67.1%, SPS: 3.9, ELAP: 24:40, ETA: 14:12 - loss: 24.4586\n",
            "6712/10000 = 67.1%, SPS: 3.9, ELAP: 24:40, ETA: 14:12 - loss: 24.2513\n",
            "6713/10000 = 67.1%, SPS: 3.9, ELAP: 24:41, ETA: 14:12 - loss: 24.1925\n",
            "6714/10000 = 67.1%, SPS: 3.9, ELAP: 24:41, ETA: 14:12 - loss: 24.0424\n",
            "6715/10000 = 67.2%, SPS: 3.9, ELAP: 24:41, ETA: 14:11 - loss: 24.5181\n",
            "6716/10000 = 67.2%, SPS: 3.9, ELAP: 24:41, ETA: 14:11 - loss: 24.0985\n",
            "6717/10000 = 67.2%, SPS: 3.9, ELAP: 24:41, ETA: 14:11 - loss: 23.5800\n",
            "6718/10000 = 67.2%, SPS: 3.9, ELAP: 24:42, ETA: 14:10 - loss: 23.6204\n",
            "6719/10000 = 67.2%, SPS: 3.9, ELAP: 24:42, ETA: 14:10 - loss: 24.4663\n",
            "6720/10000 = 67.2%, SPS: 3.9, ELAP: 24:42, ETA: 14:10 - loss: 24.6475\n",
            "6721/10000 = 67.2%, SPS: 3.9, ELAP: 24:42, ETA: 14:10 - loss: 24.5475\n",
            "6722/10000 = 67.2%, SPS: 3.9, ELAP: 24:43, ETA: 14:09 - loss: 24.3833\n",
            "6723/10000 = 67.2%, SPS: 3.9, ELAP: 24:43, ETA: 14:09 - loss: 24.1260\n",
            "6724/10000 = 67.2%, SPS: 3.9, ELAP: 24:43, ETA: 14:09 - loss: 24.1929\n",
            "6725/10000 = 67.2%, SPS: 3.9, ELAP: 24:43, ETA: 14:09 - loss: 23.9566\n",
            "6726/10000 = 67.3%, SPS: 3.9, ELAP: 24:44, ETA: 14:08 - loss: 24.2787\n",
            "6727/10000 = 67.3%, SPS: 3.9, ELAP: 24:44, ETA: 14:08 - loss: 24.5787\n",
            "6728/10000 = 67.3%, SPS: 3.9, ELAP: 24:44, ETA: 14:08 - loss: 24.8376\n",
            "6729/10000 = 67.3%, SPS: 3.9, ELAP: 24:44, ETA: 14:07 - loss: 24.0724\n",
            "6730/10000 = 67.3%, SPS: 3.9, ELAP: 24:44, ETA: 14:07 - loss: 24.4381\n",
            "6731/10000 = 67.3%, SPS: 3.9, ELAP: 24:45, ETA: 14:07 - loss: 24.3154\n",
            "6732/10000 = 67.3%, SPS: 3.9, ELAP: 24:45, ETA: 14:07 - loss: 24.2119\n",
            "6733/10000 = 67.3%, SPS: 3.9, ELAP: 24:45, ETA: 14:06 - loss: 24.3699\n",
            "6734/10000 = 67.3%, SPS: 3.9, ELAP: 24:45, ETA: 14:06 - loss: 24.8705\n",
            "6735/10000 = 67.3%, SPS: 3.9, ELAP: 24:46, ETA: 14:06 - loss: 24.2948\n",
            "6736/10000 = 67.4%, SPS: 3.9, ELAP: 24:46, ETA: 14:06 - loss: 24.1806\n",
            "6737/10000 = 67.4%, SPS: 3.9, ELAP: 24:46, ETA: 14:05 - loss: 24.0723\n",
            "6738/10000 = 67.4%, SPS: 3.9, ELAP: 24:46, ETA: 14:05 - loss: 23.8519\n",
            "6739/10000 = 67.4%, SPS: 3.9, ELAP: 24:47, ETA: 14:05 - loss: 24.0629\n",
            "6740/10000 = 67.4%, SPS: 3.9, ELAP: 24:47, ETA: 14:04 - loss: 24.1935\n",
            "6741/10000 = 67.4%, SPS: 3.9, ELAP: 24:47, ETA: 14:04 - loss: 24.6657\n",
            "6742/10000 = 67.4%, SPS: 3.9, ELAP: 24:47, ETA: 14:04 - loss: 24.5499\n",
            "6743/10000 = 67.4%, SPS: 3.9, ELAP: 24:48, ETA: 14:04 - loss: 24.3182\n",
            "6744/10000 = 67.4%, SPS: 3.9, ELAP: 24:48, ETA: 14:03 - loss: 24.3906\n",
            "6745/10000 = 67.5%, SPS: 3.9, ELAP: 24:48, ETA: 14:03 - loss: 24.1244\n",
            "6746/10000 = 67.5%, SPS: 3.9, ELAP: 24:48, ETA: 14:03 - loss: 24.5258\n",
            "6747/10000 = 67.5%, SPS: 3.9, ELAP: 24:49, ETA: 14:03 - loss: 24.1034\n",
            "6748/10000 = 67.5%, SPS: 3.9, ELAP: 24:49, ETA: 14:02 - loss: 24.4569\n",
            "6749/10000 = 67.5%, SPS: 3.9, ELAP: 24:49, ETA: 14:02 - loss: 24.2969\n",
            "6750/10000 = 67.5%, SPS: 3.9, ELAP: 24:49, ETA: 14:02 - loss: 24.6405\n",
            "6751/10000 = 67.5%, SPS: 3.9, ELAP: 24:50, ETA: 14:01 - loss: 23.7764\n",
            "6752/10000 = 67.5%, SPS: 3.9, ELAP: 24:50, ETA: 14:01 - loss: 23.7546\n",
            "6753/10000 = 67.5%, SPS: 3.9, ELAP: 24:50, ETA: 14:01 - loss: 24.2310\n",
            "6754/10000 = 67.5%, SPS: 3.9, ELAP: 24:50, ETA: 14:01 - loss: 24.5340\n",
            "6755/10000 = 67.5%, SPS: 3.9, ELAP: 24:50, ETA: 14:00 - loss: 24.0765\n",
            "6756/10000 = 67.6%, SPS: 3.9, ELAP: 24:51, ETA: 14:00 - loss: 24.0636\n",
            "6757/10000 = 67.6%, SPS: 3.9, ELAP: 24:51, ETA: 14:00 - loss: 25.1369\n",
            "6758/10000 = 67.6%, SPS: 3.9, ELAP: 24:51, ETA: 14:00 - loss: 24.7629\n",
            "6759/10000 = 67.6%, SPS: 3.9, ELAP: 24:51, ETA: 13:59 - loss: 24.2367\n",
            "6760/10000 = 67.6%, SPS: 3.9, ELAP: 24:52, ETA: 13:59 - loss: 24.3533\n",
            "6761/10000 = 67.6%, SPS: 3.9, ELAP: 24:52, ETA: 13:59 - loss: 24.3968\n",
            "6762/10000 = 67.6%, SPS: 3.9, ELAP: 24:52, ETA: 13:58 - loss: 23.8673\n",
            "6763/10000 = 67.6%, SPS: 3.9, ELAP: 24:52, ETA: 13:58 - loss: 24.1552\n",
            "6764/10000 = 67.6%, SPS: 3.9, ELAP: 24:53, ETA: 13:58 - loss: 24.0837\n",
            "6765/10000 = 67.7%, SPS: 3.9, ELAP: 24:53, ETA: 13:58 - loss: 24.2004\n",
            "6766/10000 = 67.7%, SPS: 3.9, ELAP: 24:53, ETA: 13:57 - loss: 24.0902\n",
            "6767/10000 = 67.7%, SPS: 3.9, ELAP: 24:53, ETA: 13:57 - loss: 24.2535\n",
            "6768/10000 = 67.7%, SPS: 3.9, ELAP: 24:53, ETA: 13:57 - loss: 24.2371\n",
            "6769/10000 = 67.7%, SPS: 3.9, ELAP: 24:54, ETA: 13:57 - loss: 23.6932\n",
            "6770/10000 = 67.7%, SPS: 3.9, ELAP: 24:54, ETA: 13:56 - loss: 24.0807\n",
            "6771/10000 = 67.7%, SPS: 3.9, ELAP: 24:54, ETA: 13:56 - loss: 24.4056\n",
            "6772/10000 = 67.7%, SPS: 3.9, ELAP: 24:54, ETA: 13:56 - loss: 24.2551\n",
            "6773/10000 = 67.7%, SPS: 3.9, ELAP: 24:55, ETA: 13:55 - loss: 24.5120\n",
            "6774/10000 = 67.7%, SPS: 3.9, ELAP: 24:55, ETA: 13:55 - loss: 23.8996\n",
            "6775/10000 = 67.8%, SPS: 3.9, ELAP: 24:55, ETA: 13:55 - loss: 23.7874\n",
            "6776/10000 = 67.8%, SPS: 3.9, ELAP: 24:55, ETA: 13:55 - loss: 24.6063\n",
            "6777/10000 = 67.8%, SPS: 3.9, ELAP: 24:56, ETA: 13:54 - loss: 23.9868\n",
            "6778/10000 = 67.8%, SPS: 3.9, ELAP: 24:56, ETA: 13:54 - loss: 24.0569\n",
            "6779/10000 = 67.8%, SPS: 3.9, ELAP: 24:56, ETA: 13:54 - loss: 23.7326\n",
            "6780/10000 = 67.8%, SPS: 3.9, ELAP: 24:56, ETA: 13:54 - loss: 24.7398\n",
            "6781/10000 = 67.8%, SPS: 3.9, ELAP: 24:56, ETA: 13:53 - loss: 24.3264\n",
            "6782/10000 = 67.8%, SPS: 3.9, ELAP: 24:57, ETA: 13:53 - loss: 23.9340\n",
            "6783/10000 = 67.8%, SPS: 3.9, ELAP: 24:57, ETA: 13:53 - loss: 24.6042\n",
            "6784/10000 = 67.8%, SPS: 3.9, ELAP: 24:57, ETA: 13:52 - loss: 24.1407\n",
            "6785/10000 = 67.8%, SPS: 3.9, ELAP: 24:57, ETA: 13:52 - loss: 24.0976\n",
            "6786/10000 = 67.9%, SPS: 3.9, ELAP: 24:58, ETA: 13:52 - loss: 24.3285\n",
            "6787/10000 = 67.9%, SPS: 3.9, ELAP: 24:58, ETA: 13:52 - loss: 24.3280\n",
            "6788/10000 = 67.9%, SPS: 3.9, ELAP: 24:58, ETA: 13:51 - loss: 24.0938\n",
            "6789/10000 = 67.9%, SPS: 3.9, ELAP: 24:58, ETA: 13:51 - loss: 23.9013\n",
            "6790/10000 = 67.9%, SPS: 3.9, ELAP: 24:59, ETA: 13:51 - loss: 23.8749\n",
            "6791/10000 = 67.9%, SPS: 3.9, ELAP: 24:59, ETA: 13:50 - loss: 24.1392\n",
            "6792/10000 = 67.9%, SPS: 3.9, ELAP: 24:59, ETA: 13:50 - loss: 23.0250\n",
            "6793/10000 = 67.9%, SPS: 3.9, ELAP: 24:59, ETA: 13:50 - loss: 24.6141\n",
            "6794/10000 = 67.9%, SPS: 3.9, ELAP: 24:59, ETA: 13:50 - loss: 24.0003\n",
            "6795/10000 = 68.0%, SPS: 3.9, ELAP: 25:00, ETA: 13:49 - loss: 24.9426\n",
            "6796/10000 = 68.0%, SPS: 3.9, ELAP: 25:00, ETA: 13:49 - loss: 24.2881\n",
            "6797/10000 = 68.0%, SPS: 3.9, ELAP: 25:00, ETA: 13:49 - loss: 24.6773\n",
            "6798/10000 = 68.0%, SPS: 3.9, ELAP: 25:00, ETA: 13:49 - loss: 23.9648\n",
            "6799/10000 = 68.0%, SPS: 3.9, ELAP: 25:01, ETA: 13:48 - loss: 23.5737\n",
            "6800/10000 = 68.0%, SPS: 3.9, ELAP: 25:03, ETA: 13:49 - loss: 24.4759\n",
            "6801/10000 = 68.0%, SPS: 3.9, ELAP: 25:03, ETA: 13:49 - loss: 24.1644\n",
            "6802/10000 = 68.0%, SPS: 3.9, ELAP: 25:03, ETA: 13:49 - loss: 24.1688\n",
            "6803/10000 = 68.0%, SPS: 3.9, ELAP: 25:04, ETA: 13:48 - loss: 24.3205\n",
            "6804/10000 = 68.0%, SPS: 3.9, ELAP: 25:04, ETA: 13:48 - loss: 24.3064\n",
            "6805/10000 = 68.0%, SPS: 3.9, ELAP: 25:04, ETA: 13:48 - loss: 24.0022\n",
            "6806/10000 = 68.1%, SPS: 3.9, ELAP: 25:04, ETA: 13:48 - loss: 23.7661\n",
            "6807/10000 = 68.1%, SPS: 3.9, ELAP: 25:05, ETA: 13:47 - loss: 24.8221\n",
            "6808/10000 = 68.1%, SPS: 3.9, ELAP: 25:05, ETA: 13:47 - loss: 24.4858\n",
            "6809/10000 = 68.1%, SPS: 3.9, ELAP: 25:05, ETA: 13:47 - loss: 24.1725\n",
            "6810/10000 = 68.1%, SPS: 3.9, ELAP: 25:05, ETA: 13:46 - loss: 24.1057\n",
            "6811/10000 = 68.1%, SPS: 3.9, ELAP: 25:06, ETA: 13:46 - loss: 24.4080\n",
            "6812/10000 = 68.1%, SPS: 3.9, ELAP: 25:06, ETA: 13:46 - loss: 24.0174\n",
            "6813/10000 = 68.1%, SPS: 3.9, ELAP: 25:06, ETA: 13:46 - loss: 24.4902\n",
            "6814/10000 = 68.1%, SPS: 3.9, ELAP: 25:06, ETA: 13:45 - loss: 23.8369\n",
            "6815/10000 = 68.2%, SPS: 3.9, ELAP: 25:06, ETA: 13:45 - loss: 23.8773\n",
            "6816/10000 = 68.2%, SPS: 3.9, ELAP: 25:07, ETA: 13:45 - loss: 24.7986\n",
            "6817/10000 = 68.2%, SPS: 3.9, ELAP: 25:07, ETA: 13:45 - loss: 24.4447\n",
            "6818/10000 = 68.2%, SPS: 3.9, ELAP: 25:07, ETA: 13:44 - loss: 23.9428\n",
            "6819/10000 = 68.2%, SPS: 3.9, ELAP: 25:07, ETA: 13:44 - loss: 24.2910\n",
            "6820/10000 = 68.2%, SPS: 3.9, ELAP: 25:08, ETA: 13:44 - loss: 24.1974\n",
            "6821/10000 = 68.2%, SPS: 3.9, ELAP: 25:08, ETA: 13:43 - loss: 23.8684\n",
            "6822/10000 = 68.2%, SPS: 3.9, ELAP: 25:08, ETA: 13:43 - loss: 24.0863\n",
            "6823/10000 = 68.2%, SPS: 3.9, ELAP: 25:08, ETA: 13:43 - loss: 24.7262\n",
            "6824/10000 = 68.2%, SPS: 3.9, ELAP: 25:08, ETA: 13:43 - loss: 23.6150\n",
            "6825/10000 = 68.2%, SPS: 3.9, ELAP: 25:09, ETA: 13:42 - loss: 25.2595\n",
            "6826/10000 = 68.3%, SPS: 3.9, ELAP: 25:09, ETA: 13:42 - loss: 23.5655\n",
            "6827/10000 = 68.3%, SPS: 3.9, ELAP: 25:09, ETA: 13:42 - loss: 24.3021\n",
            "6828/10000 = 68.3%, SPS: 3.9, ELAP: 25:09, ETA: 13:42 - loss: 23.9844\n",
            "6829/10000 = 68.3%, SPS: 3.9, ELAP: 25:10, ETA: 13:41 - loss: 24.4823\n",
            "6830/10000 = 68.3%, SPS: 3.9, ELAP: 25:10, ETA: 13:41 - loss: 24.8699\n",
            "6831/10000 = 68.3%, SPS: 3.9, ELAP: 25:10, ETA: 13:41 - loss: 24.2244\n",
            "6832/10000 = 68.3%, SPS: 3.9, ELAP: 25:10, ETA: 13:40 - loss: 24.3671\n",
            "6833/10000 = 68.3%, SPS: 3.9, ELAP: 25:11, ETA: 13:40 - loss: 23.4035\n",
            "6834/10000 = 68.3%, SPS: 3.9, ELAP: 25:11, ETA: 13:40 - loss: 24.1410\n",
            "6835/10000 = 68.3%, SPS: 3.9, ELAP: 25:11, ETA: 13:40 - loss: 24.2300\n",
            "6836/10000 = 68.4%, SPS: 3.9, ELAP: 25:11, ETA: 13:39 - loss: 24.4529\n",
            "6837/10000 = 68.4%, SPS: 3.9, ELAP: 25:12, ETA: 13:39 - loss: 24.3909\n",
            "6838/10000 = 68.4%, SPS: 3.9, ELAP: 25:12, ETA: 13:39 - loss: 24.0847\n",
            "6839/10000 = 68.4%, SPS: 3.9, ELAP: 25:12, ETA: 13:39 - loss: 23.7471\n",
            "6840/10000 = 68.4%, SPS: 3.9, ELAP: 25:12, ETA: 13:38 - loss: 23.7440\n",
            "6841/10000 = 68.4%, SPS: 3.9, ELAP: 25:13, ETA: 13:38 - loss: 23.5078\n",
            "6842/10000 = 68.4%, SPS: 3.9, ELAP: 25:13, ETA: 13:38 - loss: 24.4228\n",
            "6843/10000 = 68.4%, SPS: 3.9, ELAP: 25:13, ETA: 13:38 - loss: 23.4559\n",
            "6844/10000 = 68.4%, SPS: 3.9, ELAP: 25:13, ETA: 13:37 - loss: 24.7163\n",
            "6845/10000 = 68.5%, SPS: 3.9, ELAP: 25:14, ETA: 13:37 - loss: 24.5855\n",
            "6846/10000 = 68.5%, SPS: 3.9, ELAP: 25:14, ETA: 13:37 - loss: 24.4525\n",
            "6847/10000 = 68.5%, SPS: 3.9, ELAP: 25:14, ETA: 13:36 - loss: 23.8487\n",
            "6848/10000 = 68.5%, SPS: 3.9, ELAP: 25:14, ETA: 13:36 - loss: 23.8031\n",
            "6849/10000 = 68.5%, SPS: 3.9, ELAP: 25:14, ETA: 13:36 - loss: 24.3591\n",
            "6850/10000 = 68.5%, SPS: 3.9, ELAP: 25:15, ETA: 13:36 - loss: 24.2717\n",
            "6851/10000 = 68.5%, SPS: 3.9, ELAP: 25:15, ETA: 13:35 - loss: 24.1668\n",
            "6852/10000 = 68.5%, SPS: 3.9, ELAP: 25:15, ETA: 13:35 - loss: 24.3267\n",
            "6853/10000 = 68.5%, SPS: 3.9, ELAP: 25:15, ETA: 13:35 - loss: 23.9612\n",
            "6854/10000 = 68.5%, SPS: 3.9, ELAP: 25:16, ETA: 13:34 - loss: 24.3213\n",
            "6855/10000 = 68.5%, SPS: 3.9, ELAP: 25:16, ETA: 13:34 - loss: 24.4230\n",
            "6856/10000 = 68.6%, SPS: 3.9, ELAP: 25:16, ETA: 13:34 - loss: 24.1607\n",
            "6857/10000 = 68.6%, SPS: 3.9, ELAP: 25:16, ETA: 13:34 - loss: 24.4509\n",
            "6858/10000 = 68.6%, SPS: 3.9, ELAP: 25:16, ETA: 13:33 - loss: 24.4609\n",
            "6859/10000 = 68.6%, SPS: 3.9, ELAP: 25:17, ETA: 13:33 - loss: 24.0298\n",
            "6860/10000 = 68.6%, SPS: 3.9, ELAP: 25:17, ETA: 13:33 - loss: 24.3315\n",
            "6861/10000 = 68.6%, SPS: 3.9, ELAP: 25:17, ETA: 13:33 - loss: 24.2310\n",
            "6862/10000 = 68.6%, SPS: 3.9, ELAP: 25:17, ETA: 13:32 - loss: 24.2472\n",
            "6863/10000 = 68.6%, SPS: 3.9, ELAP: 25:18, ETA: 13:32 - loss: 23.6256\n",
            "6864/10000 = 68.6%, SPS: 3.9, ELAP: 25:18, ETA: 13:32 - loss: 24.3745\n",
            "6865/10000 = 68.7%, SPS: 3.9, ELAP: 25:18, ETA: 13:31 - loss: 24.9031\n",
            "6866/10000 = 68.7%, SPS: 3.9, ELAP: 25:18, ETA: 13:31 - loss: 24.9756\n",
            "6867/10000 = 68.7%, SPS: 3.9, ELAP: 25:19, ETA: 13:31 - loss: 24.7914\n",
            "6868/10000 = 68.7%, SPS: 3.9, ELAP: 25:19, ETA: 13:31 - loss: 24.0770\n",
            "6869/10000 = 68.7%, SPS: 3.9, ELAP: 25:19, ETA: 13:30 - loss: 24.2326\n",
            "6870/10000 = 68.7%, SPS: 3.9, ELAP: 25:19, ETA: 13:30 - loss: 24.1157\n",
            "6871/10000 = 68.7%, SPS: 3.9, ELAP: 25:19, ETA: 13:30 - loss: 24.7211\n",
            "6872/10000 = 68.7%, SPS: 3.9, ELAP: 25:20, ETA: 13:30 - loss: 24.0151\n",
            "6873/10000 = 68.7%, SPS: 3.9, ELAP: 25:20, ETA: 13:29 - loss: 23.8485\n",
            "6874/10000 = 68.7%, SPS: 3.9, ELAP: 25:20, ETA: 13:29 - loss: 24.0899\n",
            "6875/10000 = 68.8%, SPS: 3.9, ELAP: 25:20, ETA: 13:29 - loss: 24.3750\n",
            "6876/10000 = 68.8%, SPS: 3.9, ELAP: 25:21, ETA: 13:28 - loss: 24.5273\n",
            "6877/10000 = 68.8%, SPS: 3.9, ELAP: 25:21, ETA: 13:28 - loss: 24.1010\n",
            "6878/10000 = 68.8%, SPS: 3.9, ELAP: 25:21, ETA: 13:28 - loss: 23.9106\n",
            "6879/10000 = 68.8%, SPS: 3.9, ELAP: 25:21, ETA: 13:28 - loss: 24.2814\n",
            "6880/10000 = 68.8%, SPS: 3.9, ELAP: 25:22, ETA: 13:27 - loss: 24.1512\n",
            "6881/10000 = 68.8%, SPS: 3.9, ELAP: 25:22, ETA: 13:27 - loss: 24.1715\n",
            "6882/10000 = 68.8%, SPS: 3.9, ELAP: 25:22, ETA: 13:27 - loss: 24.4337\n",
            "6883/10000 = 68.8%, SPS: 3.9, ELAP: 25:22, ETA: 13:27 - loss: 24.6521\n",
            "6884/10000 = 68.8%, SPS: 3.9, ELAP: 25:22, ETA: 13:26 - loss: 24.1848\n",
            "6885/10000 = 68.8%, SPS: 3.9, ELAP: 25:23, ETA: 13:26 - loss: 24.3776\n",
            "6886/10000 = 68.9%, SPS: 3.9, ELAP: 25:23, ETA: 13:26 - loss: 24.0383\n",
            "6887/10000 = 68.9%, SPS: 3.9, ELAP: 25:23, ETA: 13:25 - loss: 24.0207\n",
            "6888/10000 = 68.9%, SPS: 3.9, ELAP: 25:23, ETA: 13:25 - loss: 24.5761\n",
            "6889/10000 = 68.9%, SPS: 3.9, ELAP: 25:24, ETA: 13:25 - loss: 23.9921\n",
            "6890/10000 = 68.9%, SPS: 3.9, ELAP: 25:24, ETA: 13:25 - loss: 23.8019\n",
            "6891/10000 = 68.9%, SPS: 3.9, ELAP: 25:24, ETA: 13:24 - loss: 24.4099\n",
            "6892/10000 = 68.9%, SPS: 3.9, ELAP: 25:24, ETA: 13:24 - loss: 24.5778\n",
            "6893/10000 = 68.9%, SPS: 3.9, ELAP: 25:24, ETA: 13:24 - loss: 24.3791\n",
            "6894/10000 = 68.9%, SPS: 3.9, ELAP: 25:25, ETA: 13:23 - loss: 23.9390\n",
            "6895/10000 = 69.0%, SPS: 3.9, ELAP: 25:25, ETA: 13:23 - loss: 24.1068\n",
            "6896/10000 = 69.0%, SPS: 3.9, ELAP: 25:25, ETA: 13:23 - loss: 24.4023\n",
            "6897/10000 = 69.0%, SPS: 3.9, ELAP: 25:25, ETA: 13:23 - loss: 24.3176\n",
            "6898/10000 = 69.0%, SPS: 3.9, ELAP: 25:26, ETA: 13:22 - loss: 24.2988\n",
            "6899/10000 = 69.0%, SPS: 3.9, ELAP: 25:26, ETA: 13:22 - loss: 24.6455\n",
            "6900/10000 = 69.0%, SPS: 3.9, ELAP: 25:28, ETA: 13:23 - loss: 24.2467\n",
            "6901/10000 = 69.0%, SPS: 3.9, ELAP: 25:28, ETA: 13:23 - loss: 23.7356\n",
            "6902/10000 = 69.0%, SPS: 3.9, ELAP: 25:29, ETA: 13:22 - loss: 23.8405\n",
            "6903/10000 = 69.0%, SPS: 3.9, ELAP: 25:29, ETA: 13:22 - loss: 24.1300\n",
            "6904/10000 = 69.0%, SPS: 3.9, ELAP: 25:29, ETA: 13:22 - loss: 23.7547\n",
            "6905/10000 = 69.0%, SPS: 3.9, ELAP: 25:29, ETA: 13:22 - loss: 24.1353\n",
            "6906/10000 = 69.1%, SPS: 3.9, ELAP: 25:30, ETA: 13:21 - loss: 24.1758\n",
            "6907/10000 = 69.1%, SPS: 3.9, ELAP: 25:30, ETA: 13:21 - loss: 23.9604\n",
            "6908/10000 = 69.1%, SPS: 3.9, ELAP: 25:30, ETA: 13:21 - loss: 24.1762\n",
            "6909/10000 = 69.1%, SPS: 3.9, ELAP: 25:30, ETA: 13:20 - loss: 24.1626\n",
            "6910/10000 = 69.1%, SPS: 3.9, ELAP: 25:30, ETA: 13:20 - loss: 24.0494\n",
            "6911/10000 = 69.1%, SPS: 3.9, ELAP: 25:31, ETA: 13:20 - loss: 24.6783\n",
            "6912/10000 = 69.1%, SPS: 3.9, ELAP: 25:31, ETA: 13:20 - loss: 24.6454\n",
            "6913/10000 = 69.1%, SPS: 3.9, ELAP: 25:31, ETA: 13:19 - loss: 24.0600\n",
            "6914/10000 = 69.1%, SPS: 3.9, ELAP: 25:31, ETA: 13:19 - loss: 24.4432\n",
            "6915/10000 = 69.2%, SPS: 3.9, ELAP: 25:32, ETA: 13:19 - loss: 23.4629\n",
            "6916/10000 = 69.2%, SPS: 3.9, ELAP: 25:32, ETA: 13:19 - loss: 23.8689\n",
            "6917/10000 = 69.2%, SPS: 3.9, ELAP: 25:32, ETA: 13:18 - loss: 24.8191\n",
            "6918/10000 = 69.2%, SPS: 3.9, ELAP: 25:32, ETA: 13:18 - loss: 23.6363\n",
            "6919/10000 = 69.2%, SPS: 3.9, ELAP: 25:32, ETA: 13:18 - loss: 24.2950\n",
            "6920/10000 = 69.2%, SPS: 3.9, ELAP: 25:33, ETA: 13:17 - loss: 23.7618\n",
            "6921/10000 = 69.2%, SPS: 3.9, ELAP: 25:33, ETA: 13:17 - loss: 24.4662\n",
            "6922/10000 = 69.2%, SPS: 3.9, ELAP: 25:33, ETA: 13:17 - loss: 24.2765\n",
            "6923/10000 = 69.2%, SPS: 3.9, ELAP: 25:33, ETA: 13:17 - loss: 23.5926\n",
            "6924/10000 = 69.2%, SPS: 3.9, ELAP: 25:34, ETA: 13:16 - loss: 23.4100\n",
            "6925/10000 = 69.2%, SPS: 3.9, ELAP: 25:34, ETA: 13:16 - loss: 24.3844\n",
            "6926/10000 = 69.3%, SPS: 3.9, ELAP: 25:34, ETA: 13:16 - loss: 24.1598\n",
            "6927/10000 = 69.3%, SPS: 3.9, ELAP: 25:34, ETA: 13:16 - loss: 23.8139\n",
            "6928/10000 = 69.3%, SPS: 3.9, ELAP: 25:35, ETA: 13:15 - loss: 24.2838\n",
            "6929/10000 = 69.3%, SPS: 3.9, ELAP: 25:35, ETA: 13:15 - loss: 23.9763\n",
            "6930/10000 = 69.3%, SPS: 3.9, ELAP: 25:35, ETA: 13:15 - loss: 24.2361\n",
            "6931/10000 = 69.3%, SPS: 3.9, ELAP: 25:35, ETA: 13:14 - loss: 23.8964\n",
            "6932/10000 = 69.3%, SPS: 3.9, ELAP: 25:36, ETA: 13:14 - loss: 24.2873\n",
            "6933/10000 = 69.3%, SPS: 3.9, ELAP: 25:36, ETA: 13:14 - loss: 24.5791\n",
            "6934/10000 = 69.3%, SPS: 3.9, ELAP: 25:36, ETA: 13:14 - loss: 24.3901\n",
            "6935/10000 = 69.3%, SPS: 3.9, ELAP: 25:36, ETA: 13:13 - loss: 24.2957\n",
            "6936/10000 = 69.4%, SPS: 3.9, ELAP: 25:37, ETA: 13:13 - loss: 24.0980\n",
            "6937/10000 = 69.4%, SPS: 3.9, ELAP: 25:37, ETA: 13:13 - loss: 24.3216\n",
            "6938/10000 = 69.4%, SPS: 3.9, ELAP: 25:37, ETA: 13:13 - loss: 24.1287\n",
            "6939/10000 = 69.4%, SPS: 3.9, ELAP: 25:37, ETA: 13:12 - loss: 24.7103\n",
            "6940/10000 = 69.4%, SPS: 3.9, ELAP: 25:38, ETA: 13:12 - loss: 23.4478\n",
            "6941/10000 = 69.4%, SPS: 3.9, ELAP: 25:38, ETA: 13:12 - loss: 24.2517\n",
            "6942/10000 = 69.4%, SPS: 3.9, ELAP: 25:38, ETA: 13:12 - loss: 24.7238\n",
            "6943/10000 = 69.4%, SPS: 3.9, ELAP: 25:38, ETA: 13:11 - loss: 23.9955\n",
            "6944/10000 = 69.4%, SPS: 3.9, ELAP: 25:39, ETA: 13:11 - loss: 23.6954\n",
            "6945/10000 = 69.5%, SPS: 3.9, ELAP: 25:39, ETA: 13:11 - loss: 24.5731\n",
            "6946/10000 = 69.5%, SPS: 3.9, ELAP: 25:39, ETA: 13:10 - loss: 24.6189\n",
            "6947/10000 = 69.5%, SPS: 3.9, ELAP: 25:39, ETA: 13:10 - loss: 23.7384\n",
            "6948/10000 = 69.5%, SPS: 3.9, ELAP: 25:39, ETA: 13:10 - loss: 24.2543\n",
            "6949/10000 = 69.5%, SPS: 3.9, ELAP: 25:40, ETA: 13:10 - loss: 24.2619\n",
            "6950/10000 = 69.5%, SPS: 3.9, ELAP: 25:40, ETA: 13:09 - loss: 24.8375\n",
            "6951/10000 = 69.5%, SPS: 3.9, ELAP: 25:40, ETA: 13:09 - loss: 24.9730\n",
            "6952/10000 = 69.5%, SPS: 3.9, ELAP: 25:40, ETA: 13:09 - loss: 24.1753\n",
            "6953/10000 = 69.5%, SPS: 3.9, ELAP: 25:41, ETA: 13:09 - loss: 23.7230\n",
            "6954/10000 = 69.5%, SPS: 3.9, ELAP: 25:41, ETA: 13:08 - loss: 24.2310\n",
            "6955/10000 = 69.5%, SPS: 3.9, ELAP: 25:41, ETA: 13:08 - loss: 24.2531\n",
            "6956/10000 = 69.6%, SPS: 3.9, ELAP: 25:41, ETA: 13:08 - loss: 24.4869\n",
            "6957/10000 = 69.6%, SPS: 3.9, ELAP: 25:41, ETA: 13:07 - loss: 24.2138\n",
            "6958/10000 = 69.6%, SPS: 3.9, ELAP: 25:42, ETA: 13:07 - loss: 23.8460\n",
            "6959/10000 = 69.6%, SPS: 3.9, ELAP: 25:42, ETA: 13:07 - loss: 24.1315\n",
            "6960/10000 = 69.6%, SPS: 3.9, ELAP: 25:42, ETA: 13:07 - loss: 24.0701\n",
            "6961/10000 = 69.6%, SPS: 3.9, ELAP: 25:42, ETA: 13:06 - loss: 23.7893\n",
            "6962/10000 = 69.6%, SPS: 3.9, ELAP: 25:43, ETA: 13:06 - loss: 24.1826\n",
            "6963/10000 = 69.6%, SPS: 3.9, ELAP: 25:43, ETA: 13:06 - loss: 24.2453\n",
            "6964/10000 = 69.6%, SPS: 3.9, ELAP: 25:43, ETA: 13:06 - loss: 24.4918\n",
            "6965/10000 = 69.7%, SPS: 3.9, ELAP: 25:43, ETA: 13:05 - loss: 24.4919\n",
            "6966/10000 = 69.7%, SPS: 3.9, ELAP: 25:44, ETA: 13:05 - loss: 23.7325\n",
            "6967/10000 = 69.7%, SPS: 3.9, ELAP: 25:44, ETA: 13:05 - loss: 24.2992\n",
            "6968/10000 = 69.7%, SPS: 3.9, ELAP: 25:44, ETA: 13:04 - loss: 23.8610\n",
            "6969/10000 = 69.7%, SPS: 3.9, ELAP: 25:44, ETA: 13:04 - loss: 24.3863\n",
            "6970/10000 = 69.7%, SPS: 3.9, ELAP: 25:44, ETA: 13:04 - loss: 23.8109\n",
            "6971/10000 = 69.7%, SPS: 3.9, ELAP: 25:45, ETA: 13:04 - loss: 24.4748\n",
            "6972/10000 = 69.7%, SPS: 3.9, ELAP: 25:45, ETA: 13:03 - loss: 24.5238\n",
            "6973/10000 = 69.7%, SPS: 3.9, ELAP: 25:45, ETA: 13:03 - loss: 24.1704\n",
            "6974/10000 = 69.7%, SPS: 3.9, ELAP: 25:45, ETA: 13:03 - loss: 24.3334\n",
            "6975/10000 = 69.8%, SPS: 3.9, ELAP: 25:46, ETA: 13:02 - loss: 24.4307\n",
            "6976/10000 = 69.8%, SPS: 3.9, ELAP: 25:46, ETA: 13:02 - loss: 24.6214\n",
            "6977/10000 = 69.8%, SPS: 3.9, ELAP: 25:46, ETA: 13:02 - loss: 24.8858\n",
            "6978/10000 = 69.8%, SPS: 3.9, ELAP: 25:46, ETA: 13:02 - loss: 23.6791\n",
            "6979/10000 = 69.8%, SPS: 3.9, ELAP: 25:46, ETA: 13:01 - loss: 24.6188\n",
            "6980/10000 = 69.8%, SPS: 3.9, ELAP: 25:47, ETA: 13:01 - loss: 24.3337\n",
            "6981/10000 = 69.8%, SPS: 3.9, ELAP: 25:47, ETA: 13:01 - loss: 24.3099\n",
            "6982/10000 = 69.8%, SPS: 3.9, ELAP: 25:47, ETA: 13:01 - loss: 24.2092\n",
            "6983/10000 = 69.8%, SPS: 3.9, ELAP: 25:47, ETA: 13:00 - loss: 23.9541\n",
            "6984/10000 = 69.8%, SPS: 3.9, ELAP: 25:48, ETA: 13:00 - loss: 24.5705\n",
            "6985/10000 = 69.8%, SPS: 3.9, ELAP: 25:48, ETA: 13:00 - loss: 24.1871\n",
            "6986/10000 = 69.9%, SPS: 3.9, ELAP: 25:48, ETA: 12:59 - loss: 24.5267\n",
            "6987/10000 = 69.9%, SPS: 3.9, ELAP: 25:48, ETA: 12:59 - loss: 24.3227\n",
            "6988/10000 = 69.9%, SPS: 3.9, ELAP: 25:49, ETA: 12:59 - loss: 24.0828\n",
            "6989/10000 = 69.9%, SPS: 3.9, ELAP: 25:49, ETA: 12:59 - loss: 23.4943\n",
            "6990/10000 = 69.9%, SPS: 3.9, ELAP: 25:49, ETA: 12:58 - loss: 24.6066\n",
            "6991/10000 = 69.9%, SPS: 3.9, ELAP: 25:49, ETA: 12:58 - loss: 24.0733\n",
            "6992/10000 = 69.9%, SPS: 3.9, ELAP: 25:49, ETA: 12:58 - loss: 23.7546\n",
            "6993/10000 = 69.9%, SPS: 3.9, ELAP: 25:50, ETA: 12:58 - loss: 24.2899\n",
            "6994/10000 = 69.9%, SPS: 3.9, ELAP: 25:50, ETA: 12:57 - loss: 24.9651\n",
            "6995/10000 = 70.0%, SPS: 3.9, ELAP: 25:50, ETA: 12:57 - loss: 24.4303\n",
            "6996/10000 = 70.0%, SPS: 3.9, ELAP: 25:50, ETA: 12:57 - loss: 23.9248\n",
            "6997/10000 = 70.0%, SPS: 3.9, ELAP: 25:51, ETA: 12:56 - loss: 23.3910\n",
            "6998/10000 = 70.0%, SPS: 3.9, ELAP: 25:51, ETA: 12:56 - loss: 23.5167\n",
            "6999/10000 = 70.0%, SPS: 3.9, ELAP: 25:51, ETA: 12:56 - loss: 24.5698\n",
            "7000/10000 = 70.0%, SPS: 3.9, ELAP: 25:54, ETA: 12:57 - loss: 24.9658\n",
            "7001/10000 = 70.0%, SPS: 3.9, ELAP: 25:54, ETA: 12:57 - loss: 23.9701\n",
            "7002/10000 = 70.0%, SPS: 3.9, ELAP: 25:54, ETA: 12:56 - loss: 24.0750\n",
            "7003/10000 = 70.0%, SPS: 3.9, ELAP: 25:54, ETA: 12:56 - loss: 24.6267\n",
            "7004/10000 = 70.0%, SPS: 3.9, ELAP: 25:54, ETA: 12:56 - loss: 24.0191\n",
            "7005/10000 = 70.0%, SPS: 3.9, ELAP: 25:55, ETA: 12:55 - loss: 24.0729\n",
            "7006/10000 = 70.1%, SPS: 3.9, ELAP: 25:55, ETA: 12:55 - loss: 24.3625\n",
            "7007/10000 = 70.1%, SPS: 3.9, ELAP: 25:55, ETA: 12:55 - loss: 24.0509\n",
            "7008/10000 = 70.1%, SPS: 3.9, ELAP: 25:55, ETA: 12:55 - loss: 23.5521\n",
            "7009/10000 = 70.1%, SPS: 3.9, ELAP: 25:56, ETA: 12:54 - loss: 24.6633\n",
            "7010/10000 = 70.1%, SPS: 3.9, ELAP: 25:56, ETA: 12:54 - loss: 24.2481\n",
            "7011/10000 = 70.1%, SPS: 3.9, ELAP: 25:56, ETA: 12:54 - loss: 24.1236\n",
            "7012/10000 = 70.1%, SPS: 3.9, ELAP: 25:56, ETA: 12:53 - loss: 23.9108\n",
            "7013/10000 = 70.1%, SPS: 3.9, ELAP: 25:57, ETA: 12:53 - loss: 24.3386\n",
            "7014/10000 = 70.1%, SPS: 3.9, ELAP: 25:57, ETA: 12:53 - loss: 23.7215\n",
            "7015/10000 = 70.2%, SPS: 3.9, ELAP: 25:57, ETA: 12:53 - loss: 23.9151\n",
            "7016/10000 = 70.2%, SPS: 3.9, ELAP: 25:57, ETA: 12:52 - loss: 23.8661\n",
            "7017/10000 = 70.2%, SPS: 3.9, ELAP: 25:57, ETA: 12:52 - loss: 24.0807\n",
            "7018/10000 = 70.2%, SPS: 3.9, ELAP: 25:58, ETA: 12:52 - loss: 23.4074\n",
            "7019/10000 = 70.2%, SPS: 3.9, ELAP: 25:58, ETA: 12:52 - loss: 24.9598\n",
            "7020/10000 = 70.2%, SPS: 3.9, ELAP: 25:58, ETA: 12:51 - loss: 24.0168\n",
            "7021/10000 = 70.2%, SPS: 3.9, ELAP: 25:58, ETA: 12:51 - loss: 24.3251\n",
            "7022/10000 = 70.2%, SPS: 3.9, ELAP: 25:59, ETA: 12:51 - loss: 24.4020\n",
            "7023/10000 = 70.2%, SPS: 3.9, ELAP: 25:59, ETA: 12:50 - loss: 24.0292\n",
            "7024/10000 = 70.2%, SPS: 3.9, ELAP: 25:59, ETA: 12:50 - loss: 24.2412\n",
            "7025/10000 = 70.2%, SPS: 3.9, ELAP: 25:59, ETA: 12:50 - loss: 23.5087\n",
            "7026/10000 = 70.3%, SPS: 3.9, ELAP: 25:59, ETA: 12:50 - loss: 24.1514\n",
            "7027/10000 = 70.3%, SPS: 3.9, ELAP: 26:00, ETA: 12:49 - loss: 24.4677\n",
            "7028/10000 = 70.3%, SPS: 3.9, ELAP: 26:00, ETA: 12:49 - loss: 24.3303\n",
            "7029/10000 = 70.3%, SPS: 3.9, ELAP: 26:00, ETA: 12:49 - loss: 25.0543\n",
            "7030/10000 = 70.3%, SPS: 3.9, ELAP: 26:00, ETA: 12:49 - loss: 24.1717\n",
            "7031/10000 = 70.3%, SPS: 3.9, ELAP: 26:01, ETA: 12:48 - loss: 24.0748\n",
            "7032/10000 = 70.3%, SPS: 3.9, ELAP: 26:01, ETA: 12:48 - loss: 24.1614\n",
            "7033/10000 = 70.3%, SPS: 3.9, ELAP: 26:01, ETA: 12:48 - loss: 24.4655\n",
            "7034/10000 = 70.3%, SPS: 3.9, ELAP: 26:01, ETA: 12:48 - loss: 24.3000\n",
            "7035/10000 = 70.3%, SPS: 3.9, ELAP: 26:02, ETA: 12:47 - loss: 24.1861\n",
            "7036/10000 = 70.4%, SPS: 3.9, ELAP: 26:02, ETA: 12:47 - loss: 24.3857\n",
            "7037/10000 = 70.4%, SPS: 3.9, ELAP: 26:02, ETA: 12:47 - loss: 24.8505\n",
            "7038/10000 = 70.4%, SPS: 3.9, ELAP: 26:02, ETA: 12:46 - loss: 24.1993\n",
            "7039/10000 = 70.4%, SPS: 3.9, ELAP: 26:03, ETA: 12:46 - loss: 23.9346\n",
            "7040/10000 = 70.4%, SPS: 3.9, ELAP: 26:03, ETA: 12:46 - loss: 24.1992\n",
            "7041/10000 = 70.4%, SPS: 3.9, ELAP: 26:03, ETA: 12:46 - loss: 24.6466\n",
            "7042/10000 = 70.4%, SPS: 3.9, ELAP: 26:03, ETA: 12:45 - loss: 24.4805\n",
            "7043/10000 = 70.4%, SPS: 3.9, ELAP: 26:04, ETA: 12:45 - loss: 24.4634\n",
            "7044/10000 = 70.4%, SPS: 3.9, ELAP: 26:04, ETA: 12:45 - loss: 24.4109\n",
            "7045/10000 = 70.5%, SPS: 3.9, ELAP: 26:04, ETA: 12:45 - loss: 24.0296\n",
            "7046/10000 = 70.5%, SPS: 3.9, ELAP: 26:04, ETA: 12:44 - loss: 24.5802\n",
            "7047/10000 = 70.5%, SPS: 3.9, ELAP: 26:04, ETA: 12:44 - loss: 24.2789\n",
            "7048/10000 = 70.5%, SPS: 3.9, ELAP: 26:05, ETA: 12:44 - loss: 23.9526\n",
            "7049/10000 = 70.5%, SPS: 3.9, ELAP: 26:05, ETA: 12:43 - loss: 23.6051\n",
            "7050/10000 = 70.5%, SPS: 3.9, ELAP: 26:05, ETA: 12:43 - loss: 24.6787\n",
            "7051/10000 = 70.5%, SPS: 3.9, ELAP: 26:05, ETA: 12:43 - loss: 23.9934\n",
            "7052/10000 = 70.5%, SPS: 3.9, ELAP: 26:06, ETA: 12:43 - loss: 23.9018\n",
            "7053/10000 = 70.5%, SPS: 3.9, ELAP: 26:06, ETA: 12:42 - loss: 24.4834\n",
            "7054/10000 = 70.5%, SPS: 3.9, ELAP: 26:06, ETA: 12:42 - loss: 24.0378\n",
            "7055/10000 = 70.5%, SPS: 3.9, ELAP: 26:06, ETA: 12:42 - loss: 24.1318\n",
            "7056/10000 = 70.6%, SPS: 3.9, ELAP: 26:07, ETA: 12:42 - loss: 24.0162\n",
            "7057/10000 = 70.6%, SPS: 3.9, ELAP: 26:07, ETA: 12:41 - loss: 24.1619\n",
            "7058/10000 = 70.6%, SPS: 3.9, ELAP: 26:07, ETA: 12:41 - loss: 23.7096\n",
            "7059/10000 = 70.6%, SPS: 3.9, ELAP: 26:07, ETA: 12:41 - loss: 24.5284\n",
            "7060/10000 = 70.6%, SPS: 3.9, ELAP: 26:07, ETA: 12:40 - loss: 24.1947\n",
            "7061/10000 = 70.6%, SPS: 3.9, ELAP: 26:08, ETA: 12:40 - loss: 24.7794\n",
            "7062/10000 = 70.6%, SPS: 3.9, ELAP: 26:08, ETA: 12:40 - loss: 24.4636\n",
            "7063/10000 = 70.6%, SPS: 3.9, ELAP: 26:08, ETA: 12:40 - loss: 24.3479\n",
            "7064/10000 = 70.6%, SPS: 3.9, ELAP: 26:08, ETA: 12:39 - loss: 24.5988\n",
            "7065/10000 = 70.7%, SPS: 3.9, ELAP: 26:09, ETA: 12:39 - loss: 24.3709\n",
            "7066/10000 = 70.7%, SPS: 3.9, ELAP: 26:09, ETA: 12:39 - loss: 24.1197\n",
            "7067/10000 = 70.7%, SPS: 3.9, ELAP: 26:09, ETA: 12:39 - loss: 24.1067\n",
            "7068/10000 = 70.7%, SPS: 3.9, ELAP: 26:09, ETA: 12:38 - loss: 24.0387\n",
            "7069/10000 = 70.7%, SPS: 3.9, ELAP: 26:09, ETA: 12:38 - loss: 24.1918\n",
            "7070/10000 = 70.7%, SPS: 3.9, ELAP: 26:10, ETA: 12:38 - loss: 24.4098\n",
            "7071/10000 = 70.7%, SPS: 3.9, ELAP: 26:10, ETA: 12:37 - loss: 24.1584\n",
            "7072/10000 = 70.7%, SPS: 3.9, ELAP: 26:10, ETA: 12:37 - loss: 25.1302\n",
            "7073/10000 = 70.7%, SPS: 3.9, ELAP: 26:10, ETA: 12:37 - loss: 24.5403\n",
            "7074/10000 = 70.7%, SPS: 3.9, ELAP: 26:11, ETA: 12:37 - loss: 24.3527\n",
            "7075/10000 = 70.8%, SPS: 3.9, ELAP: 26:11, ETA: 12:36 - loss: 24.6022\n",
            "7076/10000 = 70.8%, SPS: 3.9, ELAP: 26:11, ETA: 12:36 - loss: 24.1621\n",
            "7077/10000 = 70.8%, SPS: 3.9, ELAP: 26:11, ETA: 12:36 - loss: 24.1711\n",
            "7078/10000 = 70.8%, SPS: 3.9, ELAP: 26:12, ETA: 12:36 - loss: 24.0463\n",
            "7079/10000 = 70.8%, SPS: 3.9, ELAP: 26:12, ETA: 12:35 - loss: 24.5201\n",
            "7080/10000 = 70.8%, SPS: 3.9, ELAP: 26:12, ETA: 12:35 - loss: 24.5873\n",
            "7081/10000 = 70.8%, SPS: 3.9, ELAP: 26:12, ETA: 12:35 - loss: 23.8491\n",
            "7082/10000 = 70.8%, SPS: 3.9, ELAP: 26:13, ETA: 12:34 - loss: 24.4160\n",
            "7083/10000 = 70.8%, SPS: 3.9, ELAP: 26:13, ETA: 12:34 - loss: 24.4892\n",
            "7084/10000 = 70.8%, SPS: 3.9, ELAP: 26:13, ETA: 12:34 - loss: 24.3849\n",
            "7085/10000 = 70.8%, SPS: 3.9, ELAP: 26:13, ETA: 12:34 - loss: 24.2426\n",
            "7086/10000 = 70.9%, SPS: 3.9, ELAP: 26:13, ETA: 12:33 - loss: 23.5492\n",
            "7087/10000 = 70.9%, SPS: 3.9, ELAP: 26:14, ETA: 12:33 - loss: 24.0836\n",
            "7088/10000 = 70.9%, SPS: 3.9, ELAP: 26:14, ETA: 12:33 - loss: 24.4755\n",
            "7089/10000 = 70.9%, SPS: 3.9, ELAP: 26:14, ETA: 12:33 - loss: 24.0188\n",
            "7090/10000 = 70.9%, SPS: 3.9, ELAP: 26:14, ETA: 12:32 - loss: 24.8627\n",
            "7091/10000 = 70.9%, SPS: 3.9, ELAP: 26:15, ETA: 12:32 - loss: 24.0269\n",
            "7092/10000 = 70.9%, SPS: 3.9, ELAP: 26:15, ETA: 12:32 - loss: 24.3368\n",
            "7093/10000 = 70.9%, SPS: 3.9, ELAP: 26:15, ETA: 12:31 - loss: 24.1928\n",
            "7094/10000 = 70.9%, SPS: 3.9, ELAP: 26:15, ETA: 12:31 - loss: 23.6358\n",
            "7095/10000 = 71.0%, SPS: 3.9, ELAP: 26:16, ETA: 12:31 - loss: 24.4602\n",
            "7096/10000 = 71.0%, SPS: 3.9, ELAP: 26:16, ETA: 12:31 - loss: 24.4582\n",
            "7097/10000 = 71.0%, SPS: 3.9, ELAP: 26:16, ETA: 12:30 - loss: 23.7044\n",
            "7098/10000 = 71.0%, SPS: 3.9, ELAP: 26:16, ETA: 12:30 - loss: 24.4000\n",
            "7099/10000 = 71.0%, SPS: 3.9, ELAP: 26:16, ETA: 12:30 - loss: 23.8807\n",
            "7100/10000 = 71.0%, SPS: 3.9, ELAP: 26:19, ETA: 12:31 - loss: 23.6237\n",
            "7101/10000 = 71.0%, SPS: 3.9, ELAP: 26:19, ETA: 12:30 - loss: 23.7356\n",
            "7102/10000 = 71.0%, SPS: 3.9, ELAP: 26:19, ETA: 12:30 - loss: 23.2302\n",
            "7103/10000 = 71.0%, SPS: 3.9, ELAP: 26:20, ETA: 12:30 - loss: 23.9505\n",
            "7104/10000 = 71.0%, SPS: 3.9, ELAP: 26:20, ETA: 12:30 - loss: 24.4273\n",
            "7105/10000 = 71.0%, SPS: 3.9, ELAP: 26:20, ETA: 12:29 - loss: 24.0075\n",
            "7106/10000 = 71.1%, SPS: 3.9, ELAP: 26:20, ETA: 12:29 - loss: 23.9246\n",
            "7107/10000 = 71.1%, SPS: 3.9, ELAP: 26:21, ETA: 12:29 - loss: 23.6398\n",
            "7108/10000 = 71.1%, SPS: 3.9, ELAP: 26:21, ETA: 12:28 - loss: 24.5982\n",
            "7109/10000 = 71.1%, SPS: 3.9, ELAP: 26:21, ETA: 12:28 - loss: 24.3383\n",
            "7110/10000 = 71.1%, SPS: 3.9, ELAP: 26:21, ETA: 12:28 - loss: 24.4421\n",
            "7111/10000 = 71.1%, SPS: 3.9, ELAP: 26:22, ETA: 12:28 - loss: 24.4557\n",
            "7112/10000 = 71.1%, SPS: 3.9, ELAP: 26:22, ETA: 12:27 - loss: 24.1283\n",
            "7113/10000 = 71.1%, SPS: 3.9, ELAP: 26:22, ETA: 12:27 - loss: 23.5220\n",
            "7114/10000 = 71.1%, SPS: 3.9, ELAP: 26:22, ETA: 12:27 - loss: 24.2327\n",
            "7115/10000 = 71.2%, SPS: 3.9, ELAP: 26:22, ETA: 12:27 - loss: 25.3238\n",
            "7116/10000 = 71.2%, SPS: 3.9, ELAP: 26:23, ETA: 12:26 - loss: 24.4711\n",
            "7117/10000 = 71.2%, SPS: 3.9, ELAP: 26:23, ETA: 12:26 - loss: 24.3369\n",
            "7118/10000 = 71.2%, SPS: 3.9, ELAP: 26:23, ETA: 12:26 - loss: 24.2356\n",
            "7119/10000 = 71.2%, SPS: 3.9, ELAP: 26:23, ETA: 12:26 - loss: 24.4223\n",
            "7120/10000 = 71.2%, SPS: 3.9, ELAP: 26:24, ETA: 12:25 - loss: 24.2599\n",
            "7121/10000 = 71.2%, SPS: 3.9, ELAP: 26:24, ETA: 12:25 - loss: 23.8557\n",
            "7122/10000 = 71.2%, SPS: 3.9, ELAP: 26:24, ETA: 12:25 - loss: 24.5831\n",
            "7123/10000 = 71.2%, SPS: 3.9, ELAP: 26:24, ETA: 12:24 - loss: 24.6042\n",
            "7124/10000 = 71.2%, SPS: 3.9, ELAP: 26:25, ETA: 12:24 - loss: 23.9505\n",
            "7125/10000 = 71.2%, SPS: 3.9, ELAP: 26:25, ETA: 12:24 - loss: 24.2882\n",
            "7126/10000 = 71.3%, SPS: 3.9, ELAP: 26:25, ETA: 12:24 - loss: 23.2459\n",
            "7127/10000 = 71.3%, SPS: 3.9, ELAP: 26:25, ETA: 12:23 - loss: 24.3868\n",
            "7128/10000 = 71.3%, SPS: 3.9, ELAP: 26:25, ETA: 12:23 - loss: 23.7997\n",
            "7129/10000 = 71.3%, SPS: 3.9, ELAP: 26:26, ETA: 12:23 - loss: 23.4563\n",
            "7130/10000 = 71.3%, SPS: 3.9, ELAP: 26:26, ETA: 12:23 - loss: 24.6816\n",
            "7131/10000 = 71.3%, SPS: 3.9, ELAP: 26:26, ETA: 12:22 - loss: 23.9419\n",
            "7132/10000 = 71.3%, SPS: 3.9, ELAP: 26:26, ETA: 12:22 - loss: 24.0705\n",
            "7133/10000 = 71.3%, SPS: 3.9, ELAP: 26:27, ETA: 12:22 - loss: 23.6994\n",
            "7134/10000 = 71.3%, SPS: 3.9, ELAP: 26:27, ETA: 12:21 - loss: 23.7038\n",
            "7135/10000 = 71.3%, SPS: 3.9, ELAP: 26:27, ETA: 12:21 - loss: 24.0286\n",
            "7136/10000 = 71.4%, SPS: 3.9, ELAP: 26:27, ETA: 12:21 - loss: 23.5906\n",
            "7137/10000 = 71.4%, SPS: 3.9, ELAP: 26:28, ETA: 12:21 - loss: 23.5993\n",
            "7138/10000 = 71.4%, SPS: 3.9, ELAP: 26:28, ETA: 12:20 - loss: 24.7098\n",
            "7139/10000 = 71.4%, SPS: 3.9, ELAP: 26:28, ETA: 12:20 - loss: 24.1423\n",
            "7140/10000 = 71.4%, SPS: 3.9, ELAP: 26:28, ETA: 12:20 - loss: 24.5503\n",
            "7141/10000 = 71.4%, SPS: 3.9, ELAP: 26:29, ETA: 12:20 - loss: 24.2361\n",
            "7142/10000 = 71.4%, SPS: 3.9, ELAP: 26:29, ETA: 12:19 - loss: 23.9965\n",
            "7143/10000 = 71.4%, SPS: 3.9, ELAP: 26:29, ETA: 12:19 - loss: 24.4099\n",
            "7144/10000 = 71.4%, SPS: 3.9, ELAP: 26:29, ETA: 12:19 - loss: 23.8480\n",
            "7145/10000 = 71.5%, SPS: 3.9, ELAP: 26:29, ETA: 12:18 - loss: 24.8697\n",
            "7146/10000 = 71.5%, SPS: 3.9, ELAP: 26:30, ETA: 12:18 - loss: 23.6908\n",
            "7147/10000 = 71.5%, SPS: 3.9, ELAP: 26:30, ETA: 12:18 - loss: 24.8480\n",
            "7148/10000 = 71.5%, SPS: 3.9, ELAP: 26:30, ETA: 12:18 - loss: 23.9816\n",
            "7149/10000 = 71.5%, SPS: 3.9, ELAP: 26:30, ETA: 12:17 - loss: 24.8685\n",
            "7150/10000 = 71.5%, SPS: 3.9, ELAP: 26:31, ETA: 12:17 - loss: 23.9268\n",
            "7151/10000 = 71.5%, SPS: 3.9, ELAP: 26:31, ETA: 12:17 - loss: 24.0961\n",
            "7152/10000 = 71.5%, SPS: 3.9, ELAP: 26:31, ETA: 12:17 - loss: 24.4855\n",
            "7153/10000 = 71.5%, SPS: 3.9, ELAP: 26:31, ETA: 12:16 - loss: 24.0890\n",
            "7154/10000 = 71.5%, SPS: 3.9, ELAP: 26:32, ETA: 12:16 - loss: 24.4754\n",
            "7155/10000 = 71.5%, SPS: 3.9, ELAP: 26:32, ETA: 12:16 - loss: 24.0413\n",
            "7156/10000 = 71.6%, SPS: 3.9, ELAP: 26:32, ETA: 12:15 - loss: 23.9626\n",
            "7157/10000 = 71.6%, SPS: 3.9, ELAP: 26:32, ETA: 12:15 - loss: 24.4940\n",
            "7158/10000 = 71.6%, SPS: 3.9, ELAP: 26:32, ETA: 12:15 - loss: 24.1091\n",
            "7159/10000 = 71.6%, SPS: 3.9, ELAP: 26:33, ETA: 12:15 - loss: 23.9705\n",
            "7160/10000 = 71.6%, SPS: 3.9, ELAP: 26:33, ETA: 12:14 - loss: 23.7919\n",
            "7161/10000 = 71.6%, SPS: 3.9, ELAP: 26:33, ETA: 12:14 - loss: 23.7961\n",
            "7162/10000 = 71.6%, SPS: 3.9, ELAP: 26:33, ETA: 12:14 - loss: 24.0838\n",
            "7163/10000 = 71.6%, SPS: 3.9, ELAP: 26:34, ETA: 12:14 - loss: 23.5645\n",
            "7164/10000 = 71.6%, SPS: 3.9, ELAP: 26:34, ETA: 12:13 - loss: 24.2791\n",
            "7165/10000 = 71.7%, SPS: 3.9, ELAP: 26:34, ETA: 12:13 - loss: 23.8587\n",
            "7166/10000 = 71.7%, SPS: 3.9, ELAP: 26:34, ETA: 12:13 - loss: 24.4091\n",
            "7167/10000 = 71.7%, SPS: 3.9, ELAP: 26:34, ETA: 12:12 - loss: 24.9155\n",
            "7168/10000 = 71.7%, SPS: 3.9, ELAP: 26:35, ETA: 12:12 - loss: 23.9676\n",
            "7169/10000 = 71.7%, SPS: 3.9, ELAP: 26:35, ETA: 12:12 - loss: 23.8734\n",
            "7170/10000 = 71.7%, SPS: 3.9, ELAP: 26:35, ETA: 12:12 - loss: 24.6005\n",
            "7171/10000 = 71.7%, SPS: 3.9, ELAP: 26:35, ETA: 12:11 - loss: 24.1001\n",
            "7172/10000 = 71.7%, SPS: 3.9, ELAP: 26:36, ETA: 12:11 - loss: 24.5797\n",
            "7173/10000 = 71.7%, SPS: 3.9, ELAP: 26:36, ETA: 12:11 - loss: 24.6769\n",
            "7174/10000 = 71.7%, SPS: 3.9, ELAP: 26:36, ETA: 12:11 - loss: 23.8809\n",
            "7175/10000 = 71.8%, SPS: 3.9, ELAP: 26:36, ETA: 12:10 - loss: 24.0853\n",
            "7176/10000 = 71.8%, SPS: 3.9, ELAP: 26:36, ETA: 12:10 - loss: 24.2571\n",
            "7177/10000 = 71.8%, SPS: 3.9, ELAP: 26:37, ETA: 12:10 - loss: 23.6916\n",
            "7178/10000 = 71.8%, SPS: 3.9, ELAP: 26:37, ETA: 12:09 - loss: 24.7241\n",
            "7179/10000 = 71.8%, SPS: 3.9, ELAP: 26:37, ETA: 12:09 - loss: 23.9668\n",
            "7180/10000 = 71.8%, SPS: 3.9, ELAP: 26:37, ETA: 12:09 - loss: 24.2802\n",
            "7181/10000 = 71.8%, SPS: 3.9, ELAP: 26:38, ETA: 12:09 - loss: 23.7437\n",
            "7182/10000 = 71.8%, SPS: 3.9, ELAP: 26:38, ETA: 12:08 - loss: 23.8523\n",
            "7183/10000 = 71.8%, SPS: 3.9, ELAP: 26:38, ETA: 12:08 - loss: 24.0792\n",
            "7184/10000 = 71.8%, SPS: 3.9, ELAP: 26:38, ETA: 12:08 - loss: 23.9731\n",
            "7185/10000 = 71.8%, SPS: 3.9, ELAP: 26:39, ETA: 12:08 - loss: 24.4962\n",
            "7186/10000 = 71.9%, SPS: 3.9, ELAP: 26:39, ETA: 12:07 - loss: 24.2221\n",
            "7187/10000 = 71.9%, SPS: 3.9, ELAP: 26:39, ETA: 12:07 - loss: 23.6349\n",
            "7188/10000 = 71.9%, SPS: 3.9, ELAP: 26:39, ETA: 12:07 - loss: 24.4527\n",
            "7189/10000 = 71.9%, SPS: 3.9, ELAP: 26:39, ETA: 12:06 - loss: 24.8654\n",
            "7190/10000 = 71.9%, SPS: 3.9, ELAP: 26:40, ETA: 12:06 - loss: 24.0856\n",
            "7191/10000 = 71.9%, SPS: 3.9, ELAP: 26:40, ETA: 12:06 - loss: 24.9835\n",
            "7192/10000 = 71.9%, SPS: 3.9, ELAP: 26:40, ETA: 12:06 - loss: 24.1908\n",
            "7193/10000 = 71.9%, SPS: 3.9, ELAP: 26:40, ETA: 12:05 - loss: 24.1950\n",
            "7194/10000 = 71.9%, SPS: 3.9, ELAP: 26:41, ETA: 12:05 - loss: 23.8893\n",
            "7195/10000 = 72.0%, SPS: 3.9, ELAP: 26:41, ETA: 12:05 - loss: 23.9532\n",
            "7196/10000 = 72.0%, SPS: 3.9, ELAP: 26:41, ETA: 12:05 - loss: 23.9271\n",
            "7197/10000 = 72.0%, SPS: 3.9, ELAP: 26:41, ETA: 12:04 - loss: 24.1648\n",
            "7198/10000 = 72.0%, SPS: 3.9, ELAP: 26:41, ETA: 12:04 - loss: 24.0177\n",
            "7199/10000 = 72.0%, SPS: 3.9, ELAP: 26:42, ETA: 12:04 - loss: 24.3708\n",
            "7200/10000 = 72.0%, SPS: 3.9, ELAP: 26:44, ETA: 12:04 - loss: 24.1585\n",
            "7201/10000 = 72.0%, SPS: 3.9, ELAP: 26:44, ETA: 12:04 - loss: 24.1459\n",
            "7202/10000 = 72.0%, SPS: 3.9, ELAP: 26:45, ETA: 12:04 - loss: 24.4167\n",
            "7203/10000 = 72.0%, SPS: 3.9, ELAP: 26:45, ETA: 12:04 - loss: 24.2197\n",
            "7204/10000 = 72.0%, SPS: 3.9, ELAP: 26:45, ETA: 12:03 - loss: 24.1661\n",
            "7205/10000 = 72.0%, SPS: 3.9, ELAP: 26:45, ETA: 12:03 - loss: 23.2587\n",
            "7206/10000 = 72.1%, SPS: 3.9, ELAP: 26:45, ETA: 12:03 - loss: 23.5901\n",
            "7207/10000 = 72.1%, SPS: 3.9, ELAP: 26:46, ETA: 12:03 - loss: 24.5661\n",
            "7208/10000 = 72.1%, SPS: 3.9, ELAP: 26:46, ETA: 12:02 - loss: 23.3945\n",
            "7209/10000 = 72.1%, SPS: 3.9, ELAP: 26:46, ETA: 12:02 - loss: 23.7755\n",
            "7210/10000 = 72.1%, SPS: 3.9, ELAP: 26:46, ETA: 12:02 - loss: 23.7235\n",
            "7211/10000 = 72.1%, SPS: 3.9, ELAP: 26:47, ETA: 12:01 - loss: 23.7790\n",
            "7212/10000 = 72.1%, SPS: 3.9, ELAP: 26:47, ETA: 12:01 - loss: 24.5479\n",
            "7213/10000 = 72.1%, SPS: 3.9, ELAP: 26:47, ETA: 12:01 - loss: 24.0483\n",
            "7214/10000 = 72.1%, SPS: 3.9, ELAP: 26:47, ETA: 12:01 - loss: 24.2586\n",
            "7215/10000 = 72.2%, SPS: 3.9, ELAP: 26:47, ETA: 12:00 - loss: 24.0077\n",
            "7216/10000 = 72.2%, SPS: 3.9, ELAP: 26:48, ETA: 12:00 - loss: 23.7813\n",
            "7217/10000 = 72.2%, SPS: 3.9, ELAP: 26:48, ETA: 12:00 - loss: 24.1469\n",
            "7218/10000 = 72.2%, SPS: 3.9, ELAP: 26:48, ETA: 12:00 - loss: 24.2557\n",
            "7219/10000 = 72.2%, SPS: 3.9, ELAP: 26:48, ETA: 11:59 - loss: 24.0286\n",
            "7220/10000 = 72.2%, SPS: 3.9, ELAP: 26:49, ETA: 11:59 - loss: 24.4930\n",
            "7221/10000 = 72.2%, SPS: 3.9, ELAP: 26:49, ETA: 11:59 - loss: 24.2082\n",
            "7222/10000 = 72.2%, SPS: 3.9, ELAP: 26:49, ETA: 11:58 - loss: 24.5195\n",
            "7223/10000 = 72.2%, SPS: 3.9, ELAP: 26:49, ETA: 11:58 - loss: 24.3073\n",
            "7224/10000 = 72.2%, SPS: 3.9, ELAP: 26:50, ETA: 11:58 - loss: 24.0457\n",
            "7225/10000 = 72.2%, SPS: 3.9, ELAP: 26:50, ETA: 11:58 - loss: 24.2908\n",
            "7226/10000 = 72.3%, SPS: 3.9, ELAP: 26:50, ETA: 11:57 - loss: 24.3697\n",
            "7227/10000 = 72.3%, SPS: 3.9, ELAP: 26:50, ETA: 11:57 - loss: 23.9368\n",
            "7228/10000 = 72.3%, SPS: 3.9, ELAP: 26:51, ETA: 11:57 - loss: 24.2785\n",
            "7229/10000 = 72.3%, SPS: 3.9, ELAP: 26:51, ETA: 11:57 - loss: 23.4659\n",
            "7230/10000 = 72.3%, SPS: 3.9, ELAP: 26:51, ETA: 11:56 - loss: 23.9508\n",
            "7231/10000 = 72.3%, SPS: 3.9, ELAP: 26:51, ETA: 11:56 - loss: 24.2445\n",
            "7232/10000 = 72.3%, SPS: 3.9, ELAP: 26:52, ETA: 11:56 - loss: 24.7709\n",
            "7233/10000 = 72.3%, SPS: 3.9, ELAP: 26:52, ETA: 11:56 - loss: 24.4832\n",
            "7234/10000 = 72.3%, SPS: 3.9, ELAP: 26:52, ETA: 11:55 - loss: 23.9284\n",
            "7235/10000 = 72.3%, SPS: 3.9, ELAP: 26:52, ETA: 11:55 - loss: 24.0261\n",
            "7236/10000 = 72.4%, SPS: 3.9, ELAP: 26:52, ETA: 11:55 - loss: 23.6846\n",
            "7237/10000 = 72.4%, SPS: 3.9, ELAP: 26:53, ETA: 11:54 - loss: 23.7998\n",
            "7238/10000 = 72.4%, SPS: 3.9, ELAP: 26:53, ETA: 11:54 - loss: 23.6718\n",
            "7239/10000 = 72.4%, SPS: 3.9, ELAP: 26:53, ETA: 11:54 - loss: 24.1499\n",
            "7240/10000 = 72.4%, SPS: 3.9, ELAP: 26:53, ETA: 11:54 - loss: 24.6790\n",
            "7241/10000 = 72.4%, SPS: 3.9, ELAP: 26:54, ETA: 11:53 - loss: 24.5614\n",
            "7242/10000 = 72.4%, SPS: 3.9, ELAP: 26:54, ETA: 11:53 - loss: 24.2429\n",
            "7243/10000 = 72.4%, SPS: 3.9, ELAP: 26:54, ETA: 11:53 - loss: 24.1967\n",
            "7244/10000 = 72.4%, SPS: 3.9, ELAP: 26:54, ETA: 11:53 - loss: 24.2198\n",
            "7245/10000 = 72.5%, SPS: 3.9, ELAP: 26:55, ETA: 11:52 - loss: 24.5616\n",
            "7246/10000 = 72.5%, SPS: 3.9, ELAP: 26:55, ETA: 11:52 - loss: 24.3992\n",
            "7247/10000 = 72.5%, SPS: 3.9, ELAP: 26:55, ETA: 11:52 - loss: 24.2911\n",
            "7248/10000 = 72.5%, SPS: 3.9, ELAP: 26:55, ETA: 11:51 - loss: 23.8484\n",
            "7249/10000 = 72.5%, SPS: 3.9, ELAP: 26:56, ETA: 11:51 - loss: 23.5588\n",
            "7250/10000 = 72.5%, SPS: 3.9, ELAP: 26:56, ETA: 11:51 - loss: 24.5026\n",
            "7251/10000 = 72.5%, SPS: 3.9, ELAP: 26:56, ETA: 11:51 - loss: 24.0546\n",
            "7252/10000 = 72.5%, SPS: 3.9, ELAP: 26:56, ETA: 11:50 - loss: 24.3095\n",
            "7253/10000 = 72.5%, SPS: 3.9, ELAP: 26:56, ETA: 11:50 - loss: 24.3890\n",
            "7254/10000 = 72.5%, SPS: 3.9, ELAP: 26:57, ETA: 11:50 - loss: 23.9503\n",
            "7255/10000 = 72.5%, SPS: 3.9, ELAP: 26:57, ETA: 11:50 - loss: 24.6358\n",
            "7256/10000 = 72.6%, SPS: 3.9, ELAP: 26:57, ETA: 11:49 - loss: 24.4399\n",
            "7257/10000 = 72.6%, SPS: 3.9, ELAP: 26:57, ETA: 11:49 - loss: 24.0492\n",
            "7258/10000 = 72.6%, SPS: 3.9, ELAP: 26:58, ETA: 11:49 - loss: 24.2029\n",
            "7259/10000 = 72.6%, SPS: 3.9, ELAP: 26:58, ETA: 11:48 - loss: 24.1759\n",
            "7260/10000 = 72.6%, SPS: 3.9, ELAP: 26:58, ETA: 11:48 - loss: 23.9524\n",
            "7261/10000 = 72.6%, SPS: 3.9, ELAP: 26:58, ETA: 11:48 - loss: 23.5576\n",
            "7262/10000 = 72.6%, SPS: 3.9, ELAP: 26:59, ETA: 11:48 - loss: 24.3736\n",
            "7263/10000 = 72.6%, SPS: 3.9, ELAP: 26:59, ETA: 11:47 - loss: 24.3055\n",
            "7264/10000 = 72.6%, SPS: 3.9, ELAP: 26:59, ETA: 11:47 - loss: 24.2552\n",
            "7265/10000 = 72.7%, SPS: 3.9, ELAP: 26:59, ETA: 11:47 - loss: 24.1354\n",
            "7266/10000 = 72.7%, SPS: 3.9, ELAP: 26:59, ETA: 11:47 - loss: 23.9983\n",
            "7267/10000 = 72.7%, SPS: 3.9, ELAP: 27:00, ETA: 11:46 - loss: 24.3762\n",
            "7268/10000 = 72.7%, SPS: 3.9, ELAP: 27:00, ETA: 11:46 - loss: 23.7891\n",
            "7269/10000 = 72.7%, SPS: 3.9, ELAP: 27:00, ETA: 11:46 - loss: 24.1864\n",
            "7270/10000 = 72.7%, SPS: 3.9, ELAP: 27:00, ETA: 11:45 - loss: 24.5115\n",
            "7271/10000 = 72.7%, SPS: 3.9, ELAP: 27:01, ETA: 11:45 - loss: 24.0061\n",
            "7272/10000 = 72.7%, SPS: 3.9, ELAP: 27:01, ETA: 11:45 - loss: 23.8565\n",
            "7273/10000 = 72.7%, SPS: 3.9, ELAP: 27:01, ETA: 11:45 - loss: 23.9681\n",
            "7274/10000 = 72.7%, SPS: 3.9, ELAP: 27:01, ETA: 11:44 - loss: 23.7442\n",
            "7275/10000 = 72.8%, SPS: 3.9, ELAP: 27:01, ETA: 11:44 - loss: 23.5360\n",
            "7276/10000 = 72.8%, SPS: 3.9, ELAP: 27:02, ETA: 11:44 - loss: 23.9304\n",
            "7277/10000 = 72.8%, SPS: 3.9, ELAP: 27:02, ETA: 11:44 - loss: 23.5643\n",
            "7278/10000 = 72.8%, SPS: 3.9, ELAP: 27:02, ETA: 11:43 - loss: 24.3678\n",
            "7279/10000 = 72.8%, SPS: 3.9, ELAP: 27:02, ETA: 11:43 - loss: 24.4995\n",
            "7280/10000 = 72.8%, SPS: 3.9, ELAP: 27:03, ETA: 11:43 - loss: 24.2811\n",
            "7281/10000 = 72.8%, SPS: 3.9, ELAP: 27:03, ETA: 11:43 - loss: 24.2407\n",
            "7282/10000 = 72.8%, SPS: 3.9, ELAP: 27:03, ETA: 11:42 - loss: 23.7553\n",
            "7283/10000 = 72.8%, SPS: 3.9, ELAP: 27:03, ETA: 11:42 - loss: 23.9460\n",
            "7284/10000 = 72.8%, SPS: 3.9, ELAP: 27:03, ETA: 11:42 - loss: 24.6797\n",
            "7285/10000 = 72.8%, SPS: 3.9, ELAP: 27:04, ETA: 11:41 - loss: 23.5987\n",
            "7286/10000 = 72.9%, SPS: 3.9, ELAP: 27:04, ETA: 11:41 - loss: 24.5841\n",
            "7287/10000 = 72.9%, SPS: 3.9, ELAP: 27:04, ETA: 11:41 - loss: 24.7363\n",
            "7288/10000 = 72.9%, SPS: 3.9, ELAP: 27:04, ETA: 11:41 - loss: 24.4935\n",
            "7289/10000 = 72.9%, SPS: 3.9, ELAP: 27:05, ETA: 11:40 - loss: 24.4579\n",
            "7290/10000 = 72.9%, SPS: 3.9, ELAP: 27:05, ETA: 11:40 - loss: 24.4526\n",
            "7291/10000 = 72.9%, SPS: 3.9, ELAP: 27:05, ETA: 11:40 - loss: 23.9949\n",
            "7292/10000 = 72.9%, SPS: 3.9, ELAP: 27:05, ETA: 11:40 - loss: 24.5300\n",
            "7293/10000 = 72.9%, SPS: 3.9, ELAP: 27:06, ETA: 11:39 - loss: 23.6503\n",
            "7294/10000 = 72.9%, SPS: 3.9, ELAP: 27:06, ETA: 11:39 - loss: 24.0902\n",
            "7295/10000 = 73.0%, SPS: 3.9, ELAP: 27:06, ETA: 11:39 - loss: 24.1443\n",
            "7296/10000 = 73.0%, SPS: 3.9, ELAP: 27:06, ETA: 11:38 - loss: 24.0543\n",
            "7297/10000 = 73.0%, SPS: 3.9, ELAP: 27:06, ETA: 11:38 - loss: 24.5285\n",
            "7298/10000 = 73.0%, SPS: 3.9, ELAP: 27:07, ETA: 11:38 - loss: 24.1185\n",
            "7299/10000 = 73.0%, SPS: 3.9, ELAP: 27:07, ETA: 11:38 - loss: 24.3213\n",
            "7300/10000 = 73.0%, SPS: 3.9, ELAP: 27:09, ETA: 11:38 - loss: 24.1282\n",
            "7301/10000 = 73.0%, SPS: 3.9, ELAP: 27:09, ETA: 11:38 - loss: 24.2256\n",
            "7302/10000 = 73.0%, SPS: 3.9, ELAP: 27:10, ETA: 11:38 - loss: 24.1974\n",
            "7303/10000 = 73.0%, SPS: 3.9, ELAP: 27:10, ETA: 11:37 - loss: 23.6625\n",
            "7304/10000 = 73.0%, SPS: 3.9, ELAP: 27:10, ETA: 11:37 - loss: 23.8896\n",
            "7305/10000 = 73.0%, SPS: 3.9, ELAP: 27:10, ETA: 11:37 - loss: 24.2178\n",
            "7306/10000 = 73.1%, SPS: 3.9, ELAP: 27:11, ETA: 11:37 - loss: 24.2502\n",
            "7307/10000 = 73.1%, SPS: 3.9, ELAP: 27:11, ETA: 11:36 - loss: 24.0484\n",
            "7308/10000 = 73.1%, SPS: 3.9, ELAP: 27:11, ETA: 11:36 - loss: 23.4150\n",
            "7309/10000 = 73.1%, SPS: 3.9, ELAP: 27:11, ETA: 11:36 - loss: 23.9080\n",
            "7310/10000 = 73.1%, SPS: 3.9, ELAP: 27:11, ETA: 11:36 - loss: 24.2507\n",
            "7311/10000 = 73.1%, SPS: 3.9, ELAP: 27:12, ETA: 11:35 - loss: 24.4673\n",
            "7312/10000 = 73.1%, SPS: 3.9, ELAP: 27:12, ETA: 11:35 - loss: 24.3307\n",
            "7313/10000 = 73.1%, SPS: 3.9, ELAP: 27:12, ETA: 11:35 - loss: 23.6940\n",
            "7314/10000 = 73.1%, SPS: 3.9, ELAP: 27:12, ETA: 11:34 - loss: 24.0428\n",
            "7315/10000 = 73.2%, SPS: 3.9, ELAP: 27:13, ETA: 11:34 - loss: 24.0757\n",
            "7316/10000 = 73.2%, SPS: 3.9, ELAP: 27:13, ETA: 11:34 - loss: 24.6259\n",
            "7317/10000 = 73.2%, SPS: 3.9, ELAP: 27:13, ETA: 11:34 - loss: 24.5051\n",
            "7318/10000 = 73.2%, SPS: 3.9, ELAP: 27:13, ETA: 11:33 - loss: 24.8064\n",
            "7319/10000 = 73.2%, SPS: 3.9, ELAP: 27:14, ETA: 11:33 - loss: 24.2629\n",
            "7320/10000 = 73.2%, SPS: 3.9, ELAP: 27:14, ETA: 11:33 - loss: 24.0037\n",
            "7321/10000 = 73.2%, SPS: 3.9, ELAP: 27:14, ETA: 11:33 - loss: 24.0765\n",
            "7322/10000 = 73.2%, SPS: 3.9, ELAP: 27:14, ETA: 11:32 - loss: 23.7216\n",
            "7323/10000 = 73.2%, SPS: 3.9, ELAP: 27:14, ETA: 11:32 - loss: 23.8103\n",
            "7324/10000 = 73.2%, SPS: 3.9, ELAP: 27:15, ETA: 11:32 - loss: 23.8314\n",
            "7325/10000 = 73.2%, SPS: 3.9, ELAP: 27:15, ETA: 11:31 - loss: 23.8595\n",
            "7326/10000 = 73.3%, SPS: 3.9, ELAP: 27:15, ETA: 11:31 - loss: 24.7280\n",
            "7327/10000 = 73.3%, SPS: 3.9, ELAP: 27:15, ETA: 11:31 - loss: 24.8088\n",
            "7328/10000 = 73.3%, SPS: 3.9, ELAP: 27:16, ETA: 11:31 - loss: 23.8395\n",
            "7329/10000 = 73.3%, SPS: 3.9, ELAP: 27:16, ETA: 11:30 - loss: 23.4493\n",
            "7330/10000 = 73.3%, SPS: 3.9, ELAP: 27:16, ETA: 11:30 - loss: 24.4602\n",
            "7331/10000 = 73.3%, SPS: 3.9, ELAP: 27:16, ETA: 11:30 - loss: 24.7541\n",
            "7332/10000 = 73.3%, SPS: 3.9, ELAP: 27:17, ETA: 11:30 - loss: 24.6592\n",
            "7333/10000 = 73.3%, SPS: 3.9, ELAP: 27:17, ETA: 11:29 - loss: 24.8019\n",
            "7334/10000 = 73.3%, SPS: 3.9, ELAP: 27:17, ETA: 11:29 - loss: 24.3868\n",
            "7335/10000 = 73.3%, SPS: 3.9, ELAP: 27:17, ETA: 11:29 - loss: 23.8133\n",
            "7336/10000 = 73.4%, SPS: 3.9, ELAP: 27:18, ETA: 11:28 - loss: 24.1683\n",
            "7337/10000 = 73.4%, SPS: 3.9, ELAP: 27:18, ETA: 11:28 - loss: 24.0914\n",
            "7338/10000 = 73.4%, SPS: 3.9, ELAP: 27:18, ETA: 11:28 - loss: 23.7564\n",
            "7339/10000 = 73.4%, SPS: 3.9, ELAP: 27:18, ETA: 11:28 - loss: 24.6765\n",
            "7340/10000 = 73.4%, SPS: 3.9, ELAP: 27:18, ETA: 11:27 - loss: 24.1499\n",
            "7341/10000 = 73.4%, SPS: 3.9, ELAP: 27:19, ETA: 11:27 - loss: 24.5089\n",
            "7342/10000 = 73.4%, SPS: 3.9, ELAP: 27:19, ETA: 11:27 - loss: 23.7802\n",
            "7343/10000 = 73.4%, SPS: 3.9, ELAP: 27:19, ETA: 11:27 - loss: 23.6610\n",
            "7344/10000 = 73.4%, SPS: 3.9, ELAP: 27:19, ETA: 11:26 - loss: 24.2155\n",
            "7345/10000 = 73.5%, SPS: 3.9, ELAP: 27:20, ETA: 11:26 - loss: 23.4772\n",
            "7346/10000 = 73.5%, SPS: 3.9, ELAP: 27:20, ETA: 11:26 - loss: 24.2338\n",
            "7347/10000 = 73.5%, SPS: 3.9, ELAP: 27:20, ETA: 11:26 - loss: 24.4452\n",
            "7348/10000 = 73.5%, SPS: 3.9, ELAP: 27:20, ETA: 11:25 - loss: 23.9078\n",
            "7349/10000 = 73.5%, SPS: 3.9, ELAP: 27:21, ETA: 11:25 - loss: 24.0939\n",
            "7350/10000 = 73.5%, SPS: 3.9, ELAP: 27:21, ETA: 11:25 - loss: 24.0443\n",
            "7351/10000 = 73.5%, SPS: 3.9, ELAP: 27:21, ETA: 11:24 - loss: 24.4294\n",
            "7352/10000 = 73.5%, SPS: 3.9, ELAP: 27:21, ETA: 11:24 - loss: 23.7264\n",
            "7353/10000 = 73.5%, SPS: 3.9, ELAP: 27:21, ETA: 11:24 - loss: 24.3177\n",
            "7354/10000 = 73.5%, SPS: 3.9, ELAP: 27:22, ETA: 11:24 - loss: 24.1114\n",
            "7355/10000 = 73.5%, SPS: 3.9, ELAP: 27:22, ETA: 11:23 - loss: 24.3143\n",
            "7356/10000 = 73.6%, SPS: 3.9, ELAP: 27:22, ETA: 11:23 - loss: 24.0380\n",
            "7357/10000 = 73.6%, SPS: 3.9, ELAP: 27:22, ETA: 11:23 - loss: 24.2351\n",
            "7358/10000 = 73.6%, SPS: 3.9, ELAP: 27:23, ETA: 11:23 - loss: 24.1483\n",
            "7359/10000 = 73.6%, SPS: 3.9, ELAP: 27:23, ETA: 11:22 - loss: 24.3380\n",
            "7360/10000 = 73.6%, SPS: 3.9, ELAP: 27:23, ETA: 11:22 - loss: 24.3971\n",
            "7361/10000 = 73.6%, SPS: 3.9, ELAP: 27:23, ETA: 11:22 - loss: 23.9321\n",
            "7362/10000 = 73.6%, SPS: 3.9, ELAP: 27:23, ETA: 11:21 - loss: 24.0736\n",
            "7363/10000 = 73.6%, SPS: 3.9, ELAP: 27:24, ETA: 11:21 - loss: 24.6963\n",
            "7364/10000 = 73.6%, SPS: 3.9, ELAP: 27:24, ETA: 11:21 - loss: 24.0392\n",
            "7365/10000 = 73.7%, SPS: 3.9, ELAP: 27:24, ETA: 11:21 - loss: 24.4736\n",
            "7366/10000 = 73.7%, SPS: 3.9, ELAP: 27:24, ETA: 11:20 - loss: 24.5918\n",
            "7367/10000 = 73.7%, SPS: 3.9, ELAP: 27:25, ETA: 11:20 - loss: 24.5390\n",
            "7368/10000 = 73.7%, SPS: 3.9, ELAP: 27:25, ETA: 11:20 - loss: 24.5928\n",
            "7369/10000 = 73.7%, SPS: 3.9, ELAP: 27:25, ETA: 11:20 - loss: 24.3680\n",
            "7370/10000 = 73.7%, SPS: 3.9, ELAP: 27:25, ETA: 11:19 - loss: 24.0285\n",
            "7371/10000 = 73.7%, SPS: 3.9, ELAP: 27:26, ETA: 11:19 - loss: 24.6810\n",
            "7372/10000 = 73.7%, SPS: 3.9, ELAP: 27:26, ETA: 11:19 - loss: 24.1677\n",
            "7373/10000 = 73.7%, SPS: 3.9, ELAP: 27:26, ETA: 11:18 - loss: 24.1391\n",
            "7374/10000 = 73.7%, SPS: 3.9, ELAP: 27:26, ETA: 11:18 - loss: 24.3818\n",
            "7375/10000 = 73.8%, SPS: 3.9, ELAP: 27:26, ETA: 11:18 - loss: 24.7666\n",
            "7376/10000 = 73.8%, SPS: 3.9, ELAP: 27:27, ETA: 11:18 - loss: 23.9265\n",
            "7377/10000 = 73.8%, SPS: 3.9, ELAP: 27:27, ETA: 11:17 - loss: 24.6298\n",
            "7378/10000 = 73.8%, SPS: 3.9, ELAP: 27:27, ETA: 11:17 - loss: 24.4269\n",
            "7379/10000 = 73.8%, SPS: 3.9, ELAP: 27:27, ETA: 11:17 - loss: 24.3431\n",
            "7380/10000 = 73.8%, SPS: 3.9, ELAP: 27:28, ETA: 11:17 - loss: 24.2694\n",
            "7381/10000 = 73.8%, SPS: 3.9, ELAP: 27:28, ETA: 11:16 - loss: 23.5814\n",
            "7382/10000 = 73.8%, SPS: 3.9, ELAP: 27:28, ETA: 11:16 - loss: 24.3918\n",
            "7383/10000 = 73.8%, SPS: 3.9, ELAP: 27:28, ETA: 11:16 - loss: 23.5903\n",
            "7384/10000 = 73.8%, SPS: 3.9, ELAP: 27:29, ETA: 11:16 - loss: 23.9638\n",
            "7385/10000 = 73.8%, SPS: 3.9, ELAP: 27:29, ETA: 11:15 - loss: 24.2691\n",
            "7386/10000 = 73.9%, SPS: 3.9, ELAP: 27:29, ETA: 11:15 - loss: 23.9726\n",
            "7387/10000 = 73.9%, SPS: 3.9, ELAP: 27:29, ETA: 11:15 - loss: 24.6743\n",
            "7388/10000 = 73.9%, SPS: 3.9, ELAP: 27:29, ETA: 11:14 - loss: 23.8668\n",
            "7389/10000 = 73.9%, SPS: 3.9, ELAP: 27:30, ETA: 11:14 - loss: 24.0193\n",
            "7390/10000 = 73.9%, SPS: 3.9, ELAP: 27:30, ETA: 11:14 - loss: 23.7162\n",
            "7391/10000 = 73.9%, SPS: 3.9, ELAP: 27:30, ETA: 11:14 - loss: 24.0674\n",
            "7392/10000 = 73.9%, SPS: 3.9, ELAP: 27:30, ETA: 11:13 - loss: 24.1297\n",
            "7393/10000 = 73.9%, SPS: 3.9, ELAP: 27:31, ETA: 11:13 - loss: 24.2959\n",
            "7394/10000 = 73.9%, SPS: 3.9, ELAP: 27:31, ETA: 11:13 - loss: 24.1661\n",
            "7395/10000 = 74.0%, SPS: 3.9, ELAP: 27:31, ETA: 11:13 - loss: 25.0091\n",
            "7396/10000 = 74.0%, SPS: 3.9, ELAP: 27:31, ETA: 11:12 - loss: 24.1159\n",
            "7397/10000 = 74.0%, SPS: 3.9, ELAP: 27:31, ETA: 11:12 - loss: 25.0976\n",
            "7398/10000 = 74.0%, SPS: 3.9, ELAP: 27:32, ETA: 11:12 - loss: 23.1983\n",
            "7399/10000 = 74.0%, SPS: 3.9, ELAP: 27:32, ETA: 11:11 - loss: 24.3479\n",
            "7400/10000 = 74.0%, SPS: 3.9, ELAP: 27:34, ETA: 11:12 - loss: 24.3594\n",
            "7401/10000 = 74.0%, SPS: 3.9, ELAP: 27:34, ETA: 11:12 - loss: 24.1956\n",
            "7402/10000 = 74.0%, SPS: 3.9, ELAP: 27:35, ETA: 11:11 - loss: 23.9855\n",
            "7403/10000 = 74.0%, SPS: 3.9, ELAP: 27:35, ETA: 11:11 - loss: 24.4291\n",
            "7404/10000 = 74.0%, SPS: 3.9, ELAP: 27:35, ETA: 11:11 - loss: 23.9169\n",
            "7405/10000 = 74.0%, SPS: 3.9, ELAP: 27:35, ETA: 11:11 - loss: 24.0130\n",
            "7406/10000 = 74.1%, SPS: 3.9, ELAP: 27:36, ETA: 11:10 - loss: 24.2078\n",
            "7407/10000 = 74.1%, SPS: 3.9, ELAP: 27:36, ETA: 11:10 - loss: 23.8396\n",
            "7408/10000 = 74.1%, SPS: 3.9, ELAP: 27:36, ETA: 11:10 - loss: 24.0625\n",
            "7409/10000 = 74.1%, SPS: 3.9, ELAP: 27:36, ETA: 11:10 - loss: 23.3073\n",
            "7410/10000 = 74.1%, SPS: 3.9, ELAP: 27:37, ETA: 11:09 - loss: 24.8434\n",
            "7411/10000 = 74.1%, SPS: 3.9, ELAP: 27:37, ETA: 11:09 - loss: 24.7244\n",
            "7412/10000 = 74.1%, SPS: 3.9, ELAP: 27:37, ETA: 11:09 - loss: 24.5282\n",
            "7413/10000 = 74.1%, SPS: 3.9, ELAP: 27:37, ETA: 11:09 - loss: 24.4061\n",
            "7414/10000 = 74.1%, SPS: 3.9, ELAP: 27:37, ETA: 11:08 - loss: 23.9271\n",
            "7415/10000 = 74.2%, SPS: 3.9, ELAP: 27:38, ETA: 11:08 - loss: 23.4560\n",
            "7416/10000 = 74.2%, SPS: 3.9, ELAP: 27:38, ETA: 11:08 - loss: 23.7746\n",
            "7417/10000 = 74.2%, SPS: 3.9, ELAP: 27:38, ETA: 11:07 - loss: 24.4632\n",
            "7418/10000 = 74.2%, SPS: 3.9, ELAP: 27:38, ETA: 11:07 - loss: 23.7986\n",
            "7419/10000 = 74.2%, SPS: 3.9, ELAP: 27:39, ETA: 11:07 - loss: 23.9739\n",
            "7420/10000 = 74.2%, SPS: 3.9, ELAP: 27:39, ETA: 11:07 - loss: 23.9405\n",
            "7421/10000 = 74.2%, SPS: 3.9, ELAP: 27:39, ETA: 11:06 - loss: 23.9369\n",
            "7422/10000 = 74.2%, SPS: 3.9, ELAP: 27:39, ETA: 11:06 - loss: 24.1121\n",
            "7423/10000 = 74.2%, SPS: 3.9, ELAP: 27:39, ETA: 11:06 - loss: 24.2010\n",
            "7424/10000 = 74.2%, SPS: 3.9, ELAP: 27:40, ETA: 11:06 - loss: 24.8624\n",
            "7425/10000 = 74.2%, SPS: 3.9, ELAP: 27:40, ETA: 11:05 - loss: 24.0600\n",
            "7426/10000 = 74.3%, SPS: 3.9, ELAP: 27:40, ETA: 11:05 - loss: 23.8196\n",
            "7427/10000 = 74.3%, SPS: 3.9, ELAP: 27:40, ETA: 11:05 - loss: 24.2654\n",
            "7428/10000 = 74.3%, SPS: 3.9, ELAP: 27:41, ETA: 11:04 - loss: 25.0054\n",
            "7429/10000 = 74.3%, SPS: 3.9, ELAP: 27:41, ETA: 11:04 - loss: 24.3372\n",
            "7430/10000 = 74.3%, SPS: 3.9, ELAP: 27:41, ETA: 11:04 - loss: 24.1778\n",
            "7431/10000 = 74.3%, SPS: 3.9, ELAP: 27:41, ETA: 11:04 - loss: 23.8769\n",
            "7432/10000 = 74.3%, SPS: 3.9, ELAP: 27:42, ETA: 11:03 - loss: 23.9323\n",
            "7433/10000 = 74.3%, SPS: 3.9, ELAP: 27:42, ETA: 11:03 - loss: 23.9653\n",
            "7434/10000 = 74.3%, SPS: 3.9, ELAP: 27:42, ETA: 11:03 - loss: 24.1372\n",
            "7435/10000 = 74.3%, SPS: 3.9, ELAP: 27:42, ETA: 11:03 - loss: 23.7252\n",
            "7436/10000 = 74.4%, SPS: 3.9, ELAP: 27:43, ETA: 11:02 - loss: 23.9707\n",
            "7437/10000 = 74.4%, SPS: 3.9, ELAP: 27:43, ETA: 11:02 - loss: 24.3120\n",
            "7438/10000 = 74.4%, SPS: 3.9, ELAP: 27:43, ETA: 11:02 - loss: 24.3709\n",
            "7439/10000 = 74.4%, SPS: 3.9, ELAP: 27:43, ETA: 11:02 - loss: 24.3172\n",
            "7440/10000 = 74.4%, SPS: 3.9, ELAP: 27:44, ETA: 11:01 - loss: 23.9813\n",
            "7441/10000 = 74.4%, SPS: 3.9, ELAP: 27:44, ETA: 11:01 - loss: 24.4525\n",
            "7442/10000 = 74.4%, SPS: 3.9, ELAP: 27:44, ETA: 11:01 - loss: 23.6331\n",
            "7443/10000 = 74.4%, SPS: 3.9, ELAP: 27:44, ETA: 11:00 - loss: 23.8275\n",
            "7444/10000 = 74.4%, SPS: 3.9, ELAP: 27:44, ETA: 11:00 - loss: 23.4530\n",
            "7445/10000 = 74.5%, SPS: 3.9, ELAP: 27:45, ETA: 11:00 - loss: 25.0954\n",
            "7446/10000 = 74.5%, SPS: 3.9, ELAP: 27:45, ETA: 11:00 - loss: 24.1344\n",
            "7447/10000 = 74.5%, SPS: 3.9, ELAP: 27:45, ETA: 10:59 - loss: 23.7666\n",
            "7448/10000 = 74.5%, SPS: 3.9, ELAP: 27:45, ETA: 10:59 - loss: 24.3698\n",
            "7449/10000 = 74.5%, SPS: 3.9, ELAP: 27:46, ETA: 10:59 - loss: 24.4783\n",
            "7450/10000 = 74.5%, SPS: 3.9, ELAP: 27:46, ETA: 10:59 - loss: 24.5290\n",
            "7451/10000 = 74.5%, SPS: 3.9, ELAP: 27:46, ETA: 10:58 - loss: 24.2552\n",
            "7452/10000 = 74.5%, SPS: 3.9, ELAP: 27:46, ETA: 10:58 - loss: 24.3127\n",
            "7453/10000 = 74.5%, SPS: 3.9, ELAP: 27:47, ETA: 10:58 - loss: 23.8425\n",
            "7454/10000 = 74.5%, SPS: 3.9, ELAP: 27:47, ETA: 10:58 - loss: 24.6603\n",
            "7455/10000 = 74.5%, SPS: 3.9, ELAP: 27:47, ETA: 10:57 - loss: 24.6398\n",
            "7456/10000 = 74.6%, SPS: 3.9, ELAP: 27:47, ETA: 10:57 - loss: 23.9492\n",
            "7457/10000 = 74.6%, SPS: 3.9, ELAP: 27:47, ETA: 10:57 - loss: 23.9904\n",
            "7458/10000 = 74.6%, SPS: 3.9, ELAP: 27:48, ETA: 10:56 - loss: 23.5696\n",
            "7459/10000 = 74.6%, SPS: 3.9, ELAP: 27:48, ETA: 10:56 - loss: 24.4740\n",
            "7460/10000 = 74.6%, SPS: 3.9, ELAP: 27:48, ETA: 10:56 - loss: 24.4554\n",
            "7461/10000 = 74.6%, SPS: 3.9, ELAP: 27:48, ETA: 10:56 - loss: 23.7107\n",
            "7462/10000 = 74.6%, SPS: 3.9, ELAP: 27:49, ETA: 10:55 - loss: 24.4523\n",
            "7463/10000 = 74.6%, SPS: 3.9, ELAP: 27:49, ETA: 10:55 - loss: 24.5183\n",
            "7464/10000 = 74.6%, SPS: 3.9, ELAP: 27:49, ETA: 10:55 - loss: 24.7070\n",
            "7465/10000 = 74.7%, SPS: 3.9, ELAP: 27:49, ETA: 10:55 - loss: 24.5140\n",
            "7466/10000 = 74.7%, SPS: 3.9, ELAP: 27:49, ETA: 10:54 - loss: 24.0516\n",
            "7467/10000 = 74.7%, SPS: 3.9, ELAP: 27:50, ETA: 10:54 - loss: 24.2396\n",
            "7468/10000 = 74.7%, SPS: 3.9, ELAP: 27:50, ETA: 10:54 - loss: 23.7309\n",
            "7469/10000 = 74.7%, SPS: 3.9, ELAP: 27:50, ETA: 10:53 - loss: 24.3752\n",
            "7470/10000 = 74.7%, SPS: 3.9, ELAP: 27:50, ETA: 10:53 - loss: 24.4487\n",
            "7471/10000 = 74.7%, SPS: 3.9, ELAP: 27:51, ETA: 10:53 - loss: 24.4159\n",
            "7472/10000 = 74.7%, SPS: 3.9, ELAP: 27:51, ETA: 10:53 - loss: 24.2950\n",
            "7473/10000 = 74.7%, SPS: 3.9, ELAP: 27:51, ETA: 10:52 - loss: 24.4836\n",
            "7474/10000 = 74.7%, SPS: 3.9, ELAP: 27:51, ETA: 10:52 - loss: 23.6794\n",
            "7475/10000 = 74.8%, SPS: 3.9, ELAP: 27:52, ETA: 10:52 - loss: 23.5429\n",
            "7476/10000 = 74.8%, SPS: 3.9, ELAP: 27:52, ETA: 10:52 - loss: 23.7077\n",
            "7477/10000 = 74.8%, SPS: 3.9, ELAP: 27:52, ETA: 10:51 - loss: 24.1998\n",
            "7478/10000 = 74.8%, SPS: 3.9, ELAP: 27:52, ETA: 10:51 - loss: 24.8060\n",
            "7479/10000 = 74.8%, SPS: 3.9, ELAP: 27:52, ETA: 10:51 - loss: 24.0747\n",
            "7480/10000 = 74.8%, SPS: 3.9, ELAP: 27:53, ETA: 10:50 - loss: 24.5360\n",
            "7481/10000 = 74.8%, SPS: 3.9, ELAP: 27:53, ETA: 10:50 - loss: 24.1147\n",
            "7482/10000 = 74.8%, SPS: 3.9, ELAP: 27:53, ETA: 10:50 - loss: 24.7662\n",
            "7483/10000 = 74.8%, SPS: 3.9, ELAP: 27:53, ETA: 10:50 - loss: 24.0445\n",
            "7484/10000 = 74.8%, SPS: 3.9, ELAP: 27:54, ETA: 10:49 - loss: 24.0641\n",
            "7485/10000 = 74.8%, SPS: 3.9, ELAP: 27:54, ETA: 10:49 - loss: 24.2604\n",
            "7486/10000 = 74.9%, SPS: 3.9, ELAP: 27:54, ETA: 10:49 - loss: 24.0151\n",
            "7487/10000 = 74.9%, SPS: 3.9, ELAP: 27:54, ETA: 10:49 - loss: 24.4925\n",
            "7488/10000 = 74.9%, SPS: 3.9, ELAP: 27:54, ETA: 10:48 - loss: 24.6856\n",
            "7489/10000 = 74.9%, SPS: 3.9, ELAP: 27:55, ETA: 10:48 - loss: 24.1814\n",
            "7490/10000 = 74.9%, SPS: 3.9, ELAP: 27:55, ETA: 10:48 - loss: 23.7564\n",
            "7491/10000 = 74.9%, SPS: 3.9, ELAP: 27:55, ETA: 10:48 - loss: 23.7880\n",
            "7492/10000 = 74.9%, SPS: 3.9, ELAP: 27:55, ETA: 10:47 - loss: 24.4449\n",
            "7493/10000 = 74.9%, SPS: 3.9, ELAP: 27:56, ETA: 10:47 - loss: 23.9788\n",
            "7494/10000 = 74.9%, SPS: 3.9, ELAP: 27:56, ETA: 10:47 - loss: 23.9636\n",
            "7495/10000 = 75.0%, SPS: 3.9, ELAP: 27:56, ETA: 10:46 - loss: 24.5513\n",
            "7496/10000 = 75.0%, SPS: 3.9, ELAP: 27:56, ETA: 10:46 - loss: 24.1380\n",
            "7497/10000 = 75.0%, SPS: 3.9, ELAP: 27:57, ETA: 10:46 - loss: 23.6771\n",
            "7498/10000 = 75.0%, SPS: 3.9, ELAP: 27:57, ETA: 10:46 - loss: 24.2780\n",
            "7499/10000 = 75.0%, SPS: 3.9, ELAP: 27:57, ETA: 10:45 - loss: 24.8641\n",
            "7500/10000 = 75.0%, SPS: 3.9, ELAP: 27:59, ETA: 10:46 - loss: 24.3912\n",
            "7501/10000 = 75.0%, SPS: 3.9, ELAP: 28:00, ETA: 10:46 - loss: 24.8903\n",
            "7502/10000 = 75.0%, SPS: 3.9, ELAP: 28:00, ETA: 10:45 - loss: 24.6140\n",
            "7503/10000 = 75.0%, SPS: 3.9, ELAP: 28:00, ETA: 10:45 - loss: 24.2668\n",
            "7504/10000 = 75.0%, SPS: 3.9, ELAP: 28:00, ETA: 10:45 - loss: 23.6435\n",
            "7505/10000 = 75.0%, SPS: 3.9, ELAP: 28:01, ETA: 10:45 - loss: 24.0010\n",
            "7506/10000 = 75.1%, SPS: 3.9, ELAP: 28:01, ETA: 10:44 - loss: 24.1347\n",
            "7507/10000 = 75.1%, SPS: 3.9, ELAP: 28:01, ETA: 10:44 - loss: 23.5472\n",
            "7508/10000 = 75.1%, SPS: 3.9, ELAP: 28:01, ETA: 10:44 - loss: 24.2857\n",
            "7509/10000 = 75.1%, SPS: 3.9, ELAP: 28:02, ETA: 10:44 - loss: 23.9957\n",
            "7510/10000 = 75.1%, SPS: 3.9, ELAP: 28:02, ETA: 10:43 - loss: 24.5695\n",
            "7511/10000 = 75.1%, SPS: 3.9, ELAP: 28:02, ETA: 10:43 - loss: 24.0102\n",
            "7512/10000 = 75.1%, SPS: 3.9, ELAP: 28:02, ETA: 10:43 - loss: 23.7961\n",
            "7513/10000 = 75.1%, SPS: 3.9, ELAP: 28:02, ETA: 10:42 - loss: 24.6235\n",
            "7514/10000 = 75.1%, SPS: 3.9, ELAP: 28:03, ETA: 10:42 - loss: 24.2857\n",
            "7515/10000 = 75.2%, SPS: 3.9, ELAP: 28:03, ETA: 10:42 - loss: 24.2319\n",
            "7516/10000 = 75.2%, SPS: 3.9, ELAP: 28:03, ETA: 10:42 - loss: 23.2528\n",
            "7517/10000 = 75.2%, SPS: 3.9, ELAP: 28:03, ETA: 10:41 - loss: 24.0734\n",
            "7518/10000 = 75.2%, SPS: 3.9, ELAP: 28:04, ETA: 10:41 - loss: 24.3920\n",
            "7519/10000 = 75.2%, SPS: 3.9, ELAP: 28:04, ETA: 10:41 - loss: 23.7043\n",
            "7520/10000 = 75.2%, SPS: 3.9, ELAP: 28:04, ETA: 10:41 - loss: 24.0564\n",
            "7521/10000 = 75.2%, SPS: 3.9, ELAP: 28:04, ETA: 10:40 - loss: 24.5617\n",
            "7522/10000 = 75.2%, SPS: 3.9, ELAP: 28:04, ETA: 10:40 - loss: 24.2429\n",
            "7523/10000 = 75.2%, SPS: 3.9, ELAP: 28:05, ETA: 10:40 - loss: 23.1075\n",
            "7524/10000 = 75.2%, SPS: 3.9, ELAP: 28:05, ETA: 10:39 - loss: 24.0510\n",
            "7525/10000 = 75.2%, SPS: 3.9, ELAP: 28:05, ETA: 10:39 - loss: 23.6445\n",
            "7526/10000 = 75.3%, SPS: 3.9, ELAP: 28:05, ETA: 10:39 - loss: 24.0894\n",
            "7527/10000 = 75.3%, SPS: 3.9, ELAP: 28:06, ETA: 10:39 - loss: 23.7358\n",
            "7528/10000 = 75.3%, SPS: 3.9, ELAP: 28:06, ETA: 10:38 - loss: 24.3971\n",
            "7529/10000 = 75.3%, SPS: 3.9, ELAP: 28:06, ETA: 10:38 - loss: 24.8630\n",
            "7530/10000 = 75.3%, SPS: 3.9, ELAP: 28:06, ETA: 10:38 - loss: 24.2199\n",
            "7531/10000 = 75.3%, SPS: 3.9, ELAP: 28:07, ETA: 10:38 - loss: 23.8624\n",
            "7532/10000 = 75.3%, SPS: 3.9, ELAP: 28:07, ETA: 10:37 - loss: 23.4906\n",
            "7533/10000 = 75.3%, SPS: 3.9, ELAP: 28:07, ETA: 10:37 - loss: 24.5879\n",
            "7534/10000 = 75.3%, SPS: 3.9, ELAP: 28:07, ETA: 10:37 - loss: 24.6877\n",
            "7535/10000 = 75.3%, SPS: 3.9, ELAP: 28:08, ETA: 10:37 - loss: 24.0844\n",
            "7536/10000 = 75.4%, SPS: 3.9, ELAP: 28:08, ETA: 10:36 - loss: 24.7373\n",
            "7537/10000 = 75.4%, SPS: 3.9, ELAP: 28:08, ETA: 10:36 - loss: 24.5252\n",
            "7538/10000 = 75.4%, SPS: 3.9, ELAP: 28:08, ETA: 10:36 - loss: 23.9153\n",
            "7539/10000 = 75.4%, SPS: 3.9, ELAP: 28:09, ETA: 10:36 - loss: 24.2365\n",
            "7540/10000 = 75.4%, SPS: 3.9, ELAP: 28:09, ETA: 10:35 - loss: 24.0476\n",
            "7541/10000 = 75.4%, SPS: 3.9, ELAP: 28:09, ETA: 10:35 - loss: 24.3239\n",
            "7542/10000 = 75.4%, SPS: 3.9, ELAP: 28:09, ETA: 10:35 - loss: 24.3714\n",
            "7543/10000 = 75.4%, SPS: 3.9, ELAP: 28:10, ETA: 10:34 - loss: 24.7822\n",
            "7544/10000 = 75.4%, SPS: 3.9, ELAP: 28:10, ETA: 10:34 - loss: 24.6071\n",
            "7545/10000 = 75.5%, SPS: 3.9, ELAP: 28:10, ETA: 10:34 - loss: 24.4224\n",
            "7546/10000 = 75.5%, SPS: 3.9, ELAP: 28:10, ETA: 10:34 - loss: 23.7716\n",
            "7547/10000 = 75.5%, SPS: 3.9, ELAP: 28:10, ETA: 10:33 - loss: 24.5578\n",
            "7548/10000 = 75.5%, SPS: 3.9, ELAP: 28:11, ETA: 10:33 - loss: 24.5252\n",
            "7549/10000 = 75.5%, SPS: 3.9, ELAP: 28:11, ETA: 10:33 - loss: 23.8900\n",
            "7550/10000 = 75.5%, SPS: 3.9, ELAP: 28:11, ETA: 10:33 - loss: 24.2215\n",
            "7551/10000 = 75.5%, SPS: 3.9, ELAP: 28:11, ETA: 10:32 - loss: 24.0507\n",
            "7552/10000 = 75.5%, SPS: 3.9, ELAP: 28:12, ETA: 10:32 - loss: 24.0093\n",
            "7553/10000 = 75.5%, SPS: 3.9, ELAP: 28:12, ETA: 10:32 - loss: 24.2305\n",
            "7554/10000 = 75.5%, SPS: 3.9, ELAP: 28:12, ETA: 10:31 - loss: 24.0024\n",
            "7555/10000 = 75.5%, SPS: 3.9, ELAP: 28:12, ETA: 10:31 - loss: 24.5352\n",
            "7556/10000 = 75.6%, SPS: 3.9, ELAP: 28:12, ETA: 10:31 - loss: 24.0511\n",
            "7557/10000 = 75.6%, SPS: 3.9, ELAP: 28:13, ETA: 10:31 - loss: 23.9489\n",
            "7558/10000 = 75.6%, SPS: 3.9, ELAP: 28:13, ETA: 10:30 - loss: 23.5112\n",
            "7559/10000 = 75.6%, SPS: 3.9, ELAP: 28:13, ETA: 10:30 - loss: 24.7652\n",
            "7560/10000 = 75.6%, SPS: 3.9, ELAP: 28:13, ETA: 10:30 - loss: 24.6387\n",
            "7561/10000 = 75.6%, SPS: 3.9, ELAP: 28:14, ETA: 10:30 - loss: 24.0690\n",
            "7562/10000 = 75.6%, SPS: 3.9, ELAP: 28:14, ETA: 10:29 - loss: 24.4464\n",
            "7563/10000 = 75.6%, SPS: 3.9, ELAP: 28:14, ETA: 10:29 - loss: 24.4246\n",
            "7564/10000 = 75.6%, SPS: 3.9, ELAP: 28:14, ETA: 10:29 - loss: 23.4432\n",
            "7565/10000 = 75.7%, SPS: 3.9, ELAP: 28:15, ETA: 10:29 - loss: 23.5625\n",
            "7566/10000 = 75.7%, SPS: 3.9, ELAP: 28:15, ETA: 10:28 - loss: 23.2128\n",
            "7567/10000 = 75.7%, SPS: 3.9, ELAP: 28:15, ETA: 10:28 - loss: 23.7505\n",
            "7568/10000 = 75.7%, SPS: 3.9, ELAP: 28:15, ETA: 10:28 - loss: 24.3988\n",
            "7569/10000 = 75.7%, SPS: 3.9, ELAP: 28:15, ETA: 10:27 - loss: 23.9300\n",
            "7570/10000 = 75.7%, SPS: 3.9, ELAP: 28:16, ETA: 10:27 - loss: 24.0783\n",
            "7571/10000 = 75.7%, SPS: 3.9, ELAP: 28:16, ETA: 10:27 - loss: 23.8057\n",
            "7572/10000 = 75.7%, SPS: 3.9, ELAP: 28:16, ETA: 10:27 - loss: 24.1146\n",
            "7573/10000 = 75.7%, SPS: 3.9, ELAP: 28:16, ETA: 10:26 - loss: 24.0258\n",
            "7574/10000 = 75.7%, SPS: 3.9, ELAP: 28:17, ETA: 10:26 - loss: 24.2196\n",
            "7575/10000 = 75.8%, SPS: 3.9, ELAP: 28:17, ETA: 10:26 - loss: 24.0025\n",
            "7576/10000 = 75.8%, SPS: 3.9, ELAP: 28:17, ETA: 10:26 - loss: 24.2765\n",
            "7577/10000 = 75.8%, SPS: 3.9, ELAP: 28:17, ETA: 10:25 - loss: 23.9590\n",
            "7578/10000 = 75.8%, SPS: 3.9, ELAP: 28:17, ETA: 10:25 - loss: 23.8597\n",
            "7579/10000 = 75.8%, SPS: 3.9, ELAP: 28:18, ETA: 10:25 - loss: 23.8172\n",
            "7580/10000 = 75.8%, SPS: 3.9, ELAP: 28:18, ETA: 10:24 - loss: 24.2985\n",
            "7581/10000 = 75.8%, SPS: 3.9, ELAP: 28:18, ETA: 10:24 - loss: 23.4294\n",
            "7582/10000 = 75.8%, SPS: 3.9, ELAP: 28:18, ETA: 10:24 - loss: 23.9678\n",
            "7583/10000 = 75.8%, SPS: 3.9, ELAP: 28:19, ETA: 10:24 - loss: 24.2183\n",
            "7584/10000 = 75.8%, SPS: 3.9, ELAP: 28:19, ETA: 10:23 - loss: 24.2185\n",
            "7585/10000 = 75.8%, SPS: 3.9, ELAP: 28:19, ETA: 10:23 - loss: 23.8632\n",
            "7586/10000 = 75.9%, SPS: 3.9, ELAP: 28:19, ETA: 10:23 - loss: 24.1317\n",
            "7587/10000 = 75.9%, SPS: 3.9, ELAP: 28:20, ETA: 10:23 - loss: 23.9471\n",
            "7588/10000 = 75.9%, SPS: 3.9, ELAP: 28:20, ETA: 10:22 - loss: 24.1519\n",
            "7589/10000 = 75.9%, SPS: 3.9, ELAP: 28:20, ETA: 10:22 - loss: 24.0727\n",
            "7590/10000 = 75.9%, SPS: 3.9, ELAP: 28:20, ETA: 10:22 - loss: 24.2220\n",
            "7591/10000 = 75.9%, SPS: 3.9, ELAP: 28:20, ETA: 10:22 - loss: 24.1177\n",
            "7592/10000 = 75.9%, SPS: 3.9, ELAP: 28:21, ETA: 10:21 - loss: 24.4051\n",
            "7593/10000 = 75.9%, SPS: 3.9, ELAP: 28:21, ETA: 10:21 - loss: 23.9759\n",
            "7594/10000 = 75.9%, SPS: 3.9, ELAP: 28:21, ETA: 10:21 - loss: 23.8895\n",
            "7595/10000 = 76.0%, SPS: 3.9, ELAP: 28:21, ETA: 10:20 - loss: 24.1486\n",
            "7596/10000 = 76.0%, SPS: 3.9, ELAP: 28:22, ETA: 10:20 - loss: 24.0690\n",
            "7597/10000 = 76.0%, SPS: 3.9, ELAP: 28:22, ETA: 10:20 - loss: 23.3169\n",
            "7598/10000 = 76.0%, SPS: 3.9, ELAP: 28:22, ETA: 10:20 - loss: 23.2786\n",
            "7599/10000 = 76.0%, SPS: 3.9, ELAP: 28:22, ETA: 10:19 - loss: 24.2288\n",
            "7600/10000 = 76.0%, SPS: 3.9, ELAP: 28:25, ETA: 10:20 - loss: 23.9603\n",
            "7601/10000 = 76.0%, SPS: 3.9, ELAP: 28:25, ETA: 10:20 - loss: 24.3777\n",
            "7602/10000 = 76.0%, SPS: 3.9, ELAP: 28:25, ETA: 10:19 - loss: 23.5374\n",
            "7603/10000 = 76.0%, SPS: 3.9, ELAP: 28:25, ETA: 10:19 - loss: 24.1108\n",
            "7604/10000 = 76.0%, SPS: 3.9, ELAP: 28:26, ETA: 10:19 - loss: 24.0104\n",
            "7605/10000 = 76.0%, SPS: 3.9, ELAP: 28:26, ETA: 10:19 - loss: 24.1090\n",
            "7606/10000 = 76.1%, SPS: 3.9, ELAP: 28:26, ETA: 10:18 - loss: 23.8480\n",
            "7607/10000 = 76.1%, SPS: 3.9, ELAP: 28:26, ETA: 10:18 - loss: 24.4169\n",
            "7608/10000 = 76.1%, SPS: 3.9, ELAP: 28:27, ETA: 10:18 - loss: 23.2172\n",
            "7609/10000 = 76.1%, SPS: 3.9, ELAP: 28:27, ETA: 10:17 - loss: 24.1943\n",
            "7610/10000 = 76.1%, SPS: 3.9, ELAP: 28:27, ETA: 10:17 - loss: 24.3089\n",
            "7611/10000 = 76.1%, SPS: 3.9, ELAP: 28:27, ETA: 10:17 - loss: 24.2227\n",
            "7612/10000 = 76.1%, SPS: 3.9, ELAP: 28:27, ETA: 10:17 - loss: 24.3892\n",
            "7613/10000 = 76.1%, SPS: 3.9, ELAP: 28:28, ETA: 10:16 - loss: 24.1181\n",
            "7614/10000 = 76.1%, SPS: 3.9, ELAP: 28:28, ETA: 10:16 - loss: 23.6540\n",
            "7615/10000 = 76.2%, SPS: 3.9, ELAP: 28:28, ETA: 10:16 - loss: 24.3871\n",
            "7616/10000 = 76.2%, SPS: 3.9, ELAP: 28:28, ETA: 10:16 - loss: 24.1719\n",
            "7617/10000 = 76.2%, SPS: 3.9, ELAP: 28:29, ETA: 10:15 - loss: 23.8012\n",
            "7618/10000 = 76.2%, SPS: 3.9, ELAP: 28:29, ETA: 10:15 - loss: 24.4024\n",
            "7619/10000 = 76.2%, SPS: 3.9, ELAP: 28:29, ETA: 10:15 - loss: 23.3040\n",
            "7620/10000 = 76.2%, SPS: 3.9, ELAP: 28:29, ETA: 10:15 - loss: 24.3047\n",
            "7621/10000 = 76.2%, SPS: 3.9, ELAP: 28:30, ETA: 10:14 - loss: 23.7430\n",
            "7622/10000 = 76.2%, SPS: 3.9, ELAP: 28:30, ETA: 10:14 - loss: 24.0639\n",
            "7623/10000 = 76.2%, SPS: 3.9, ELAP: 28:30, ETA: 10:14 - loss: 24.2003\n",
            "7624/10000 = 76.2%, SPS: 3.9, ELAP: 28:30, ETA: 10:13 - loss: 24.3508\n",
            "7625/10000 = 76.2%, SPS: 3.9, ELAP: 28:31, ETA: 10:13 - loss: 24.5001\n",
            "7626/10000 = 76.3%, SPS: 3.9, ELAP: 28:31, ETA: 10:13 - loss: 23.5469\n",
            "7627/10000 = 76.3%, SPS: 3.9, ELAP: 28:31, ETA: 10:13 - loss: 24.3526\n",
            "7628/10000 = 76.3%, SPS: 3.9, ELAP: 28:31, ETA: 10:12 - loss: 24.4215\n",
            "7629/10000 = 76.3%, SPS: 3.9, ELAP: 28:32, ETA: 10:12 - loss: 23.9799\n",
            "7630/10000 = 76.3%, SPS: 3.9, ELAP: 28:32, ETA: 10:12 - loss: 24.4003\n",
            "7631/10000 = 76.3%, SPS: 3.9, ELAP: 28:32, ETA: 10:12 - loss: 24.1420\n",
            "7632/10000 = 76.3%, SPS: 3.9, ELAP: 28:32, ETA: 10:11 - loss: 24.6082\n",
            "7633/10000 = 76.3%, SPS: 3.9, ELAP: 28:33, ETA: 10:11 - loss: 24.3596\n",
            "7634/10000 = 76.3%, SPS: 3.9, ELAP: 28:33, ETA: 10:11 - loss: 23.5068\n",
            "7635/10000 = 76.3%, SPS: 3.9, ELAP: 28:33, ETA: 10:11 - loss: 23.9519\n",
            "7636/10000 = 76.4%, SPS: 3.9, ELAP: 28:33, ETA: 10:10 - loss: 23.9285\n",
            "7637/10000 = 76.4%, SPS: 3.9, ELAP: 28:33, ETA: 10:10 - loss: 24.3017\n",
            "7638/10000 = 76.4%, SPS: 3.9, ELAP: 28:34, ETA: 10:10 - loss: 24.2174\n",
            "7639/10000 = 76.4%, SPS: 3.9, ELAP: 28:34, ETA: 10:10 - loss: 23.9632\n",
            "7640/10000 = 76.4%, SPS: 3.9, ELAP: 28:34, ETA: 10:09 - loss: 24.1220\n",
            "7641/10000 = 76.4%, SPS: 3.9, ELAP: 28:34, ETA: 10:09 - loss: 23.9060\n",
            "7642/10000 = 76.4%, SPS: 3.9, ELAP: 28:35, ETA: 10:09 - loss: 23.4148\n",
            "7643/10000 = 76.4%, SPS: 3.9, ELAP: 28:35, ETA: 10:08 - loss: 23.5010\n",
            "7644/10000 = 76.4%, SPS: 3.9, ELAP: 28:35, ETA: 10:08 - loss: 24.0271\n",
            "7645/10000 = 76.5%, SPS: 3.9, ELAP: 28:35, ETA: 10:08 - loss: 24.1744\n",
            "7646/10000 = 76.5%, SPS: 3.9, ELAP: 28:36, ETA: 10:08 - loss: 23.9973\n",
            "7647/10000 = 76.5%, SPS: 3.9, ELAP: 28:36, ETA: 10:07 - loss: 24.4412\n",
            "7648/10000 = 76.5%, SPS: 3.9, ELAP: 28:36, ETA: 10:07 - loss: 23.7173\n",
            "7649/10000 = 76.5%, SPS: 3.9, ELAP: 28:36, ETA: 10:07 - loss: 23.8770\n",
            "7650/10000 = 76.5%, SPS: 3.9, ELAP: 28:36, ETA: 10:07 - loss: 23.9623\n",
            "7651/10000 = 76.5%, SPS: 3.9, ELAP: 28:37, ETA: 10:06 - loss: 23.8554\n",
            "7652/10000 = 76.5%, SPS: 3.9, ELAP: 28:37, ETA: 10:06 - loss: 23.5224\n",
            "7653/10000 = 76.5%, SPS: 3.9, ELAP: 28:37, ETA: 10:06 - loss: 23.2753\n",
            "7654/10000 = 76.5%, SPS: 3.9, ELAP: 28:37, ETA: 10:05 - loss: 24.3485\n",
            "7655/10000 = 76.5%, SPS: 3.9, ELAP: 28:38, ETA: 10:05 - loss: 24.0820\n",
            "7656/10000 = 76.6%, SPS: 3.9, ELAP: 28:38, ETA: 10:05 - loss: 23.9199\n",
            "7657/10000 = 76.6%, SPS: 3.9, ELAP: 28:38, ETA: 10:05 - loss: 24.5619\n",
            "7658/10000 = 76.6%, SPS: 3.9, ELAP: 28:38, ETA: 10:04 - loss: 24.0264\n",
            "7659/10000 = 76.6%, SPS: 3.9, ELAP: 28:38, ETA: 10:04 - loss: 24.4137\n",
            "7660/10000 = 76.6%, SPS: 3.9, ELAP: 28:39, ETA: 10:04 - loss: 23.5992\n",
            "7661/10000 = 76.6%, SPS: 3.9, ELAP: 28:39, ETA: 10:04 - loss: 24.0621\n",
            "7662/10000 = 76.6%, SPS: 3.9, ELAP: 28:39, ETA: 10:03 - loss: 23.8183\n",
            "7663/10000 = 76.6%, SPS: 3.9, ELAP: 28:39, ETA: 10:03 - loss: 24.4391\n",
            "7664/10000 = 76.6%, SPS: 3.9, ELAP: 28:40, ETA: 10:03 - loss: 23.9884\n",
            "7665/10000 = 76.7%, SPS: 3.9, ELAP: 28:40, ETA: 10:03 - loss: 23.9440\n",
            "7666/10000 = 76.7%, SPS: 3.9, ELAP: 28:40, ETA: 10:02 - loss: 23.5983\n",
            "7667/10000 = 76.7%, SPS: 3.9, ELAP: 28:40, ETA: 10:02 - loss: 23.6514\n",
            "7668/10000 = 76.7%, SPS: 3.9, ELAP: 28:41, ETA: 10:02 - loss: 23.9180\n",
            "7669/10000 = 76.7%, SPS: 3.9, ELAP: 28:41, ETA: 10:01 - loss: 23.9724\n",
            "7670/10000 = 76.7%, SPS: 3.9, ELAP: 28:41, ETA: 10:01 - loss: 23.6703\n",
            "7671/10000 = 76.7%, SPS: 3.9, ELAP: 28:41, ETA: 10:01 - loss: 24.3495\n",
            "7672/10000 = 76.7%, SPS: 3.9, ELAP: 28:41, ETA: 10:01 - loss: 24.2723\n",
            "7673/10000 = 76.7%, SPS: 3.9, ELAP: 28:42, ETA: 10:00 - loss: 24.5775\n",
            "7674/10000 = 76.7%, SPS: 3.9, ELAP: 28:42, ETA: 10:00 - loss: 24.3419\n",
            "7675/10000 = 76.8%, SPS: 3.9, ELAP: 28:42, ETA: 10:00 - loss: 24.5415\n",
            "7676/10000 = 76.8%, SPS: 3.9, ELAP: 28:42, ETA: 10:00 - loss: 23.8746\n",
            "7677/10000 = 76.8%, SPS: 3.9, ELAP: 28:43, ETA: 9:59 - loss: 23.8854\n",
            "7678/10000 = 76.8%, SPS: 3.9, ELAP: 28:43, ETA: 9:59 - loss: 23.9930\n",
            "7679/10000 = 76.8%, SPS: 3.9, ELAP: 28:43, ETA: 9:59 - loss: 23.6629\n",
            "7680/10000 = 76.8%, SPS: 3.9, ELAP: 28:43, ETA: 9:58 - loss: 24.6281\n",
            "7681/10000 = 76.8%, SPS: 3.9, ELAP: 28:43, ETA: 9:58 - loss: 23.9029\n",
            "7682/10000 = 76.8%, SPS: 3.9, ELAP: 28:44, ETA: 9:58 - loss: 24.4311\n",
            "7683/10000 = 76.8%, SPS: 3.9, ELAP: 28:44, ETA: 9:58 - loss: 24.1702\n",
            "7684/10000 = 76.8%, SPS: 3.9, ELAP: 28:44, ETA: 9:57 - loss: 24.5415\n",
            "7685/10000 = 76.8%, SPS: 3.9, ELAP: 28:44, ETA: 9:57 - loss: 24.2279\n",
            "7686/10000 = 76.9%, SPS: 3.9, ELAP: 28:45, ETA: 9:57 - loss: 23.5194\n",
            "7687/10000 = 76.9%, SPS: 3.9, ELAP: 28:45, ETA: 9:57 - loss: 23.9198\n",
            "7688/10000 = 76.9%, SPS: 3.9, ELAP: 28:45, ETA: 9:56 - loss: 24.3711\n",
            "7689/10000 = 76.9%, SPS: 3.9, ELAP: 28:45, ETA: 9:56 - loss: 23.6918\n",
            "7690/10000 = 76.9%, SPS: 3.9, ELAP: 28:46, ETA: 9:56 - loss: 23.7457\n",
            "7691/10000 = 76.9%, SPS: 3.9, ELAP: 28:46, ETA: 9:56 - loss: 23.7391\n",
            "7692/10000 = 76.9%, SPS: 3.9, ELAP: 28:46, ETA: 9:55 - loss: 24.6088\n",
            "7693/10000 = 76.9%, SPS: 3.9, ELAP: 28:46, ETA: 9:55 - loss: 23.6970\n",
            "7694/10000 = 76.9%, SPS: 3.9, ELAP: 28:46, ETA: 9:55 - loss: 24.1427\n",
            "7695/10000 = 77.0%, SPS: 3.9, ELAP: 28:47, ETA: 9:54 - loss: 24.2353\n",
            "7696/10000 = 77.0%, SPS: 3.9, ELAP: 28:47, ETA: 9:54 - loss: 24.5062\n",
            "7697/10000 = 77.0%, SPS: 3.9, ELAP: 28:47, ETA: 9:54 - loss: 24.3923\n",
            "7698/10000 = 77.0%, SPS: 3.9, ELAP: 28:47, ETA: 9:54 - loss: 23.7815\n",
            "7699/10000 = 77.0%, SPS: 3.9, ELAP: 28:48, ETA: 9:53 - loss: 24.1772\n",
            "7700/10000 = 77.0%, SPS: 3.9, ELAP: 28:50, ETA: 9:54 - loss: 23.7222\n",
            "7701/10000 = 77.0%, SPS: 3.9, ELAP: 28:50, ETA: 9:54 - loss: 24.3930\n",
            "7702/10000 = 77.0%, SPS: 3.9, ELAP: 28:50, ETA: 9:53 - loss: 23.5061\n",
            "7703/10000 = 77.0%, SPS: 3.9, ELAP: 28:51, ETA: 9:53 - loss: 24.0325\n",
            "7704/10000 = 77.0%, SPS: 3.9, ELAP: 28:51, ETA: 9:53 - loss: 23.7547\n",
            "7705/10000 = 77.0%, SPS: 3.9, ELAP: 28:51, ETA: 9:53 - loss: 24.2854\n",
            "7706/10000 = 77.1%, SPS: 3.9, ELAP: 28:51, ETA: 9:52 - loss: 23.9744\n",
            "7707/10000 = 77.1%, SPS: 3.9, ELAP: 28:52, ETA: 9:52 - loss: 25.0338\n",
            "7708/10000 = 77.1%, SPS: 3.9, ELAP: 28:52, ETA: 9:52 - loss: 23.8516\n",
            "7709/10000 = 77.1%, SPS: 3.9, ELAP: 28:52, ETA: 9:51 - loss: 24.3209\n",
            "7710/10000 = 77.1%, SPS: 3.9, ELAP: 28:52, ETA: 9:51 - loss: 24.5946\n",
            "7711/10000 = 77.1%, SPS: 3.9, ELAP: 28:52, ETA: 9:51 - loss: 23.6572\n",
            "7712/10000 = 77.1%, SPS: 3.9, ELAP: 28:53, ETA: 9:51 - loss: 23.5975\n",
            "7713/10000 = 77.1%, SPS: 3.9, ELAP: 28:53, ETA: 9:50 - loss: 24.1157\n",
            "7714/10000 = 77.1%, SPS: 3.9, ELAP: 28:53, ETA: 9:50 - loss: 23.8108\n",
            "7715/10000 = 77.2%, SPS: 3.9, ELAP: 28:53, ETA: 9:50 - loss: 24.1078\n",
            "7716/10000 = 77.2%, SPS: 3.9, ELAP: 28:54, ETA: 9:50 - loss: 24.0203\n",
            "7717/10000 = 77.2%, SPS: 3.9, ELAP: 28:54, ETA: 9:49 - loss: 23.6970\n",
            "7718/10000 = 77.2%, SPS: 3.9, ELAP: 28:54, ETA: 9:49 - loss: 23.8196\n",
            "7719/10000 = 77.2%, SPS: 3.9, ELAP: 28:54, ETA: 9:49 - loss: 23.5135\n",
            "7720/10000 = 77.2%, SPS: 3.9, ELAP: 28:55, ETA: 9:48 - loss: 23.7877\n",
            "7721/10000 = 77.2%, SPS: 3.9, ELAP: 28:55, ETA: 9:48 - loss: 24.0885\n",
            "7722/10000 = 77.2%, SPS: 3.9, ELAP: 28:55, ETA: 9:48 - loss: 23.4278\n",
            "7723/10000 = 77.2%, SPS: 3.9, ELAP: 28:55, ETA: 9:48 - loss: 24.4253\n",
            "7724/10000 = 77.2%, SPS: 3.9, ELAP: 28:56, ETA: 9:47 - loss: 24.2605\n",
            "7725/10000 = 77.2%, SPS: 3.9, ELAP: 28:56, ETA: 9:47 - loss: 24.1755\n",
            "7726/10000 = 77.3%, SPS: 3.9, ELAP: 28:56, ETA: 9:47 - loss: 23.7497\n",
            "7727/10000 = 77.3%, SPS: 3.9, ELAP: 28:56, ETA: 9:47 - loss: 23.7404\n",
            "7728/10000 = 77.3%, SPS: 3.9, ELAP: 28:57, ETA: 9:46 - loss: 23.5057\n",
            "7729/10000 = 77.3%, SPS: 3.9, ELAP: 28:57, ETA: 9:46 - loss: 24.2409\n",
            "7730/10000 = 77.3%, SPS: 3.9, ELAP: 28:57, ETA: 9:46 - loss: 24.2141\n",
            "7731/10000 = 77.3%, SPS: 3.9, ELAP: 28:57, ETA: 9:46 - loss: 24.2596\n",
            "7732/10000 = 77.3%, SPS: 3.9, ELAP: 28:58, ETA: 9:45 - loss: 24.1866\n",
            "7733/10000 = 77.3%, SPS: 3.9, ELAP: 28:58, ETA: 9:45 - loss: 24.0197\n",
            "7734/10000 = 77.3%, SPS: 3.9, ELAP: 28:58, ETA: 9:45 - loss: 24.2199\n",
            "7735/10000 = 77.3%, SPS: 3.9, ELAP: 28:58, ETA: 9:45 - loss: 24.3169\n",
            "7736/10000 = 77.4%, SPS: 3.9, ELAP: 28:58, ETA: 9:44 - loss: 23.8858\n",
            "7737/10000 = 77.4%, SPS: 3.9, ELAP: 28:59, ETA: 9:44 - loss: 24.2627\n",
            "7738/10000 = 77.4%, SPS: 3.9, ELAP: 28:59, ETA: 9:44 - loss: 23.9916\n",
            "7739/10000 = 77.4%, SPS: 3.9, ELAP: 28:59, ETA: 9:44 - loss: 24.3985\n",
            "7740/10000 = 77.4%, SPS: 3.9, ELAP: 28:59, ETA: 9:43 - loss: 23.9359\n",
            "7741/10000 = 77.4%, SPS: 3.9, ELAP: 29:00, ETA: 9:43 - loss: 23.5557\n",
            "7742/10000 = 77.4%, SPS: 3.9, ELAP: 29:00, ETA: 9:43 - loss: 24.1569\n",
            "7743/10000 = 77.4%, SPS: 3.9, ELAP: 29:00, ETA: 9:42 - loss: 24.5195\n",
            "7744/10000 = 77.4%, SPS: 3.9, ELAP: 29:00, ETA: 9:42 - loss: 24.0124\n",
            "7745/10000 = 77.5%, SPS: 3.9, ELAP: 29:01, ETA: 9:42 - loss: 24.2029\n",
            "7746/10000 = 77.5%, SPS: 3.9, ELAP: 29:01, ETA: 9:42 - loss: 24.1661\n",
            "7747/10000 = 77.5%, SPS: 3.9, ELAP: 29:01, ETA: 9:41 - loss: 24.4577\n",
            "7748/10000 = 77.5%, SPS: 3.9, ELAP: 29:01, ETA: 9:41 - loss: 23.5549\n",
            "7749/10000 = 77.5%, SPS: 3.9, ELAP: 29:01, ETA: 9:41 - loss: 24.0093\n",
            "7750/10000 = 77.5%, SPS: 3.9, ELAP: 29:02, ETA: 9:41 - loss: 23.8652\n",
            "7751/10000 = 77.5%, SPS: 3.9, ELAP: 29:02, ETA: 9:40 - loss: 24.0755\n",
            "7752/10000 = 77.5%, SPS: 3.9, ELAP: 29:02, ETA: 9:40 - loss: 24.2940\n",
            "7753/10000 = 77.5%, SPS: 3.9, ELAP: 29:02, ETA: 9:40 - loss: 24.4821\n",
            "7754/10000 = 77.5%, SPS: 3.9, ELAP: 29:03, ETA: 9:39 - loss: 24.4296\n",
            "7755/10000 = 77.5%, SPS: 3.9, ELAP: 29:03, ETA: 9:39 - loss: 24.0987\n",
            "7756/10000 = 77.6%, SPS: 3.9, ELAP: 29:03, ETA: 9:39 - loss: 24.3403\n",
            "7757/10000 = 77.6%, SPS: 3.9, ELAP: 29:03, ETA: 9:39 - loss: 23.8695\n",
            "7758/10000 = 77.6%, SPS: 3.9, ELAP: 29:04, ETA: 9:38 - loss: 24.3059\n",
            "7759/10000 = 77.6%, SPS: 3.9, ELAP: 29:04, ETA: 9:38 - loss: 23.5191\n",
            "7760/10000 = 77.6%, SPS: 3.9, ELAP: 29:04, ETA: 9:38 - loss: 24.0235\n",
            "7761/10000 = 77.6%, SPS: 3.9, ELAP: 29:04, ETA: 9:38 - loss: 24.6516\n",
            "7762/10000 = 77.6%, SPS: 3.9, ELAP: 29:04, ETA: 9:37 - loss: 24.3268\n",
            "7763/10000 = 77.6%, SPS: 3.9, ELAP: 29:05, ETA: 9:37 - loss: 23.6046\n",
            "7764/10000 = 77.6%, SPS: 3.9, ELAP: 29:05, ETA: 9:37 - loss: 24.0781\n",
            "7765/10000 = 77.7%, SPS: 3.9, ELAP: 29:05, ETA: 9:37 - loss: 24.2778\n",
            "7766/10000 = 77.7%, SPS: 3.9, ELAP: 29:05, ETA: 9:36 - loss: 24.2337\n",
            "7767/10000 = 77.7%, SPS: 3.9, ELAP: 29:06, ETA: 9:36 - loss: 24.2569\n",
            "7768/10000 = 77.7%, SPS: 3.9, ELAP: 29:06, ETA: 9:36 - loss: 23.7480\n",
            "7769/10000 = 77.7%, SPS: 3.9, ELAP: 29:06, ETA: 9:35 - loss: 23.9300\n",
            "7770/10000 = 77.7%, SPS: 3.9, ELAP: 29:06, ETA: 9:35 - loss: 24.4191\n",
            "7771/10000 = 77.7%, SPS: 3.9, ELAP: 29:06, ETA: 9:35 - loss: 23.9693\n",
            "7772/10000 = 77.7%, SPS: 3.9, ELAP: 29:07, ETA: 9:35 - loss: 24.0314\n",
            "7773/10000 = 77.7%, SPS: 3.9, ELAP: 29:07, ETA: 9:34 - loss: 24.0480\n",
            "7774/10000 = 77.7%, SPS: 3.9, ELAP: 29:07, ETA: 9:34 - loss: 24.3493\n",
            "7775/10000 = 77.8%, SPS: 3.9, ELAP: 29:07, ETA: 9:34 - loss: 24.0833\n",
            "7776/10000 = 77.8%, SPS: 3.9, ELAP: 29:08, ETA: 9:34 - loss: 24.0514\n",
            "7777/10000 = 77.8%, SPS: 3.9, ELAP: 29:08, ETA: 9:33 - loss: 24.6740\n",
            "7778/10000 = 77.8%, SPS: 3.9, ELAP: 29:08, ETA: 9:33 - loss: 24.0728\n",
            "7779/10000 = 77.8%, SPS: 3.9, ELAP: 29:08, ETA: 9:33 - loss: 23.9626\n",
            "7780/10000 = 77.8%, SPS: 3.9, ELAP: 29:09, ETA: 9:33 - loss: 24.2534\n",
            "7781/10000 = 77.8%, SPS: 3.9, ELAP: 29:09, ETA: 9:32 - loss: 24.0259\n",
            "7782/10000 = 77.8%, SPS: 3.9, ELAP: 29:09, ETA: 9:32 - loss: 24.4684\n",
            "7783/10000 = 77.8%, SPS: 3.9, ELAP: 29:09, ETA: 9:32 - loss: 23.7730\n",
            "7784/10000 = 77.8%, SPS: 3.9, ELAP: 29:09, ETA: 9:31 - loss: 24.5016\n",
            "7785/10000 = 77.8%, SPS: 3.9, ELAP: 29:10, ETA: 9:31 - loss: 24.1996\n",
            "7786/10000 = 77.9%, SPS: 3.9, ELAP: 29:10, ETA: 9:31 - loss: 23.0415\n",
            "7787/10000 = 77.9%, SPS: 3.9, ELAP: 29:10, ETA: 9:31 - loss: 23.7963\n",
            "7788/10000 = 77.9%, SPS: 3.9, ELAP: 29:10, ETA: 9:30 - loss: 24.7773\n",
            "7789/10000 = 77.9%, SPS: 3.9, ELAP: 29:11, ETA: 9:30 - loss: 24.4131\n",
            "7790/10000 = 77.9%, SPS: 3.9, ELAP: 29:11, ETA: 9:30 - loss: 23.8948\n",
            "7791/10000 = 77.9%, SPS: 3.9, ELAP: 29:11, ETA: 9:30 - loss: 23.7678\n",
            "7792/10000 = 77.9%, SPS: 3.9, ELAP: 29:11, ETA: 9:29 - loss: 24.4392\n",
            "7793/10000 = 77.9%, SPS: 3.9, ELAP: 29:11, ETA: 9:29 - loss: 23.6436\n",
            "7794/10000 = 77.9%, SPS: 3.9, ELAP: 29:12, ETA: 9:29 - loss: 24.3776\n",
            "7795/10000 = 78.0%, SPS: 3.9, ELAP: 29:12, ETA: 9:29 - loss: 23.9093\n",
            "7796/10000 = 78.0%, SPS: 3.9, ELAP: 29:12, ETA: 9:28 - loss: 23.8310\n",
            "7797/10000 = 78.0%, SPS: 3.9, ELAP: 29:12, ETA: 9:28 - loss: 24.2238\n",
            "7798/10000 = 78.0%, SPS: 3.9, ELAP: 29:13, ETA: 9:28 - loss: 24.1867\n",
            "7799/10000 = 78.0%, SPS: 3.9, ELAP: 29:13, ETA: 9:27 - loss: 24.1903\n",
            "7800/10000 = 78.0%, SPS: 3.9, ELAP: 29:15, ETA: 9:28 - loss: 24.0348\n",
            "7801/10000 = 78.0%, SPS: 3.9, ELAP: 29:15, ETA: 9:28 - loss: 23.8330\n",
            "7802/10000 = 78.0%, SPS: 3.9, ELAP: 29:16, ETA: 9:27 - loss: 24.7249\n",
            "7803/10000 = 78.0%, SPS: 3.9, ELAP: 29:16, ETA: 9:27 - loss: 24.0903\n",
            "7804/10000 = 78.0%, SPS: 3.9, ELAP: 29:16, ETA: 9:27 - loss: 24.4040\n",
            "7805/10000 = 78.0%, SPS: 3.9, ELAP: 29:16, ETA: 9:27 - loss: 23.9068\n",
            "7806/10000 = 78.1%, SPS: 3.9, ELAP: 29:17, ETA: 9:26 - loss: 24.4120\n",
            "7807/10000 = 78.1%, SPS: 3.9, ELAP: 29:17, ETA: 9:26 - loss: 24.6407\n",
            "7808/10000 = 78.1%, SPS: 3.9, ELAP: 29:17, ETA: 9:26 - loss: 23.6708\n",
            "7809/10000 = 78.1%, SPS: 3.9, ELAP: 29:17, ETA: 9:25 - loss: 23.7193\n",
            "7810/10000 = 78.1%, SPS: 3.9, ELAP: 29:18, ETA: 9:25 - loss: 23.6246\n",
            "7811/10000 = 78.1%, SPS: 3.9, ELAP: 29:18, ETA: 9:25 - loss: 23.7622\n",
            "7812/10000 = 78.1%, SPS: 3.9, ELAP: 29:18, ETA: 9:25 - loss: 22.9973\n",
            "7813/10000 = 78.1%, SPS: 3.9, ELAP: 29:18, ETA: 9:24 - loss: 23.7029\n",
            "7814/10000 = 78.1%, SPS: 3.9, ELAP: 29:18, ETA: 9:24 - loss: 24.5474\n",
            "7815/10000 = 78.2%, SPS: 3.9, ELAP: 29:19, ETA: 9:24 - loss: 23.9360\n",
            "7816/10000 = 78.2%, SPS: 3.9, ELAP: 29:19, ETA: 9:24 - loss: 24.5295\n",
            "7817/10000 = 78.2%, SPS: 3.9, ELAP: 29:19, ETA: 9:23 - loss: 24.1949\n",
            "7818/10000 = 78.2%, SPS: 3.9, ELAP: 29:19, ETA: 9:23 - loss: 24.2227\n",
            "7819/10000 = 78.2%, SPS: 3.9, ELAP: 29:20, ETA: 9:23 - loss: 24.1252\n",
            "7820/10000 = 78.2%, SPS: 3.9, ELAP: 29:20, ETA: 9:23 - loss: 24.0802\n",
            "7821/10000 = 78.2%, SPS: 3.9, ELAP: 29:20, ETA: 9:22 - loss: 23.8983\n",
            "7822/10000 = 78.2%, SPS: 3.9, ELAP: 29:20, ETA: 9:22 - loss: 24.1171\n",
            "7823/10000 = 78.2%, SPS: 3.9, ELAP: 29:21, ETA: 9:22 - loss: 24.8859\n",
            "7824/10000 = 78.2%, SPS: 3.9, ELAP: 29:21, ETA: 9:21 - loss: 23.9834\n",
            "7825/10000 = 78.2%, SPS: 3.9, ELAP: 29:21, ETA: 9:21 - loss: 24.3064\n",
            "7826/10000 = 78.3%, SPS: 3.9, ELAP: 29:21, ETA: 9:21 - loss: 23.9977\n",
            "7827/10000 = 78.3%, SPS: 3.9, ELAP: 29:22, ETA: 9:21 - loss: 23.9310\n",
            "7828/10000 = 78.3%, SPS: 3.9, ELAP: 29:22, ETA: 9:20 - loss: 24.4557\n",
            "7829/10000 = 78.3%, SPS: 3.9, ELAP: 29:22, ETA: 9:20 - loss: 23.9993\n",
            "7830/10000 = 78.3%, SPS: 3.9, ELAP: 29:22, ETA: 9:20 - loss: 24.0111\n",
            "7831/10000 = 78.3%, SPS: 3.9, ELAP: 29:22, ETA: 9:20 - loss: 24.1201\n",
            "7832/10000 = 78.3%, SPS: 3.9, ELAP: 29:23, ETA: 9:19 - loss: 23.9545\n",
            "7833/10000 = 78.3%, SPS: 3.9, ELAP: 29:23, ETA: 9:19 - loss: 23.6590\n",
            "7834/10000 = 78.3%, SPS: 3.9, ELAP: 29:23, ETA: 9:19 - loss: 23.7626\n",
            "7835/10000 = 78.3%, SPS: 3.9, ELAP: 29:23, ETA: 9:19 - loss: 23.9853\n",
            "7836/10000 = 78.4%, SPS: 3.9, ELAP: 29:24, ETA: 9:18 - loss: 23.7409\n",
            "7837/10000 = 78.4%, SPS: 3.9, ELAP: 29:24, ETA: 9:18 - loss: 23.3876\n",
            "7838/10000 = 78.4%, SPS: 3.9, ELAP: 29:24, ETA: 9:18 - loss: 24.0228\n",
            "7839/10000 = 78.4%, SPS: 3.9, ELAP: 29:24, ETA: 9:17 - loss: 23.6645\n",
            "7840/10000 = 78.4%, SPS: 3.9, ELAP: 29:25, ETA: 9:17 - loss: 23.3561\n",
            "7841/10000 = 78.4%, SPS: 3.9, ELAP: 29:25, ETA: 9:17 - loss: 24.2866\n",
            "7842/10000 = 78.4%, SPS: 3.9, ELAP: 29:25, ETA: 9:17 - loss: 24.8585\n",
            "7843/10000 = 78.4%, SPS: 3.9, ELAP: 29:25, ETA: 9:16 - loss: 24.4825\n",
            "7844/10000 = 78.4%, SPS: 3.9, ELAP: 29:25, ETA: 9:16 - loss: 24.3737\n",
            "7845/10000 = 78.5%, SPS: 3.9, ELAP: 29:26, ETA: 9:16 - loss: 23.5397\n",
            "7846/10000 = 78.5%, SPS: 3.9, ELAP: 29:26, ETA: 9:16 - loss: 24.3570\n",
            "7847/10000 = 78.5%, SPS: 3.9, ELAP: 29:26, ETA: 9:15 - loss: 23.7087\n",
            "7848/10000 = 78.5%, SPS: 3.9, ELAP: 29:26, ETA: 9:15 - loss: 24.1395\n",
            "7849/10000 = 78.5%, SPS: 3.9, ELAP: 29:27, ETA: 9:15 - loss: 24.0350\n",
            "7850/10000 = 78.5%, SPS: 3.9, ELAP: 29:27, ETA: 9:15 - loss: 23.9070\n",
            "7851/10000 = 78.5%, SPS: 3.9, ELAP: 29:27, ETA: 9:14 - loss: 23.6144\n",
            "7852/10000 = 78.5%, SPS: 3.9, ELAP: 29:27, ETA: 9:14 - loss: 24.2813\n",
            "7853/10000 = 78.5%, SPS: 3.9, ELAP: 29:28, ETA: 9:14 - loss: 23.9112\n",
            "7854/10000 = 78.5%, SPS: 3.9, ELAP: 29:28, ETA: 9:13 - loss: 24.1476\n",
            "7855/10000 = 78.5%, SPS: 3.9, ELAP: 29:28, ETA: 9:13 - loss: 23.7491\n",
            "7856/10000 = 78.6%, SPS: 3.9, ELAP: 29:28, ETA: 9:13 - loss: 23.9329\n",
            "7857/10000 = 78.6%, SPS: 3.9, ELAP: 29:28, ETA: 9:13 - loss: 24.4936\n",
            "7858/10000 = 78.6%, SPS: 3.9, ELAP: 29:29, ETA: 9:12 - loss: 24.0324\n",
            "7859/10000 = 78.6%, SPS: 3.9, ELAP: 29:29, ETA: 9:12 - loss: 24.0147\n",
            "7860/10000 = 78.6%, SPS: 3.9, ELAP: 29:29, ETA: 9:12 - loss: 23.6767\n",
            "7861/10000 = 78.6%, SPS: 3.9, ELAP: 29:29, ETA: 9:12 - loss: 23.8408\n",
            "7862/10000 = 78.6%, SPS: 3.9, ELAP: 29:30, ETA: 9:11 - loss: 23.9518\n",
            "7863/10000 = 78.6%, SPS: 3.9, ELAP: 29:30, ETA: 9:11 - loss: 24.3679\n",
            "7864/10000 = 78.6%, SPS: 3.9, ELAP: 29:30, ETA: 9:11 - loss: 24.2377\n",
            "7865/10000 = 78.7%, SPS: 3.9, ELAP: 29:30, ETA: 9:11 - loss: 24.3964\n",
            "7866/10000 = 78.7%, SPS: 3.9, ELAP: 29:31, ETA: 9:10 - loss: 24.2906\n",
            "7867/10000 = 78.7%, SPS: 3.9, ELAP: 29:31, ETA: 9:10 - loss: 23.8710\n",
            "7868/10000 = 78.7%, SPS: 3.9, ELAP: 29:31, ETA: 9:10 - loss: 24.5999\n",
            "7869/10000 = 78.7%, SPS: 3.9, ELAP: 29:31, ETA: 9:09 - loss: 23.7764\n",
            "7870/10000 = 78.7%, SPS: 3.9, ELAP: 29:31, ETA: 9:09 - loss: 23.6278\n",
            "7871/10000 = 78.7%, SPS: 3.9, ELAP: 29:32, ETA: 9:09 - loss: 23.6792\n",
            "7872/10000 = 78.7%, SPS: 3.9, ELAP: 29:32, ETA: 9:09 - loss: 23.8975\n",
            "7873/10000 = 78.7%, SPS: 3.9, ELAP: 29:32, ETA: 9:08 - loss: 23.7053\n",
            "7874/10000 = 78.7%, SPS: 3.9, ELAP: 29:32, ETA: 9:08 - loss: 23.6890\n",
            "7875/10000 = 78.8%, SPS: 3.9, ELAP: 29:33, ETA: 9:08 - loss: 24.1521\n",
            "7876/10000 = 78.8%, SPS: 3.9, ELAP: 29:33, ETA: 9:08 - loss: 23.8633\n",
            "7877/10000 = 78.8%, SPS: 3.9, ELAP: 29:33, ETA: 9:07 - loss: 23.7540\n",
            "7878/10000 = 78.8%, SPS: 3.9, ELAP: 29:33, ETA: 9:07 - loss: 23.3309\n",
            "7879/10000 = 78.8%, SPS: 3.9, ELAP: 29:33, ETA: 9:07 - loss: 24.3199\n",
            "7880/10000 = 78.8%, SPS: 3.9, ELAP: 29:34, ETA: 9:07 - loss: 23.9496\n",
            "7881/10000 = 78.8%, SPS: 3.9, ELAP: 29:34, ETA: 9:06 - loss: 24.4401\n",
            "7882/10000 = 78.8%, SPS: 3.9, ELAP: 29:34, ETA: 9:06 - loss: 24.4778\n",
            "7883/10000 = 78.8%, SPS: 3.9, ELAP: 29:34, ETA: 9:06 - loss: 23.6217\n",
            "7884/10000 = 78.8%, SPS: 3.9, ELAP: 29:35, ETA: 9:05 - loss: 24.2768\n",
            "7885/10000 = 78.8%, SPS: 3.9, ELAP: 29:35, ETA: 9:05 - loss: 23.8513\n",
            "7886/10000 = 78.9%, SPS: 3.9, ELAP: 29:35, ETA: 9:05 - loss: 23.4667\n",
            "7887/10000 = 78.9%, SPS: 3.9, ELAP: 29:35, ETA: 9:05 - loss: 23.7950\n",
            "7888/10000 = 78.9%, SPS: 3.9, ELAP: 29:36, ETA: 9:04 - loss: 24.0998\n",
            "7889/10000 = 78.9%, SPS: 3.9, ELAP: 29:36, ETA: 9:04 - loss: 23.6953\n",
            "7890/10000 = 78.9%, SPS: 3.9, ELAP: 29:36, ETA: 9:04 - loss: 23.6292\n",
            "7891/10000 = 78.9%, SPS: 3.9, ELAP: 29:36, ETA: 9:04 - loss: 23.8886\n",
            "7892/10000 = 78.9%, SPS: 3.9, ELAP: 29:36, ETA: 9:03 - loss: 24.5839\n",
            "7893/10000 = 78.9%, SPS: 3.9, ELAP: 29:37, ETA: 9:03 - loss: 24.1900\n",
            "7894/10000 = 78.9%, SPS: 3.9, ELAP: 29:37, ETA: 9:03 - loss: 23.6029\n",
            "7895/10000 = 79.0%, SPS: 3.9, ELAP: 29:37, ETA: 9:03 - loss: 23.7403\n",
            "7896/10000 = 79.0%, SPS: 3.9, ELAP: 29:37, ETA: 9:02 - loss: 23.5043\n",
            "7897/10000 = 79.0%, SPS: 3.9, ELAP: 29:38, ETA: 9:02 - loss: 23.5525\n",
            "7898/10000 = 79.0%, SPS: 3.9, ELAP: 29:38, ETA: 9:02 - loss: 24.4666\n",
            "7899/10000 = 79.0%, SPS: 3.9, ELAP: 29:38, ETA: 9:01 - loss: 24.0742\n",
            "7900/10000 = 79.0%, SPS: 3.9, ELAP: 29:40, ETA: 9:02 - loss: 24.5694\n",
            "7901/10000 = 79.0%, SPS: 3.9, ELAP: 29:41, ETA: 9:02 - loss: 23.8199\n",
            "7902/10000 = 79.0%, SPS: 3.9, ELAP: 29:41, ETA: 9:01 - loss: 23.1948\n",
            "7903/10000 = 79.0%, SPS: 3.9, ELAP: 29:41, ETA: 9:01 - loss: 23.8899\n",
            "7904/10000 = 79.0%, SPS: 3.9, ELAP: 29:41, ETA: 9:01 - loss: 24.2203\n",
            "7905/10000 = 79.0%, SPS: 3.9, ELAP: 29:42, ETA: 9:01 - loss: 24.0054\n",
            "7906/10000 = 79.1%, SPS: 3.9, ELAP: 29:42, ETA: 9:00 - loss: 23.5325\n",
            "7907/10000 = 79.1%, SPS: 3.9, ELAP: 29:42, ETA: 9:00 - loss: 23.9992\n",
            "7908/10000 = 79.1%, SPS: 3.9, ELAP: 29:42, ETA: 9:00 - loss: 23.4499\n",
            "7909/10000 = 79.1%, SPS: 3.9, ELAP: 29:42, ETA: 8:59 - loss: 24.3904\n",
            "7910/10000 = 79.1%, SPS: 3.9, ELAP: 29:43, ETA: 8:59 - loss: 23.7028\n",
            "7911/10000 = 79.1%, SPS: 3.9, ELAP: 29:43, ETA: 8:59 - loss: 23.6131\n",
            "7912/10000 = 79.1%, SPS: 3.9, ELAP: 29:43, ETA: 8:59 - loss: 24.7135\n",
            "7913/10000 = 79.1%, SPS: 3.9, ELAP: 29:43, ETA: 8:58 - loss: 24.0899\n",
            "7914/10000 = 79.1%, SPS: 3.9, ELAP: 29:44, ETA: 8:58 - loss: 23.9078\n",
            "7915/10000 = 79.2%, SPS: 3.9, ELAP: 29:44, ETA: 8:58 - loss: 24.1342\n",
            "7916/10000 = 79.2%, SPS: 3.9, ELAP: 29:44, ETA: 8:58 - loss: 24.0776\n",
            "7917/10000 = 79.2%, SPS: 3.9, ELAP: 29:44, ETA: 8:57 - loss: 24.0885\n",
            "7918/10000 = 79.2%, SPS: 3.9, ELAP: 29:45, ETA: 8:57 - loss: 23.6736\n",
            "7919/10000 = 79.2%, SPS: 3.9, ELAP: 29:45, ETA: 8:57 - loss: 23.7823\n",
            "7920/10000 = 79.2%, SPS: 3.9, ELAP: 29:45, ETA: 8:57 - loss: 24.2648\n",
            "7921/10000 = 79.2%, SPS: 3.9, ELAP: 29:45, ETA: 8:56 - loss: 23.9423\n",
            "7922/10000 = 79.2%, SPS: 3.9, ELAP: 29:46, ETA: 8:56 - loss: 23.6735\n",
            "7923/10000 = 79.2%, SPS: 3.9, ELAP: 29:46, ETA: 8:56 - loss: 24.5884\n",
            "7924/10000 = 79.2%, SPS: 3.9, ELAP: 29:46, ETA: 8:55 - loss: 23.5396\n",
            "7925/10000 = 79.2%, SPS: 3.9, ELAP: 29:46, ETA: 8:55 - loss: 24.2160\n",
            "7926/10000 = 79.3%, SPS: 3.9, ELAP: 29:46, ETA: 8:55 - loss: 23.5889\n",
            "7927/10000 = 79.3%, SPS: 3.9, ELAP: 29:47, ETA: 8:55 - loss: 23.7520\n",
            "7928/10000 = 79.3%, SPS: 3.9, ELAP: 29:47, ETA: 8:54 - loss: 24.0738\n",
            "7929/10000 = 79.3%, SPS: 3.9, ELAP: 29:47, ETA: 8:54 - loss: 24.4709\n",
            "7930/10000 = 79.3%, SPS: 3.9, ELAP: 29:47, ETA: 8:54 - loss: 24.1028\n",
            "7931/10000 = 79.3%, SPS: 3.9, ELAP: 29:48, ETA: 8:54 - loss: 24.1346\n",
            "7932/10000 = 79.3%, SPS: 3.9, ELAP: 29:48, ETA: 8:53 - loss: 23.9440\n",
            "7933/10000 = 79.3%, SPS: 3.9, ELAP: 29:48, ETA: 8:53 - loss: 23.9571\n",
            "7934/10000 = 79.3%, SPS: 3.9, ELAP: 29:48, ETA: 8:53 - loss: 23.8224\n",
            "7935/10000 = 79.3%, SPS: 3.9, ELAP: 29:49, ETA: 8:53 - loss: 24.0018\n",
            "7936/10000 = 79.4%, SPS: 3.9, ELAP: 29:49, ETA: 8:52 - loss: 23.8337\n",
            "7937/10000 = 79.4%, SPS: 3.9, ELAP: 29:49, ETA: 8:52 - loss: 24.0370\n",
            "7938/10000 = 79.4%, SPS: 3.9, ELAP: 29:49, ETA: 8:52 - loss: 23.8520\n",
            "7939/10000 = 79.4%, SPS: 3.9, ELAP: 29:50, ETA: 8:52 - loss: 24.2734\n",
            "7940/10000 = 79.4%, SPS: 3.9, ELAP: 29:50, ETA: 8:51 - loss: 24.4200\n",
            "7941/10000 = 79.4%, SPS: 3.9, ELAP: 29:50, ETA: 8:51 - loss: 23.8553\n",
            "7942/10000 = 79.4%, SPS: 3.9, ELAP: 29:50, ETA: 8:51 - loss: 24.5661\n",
            "7943/10000 = 79.4%, SPS: 3.9, ELAP: 29:51, ETA: 8:50 - loss: 23.5124\n",
            "7944/10000 = 79.4%, SPS: 3.9, ELAP: 29:51, ETA: 8:50 - loss: 23.5091\n",
            "7945/10000 = 79.5%, SPS: 3.9, ELAP: 29:51, ETA: 8:50 - loss: 24.1740\n",
            "7946/10000 = 79.5%, SPS: 3.9, ELAP: 29:51, ETA: 8:50 - loss: 24.2384\n",
            "7947/10000 = 79.5%, SPS: 3.9, ELAP: 29:51, ETA: 8:49 - loss: 24.0903\n",
            "7948/10000 = 79.5%, SPS: 3.9, ELAP: 29:52, ETA: 8:49 - loss: 24.1691\n",
            "7949/10000 = 79.5%, SPS: 3.9, ELAP: 29:52, ETA: 8:49 - loss: 23.5828\n",
            "7950/10000 = 79.5%, SPS: 3.9, ELAP: 29:52, ETA: 8:49 - loss: 24.5658\n",
            "7951/10000 = 79.5%, SPS: 3.9, ELAP: 29:52, ETA: 8:48 - loss: 23.8752\n",
            "7952/10000 = 79.5%, SPS: 3.9, ELAP: 29:53, ETA: 8:48 - loss: 24.4860\n",
            "7953/10000 = 79.5%, SPS: 3.9, ELAP: 29:53, ETA: 8:48 - loss: 24.1079\n",
            "7954/10000 = 79.5%, SPS: 3.9, ELAP: 29:53, ETA: 8:48 - loss: 24.0071\n",
            "7955/10000 = 79.5%, SPS: 3.9, ELAP: 29:53, ETA: 8:47 - loss: 23.5776\n",
            "7956/10000 = 79.6%, SPS: 3.9, ELAP: 29:53, ETA: 8:47 - loss: 24.5095\n",
            "7957/10000 = 79.6%, SPS: 3.9, ELAP: 29:54, ETA: 8:47 - loss: 23.3661\n",
            "7958/10000 = 79.6%, SPS: 3.9, ELAP: 29:54, ETA: 8:46 - loss: 23.2706\n",
            "7959/10000 = 79.6%, SPS: 3.9, ELAP: 29:54, ETA: 8:46 - loss: 23.5323\n",
            "7960/10000 = 79.6%, SPS: 3.9, ELAP: 29:54, ETA: 8:46 - loss: 23.5601\n",
            "7961/10000 = 79.6%, SPS: 3.9, ELAP: 29:55, ETA: 8:46 - loss: 23.9001\n",
            "7962/10000 = 79.6%, SPS: 3.9, ELAP: 29:55, ETA: 8:45 - loss: 24.0522\n",
            "7963/10000 = 79.6%, SPS: 3.9, ELAP: 29:55, ETA: 8:45 - loss: 23.9601\n",
            "7964/10000 = 79.6%, SPS: 3.9, ELAP: 29:55, ETA: 8:45 - loss: 23.3434\n",
            "7965/10000 = 79.7%, SPS: 3.9, ELAP: 29:56, ETA: 8:45 - loss: 24.1824\n",
            "7966/10000 = 79.7%, SPS: 3.9, ELAP: 29:56, ETA: 8:44 - loss: 23.7726\n",
            "7967/10000 = 79.7%, SPS: 3.9, ELAP: 29:56, ETA: 8:44 - loss: 24.2379\n",
            "7968/10000 = 79.7%, SPS: 3.9, ELAP: 29:56, ETA: 8:44 - loss: 24.1803\n",
            "7969/10000 = 79.7%, SPS: 3.9, ELAP: 29:56, ETA: 8:44 - loss: 24.5531\n",
            "7970/10000 = 79.7%, SPS: 3.9, ELAP: 29:57, ETA: 8:43 - loss: 23.8433\n",
            "7971/10000 = 79.7%, SPS: 3.9, ELAP: 29:57, ETA: 8:43 - loss: 24.3602\n",
            "7972/10000 = 79.7%, SPS: 3.9, ELAP: 29:57, ETA: 8:43 - loss: 23.5611\n",
            "7973/10000 = 79.7%, SPS: 3.9, ELAP: 29:57, ETA: 8:42 - loss: 23.8290\n",
            "7974/10000 = 79.7%, SPS: 3.9, ELAP: 29:58, ETA: 8:42 - loss: 24.3866\n",
            "7975/10000 = 79.8%, SPS: 3.9, ELAP: 29:58, ETA: 8:42 - loss: 23.8416\n",
            "7976/10000 = 79.8%, SPS: 3.9, ELAP: 29:58, ETA: 8:42 - loss: 23.8511\n",
            "7977/10000 = 79.8%, SPS: 3.9, ELAP: 29:58, ETA: 8:41 - loss: 23.7214\n",
            "7978/10000 = 79.8%, SPS: 3.9, ELAP: 29:59, ETA: 8:41 - loss: 24.2815\n",
            "7979/10000 = 79.8%, SPS: 3.9, ELAP: 29:59, ETA: 8:41 - loss: 24.2563\n",
            "7980/10000 = 79.8%, SPS: 3.9, ELAP: 29:59, ETA: 8:41 - loss: 24.0699\n",
            "7981/10000 = 79.8%, SPS: 3.9, ELAP: 29:59, ETA: 8:40 - loss: 23.9594\n",
            "7982/10000 = 79.8%, SPS: 3.9, ELAP: 29:59, ETA: 8:40 - loss: 24.5640\n",
            "7983/10000 = 79.8%, SPS: 3.9, ELAP: 30:00, ETA: 8:40 - loss: 24.0080\n",
            "7984/10000 = 79.8%, SPS: 3.9, ELAP: 30:00, ETA: 8:40 - loss: 23.8767\n",
            "7985/10000 = 79.8%, SPS: 3.9, ELAP: 30:00, ETA: 8:39 - loss: 24.5001\n",
            "7986/10000 = 79.9%, SPS: 3.9, ELAP: 30:00, ETA: 8:39 - loss: 24.4979\n",
            "7987/10000 = 79.9%, SPS: 3.9, ELAP: 30:01, ETA: 8:39 - loss: 24.1818\n",
            "7988/10000 = 79.9%, SPS: 3.9, ELAP: 30:01, ETA: 8:38 - loss: 23.6650\n",
            "7989/10000 = 79.9%, SPS: 3.9, ELAP: 30:01, ETA: 8:38 - loss: 23.3764\n",
            "7990/10000 = 79.9%, SPS: 3.9, ELAP: 30:01, ETA: 8:38 - loss: 23.9043\n",
            "7991/10000 = 79.9%, SPS: 3.9, ELAP: 30:01, ETA: 8:38 - loss: 23.6828\n",
            "7992/10000 = 79.9%, SPS: 3.9, ELAP: 30:02, ETA: 8:37 - loss: 24.4699\n",
            "7993/10000 = 79.9%, SPS: 3.9, ELAP: 30:02, ETA: 8:37 - loss: 24.1260\n",
            "7994/10000 = 79.9%, SPS: 3.9, ELAP: 30:02, ETA: 8:37 - loss: 24.2118\n",
            "7995/10000 = 80.0%, SPS: 3.9, ELAP: 30:02, ETA: 8:37 - loss: 24.2238\n",
            "7996/10000 = 80.0%, SPS: 3.9, ELAP: 30:03, ETA: 8:36 - loss: 24.6477\n",
            "7997/10000 = 80.0%, SPS: 3.9, ELAP: 30:03, ETA: 8:36 - loss: 23.7391\n",
            "7998/10000 = 80.0%, SPS: 3.9, ELAP: 30:03, ETA: 8:36 - loss: 24.4551\n",
            "7999/10000 = 80.0%, SPS: 3.9, ELAP: 30:03, ETA: 8:36 - loss: 24.0947\n",
            "8000/10000 = 80.0%, SPS: 3.9, ELAP: 30:06, ETA: 8:36 - loss: 23.4937\n",
            "8001/10000 = 80.0%, SPS: 3.9, ELAP: 30:06, ETA: 8:36 - loss: 24.4644\n",
            "8002/10000 = 80.0%, SPS: 3.9, ELAP: 30:06, ETA: 8:35 - loss: 23.8123\n",
            "8003/10000 = 80.0%, SPS: 3.9, ELAP: 30:06, ETA: 8:35 - loss: 24.2010\n",
            "8004/10000 = 80.0%, SPS: 3.9, ELAP: 30:07, ETA: 8:35 - loss: 24.1633\n",
            "8005/10000 = 80.0%, SPS: 3.9, ELAP: 30:07, ETA: 8:35 - loss: 24.3818\n",
            "8006/10000 = 80.1%, SPS: 3.9, ELAP: 30:07, ETA: 8:34 - loss: 24.2000\n",
            "8007/10000 = 80.1%, SPS: 3.9, ELAP: 30:07, ETA: 8:34 - loss: 24.3827\n",
            "8008/10000 = 80.1%, SPS: 3.9, ELAP: 30:08, ETA: 8:34 - loss: 23.9037\n",
            "8009/10000 = 80.1%, SPS: 3.9, ELAP: 30:08, ETA: 8:34 - loss: 24.4623\n",
            "8010/10000 = 80.1%, SPS: 3.9, ELAP: 30:08, ETA: 8:33 - loss: 24.7484\n",
            "8011/10000 = 80.1%, SPS: 3.9, ELAP: 30:08, ETA: 8:33 - loss: 24.1920\n",
            "8012/10000 = 80.1%, SPS: 3.9, ELAP: 30:08, ETA: 8:33 - loss: 23.9328\n",
            "8013/10000 = 80.1%, SPS: 3.9, ELAP: 30:09, ETA: 8:32 - loss: 23.4732\n",
            "8014/10000 = 80.1%, SPS: 3.9, ELAP: 30:09, ETA: 8:32 - loss: 24.1336\n",
            "8015/10000 = 80.2%, SPS: 3.9, ELAP: 30:09, ETA: 8:32 - loss: 24.3132\n",
            "8016/10000 = 80.2%, SPS: 3.9, ELAP: 30:09, ETA: 8:32 - loss: 23.4380\n",
            "8017/10000 = 80.2%, SPS: 3.9, ELAP: 30:10, ETA: 8:31 - loss: 23.1235\n",
            "8018/10000 = 80.2%, SPS: 3.9, ELAP: 30:10, ETA: 8:31 - loss: 24.0007\n",
            "8019/10000 = 80.2%, SPS: 3.9, ELAP: 30:10, ETA: 8:31 - loss: 24.0107\n",
            "8020/10000 = 80.2%, SPS: 3.9, ELAP: 30:10, ETA: 8:31 - loss: 24.6783\n",
            "8021/10000 = 80.2%, SPS: 3.9, ELAP: 30:11, ETA: 8:30 - loss: 23.8969\n",
            "8022/10000 = 80.2%, SPS: 3.9, ELAP: 30:11, ETA: 8:30 - loss: 23.8627\n",
            "8023/10000 = 80.2%, SPS: 3.9, ELAP: 30:11, ETA: 8:30 - loss: 24.4917\n",
            "8024/10000 = 80.2%, SPS: 3.9, ELAP: 30:11, ETA: 8:30 - loss: 24.2298\n",
            "8025/10000 = 80.2%, SPS: 3.9, ELAP: 30:12, ETA: 8:29 - loss: 25.0956\n",
            "8026/10000 = 80.3%, SPS: 3.9, ELAP: 30:12, ETA: 8:29 - loss: 23.9846\n",
            "8027/10000 = 80.3%, SPS: 3.9, ELAP: 30:12, ETA: 8:29 - loss: 24.2729\n",
            "8028/10000 = 80.3%, SPS: 3.9, ELAP: 30:12, ETA: 8:29 - loss: 24.0889\n",
            "8029/10000 = 80.3%, SPS: 3.9, ELAP: 30:13, ETA: 8:28 - loss: 23.3759\n",
            "8030/10000 = 80.3%, SPS: 3.9, ELAP: 30:13, ETA: 8:28 - loss: 23.5648\n",
            "8031/10000 = 80.3%, SPS: 3.9, ELAP: 30:13, ETA: 8:28 - loss: 23.7764\n",
            "8032/10000 = 80.3%, SPS: 3.9, ELAP: 30:13, ETA: 8:27 - loss: 24.0349\n",
            "8033/10000 = 80.3%, SPS: 3.9, ELAP: 30:13, ETA: 8:27 - loss: 23.6142\n",
            "8034/10000 = 80.3%, SPS: 3.9, ELAP: 30:14, ETA: 8:27 - loss: 24.0899\n",
            "8035/10000 = 80.3%, SPS: 3.9, ELAP: 30:14, ETA: 8:27 - loss: 24.4403\n",
            "8036/10000 = 80.4%, SPS: 3.9, ELAP: 30:14, ETA: 8:26 - loss: 24.3025\n",
            "8037/10000 = 80.4%, SPS: 3.9, ELAP: 30:14, ETA: 8:26 - loss: 24.3256\n",
            "8038/10000 = 80.4%, SPS: 3.9, ELAP: 30:15, ETA: 8:26 - loss: 23.5475\n",
            "8039/10000 = 80.4%, SPS: 3.9, ELAP: 30:15, ETA: 8:26 - loss: 24.2850\n",
            "8040/10000 = 80.4%, SPS: 3.9, ELAP: 30:15, ETA: 8:25 - loss: 24.1522\n",
            "8041/10000 = 80.4%, SPS: 3.9, ELAP: 30:15, ETA: 8:25 - loss: 23.8966\n",
            "8042/10000 = 80.4%, SPS: 3.9, ELAP: 30:15, ETA: 8:25 - loss: 23.6919\n",
            "8043/10000 = 80.4%, SPS: 3.9, ELAP: 30:16, ETA: 8:25 - loss: 24.3785\n",
            "8044/10000 = 80.4%, SPS: 3.9, ELAP: 30:16, ETA: 8:24 - loss: 23.8606\n",
            "8045/10000 = 80.5%, SPS: 3.9, ELAP: 30:16, ETA: 8:24 - loss: 23.4737\n",
            "8046/10000 = 80.5%, SPS: 3.9, ELAP: 30:16, ETA: 8:24 - loss: 23.8614\n",
            "8047/10000 = 80.5%, SPS: 3.9, ELAP: 30:17, ETA: 8:23 - loss: 23.8443\n",
            "8048/10000 = 80.5%, SPS: 3.9, ELAP: 30:17, ETA: 8:23 - loss: 23.9992\n",
            "8049/10000 = 80.5%, SPS: 3.9, ELAP: 30:17, ETA: 8:23 - loss: 23.9142\n",
            "8050/10000 = 80.5%, SPS: 3.9, ELAP: 30:17, ETA: 8:23 - loss: 23.8370\n",
            "8051/10000 = 80.5%, SPS: 3.9, ELAP: 30:18, ETA: 8:22 - loss: 23.4612\n",
            "8052/10000 = 80.5%, SPS: 3.9, ELAP: 30:18, ETA: 8:22 - loss: 24.0488\n",
            "8053/10000 = 80.5%, SPS: 3.9, ELAP: 30:18, ETA: 8:22 - loss: 24.1463\n",
            "8054/10000 = 80.5%, SPS: 3.9, ELAP: 30:18, ETA: 8:22 - loss: 24.5472\n",
            "8055/10000 = 80.5%, SPS: 3.9, ELAP: 30:18, ETA: 8:21 - loss: 24.1419\n",
            "8056/10000 = 80.6%, SPS: 3.9, ELAP: 30:19, ETA: 8:21 - loss: 23.8042\n",
            "8057/10000 = 80.6%, SPS: 3.9, ELAP: 30:19, ETA: 8:21 - loss: 23.8481\n",
            "8058/10000 = 80.6%, SPS: 3.9, ELAP: 30:19, ETA: 8:21 - loss: 24.4214\n",
            "8059/10000 = 80.6%, SPS: 3.9, ELAP: 30:19, ETA: 8:20 - loss: 23.8199\n",
            "8060/10000 = 80.6%, SPS: 3.9, ELAP: 30:20, ETA: 8:20 - loss: 24.7223\n",
            "8061/10000 = 80.6%, SPS: 3.9, ELAP: 30:20, ETA: 8:20 - loss: 24.3891\n",
            "8062/10000 = 80.6%, SPS: 3.9, ELAP: 30:20, ETA: 8:19 - loss: 23.3610\n",
            "8063/10000 = 80.6%, SPS: 3.9, ELAP: 30:20, ETA: 8:19 - loss: 24.2950\n",
            "8064/10000 = 80.6%, SPS: 3.9, ELAP: 30:21, ETA: 8:19 - loss: 23.8101\n",
            "8065/10000 = 80.7%, SPS: 3.9, ELAP: 30:21, ETA: 8:19 - loss: 24.2036\n",
            "8066/10000 = 80.7%, SPS: 3.9, ELAP: 30:21, ETA: 8:18 - loss: 24.5783\n",
            "8067/10000 = 80.7%, SPS: 3.9, ELAP: 30:21, ETA: 8:18 - loss: 24.8230\n",
            "8068/10000 = 80.7%, SPS: 3.9, ELAP: 30:21, ETA: 8:18 - loss: 23.8516\n",
            "8069/10000 = 80.7%, SPS: 3.9, ELAP: 30:22, ETA: 8:18 - loss: 23.8054\n",
            "8070/10000 = 80.7%, SPS: 3.9, ELAP: 30:22, ETA: 8:17 - loss: 24.4640\n",
            "8071/10000 = 80.7%, SPS: 3.9, ELAP: 30:22, ETA: 8:17 - loss: 23.4035\n",
            "8072/10000 = 80.7%, SPS: 3.9, ELAP: 30:22, ETA: 8:17 - loss: 24.6818\n",
            "8073/10000 = 80.7%, SPS: 3.9, ELAP: 30:23, ETA: 8:17 - loss: 23.8751\n",
            "8074/10000 = 80.7%, SPS: 3.9, ELAP: 30:23, ETA: 8:16 - loss: 24.0292\n",
            "8075/10000 = 80.8%, SPS: 3.9, ELAP: 30:23, ETA: 8:16 - loss: 24.1308\n",
            "8076/10000 = 80.8%, SPS: 3.9, ELAP: 30:23, ETA: 8:16 - loss: 24.3325\n",
            "8077/10000 = 80.8%, SPS: 3.9, ELAP: 30:23, ETA: 8:15 - loss: 23.2301\n",
            "8078/10000 = 80.8%, SPS: 3.9, ELAP: 30:24, ETA: 8:15 - loss: 23.6332\n",
            "8079/10000 = 80.8%, SPS: 3.9, ELAP: 30:24, ETA: 8:15 - loss: 24.3627\n",
            "8080/10000 = 80.8%, SPS: 3.9, ELAP: 30:24, ETA: 8:15 - loss: 23.7514\n",
            "8081/10000 = 80.8%, SPS: 3.9, ELAP: 30:24, ETA: 8:14 - loss: 24.4269\n",
            "8082/10000 = 80.8%, SPS: 3.9, ELAP: 30:25, ETA: 8:14 - loss: 24.2077\n",
            "8083/10000 = 80.8%, SPS: 3.9, ELAP: 30:25, ETA: 8:14 - loss: 24.1081\n",
            "8084/10000 = 80.8%, SPS: 3.9, ELAP: 30:25, ETA: 8:14 - loss: 24.4934\n",
            "8085/10000 = 80.8%, SPS: 3.9, ELAP: 30:25, ETA: 8:13 - loss: 23.5047\n",
            "8086/10000 = 80.9%, SPS: 3.9, ELAP: 30:26, ETA: 8:13 - loss: 23.9268\n",
            "8087/10000 = 80.9%, SPS: 3.9, ELAP: 30:26, ETA: 8:13 - loss: 24.4563\n",
            "8088/10000 = 80.9%, SPS: 3.9, ELAP: 30:26, ETA: 8:13 - loss: 23.7379\n",
            "8089/10000 = 80.9%, SPS: 3.9, ELAP: 30:26, ETA: 8:12 - loss: 24.3070\n",
            "8090/10000 = 80.9%, SPS: 3.9, ELAP: 30:26, ETA: 8:12 - loss: 24.0353\n",
            "8091/10000 = 80.9%, SPS: 3.9, ELAP: 30:27, ETA: 8:12 - loss: 24.0515\n",
            "8092/10000 = 80.9%, SPS: 3.9, ELAP: 30:27, ETA: 8:11 - loss: 24.3298\n",
            "8093/10000 = 80.9%, SPS: 3.9, ELAP: 30:27, ETA: 8:11 - loss: 24.3735\n",
            "8094/10000 = 80.9%, SPS: 3.9, ELAP: 30:27, ETA: 8:11 - loss: 24.3969\n",
            "8095/10000 = 81.0%, SPS: 3.9, ELAP: 30:28, ETA: 8:11 - loss: 24.4306\n",
            "8096/10000 = 81.0%, SPS: 3.9, ELAP: 30:28, ETA: 8:10 - loss: 24.0388\n",
            "8097/10000 = 81.0%, SPS: 3.9, ELAP: 30:28, ETA: 8:10 - loss: 23.7491\n",
            "8098/10000 = 81.0%, SPS: 3.9, ELAP: 30:28, ETA: 8:10 - loss: 23.8325\n",
            "8099/10000 = 81.0%, SPS: 3.9, ELAP: 30:28, ETA: 8:10 - loss: 24.6949\n",
            "8100/10000 = 81.0%, SPS: 3.9, ELAP: 30:31, ETA: 8:10 - loss: 24.4049\n",
            "8101/10000 = 81.0%, SPS: 3.9, ELAP: 30:31, ETA: 8:10 - loss: 23.6578\n",
            "8102/10000 = 81.0%, SPS: 3.9, ELAP: 30:31, ETA: 8:09 - loss: 23.8151\n",
            "8103/10000 = 81.0%, SPS: 3.9, ELAP: 30:32, ETA: 8:09 - loss: 24.2043\n",
            "8104/10000 = 81.0%, SPS: 3.9, ELAP: 30:32, ETA: 8:09 - loss: 24.4356\n",
            "8105/10000 = 81.0%, SPS: 3.9, ELAP: 30:32, ETA: 8:09 - loss: 23.9547\n",
            "8106/10000 = 81.1%, SPS: 3.9, ELAP: 30:32, ETA: 8:08 - loss: 23.8501\n",
            "8107/10000 = 81.1%, SPS: 3.9, ELAP: 30:33, ETA: 8:08 - loss: 24.2116\n",
            "8108/10000 = 81.1%, SPS: 3.9, ELAP: 30:33, ETA: 8:08 - loss: 24.1420\n",
            "8109/10000 = 81.1%, SPS: 3.9, ELAP: 30:33, ETA: 8:08 - loss: 24.3642\n",
            "8110/10000 = 81.1%, SPS: 3.9, ELAP: 30:33, ETA: 8:07 - loss: 24.1224\n",
            "8111/10000 = 81.1%, SPS: 3.9, ELAP: 30:33, ETA: 8:07 - loss: 24.0671\n",
            "8112/10000 = 81.1%, SPS: 3.9, ELAP: 30:34, ETA: 8:07 - loss: 24.0460\n",
            "8113/10000 = 81.1%, SPS: 3.9, ELAP: 30:34, ETA: 8:07 - loss: 24.5385\n",
            "8114/10000 = 81.1%, SPS: 3.9, ELAP: 30:34, ETA: 8:06 - loss: 24.0768\n",
            "8115/10000 = 81.2%, SPS: 3.9, ELAP: 30:34, ETA: 8:06 - loss: 23.9359\n",
            "8116/10000 = 81.2%, SPS: 3.9, ELAP: 30:35, ETA: 8:06 - loss: 23.5661\n",
            "8117/10000 = 81.2%, SPS: 3.9, ELAP: 30:35, ETA: 8:05 - loss: 24.6517\n",
            "8118/10000 = 81.2%, SPS: 3.9, ELAP: 30:35, ETA: 8:05 - loss: 24.1224\n",
            "8119/10000 = 81.2%, SPS: 3.9, ELAP: 30:35, ETA: 8:05 - loss: 23.6204\n",
            "8120/10000 = 81.2%, SPS: 3.9, ELAP: 30:36, ETA: 8:05 - loss: 23.3557\n",
            "8121/10000 = 81.2%, SPS: 3.9, ELAP: 30:36, ETA: 8:04 - loss: 23.8101\n",
            "8122/10000 = 81.2%, SPS: 3.9, ELAP: 30:36, ETA: 8:04 - loss: 23.3627\n",
            "8123/10000 = 81.2%, SPS: 3.9, ELAP: 30:36, ETA: 8:04 - loss: 24.0478\n",
            "8124/10000 = 81.2%, SPS: 3.9, ELAP: 30:37, ETA: 8:04 - loss: 23.7755\n",
            "8125/10000 = 81.2%, SPS: 3.9, ELAP: 30:37, ETA: 8:03 - loss: 23.5782\n",
            "8126/10000 = 81.3%, SPS: 3.9, ELAP: 30:37, ETA: 8:03 - loss: 23.3847\n",
            "8127/10000 = 81.3%, SPS: 3.9, ELAP: 30:37, ETA: 8:03 - loss: 24.0377\n",
            "8128/10000 = 81.3%, SPS: 3.9, ELAP: 30:38, ETA: 8:03 - loss: 23.8860\n",
            "8129/10000 = 81.3%, SPS: 3.9, ELAP: 30:38, ETA: 8:02 - loss: 23.7471\n",
            "8130/10000 = 81.3%, SPS: 3.9, ELAP: 30:38, ETA: 8:02 - loss: 23.3845\n",
            "8131/10000 = 81.3%, SPS: 3.9, ELAP: 30:38, ETA: 8:02 - loss: 23.8594\n",
            "8132/10000 = 81.3%, SPS: 3.9, ELAP: 30:38, ETA: 8:02 - loss: 23.9297\n",
            "8133/10000 = 81.3%, SPS: 3.9, ELAP: 30:39, ETA: 8:01 - loss: 24.4586\n",
            "8134/10000 = 81.3%, SPS: 3.9, ELAP: 30:39, ETA: 8:01 - loss: 24.7306\n",
            "8135/10000 = 81.3%, SPS: 3.9, ELAP: 30:39, ETA: 8:01 - loss: 24.7122\n",
            "8136/10000 = 81.4%, SPS: 3.9, ELAP: 30:39, ETA: 8:00 - loss: 23.8051\n",
            "8137/10000 = 81.4%, SPS: 3.9, ELAP: 30:40, ETA: 8:00 - loss: 23.8377\n",
            "8138/10000 = 81.4%, SPS: 3.9, ELAP: 30:40, ETA: 8:00 - loss: 23.7463\n",
            "8139/10000 = 81.4%, SPS: 3.9, ELAP: 30:40, ETA: 8:00 - loss: 24.0206\n",
            "8140/10000 = 81.4%, SPS: 3.9, ELAP: 30:40, ETA: 7:59 - loss: 24.1629\n",
            "8141/10000 = 81.4%, SPS: 3.9, ELAP: 30:41, ETA: 7:59 - loss: 23.8049\n",
            "8142/10000 = 81.4%, SPS: 3.9, ELAP: 30:41, ETA: 7:59 - loss: 24.4250\n",
            "8143/10000 = 81.4%, SPS: 3.9, ELAP: 30:41, ETA: 7:59 - loss: 23.6601\n",
            "8144/10000 = 81.4%, SPS: 3.9, ELAP: 30:41, ETA: 7:58 - loss: 23.4846\n",
            "8145/10000 = 81.5%, SPS: 3.9, ELAP: 30:41, ETA: 7:58 - loss: 23.5083\n",
            "8146/10000 = 81.5%, SPS: 3.9, ELAP: 30:42, ETA: 7:58 - loss: 24.0613\n",
            "8147/10000 = 81.5%, SPS: 3.9, ELAP: 30:42, ETA: 7:58 - loss: 24.4533\n",
            "8148/10000 = 81.5%, SPS: 3.9, ELAP: 30:42, ETA: 7:57 - loss: 24.1986\n",
            "8149/10000 = 81.5%, SPS: 3.9, ELAP: 30:42, ETA: 7:57 - loss: 23.9153\n",
            "8150/10000 = 81.5%, SPS: 3.9, ELAP: 30:43, ETA: 7:57 - loss: 23.7995\n",
            "8151/10000 = 81.5%, SPS: 3.9, ELAP: 30:43, ETA: 7:56 - loss: 24.0936\n",
            "8152/10000 = 81.5%, SPS: 3.9, ELAP: 30:43, ETA: 7:56 - loss: 23.3346\n",
            "8153/10000 = 81.5%, SPS: 3.9, ELAP: 30:43, ETA: 7:56 - loss: 23.9773\n",
            "8154/10000 = 81.5%, SPS: 3.9, ELAP: 30:43, ETA: 7:56 - loss: 24.1062\n",
            "8155/10000 = 81.5%, SPS: 3.9, ELAP: 30:44, ETA: 7:55 - loss: 24.1389\n",
            "8156/10000 = 81.6%, SPS: 3.9, ELAP: 30:44, ETA: 7:55 - loss: 24.3021\n",
            "8157/10000 = 81.6%, SPS: 3.9, ELAP: 30:44, ETA: 7:55 - loss: 24.2323\n",
            "8158/10000 = 81.6%, SPS: 3.9, ELAP: 30:44, ETA: 7:55 - loss: 23.7676\n",
            "8159/10000 = 81.6%, SPS: 3.9, ELAP: 30:45, ETA: 7:54 - loss: 24.6627\n",
            "8160/10000 = 81.6%, SPS: 3.9, ELAP: 30:45, ETA: 7:54 - loss: 24.2473\n",
            "8161/10000 = 81.6%, SPS: 3.9, ELAP: 30:45, ETA: 7:54 - loss: 23.7942\n",
            "8162/10000 = 81.6%, SPS: 3.9, ELAP: 30:45, ETA: 7:54 - loss: 24.0722\n",
            "8163/10000 = 81.6%, SPS: 3.9, ELAP: 30:46, ETA: 7:53 - loss: 24.6393\n",
            "8164/10000 = 81.6%, SPS: 3.9, ELAP: 30:46, ETA: 7:53 - loss: 24.3290\n",
            "8165/10000 = 81.7%, SPS: 3.9, ELAP: 30:46, ETA: 7:53 - loss: 24.8934\n",
            "8166/10000 = 81.7%, SPS: 3.9, ELAP: 30:46, ETA: 7:53 - loss: 24.5098\n",
            "8167/10000 = 81.7%, SPS: 3.9, ELAP: 30:46, ETA: 7:52 - loss: 24.0473\n",
            "8168/10000 = 81.7%, SPS: 3.9, ELAP: 30:47, ETA: 7:52 - loss: 24.3368\n",
            "8169/10000 = 81.7%, SPS: 3.9, ELAP: 30:47, ETA: 7:52 - loss: 23.7159\n",
            "8170/10000 = 81.7%, SPS: 3.9, ELAP: 30:47, ETA: 7:51 - loss: 23.6866\n",
            "8171/10000 = 81.7%, SPS: 3.9, ELAP: 30:47, ETA: 7:51 - loss: 24.0066\n",
            "8172/10000 = 81.7%, SPS: 3.9, ELAP: 30:48, ETA: 7:51 - loss: 23.7690\n",
            "8173/10000 = 81.7%, SPS: 3.9, ELAP: 30:48, ETA: 7:51 - loss: 23.8258\n",
            "8174/10000 = 81.7%, SPS: 3.9, ELAP: 30:48, ETA: 7:50 - loss: 23.8117\n",
            "8175/10000 = 81.8%, SPS: 3.9, ELAP: 30:48, ETA: 7:50 - loss: 23.8784\n",
            "8176/10000 = 81.8%, SPS: 3.9, ELAP: 30:48, ETA: 7:50 - loss: 24.0027\n",
            "8177/10000 = 81.8%, SPS: 3.9, ELAP: 30:49, ETA: 7:50 - loss: 23.8792\n",
            "8178/10000 = 81.8%, SPS: 3.9, ELAP: 30:49, ETA: 7:49 - loss: 23.8398\n",
            "8179/10000 = 81.8%, SPS: 3.9, ELAP: 30:49, ETA: 7:49 - loss: 24.5304\n",
            "8180/10000 = 81.8%, SPS: 3.9, ELAP: 30:49, ETA: 7:49 - loss: 24.5225\n",
            "8181/10000 = 81.8%, SPS: 3.9, ELAP: 30:50, ETA: 7:49 - loss: 24.2795\n",
            "8182/10000 = 81.8%, SPS: 3.9, ELAP: 30:50, ETA: 7:48 - loss: 23.6529\n",
            "8183/10000 = 81.8%, SPS: 3.9, ELAP: 30:50, ETA: 7:48 - loss: 24.1921\n",
            "8184/10000 = 81.8%, SPS: 3.9, ELAP: 30:50, ETA: 7:48 - loss: 23.4821\n",
            "8185/10000 = 81.8%, SPS: 3.9, ELAP: 30:51, ETA: 7:47 - loss: 23.9527\n",
            "8186/10000 = 81.9%, SPS: 3.9, ELAP: 30:51, ETA: 7:47 - loss: 24.1268\n",
            "8187/10000 = 81.9%, SPS: 3.9, ELAP: 30:51, ETA: 7:47 - loss: 23.5088\n",
            "8188/10000 = 81.9%, SPS: 3.9, ELAP: 30:51, ETA: 7:47 - loss: 23.3448\n",
            "8189/10000 = 81.9%, SPS: 3.9, ELAP: 30:51, ETA: 7:46 - loss: 22.9937\n",
            "8190/10000 = 81.9%, SPS: 3.9, ELAP: 30:52, ETA: 7:46 - loss: 24.0888\n",
            "8191/10000 = 81.9%, SPS: 3.9, ELAP: 30:52, ETA: 7:46 - loss: 24.2008\n",
            "8192/10000 = 81.9%, SPS: 3.9, ELAP: 30:52, ETA: 7:46 - loss: 24.2411\n",
            "8193/10000 = 81.9%, SPS: 3.9, ELAP: 30:52, ETA: 7:45 - loss: 23.9773\n",
            "8194/10000 = 81.9%, SPS: 3.9, ELAP: 30:53, ETA: 7:45 - loss: 24.1713\n",
            "8195/10000 = 82.0%, SPS: 3.9, ELAP: 30:53, ETA: 7:45 - loss: 23.9333\n",
            "8196/10000 = 82.0%, SPS: 3.9, ELAP: 30:53, ETA: 7:45 - loss: 23.8166\n",
            "8197/10000 = 82.0%, SPS: 3.9, ELAP: 30:53, ETA: 7:44 - loss: 23.7290\n",
            "8198/10000 = 82.0%, SPS: 3.9, ELAP: 30:54, ETA: 7:44 - loss: 24.3356\n",
            "8199/10000 = 82.0%, SPS: 3.9, ELAP: 30:54, ETA: 7:44 - loss: 24.1256\n",
            "8200/10000 = 82.0%, SPS: 3.9, ELAP: 30:56, ETA: 7:44 - loss: 23.8114\n",
            "8201/10000 = 82.0%, SPS: 3.9, ELAP: 30:56, ETA: 7:44 - loss: 23.6169\n",
            "8202/10000 = 82.0%, SPS: 3.9, ELAP: 30:57, ETA: 7:44 - loss: 24.3424\n",
            "8203/10000 = 82.0%, SPS: 3.9, ELAP: 30:57, ETA: 7:43 - loss: 23.6799\n",
            "8204/10000 = 82.0%, SPS: 3.9, ELAP: 30:57, ETA: 7:43 - loss: 23.8606\n",
            "8205/10000 = 82.0%, SPS: 3.9, ELAP: 30:57, ETA: 7:43 - loss: 24.3427\n",
            "8206/10000 = 82.1%, SPS: 3.9, ELAP: 30:58, ETA: 7:42 - loss: 23.9910\n",
            "8207/10000 = 82.1%, SPS: 3.9, ELAP: 30:58, ETA: 7:42 - loss: 23.8128\n",
            "8208/10000 = 82.1%, SPS: 3.9, ELAP: 30:58, ETA: 7:42 - loss: 24.3089\n",
            "8209/10000 = 82.1%, SPS: 3.9, ELAP: 30:58, ETA: 7:42 - loss: 24.6456\n",
            "8210/10000 = 82.1%, SPS: 3.9, ELAP: 30:58, ETA: 7:41 - loss: 24.4388\n",
            "8211/10000 = 82.1%, SPS: 3.9, ELAP: 30:59, ETA: 7:41 - loss: 23.9295\n",
            "8212/10000 = 82.1%, SPS: 3.9, ELAP: 30:59, ETA: 7:41 - loss: 23.6525\n",
            "8213/10000 = 82.1%, SPS: 3.9, ELAP: 30:59, ETA: 7:41 - loss: 23.5183\n",
            "8214/10000 = 82.1%, SPS: 3.9, ELAP: 30:59, ETA: 7:40 - loss: 24.1698\n",
            "8215/10000 = 82.2%, SPS: 3.9, ELAP: 31:00, ETA: 7:40 - loss: 24.1272\n",
            "8216/10000 = 82.2%, SPS: 3.9, ELAP: 31:00, ETA: 7:40 - loss: 23.5528\n",
            "8217/10000 = 82.2%, SPS: 3.9, ELAP: 31:00, ETA: 7:40 - loss: 23.7010\n",
            "8218/10000 = 82.2%, SPS: 3.9, ELAP: 31:00, ETA: 7:39 - loss: 24.3392\n",
            "8219/10000 = 82.2%, SPS: 3.9, ELAP: 31:01, ETA: 7:39 - loss: 23.9982\n",
            "8220/10000 = 82.2%, SPS: 3.9, ELAP: 31:01, ETA: 7:39 - loss: 23.4415\n",
            "8221/10000 = 82.2%, SPS: 3.9, ELAP: 31:01, ETA: 7:39 - loss: 24.1684\n",
            "8222/10000 = 82.2%, SPS: 3.9, ELAP: 31:01, ETA: 7:38 - loss: 23.8252\n",
            "8223/10000 = 82.2%, SPS: 3.9, ELAP: 31:02, ETA: 7:38 - loss: 24.2635\n",
            "8224/10000 = 82.2%, SPS: 3.9, ELAP: 31:02, ETA: 7:38 - loss: 24.3055\n",
            "8225/10000 = 82.2%, SPS: 3.9, ELAP: 31:02, ETA: 7:37 - loss: 24.2373\n",
            "8226/10000 = 82.3%, SPS: 3.9, ELAP: 31:02, ETA: 7:37 - loss: 23.9386\n",
            "8227/10000 = 82.3%, SPS: 3.9, ELAP: 31:03, ETA: 7:37 - loss: 23.9422\n",
            "8228/10000 = 82.3%, SPS: 3.9, ELAP: 31:03, ETA: 7:37 - loss: 23.5790\n",
            "8229/10000 = 82.3%, SPS: 3.9, ELAP: 31:03, ETA: 7:36 - loss: 24.1433\n",
            "8230/10000 = 82.3%, SPS: 3.9, ELAP: 31:03, ETA: 7:36 - loss: 24.1728\n",
            "8231/10000 = 82.3%, SPS: 3.9, ELAP: 31:03, ETA: 7:36 - loss: 24.1564\n",
            "8232/10000 = 82.3%, SPS: 3.9, ELAP: 31:04, ETA: 7:36 - loss: 23.8796\n",
            "8233/10000 = 82.3%, SPS: 3.9, ELAP: 31:04, ETA: 7:35 - loss: 24.0150\n",
            "8234/10000 = 82.3%, SPS: 3.9, ELAP: 31:04, ETA: 7:35 - loss: 24.3112\n",
            "8235/10000 = 82.3%, SPS: 3.9, ELAP: 31:04, ETA: 7:35 - loss: 24.4147\n",
            "8236/10000 = 82.4%, SPS: 3.9, ELAP: 31:05, ETA: 7:35 - loss: 24.4626\n",
            "8237/10000 = 82.4%, SPS: 3.9, ELAP: 31:05, ETA: 7:34 - loss: 24.0336\n",
            "8238/10000 = 82.4%, SPS: 3.9, ELAP: 31:05, ETA: 7:34 - loss: 24.3149\n",
            "8239/10000 = 82.4%, SPS: 3.9, ELAP: 31:05, ETA: 7:34 - loss: 23.7076\n",
            "8240/10000 = 82.4%, SPS: 3.9, ELAP: 31:06, ETA: 7:33 - loss: 24.2400\n",
            "8241/10000 = 82.4%, SPS: 3.9, ELAP: 31:06, ETA: 7:33 - loss: 23.8762\n",
            "8242/10000 = 82.4%, SPS: 3.9, ELAP: 31:06, ETA: 7:33 - loss: 24.2024\n",
            "8243/10000 = 82.4%, SPS: 3.9, ELAP: 31:06, ETA: 7:33 - loss: 23.8755\n",
            "8244/10000 = 82.4%, SPS: 3.9, ELAP: 31:06, ETA: 7:32 - loss: 24.1095\n",
            "8245/10000 = 82.5%, SPS: 3.9, ELAP: 31:07, ETA: 7:32 - loss: 24.2034\n",
            "8246/10000 = 82.5%, SPS: 3.9, ELAP: 31:07, ETA: 7:32 - loss: 24.6434\n",
            "8247/10000 = 82.5%, SPS: 3.9, ELAP: 31:07, ETA: 7:32 - loss: 23.8320\n",
            "8248/10000 = 82.5%, SPS: 3.9, ELAP: 31:07, ETA: 7:31 - loss: 24.4700\n",
            "8249/10000 = 82.5%, SPS: 3.9, ELAP: 31:08, ETA: 7:31 - loss: 24.0300\n",
            "8250/10000 = 82.5%, SPS: 3.9, ELAP: 31:08, ETA: 7:31 - loss: 23.9804\n",
            "8251/10000 = 82.5%, SPS: 3.9, ELAP: 31:08, ETA: 7:31 - loss: 24.2106\n",
            "8252/10000 = 82.5%, SPS: 3.9, ELAP: 31:08, ETA: 7:30 - loss: 23.6049\n",
            "8253/10000 = 82.5%, SPS: 3.9, ELAP: 31:08, ETA: 7:30 - loss: 23.8620\n",
            "8254/10000 = 82.5%, SPS: 3.9, ELAP: 31:09, ETA: 7:30 - loss: 24.2925\n",
            "8255/10000 = 82.5%, SPS: 3.9, ELAP: 31:09, ETA: 7:30 - loss: 24.1733\n",
            "8256/10000 = 82.6%, SPS: 3.9, ELAP: 31:09, ETA: 7:29 - loss: 24.4014\n",
            "8257/10000 = 82.6%, SPS: 3.9, ELAP: 31:09, ETA: 7:29 - loss: 23.9436\n",
            "8258/10000 = 82.6%, SPS: 3.9, ELAP: 31:10, ETA: 7:29 - loss: 24.2571\n",
            "8259/10000 = 82.6%, SPS: 3.9, ELAP: 31:10, ETA: 7:28 - loss: 24.1520\n",
            "8260/10000 = 82.6%, SPS: 3.9, ELAP: 31:10, ETA: 7:28 - loss: 24.6174\n",
            "8261/10000 = 82.6%, SPS: 3.9, ELAP: 31:10, ETA: 7:28 - loss: 24.4665\n",
            "8262/10000 = 82.6%, SPS: 3.9, ELAP: 31:11, ETA: 7:28 - loss: 24.9084\n",
            "8263/10000 = 82.6%, SPS: 3.9, ELAP: 31:11, ETA: 7:27 - loss: 23.9722\n",
            "8264/10000 = 82.6%, SPS: 3.9, ELAP: 31:11, ETA: 7:27 - loss: 24.0746\n",
            "8265/10000 = 82.7%, SPS: 3.9, ELAP: 31:11, ETA: 7:27 - loss: 23.5789\n",
            "8266/10000 = 82.7%, SPS: 3.9, ELAP: 31:11, ETA: 7:27 - loss: 23.7559\n",
            "8267/10000 = 82.7%, SPS: 3.9, ELAP: 31:12, ETA: 7:26 - loss: 23.7339\n",
            "8268/10000 = 82.7%, SPS: 3.9, ELAP: 31:12, ETA: 7:26 - loss: 23.7337\n",
            "8269/10000 = 82.7%, SPS: 3.9, ELAP: 31:12, ETA: 7:26 - loss: 23.6587\n",
            "8270/10000 = 82.7%, SPS: 3.9, ELAP: 31:12, ETA: 7:26 - loss: 23.8022\n",
            "8271/10000 = 82.7%, SPS: 3.9, ELAP: 31:13, ETA: 7:25 - loss: 24.0878\n",
            "8272/10000 = 82.7%, SPS: 3.9, ELAP: 31:13, ETA: 7:25 - loss: 24.0198\n",
            "8273/10000 = 82.7%, SPS: 3.9, ELAP: 31:13, ETA: 7:25 - loss: 23.9846\n",
            "8274/10000 = 82.7%, SPS: 3.9, ELAP: 31:13, ETA: 7:24 - loss: 23.6166\n",
            "8275/10000 = 82.8%, SPS: 3.9, ELAP: 31:13, ETA: 7:24 - loss: 23.9124\n",
            "8276/10000 = 82.8%, SPS: 3.9, ELAP: 31:14, ETA: 7:24 - loss: 24.0631\n",
            "8277/10000 = 82.8%, SPS: 3.9, ELAP: 31:14, ETA: 7:24 - loss: 23.4576\n",
            "8278/10000 = 82.8%, SPS: 3.9, ELAP: 31:14, ETA: 7:23 - loss: 24.3577\n",
            "8279/10000 = 82.8%, SPS: 3.9, ELAP: 31:14, ETA: 7:23 - loss: 23.4499\n",
            "8280/10000 = 82.8%, SPS: 3.9, ELAP: 31:15, ETA: 7:23 - loss: 23.8007\n",
            "8281/10000 = 82.8%, SPS: 3.9, ELAP: 31:15, ETA: 7:23 - loss: 24.1505\n",
            "8282/10000 = 82.8%, SPS: 3.9, ELAP: 31:15, ETA: 7:22 - loss: 23.7381\n",
            "8283/10000 = 82.8%, SPS: 3.9, ELAP: 31:15, ETA: 7:22 - loss: 24.1923\n",
            "8284/10000 = 82.8%, SPS: 3.9, ELAP: 31:16, ETA: 7:22 - loss: 23.6438\n",
            "8285/10000 = 82.8%, SPS: 3.9, ELAP: 31:16, ETA: 7:22 - loss: 23.7670\n",
            "8286/10000 = 82.9%, SPS: 3.9, ELAP: 31:16, ETA: 7:21 - loss: 24.2211\n",
            "8287/10000 = 82.9%, SPS: 3.9, ELAP: 31:16, ETA: 7:21 - loss: 24.3108\n",
            "8288/10000 = 82.9%, SPS: 3.9, ELAP: 31:16, ETA: 7:21 - loss: 23.9115\n",
            "8289/10000 = 82.9%, SPS: 3.9, ELAP: 31:17, ETA: 7:21 - loss: 23.9647\n",
            "8290/10000 = 82.9%, SPS: 3.9, ELAP: 31:17, ETA: 7:20 - loss: 23.4924\n",
            "8291/10000 = 82.9%, SPS: 3.9, ELAP: 31:17, ETA: 7:20 - loss: 23.9199\n",
            "8292/10000 = 82.9%, SPS: 3.9, ELAP: 31:17, ETA: 7:20 - loss: 24.5679\n",
            "8293/10000 = 82.9%, SPS: 3.9, ELAP: 31:18, ETA: 7:19 - loss: 24.3804\n",
            "8294/10000 = 82.9%, SPS: 3.9, ELAP: 31:18, ETA: 7:19 - loss: 23.3616\n",
            "8295/10000 = 83.0%, SPS: 3.9, ELAP: 31:18, ETA: 7:19 - loss: 23.9327\n",
            "8296/10000 = 83.0%, SPS: 3.9, ELAP: 31:18, ETA: 7:19 - loss: 23.8421\n",
            "8297/10000 = 83.0%, SPS: 3.9, ELAP: 31:19, ETA: 7:18 - loss: 23.3600\n",
            "8298/10000 = 83.0%, SPS: 3.9, ELAP: 31:19, ETA: 7:18 - loss: 23.6389\n",
            "8299/10000 = 83.0%, SPS: 3.9, ELAP: 31:19, ETA: 7:18 - loss: 24.3396\n",
            "8300/10000 = 83.0%, SPS: 3.9, ELAP: 31:22, ETA: 7:18 - loss: 24.2953\n",
            "8301/10000 = 83.0%, SPS: 3.9, ELAP: 31:22, ETA: 7:18 - loss: 23.5843\n",
            "8302/10000 = 83.0%, SPS: 3.9, ELAP: 31:22, ETA: 7:18 - loss: 23.3999\n",
            "8303/10000 = 83.0%, SPS: 3.9, ELAP: 31:22, ETA: 7:17 - loss: 23.6366\n",
            "8304/10000 = 83.0%, SPS: 3.9, ELAP: 31:23, ETA: 7:17 - loss: 23.4830\n",
            "8305/10000 = 83.0%, SPS: 3.9, ELAP: 31:23, ETA: 7:17 - loss: 23.7550\n",
            "8306/10000 = 83.1%, SPS: 3.9, ELAP: 31:23, ETA: 7:17 - loss: 24.0625\n",
            "8307/10000 = 83.1%, SPS: 3.9, ELAP: 31:23, ETA: 7:16 - loss: 24.4046\n",
            "8308/10000 = 83.1%, SPS: 3.9, ELAP: 31:24, ETA: 7:16 - loss: 23.5811\n",
            "8309/10000 = 83.1%, SPS: 3.9, ELAP: 31:24, ETA: 7:16 - loss: 23.9663\n",
            "8310/10000 = 83.1%, SPS: 3.9, ELAP: 31:24, ETA: 7:16 - loss: 24.2583\n",
            "8311/10000 = 83.1%, SPS: 3.9, ELAP: 31:24, ETA: 7:15 - loss: 23.8590\n",
            "8312/10000 = 83.1%, SPS: 3.9, ELAP: 31:25, ETA: 7:15 - loss: 23.7498\n",
            "8313/10000 = 83.1%, SPS: 3.9, ELAP: 31:25, ETA: 7:15 - loss: 24.0838\n",
            "8314/10000 = 83.1%, SPS: 3.9, ELAP: 31:25, ETA: 7:15 - loss: 24.1500\n",
            "8315/10000 = 83.2%, SPS: 3.9, ELAP: 31:25, ETA: 7:14 - loss: 24.3617\n",
            "8316/10000 = 83.2%, SPS: 3.9, ELAP: 31:25, ETA: 7:14 - loss: 23.5720\n",
            "8317/10000 = 83.2%, SPS: 3.9, ELAP: 31:26, ETA: 7:14 - loss: 24.4609\n",
            "8318/10000 = 83.2%, SPS: 3.9, ELAP: 31:26, ETA: 7:13 - loss: 23.3139\n",
            "8319/10000 = 83.2%, SPS: 3.9, ELAP: 31:26, ETA: 7:13 - loss: 23.9810\n",
            "8320/10000 = 83.2%, SPS: 3.9, ELAP: 31:26, ETA: 7:13 - loss: 23.9967\n",
            "8321/10000 = 83.2%, SPS: 3.9, ELAP: 31:27, ETA: 7:13 - loss: 24.2961\n",
            "8322/10000 = 83.2%, SPS: 3.9, ELAP: 31:27, ETA: 7:12 - loss: 24.0505\n",
            "8323/10000 = 83.2%, SPS: 3.9, ELAP: 31:27, ETA: 7:12 - loss: 24.1427\n",
            "8324/10000 = 83.2%, SPS: 3.9, ELAP: 31:27, ETA: 7:12 - loss: 23.9625\n",
            "8325/10000 = 83.2%, SPS: 3.9, ELAP: 31:28, ETA: 7:12 - loss: 24.1637\n",
            "8326/10000 = 83.3%, SPS: 3.9, ELAP: 31:28, ETA: 7:11 - loss: 23.9997\n",
            "8327/10000 = 83.3%, SPS: 3.9, ELAP: 31:28, ETA: 7:11 - loss: 23.9002\n",
            "8328/10000 = 83.3%, SPS: 3.9, ELAP: 31:28, ETA: 7:11 - loss: 24.1074\n",
            "8329/10000 = 83.3%, SPS: 3.9, ELAP: 31:29, ETA: 7:11 - loss: 24.3424\n",
            "8330/10000 = 83.3%, SPS: 3.9, ELAP: 31:29, ETA: 7:10 - loss: 23.7809\n",
            "8331/10000 = 83.3%, SPS: 3.9, ELAP: 31:29, ETA: 7:10 - loss: 24.3032\n",
            "8332/10000 = 83.3%, SPS: 3.9, ELAP: 31:29, ETA: 7:10 - loss: 24.6733\n",
            "8333/10000 = 83.3%, SPS: 3.9, ELAP: 31:30, ETA: 7:10 - loss: 24.3384\n",
            "8334/10000 = 83.3%, SPS: 3.9, ELAP: 31:30, ETA: 7:09 - loss: 24.7610\n",
            "8335/10000 = 83.3%, SPS: 3.9, ELAP: 31:30, ETA: 7:09 - loss: 23.3977\n",
            "8336/10000 = 83.4%, SPS: 3.9, ELAP: 31:30, ETA: 7:09 - loss: 24.1214\n",
            "8337/10000 = 83.4%, SPS: 3.9, ELAP: 31:31, ETA: 7:09 - loss: 24.0280\n",
            "8338/10000 = 83.4%, SPS: 3.9, ELAP: 31:31, ETA: 7:08 - loss: 23.8347\n",
            "8339/10000 = 83.4%, SPS: 3.9, ELAP: 31:31, ETA: 7:08 - loss: 23.9228\n",
            "8340/10000 = 83.4%, SPS: 3.9, ELAP: 31:31, ETA: 7:08 - loss: 23.7869\n",
            "8341/10000 = 83.4%, SPS: 3.9, ELAP: 31:31, ETA: 7:07 - loss: 24.4346\n",
            "8342/10000 = 83.4%, SPS: 3.9, ELAP: 31:32, ETA: 7:07 - loss: 23.5599\n",
            "8343/10000 = 83.4%, SPS: 3.9, ELAP: 31:32, ETA: 7:07 - loss: 24.1766\n",
            "8344/10000 = 83.4%, SPS: 3.9, ELAP: 31:32, ETA: 7:07 - loss: 23.5048\n",
            "8345/10000 = 83.5%, SPS: 3.9, ELAP: 31:32, ETA: 7:06 - loss: 23.7451\n",
            "8346/10000 = 83.5%, SPS: 3.9, ELAP: 31:33, ETA: 7:06 - loss: 23.5213\n",
            "8347/10000 = 83.5%, SPS: 3.9, ELAP: 31:33, ETA: 7:06 - loss: 23.6479\n",
            "8348/10000 = 83.5%, SPS: 3.9, ELAP: 31:33, ETA: 7:06 - loss: 23.6487\n",
            "8349/10000 = 83.5%, SPS: 3.9, ELAP: 31:33, ETA: 7:05 - loss: 24.4948\n",
            "8350/10000 = 83.5%, SPS: 3.9, ELAP: 31:34, ETA: 7:05 - loss: 24.1194\n",
            "8351/10000 = 83.5%, SPS: 3.9, ELAP: 31:34, ETA: 7:05 - loss: 23.5083\n",
            "8352/10000 = 83.5%, SPS: 3.9, ELAP: 31:34, ETA: 7:05 - loss: 23.5963\n",
            "8353/10000 = 83.5%, SPS: 3.9, ELAP: 31:34, ETA: 7:04 - loss: 24.0776\n",
            "8354/10000 = 83.5%, SPS: 3.9, ELAP: 31:34, ETA: 7:04 - loss: 24.1831\n",
            "8355/10000 = 83.5%, SPS: 3.9, ELAP: 31:35, ETA: 7:04 - loss: 23.2025\n",
            "8356/10000 = 83.6%, SPS: 3.9, ELAP: 31:35, ETA: 7:03 - loss: 24.3799\n",
            "8357/10000 = 83.6%, SPS: 3.9, ELAP: 31:35, ETA: 7:03 - loss: 24.2455\n",
            "8358/10000 = 83.6%, SPS: 3.9, ELAP: 31:35, ETA: 7:03 - loss: 24.2427\n",
            "8359/10000 = 83.6%, SPS: 3.9, ELAP: 31:36, ETA: 7:03 - loss: 23.7806\n",
            "8360/10000 = 83.6%, SPS: 3.9, ELAP: 31:36, ETA: 7:02 - loss: 23.0393\n",
            "8361/10000 = 83.6%, SPS: 3.9, ELAP: 31:36, ETA: 7:02 - loss: 23.7854\n",
            "8362/10000 = 83.6%, SPS: 3.9, ELAP: 31:36, ETA: 7:02 - loss: 23.6646\n",
            "8363/10000 = 83.6%, SPS: 3.9, ELAP: 31:37, ETA: 7:02 - loss: 24.0104\n",
            "8364/10000 = 83.6%, SPS: 3.9, ELAP: 31:37, ETA: 7:01 - loss: 23.9783\n",
            "8365/10000 = 83.7%, SPS: 3.9, ELAP: 31:37, ETA: 7:01 - loss: 24.3129\n",
            "8366/10000 = 83.7%, SPS: 3.9, ELAP: 31:37, ETA: 7:01 - loss: 23.9894\n",
            "8367/10000 = 83.7%, SPS: 3.9, ELAP: 31:37, ETA: 7:01 - loss: 23.8752\n",
            "8368/10000 = 83.7%, SPS: 3.9, ELAP: 31:38, ETA: 7:00 - loss: 23.8136\n",
            "8369/10000 = 83.7%, SPS: 3.9, ELAP: 31:38, ETA: 7:00 - loss: 24.5429\n",
            "8370/10000 = 83.7%, SPS: 3.9, ELAP: 31:38, ETA: 7:00 - loss: 23.3283\n",
            "8371/10000 = 83.7%, SPS: 3.9, ELAP: 31:38, ETA: 7:00 - loss: 23.9986\n",
            "8372/10000 = 83.7%, SPS: 3.9, ELAP: 31:39, ETA: 6:59 - loss: 24.0210\n",
            "8373/10000 = 83.7%, SPS: 3.9, ELAP: 31:39, ETA: 6:59 - loss: 23.5804\n",
            "8374/10000 = 83.7%, SPS: 3.9, ELAP: 31:39, ETA: 6:59 - loss: 23.8627\n",
            "8375/10000 = 83.8%, SPS: 3.9, ELAP: 31:39, ETA: 6:58 - loss: 23.7867\n",
            "8376/10000 = 83.8%, SPS: 3.9, ELAP: 31:39, ETA: 6:58 - loss: 24.0945\n",
            "8377/10000 = 83.8%, SPS: 3.9, ELAP: 31:40, ETA: 6:58 - loss: 24.3035\n",
            "8378/10000 = 83.8%, SPS: 3.9, ELAP: 31:40, ETA: 6:58 - loss: 24.1151\n",
            "8379/10000 = 83.8%, SPS: 3.9, ELAP: 31:40, ETA: 6:57 - loss: 24.3209\n",
            "8380/10000 = 83.8%, SPS: 3.9, ELAP: 31:40, ETA: 6:57 - loss: 24.6795\n",
            "8381/10000 = 83.8%, SPS: 3.9, ELAP: 31:41, ETA: 6:57 - loss: 24.3662\n",
            "8382/10000 = 83.8%, SPS: 3.9, ELAP: 31:41, ETA: 6:57 - loss: 23.6968\n",
            "8383/10000 = 83.8%, SPS: 3.9, ELAP: 31:41, ETA: 6:56 - loss: 23.5996\n",
            "8384/10000 = 83.8%, SPS: 3.9, ELAP: 31:41, ETA: 6:56 - loss: 24.6457\n",
            "8385/10000 = 83.8%, SPS: 3.9, ELAP: 31:42, ETA: 6:56 - loss: 24.4250\n",
            "8386/10000 = 83.9%, SPS: 3.9, ELAP: 31:42, ETA: 6:56 - loss: 23.6064\n",
            "8387/10000 = 83.9%, SPS: 3.9, ELAP: 31:42, ETA: 6:55 - loss: 24.5211\n",
            "8388/10000 = 83.9%, SPS: 3.9, ELAP: 31:42, ETA: 6:55 - loss: 23.6200\n",
            "8389/10000 = 83.9%, SPS: 3.9, ELAP: 31:42, ETA: 6:55 - loss: 24.5461\n",
            "8390/10000 = 83.9%, SPS: 3.9, ELAP: 31:43, ETA: 6:55 - loss: 24.0116\n",
            "8391/10000 = 83.9%, SPS: 3.9, ELAP: 31:43, ETA: 6:54 - loss: 24.4156\n",
            "8392/10000 = 83.9%, SPS: 3.9, ELAP: 31:43, ETA: 6:54 - loss: 23.9018\n",
            "8393/10000 = 83.9%, SPS: 3.9, ELAP: 31:43, ETA: 6:54 - loss: 24.4576\n",
            "8394/10000 = 83.9%, SPS: 3.9, ELAP: 31:44, ETA: 6:53 - loss: 23.9701\n",
            "8395/10000 = 84.0%, SPS: 3.9, ELAP: 31:44, ETA: 6:53 - loss: 23.8906\n",
            "8396/10000 = 84.0%, SPS: 3.9, ELAP: 31:44, ETA: 6:53 - loss: 23.2610\n",
            "8397/10000 = 84.0%, SPS: 3.9, ELAP: 31:44, ETA: 6:53 - loss: 24.3716\n",
            "8398/10000 = 84.0%, SPS: 3.9, ELAP: 31:44, ETA: 6:52 - loss: 24.2124\n",
            "8399/10000 = 84.0%, SPS: 3.9, ELAP: 31:45, ETA: 6:52 - loss: 24.4449\n",
            "8400/10000 = 84.0%, SPS: 3.9, ELAP: 31:47, ETA: 6:52 - loss: 22.9972\n",
            "8401/10000 = 84.0%, SPS: 3.9, ELAP: 31:47, ETA: 6:52 - loss: 24.5891\n",
            "8402/10000 = 84.0%, SPS: 3.9, ELAP: 31:48, ETA: 6:52 - loss: 24.5105\n",
            "8403/10000 = 84.0%, SPS: 3.9, ELAP: 31:48, ETA: 6:52 - loss: 23.4823\n",
            "8404/10000 = 84.0%, SPS: 3.9, ELAP: 31:48, ETA: 6:51 - loss: 23.0995\n",
            "8405/10000 = 84.0%, SPS: 3.9, ELAP: 31:48, ETA: 6:51 - loss: 23.5420\n",
            "8406/10000 = 84.1%, SPS: 3.9, ELAP: 31:48, ETA: 6:51 - loss: 23.9874\n",
            "8407/10000 = 84.1%, SPS: 3.9, ELAP: 31:49, ETA: 6:50 - loss: 24.6477\n",
            "8408/10000 = 84.1%, SPS: 3.9, ELAP: 31:49, ETA: 6:50 - loss: 24.2339\n",
            "8409/10000 = 84.1%, SPS: 3.9, ELAP: 31:49, ETA: 6:50 - loss: 24.0093\n",
            "8410/10000 = 84.1%, SPS: 3.9, ELAP: 31:49, ETA: 6:50 - loss: 23.0432\n",
            "8411/10000 = 84.1%, SPS: 3.9, ELAP: 31:50, ETA: 6:49 - loss: 24.2761\n",
            "8412/10000 = 84.1%, SPS: 3.9, ELAP: 31:50, ETA: 6:49 - loss: 23.7154\n",
            "8413/10000 = 84.1%, SPS: 3.9, ELAP: 31:50, ETA: 6:49 - loss: 24.6062\n",
            "8414/10000 = 84.1%, SPS: 3.9, ELAP: 31:50, ETA: 6:49 - loss: 23.6897\n",
            "8415/10000 = 84.2%, SPS: 3.9, ELAP: 31:51, ETA: 6:48 - loss: 23.8981\n",
            "8416/10000 = 84.2%, SPS: 3.9, ELAP: 31:51, ETA: 6:48 - loss: 24.1351\n",
            "8417/10000 = 84.2%, SPS: 3.9, ELAP: 31:51, ETA: 6:48 - loss: 24.2044\n",
            "8418/10000 = 84.2%, SPS: 3.9, ELAP: 31:51, ETA: 6:48 - loss: 24.4933\n",
            "8419/10000 = 84.2%, SPS: 3.9, ELAP: 31:51, ETA: 6:47 - loss: 23.7706\n",
            "8420/10000 = 84.2%, SPS: 3.9, ELAP: 31:52, ETA: 6:47 - loss: 24.2447\n",
            "8421/10000 = 84.2%, SPS: 3.9, ELAP: 31:52, ETA: 6:47 - loss: 24.5836\n",
            "8422/10000 = 84.2%, SPS: 3.9, ELAP: 31:52, ETA: 6:47 - loss: 23.8897\n",
            "8423/10000 = 84.2%, SPS: 3.9, ELAP: 31:52, ETA: 6:46 - loss: 23.7101\n",
            "8424/10000 = 84.2%, SPS: 3.9, ELAP: 31:53, ETA: 6:46 - loss: 24.7228\n",
            "8425/10000 = 84.2%, SPS: 3.9, ELAP: 31:53, ETA: 6:46 - loss: 24.2615\n",
            "8426/10000 = 84.3%, SPS: 3.9, ELAP: 31:53, ETA: 6:45 - loss: 24.2866\n",
            "8427/10000 = 84.3%, SPS: 3.9, ELAP: 31:53, ETA: 6:45 - loss: 24.1731\n",
            "8428/10000 = 84.3%, SPS: 3.9, ELAP: 31:54, ETA: 6:45 - loss: 23.6995\n",
            "8429/10000 = 84.3%, SPS: 3.9, ELAP: 31:54, ETA: 6:45 - loss: 23.3603\n",
            "8430/10000 = 84.3%, SPS: 3.9, ELAP: 31:54, ETA: 6:44 - loss: 23.7728\n",
            "8431/10000 = 84.3%, SPS: 3.9, ELAP: 31:54, ETA: 6:44 - loss: 23.6652\n",
            "8432/10000 = 84.3%, SPS: 3.9, ELAP: 31:54, ETA: 6:44 - loss: 24.6660\n",
            "8433/10000 = 84.3%, SPS: 3.9, ELAP: 31:55, ETA: 6:44 - loss: 24.3292\n",
            "8434/10000 = 84.3%, SPS: 3.9, ELAP: 31:55, ETA: 6:43 - loss: 24.2299\n",
            "8435/10000 = 84.3%, SPS: 3.9, ELAP: 31:55, ETA: 6:43 - loss: 23.3741\n",
            "8436/10000 = 84.4%, SPS: 3.9, ELAP: 31:55, ETA: 6:43 - loss: 24.1026\n",
            "8437/10000 = 84.4%, SPS: 3.9, ELAP: 31:56, ETA: 6:43 - loss: 24.7674\n",
            "8438/10000 = 84.4%, SPS: 3.9, ELAP: 31:56, ETA: 6:42 - loss: 24.1268\n",
            "8439/10000 = 84.4%, SPS: 3.9, ELAP: 31:56, ETA: 6:42 - loss: 24.4440\n",
            "8440/10000 = 84.4%, SPS: 3.9, ELAP: 31:56, ETA: 6:42 - loss: 23.7774\n",
            "8441/10000 = 84.4%, SPS: 3.9, ELAP: 31:57, ETA: 6:42 - loss: 24.4883\n",
            "8442/10000 = 84.4%, SPS: 3.9, ELAP: 31:57, ETA: 6:41 - loss: 23.9878\n",
            "8443/10000 = 84.4%, SPS: 3.9, ELAP: 31:57, ETA: 6:41 - loss: 23.6712\n",
            "8444/10000 = 84.4%, SPS: 3.9, ELAP: 31:57, ETA: 6:41 - loss: 23.7377\n",
            "8445/10000 = 84.5%, SPS: 3.9, ELAP: 31:58, ETA: 6:41 - loss: 23.5768\n",
            "8446/10000 = 84.5%, SPS: 3.9, ELAP: 31:58, ETA: 6:40 - loss: 24.3045\n",
            "8447/10000 = 84.5%, SPS: 3.9, ELAP: 31:58, ETA: 6:40 - loss: 23.9800\n",
            "8448/10000 = 84.5%, SPS: 3.9, ELAP: 31:58, ETA: 6:40 - loss: 23.8224\n",
            "8449/10000 = 84.5%, SPS: 3.9, ELAP: 31:59, ETA: 6:39 - loss: 23.8181\n",
            "8450/10000 = 84.5%, SPS: 3.9, ELAP: 31:59, ETA: 6:39 - loss: 23.3469\n",
            "8451/10000 = 84.5%, SPS: 3.9, ELAP: 31:59, ETA: 6:39 - loss: 23.4219\n",
            "8452/10000 = 84.5%, SPS: 3.9, ELAP: 31:59, ETA: 6:39 - loss: 24.1671\n",
            "8453/10000 = 84.5%, SPS: 3.9, ELAP: 32:00, ETA: 6:38 - loss: 23.7617\n",
            "8454/10000 = 84.5%, SPS: 3.9, ELAP: 32:00, ETA: 6:38 - loss: 23.7240\n",
            "8455/10000 = 84.5%, SPS: 3.9, ELAP: 32:00, ETA: 6:38 - loss: 24.4309\n",
            "8456/10000 = 84.6%, SPS: 3.9, ELAP: 32:00, ETA: 6:38 - loss: 23.9608\n",
            "8457/10000 = 84.6%, SPS: 3.9, ELAP: 32:00, ETA: 6:37 - loss: 23.6723\n",
            "8458/10000 = 84.6%, SPS: 3.9, ELAP: 32:01, ETA: 6:37 - loss: 24.1282\n",
            "8459/10000 = 84.6%, SPS: 3.9, ELAP: 32:01, ETA: 6:37 - loss: 23.1569\n",
            "8460/10000 = 84.6%, SPS: 3.9, ELAP: 32:01, ETA: 6:37 - loss: 23.7336\n",
            "8461/10000 = 84.6%, SPS: 3.9, ELAP: 32:01, ETA: 6:36 - loss: 22.9162\n",
            "8462/10000 = 84.6%, SPS: 3.9, ELAP: 32:02, ETA: 6:36 - loss: 24.3799\n",
            "8463/10000 = 84.6%, SPS: 3.9, ELAP: 32:02, ETA: 6:36 - loss: 24.0633\n",
            "8464/10000 = 84.6%, SPS: 3.9, ELAP: 32:02, ETA: 6:36 - loss: 23.8733\n",
            "8465/10000 = 84.7%, SPS: 3.9, ELAP: 32:02, ETA: 6:35 - loss: 23.4704\n",
            "8466/10000 = 84.7%, SPS: 3.9, ELAP: 32:02, ETA: 6:35 - loss: 23.3114\n",
            "8467/10000 = 84.7%, SPS: 3.9, ELAP: 32:03, ETA: 6:35 - loss: 23.5632\n",
            "8468/10000 = 84.7%, SPS: 3.9, ELAP: 32:03, ETA: 6:34 - loss: 23.5804\n",
            "8469/10000 = 84.7%, SPS: 3.9, ELAP: 32:03, ETA: 6:34 - loss: 24.5019\n",
            "8470/10000 = 84.7%, SPS: 3.9, ELAP: 32:03, ETA: 6:34 - loss: 24.0645\n",
            "8471/10000 = 84.7%, SPS: 3.9, ELAP: 32:04, ETA: 6:34 - loss: 23.5963\n",
            "8472/10000 = 84.7%, SPS: 3.9, ELAP: 32:04, ETA: 6:33 - loss: 23.7551\n",
            "8473/10000 = 84.7%, SPS: 3.9, ELAP: 32:04, ETA: 6:33 - loss: 24.1078\n",
            "8474/10000 = 84.7%, SPS: 3.9, ELAP: 32:04, ETA: 6:33 - loss: 24.4534\n",
            "8475/10000 = 84.8%, SPS: 3.9, ELAP: 32:05, ETA: 6:33 - loss: 23.7742\n",
            "8476/10000 = 84.8%, SPS: 3.9, ELAP: 32:05, ETA: 6:32 - loss: 23.6045\n",
            "8477/10000 = 84.8%, SPS: 3.9, ELAP: 32:05, ETA: 6:32 - loss: 24.2195\n",
            "8478/10000 = 84.8%, SPS: 3.9, ELAP: 32:05, ETA: 6:32 - loss: 24.2233\n",
            "8479/10000 = 84.8%, SPS: 3.9, ELAP: 32:05, ETA: 6:32 - loss: 24.0450\n",
            "8480/10000 = 84.8%, SPS: 3.9, ELAP: 32:06, ETA: 6:31 - loss: 24.1710\n",
            "8481/10000 = 84.8%, SPS: 3.9, ELAP: 32:06, ETA: 6:31 - loss: 24.4993\n",
            "8482/10000 = 84.8%, SPS: 3.9, ELAP: 32:06, ETA: 6:31 - loss: 24.5474\n",
            "8483/10000 = 84.8%, SPS: 3.9, ELAP: 32:06, ETA: 6:31 - loss: 23.9594\n",
            "8484/10000 = 84.8%, SPS: 3.9, ELAP: 32:07, ETA: 6:30 - loss: 24.3540\n",
            "8485/10000 = 84.8%, SPS: 3.9, ELAP: 32:07, ETA: 6:30 - loss: 24.2646\n",
            "8486/10000 = 84.9%, SPS: 3.9, ELAP: 32:07, ETA: 6:30 - loss: 24.1762\n",
            "8487/10000 = 84.9%, SPS: 3.9, ELAP: 32:07, ETA: 6:29 - loss: 24.3928\n",
            "8488/10000 = 84.9%, SPS: 3.9, ELAP: 32:08, ETA: 6:29 - loss: 23.7673\n",
            "8489/10000 = 84.9%, SPS: 3.9, ELAP: 32:08, ETA: 6:29 - loss: 23.8139\n",
            "8490/10000 = 84.9%, SPS: 3.9, ELAP: 32:08, ETA: 6:29 - loss: 23.5545\n",
            "8491/10000 = 84.9%, SPS: 3.9, ELAP: 32:08, ETA: 6:28 - loss: 23.6584\n",
            "8492/10000 = 84.9%, SPS: 3.9, ELAP: 32:08, ETA: 6:28 - loss: 24.1116\n",
            "8493/10000 = 84.9%, SPS: 3.9, ELAP: 32:09, ETA: 6:28 - loss: 24.0424\n",
            "8494/10000 = 84.9%, SPS: 3.9, ELAP: 32:09, ETA: 6:28 - loss: 24.4473\n",
            "8495/10000 = 85.0%, SPS: 3.9, ELAP: 32:09, ETA: 6:27 - loss: 23.6921\n",
            "8496/10000 = 85.0%, SPS: 3.9, ELAP: 32:09, ETA: 6:27 - loss: 23.8463\n",
            "8497/10000 = 85.0%, SPS: 3.9, ELAP: 32:10, ETA: 6:27 - loss: 23.6410\n",
            "8498/10000 = 85.0%, SPS: 3.9, ELAP: 32:10, ETA: 6:27 - loss: 23.6092\n",
            "8499/10000 = 85.0%, SPS: 3.9, ELAP: 32:10, ETA: 6:26 - loss: 23.3077\n",
            "8500/10000 = 85.0%, SPS: 3.9, ELAP: 32:12, ETA: 6:26 - loss: 23.9262\n",
            "8501/10000 = 85.0%, SPS: 3.9, ELAP: 32:13, ETA: 6:26 - loss: 24.7402\n",
            "8502/10000 = 85.0%, SPS: 3.9, ELAP: 32:13, ETA: 6:26 - loss: 23.7446\n",
            "8503/10000 = 85.0%, SPS: 3.9, ELAP: 32:13, ETA: 6:26 - loss: 24.5973\n",
            "8504/10000 = 85.0%, SPS: 3.9, ELAP: 32:13, ETA: 6:25 - loss: 23.9472\n",
            "8505/10000 = 85.0%, SPS: 3.9, ELAP: 32:14, ETA: 6:25 - loss: 23.8227\n",
            "8506/10000 = 85.1%, SPS: 3.9, ELAP: 32:14, ETA: 6:25 - loss: 23.8511\n",
            "8507/10000 = 85.1%, SPS: 3.9, ELAP: 32:14, ETA: 6:25 - loss: 24.4985\n",
            "8508/10000 = 85.1%, SPS: 3.9, ELAP: 32:14, ETA: 6:24 - loss: 23.6783\n",
            "8509/10000 = 85.1%, SPS: 3.9, ELAP: 32:14, ETA: 6:24 - loss: 23.7654\n",
            "8510/10000 = 85.1%, SPS: 3.9, ELAP: 32:15, ETA: 6:24 - loss: 23.3944\n",
            "8511/10000 = 85.1%, SPS: 3.9, ELAP: 32:15, ETA: 6:24 - loss: 24.0331\n",
            "8512/10000 = 85.1%, SPS: 3.9, ELAP: 32:15, ETA: 6:23 - loss: 24.1955\n",
            "8513/10000 = 85.1%, SPS: 3.9, ELAP: 32:15, ETA: 6:23 - loss: 24.2131\n",
            "8514/10000 = 85.1%, SPS: 3.9, ELAP: 32:16, ETA: 6:23 - loss: 24.0526\n",
            "8515/10000 = 85.2%, SPS: 3.9, ELAP: 32:16, ETA: 6:23 - loss: 23.4083\n",
            "8516/10000 = 85.2%, SPS: 3.9, ELAP: 32:16, ETA: 6:22 - loss: 22.9746\n",
            "8517/10000 = 85.2%, SPS: 3.9, ELAP: 32:16, ETA: 6:22 - loss: 23.2145\n",
            "8518/10000 = 85.2%, SPS: 3.9, ELAP: 32:17, ETA: 6:22 - loss: 24.3432\n",
            "8519/10000 = 85.2%, SPS: 3.9, ELAP: 32:17, ETA: 6:21 - loss: 23.6486\n",
            "8520/10000 = 85.2%, SPS: 3.9, ELAP: 32:17, ETA: 6:21 - loss: 23.7973\n",
            "8521/10000 = 85.2%, SPS: 3.9, ELAP: 32:17, ETA: 6:21 - loss: 23.7031\n",
            "8522/10000 = 85.2%, SPS: 3.9, ELAP: 32:17, ETA: 6:21 - loss: 23.5491\n",
            "8523/10000 = 85.2%, SPS: 3.9, ELAP: 32:18, ETA: 6:20 - loss: 23.9319\n",
            "8524/10000 = 85.2%, SPS: 3.9, ELAP: 32:18, ETA: 6:20 - loss: 23.0857\n",
            "8525/10000 = 85.2%, SPS: 3.9, ELAP: 32:18, ETA: 6:20 - loss: 23.9703\n",
            "8526/10000 = 85.3%, SPS: 3.9, ELAP: 32:18, ETA: 6:20 - loss: 23.8352\n",
            "8527/10000 = 85.3%, SPS: 3.9, ELAP: 32:19, ETA: 6:19 - loss: 24.3763\n",
            "8528/10000 = 85.3%, SPS: 3.9, ELAP: 32:19, ETA: 6:19 - loss: 24.5326\n",
            "8529/10000 = 85.3%, SPS: 3.9, ELAP: 32:19, ETA: 6:19 - loss: 24.4103\n",
            "8530/10000 = 85.3%, SPS: 3.9, ELAP: 32:19, ETA: 6:19 - loss: 23.9654\n",
            "8531/10000 = 85.3%, SPS: 3.9, ELAP: 32:20, ETA: 6:18 - loss: 23.7932\n",
            "8532/10000 = 85.3%, SPS: 3.9, ELAP: 32:20, ETA: 6:18 - loss: 24.3764\n",
            "8533/10000 = 85.3%, SPS: 3.9, ELAP: 32:20, ETA: 6:18 - loss: 23.4507\n",
            "8534/10000 = 85.3%, SPS: 3.9, ELAP: 32:20, ETA: 6:18 - loss: 24.0719\n",
            "8535/10000 = 85.3%, SPS: 3.9, ELAP: 32:21, ETA: 6:17 - loss: 24.6615\n",
            "8536/10000 = 85.4%, SPS: 3.9, ELAP: 32:21, ETA: 6:17 - loss: 23.8581\n",
            "8537/10000 = 85.4%, SPS: 3.9, ELAP: 32:21, ETA: 6:17 - loss: 24.1483\n",
            "8538/10000 = 85.4%, SPS: 3.9, ELAP: 32:21, ETA: 6:17 - loss: 23.5895\n",
            "8539/10000 = 85.4%, SPS: 3.9, ELAP: 32:21, ETA: 6:16 - loss: 23.3929\n",
            "8540/10000 = 85.4%, SPS: 3.9, ELAP: 32:22, ETA: 6:16 - loss: 24.5851\n",
            "8541/10000 = 85.4%, SPS: 3.9, ELAP: 32:22, ETA: 6:16 - loss: 23.4391\n",
            "8542/10000 = 85.4%, SPS: 3.9, ELAP: 32:22, ETA: 6:15 - loss: 24.0088\n",
            "8543/10000 = 85.4%, SPS: 3.9, ELAP: 32:22, ETA: 6:15 - loss: 23.7675\n",
            "8544/10000 = 85.4%, SPS: 3.9, ELAP: 32:23, ETA: 6:15 - loss: 22.8069\n",
            "8545/10000 = 85.5%, SPS: 3.9, ELAP: 32:23, ETA: 6:15 - loss: 23.9839\n",
            "8546/10000 = 85.5%, SPS: 3.9, ELAP: 32:23, ETA: 6:14 - loss: 24.1713\n",
            "8547/10000 = 85.5%, SPS: 3.9, ELAP: 32:23, ETA: 6:14 - loss: 23.8219\n",
            "8548/10000 = 85.5%, SPS: 3.9, ELAP: 32:24, ETA: 6:14 - loss: 23.9351\n",
            "8549/10000 = 85.5%, SPS: 3.9, ELAP: 32:24, ETA: 6:14 - loss: 23.8397\n",
            "8550/10000 = 85.5%, SPS: 3.9, ELAP: 32:24, ETA: 6:13 - loss: 24.4867\n",
            "8551/10000 = 85.5%, SPS: 3.9, ELAP: 32:24, ETA: 6:13 - loss: 24.6026\n",
            "8552/10000 = 85.5%, SPS: 3.9, ELAP: 32:25, ETA: 6:13 - loss: 24.0114\n",
            "8553/10000 = 85.5%, SPS: 3.9, ELAP: 32:25, ETA: 6:13 - loss: 23.9544\n",
            "8554/10000 = 85.5%, SPS: 3.9, ELAP: 32:25, ETA: 6:12 - loss: 24.3446\n",
            "8555/10000 = 85.5%, SPS: 3.9, ELAP: 32:25, ETA: 6:12 - loss: 24.5905\n",
            "8556/10000 = 85.6%, SPS: 3.9, ELAP: 32:25, ETA: 6:12 - loss: 23.5085\n",
            "8557/10000 = 85.6%, SPS: 3.9, ELAP: 32:26, ETA: 6:12 - loss: 23.9843\n",
            "8558/10000 = 85.6%, SPS: 3.9, ELAP: 32:26, ETA: 6:11 - loss: 24.0421\n",
            "8559/10000 = 85.6%, SPS: 3.9, ELAP: 32:26, ETA: 6:11 - loss: 23.8120\n",
            "8560/10000 = 85.6%, SPS: 3.9, ELAP: 32:26, ETA: 6:11 - loss: 23.3540\n",
            "8561/10000 = 85.6%, SPS: 3.9, ELAP: 32:27, ETA: 6:10 - loss: 24.1650\n",
            "8562/10000 = 85.6%, SPS: 3.9, ELAP: 32:27, ETA: 6:10 - loss: 24.2668\n",
            "8563/10000 = 85.6%, SPS: 3.9, ELAP: 32:27, ETA: 6:10 - loss: 24.5201\n",
            "8564/10000 = 85.6%, SPS: 3.9, ELAP: 32:27, ETA: 6:10 - loss: 23.9367\n",
            "8565/10000 = 85.7%, SPS: 3.9, ELAP: 32:28, ETA: 6:09 - loss: 23.8130\n",
            "8566/10000 = 85.7%, SPS: 3.9, ELAP: 32:28, ETA: 6:09 - loss: 24.2600\n",
            "8567/10000 = 85.7%, SPS: 3.9, ELAP: 32:28, ETA: 6:09 - loss: 23.8886\n",
            "8568/10000 = 85.7%, SPS: 3.9, ELAP: 32:28, ETA: 6:09 - loss: 23.3520\n",
            "8569/10000 = 85.7%, SPS: 3.9, ELAP: 32:28, ETA: 6:08 - loss: 23.5065\n",
            "8570/10000 = 85.7%, SPS: 3.9, ELAP: 32:29, ETA: 6:08 - loss: 24.3868\n",
            "8571/10000 = 85.7%, SPS: 3.9, ELAP: 32:29, ETA: 6:08 - loss: 24.0431\n",
            "8572/10000 = 85.7%, SPS: 3.9, ELAP: 32:29, ETA: 6:08 - loss: 24.0045\n",
            "8573/10000 = 85.7%, SPS: 3.9, ELAP: 32:29, ETA: 6:07 - loss: 23.7212\n",
            "8574/10000 = 85.7%, SPS: 3.9, ELAP: 32:30, ETA: 6:07 - loss: 24.0391\n",
            "8575/10000 = 85.8%, SPS: 3.9, ELAP: 32:30, ETA: 6:07 - loss: 24.6370\n",
            "8576/10000 = 85.8%, SPS: 3.9, ELAP: 32:30, ETA: 6:07 - loss: 23.8084\n",
            "8577/10000 = 85.8%, SPS: 3.9, ELAP: 32:30, ETA: 6:06 - loss: 24.1892\n",
            "8578/10000 = 85.8%, SPS: 3.9, ELAP: 32:31, ETA: 6:06 - loss: 23.7754\n",
            "8579/10000 = 85.8%, SPS: 3.9, ELAP: 32:31, ETA: 6:06 - loss: 23.3551\n",
            "8580/10000 = 85.8%, SPS: 3.9, ELAP: 32:31, ETA: 6:05 - loss: 23.5227\n",
            "8581/10000 = 85.8%, SPS: 3.9, ELAP: 32:31, ETA: 6:05 - loss: 23.7460\n",
            "8582/10000 = 85.8%, SPS: 3.9, ELAP: 32:31, ETA: 6:05 - loss: 24.0376\n",
            "8583/10000 = 85.8%, SPS: 3.9, ELAP: 32:32, ETA: 6:05 - loss: 23.7710\n",
            "8584/10000 = 85.8%, SPS: 3.9, ELAP: 32:32, ETA: 6:04 - loss: 24.3498\n",
            "8585/10000 = 85.8%, SPS: 3.9, ELAP: 32:32, ETA: 6:04 - loss: 23.4535\n",
            "8586/10000 = 85.9%, SPS: 3.9, ELAP: 32:32, ETA: 6:04 - loss: 24.0777\n",
            "8587/10000 = 85.9%, SPS: 3.9, ELAP: 32:33, ETA: 6:04 - loss: 24.4107\n",
            "8588/10000 = 85.9%, SPS: 3.9, ELAP: 32:33, ETA: 6:03 - loss: 23.4969\n",
            "8589/10000 = 85.9%, SPS: 3.9, ELAP: 32:33, ETA: 6:03 - loss: 24.4475\n",
            "8590/10000 = 85.9%, SPS: 3.9, ELAP: 32:33, ETA: 6:03 - loss: 23.7838\n",
            "8591/10000 = 85.9%, SPS: 3.9, ELAP: 32:34, ETA: 6:03 - loss: 23.9743\n",
            "8592/10000 = 85.9%, SPS: 3.9, ELAP: 32:34, ETA: 6:02 - loss: 23.3944\n",
            "8593/10000 = 85.9%, SPS: 3.9, ELAP: 32:34, ETA: 6:02 - loss: 24.4828\n",
            "8594/10000 = 85.9%, SPS: 3.9, ELAP: 32:34, ETA: 6:02 - loss: 23.4502\n",
            "8595/10000 = 86.0%, SPS: 3.9, ELAP: 32:34, ETA: 6:02 - loss: 23.9287\n",
            "8596/10000 = 86.0%, SPS: 3.9, ELAP: 32:35, ETA: 6:01 - loss: 23.8492\n",
            "8597/10000 = 86.0%, SPS: 3.9, ELAP: 32:35, ETA: 6:01 - loss: 23.2784\n",
            "8598/10000 = 86.0%, SPS: 3.9, ELAP: 32:35, ETA: 6:01 - loss: 24.6260\n",
            "8599/10000 = 86.0%, SPS: 3.9, ELAP: 32:35, ETA: 6:01 - loss: 24.1941\n",
            "8600/10000 = 86.0%, SPS: 3.9, ELAP: 32:38, ETA: 6:01 - loss: 23.2318\n",
            "8601/10000 = 86.0%, SPS: 3.9, ELAP: 32:38, ETA: 6:00 - loss: 23.8454\n",
            "8602/10000 = 86.0%, SPS: 3.9, ELAP: 32:38, ETA: 6:00 - loss: 23.5195\n",
            "8603/10000 = 86.0%, SPS: 3.9, ELAP: 32:38, ETA: 6:00 - loss: 23.6327\n",
            "8604/10000 = 86.0%, SPS: 3.9, ELAP: 32:39, ETA: 6:00 - loss: 23.3215\n",
            "8605/10000 = 86.0%, SPS: 3.9, ELAP: 32:39, ETA: 5:59 - loss: 24.1231\n",
            "8606/10000 = 86.1%, SPS: 3.9, ELAP: 32:39, ETA: 5:59 - loss: 23.7147\n",
            "8607/10000 = 86.1%, SPS: 3.9, ELAP: 32:39, ETA: 5:59 - loss: 24.1326\n",
            "8608/10000 = 86.1%, SPS: 3.9, ELAP: 32:40, ETA: 5:59 - loss: 24.4507\n",
            "8609/10000 = 86.1%, SPS: 3.9, ELAP: 32:40, ETA: 5:58 - loss: 23.7586\n",
            "8610/10000 = 86.1%, SPS: 3.9, ELAP: 32:40, ETA: 5:58 - loss: 23.3277\n",
            "8611/10000 = 86.1%, SPS: 3.9, ELAP: 32:40, ETA: 5:58 - loss: 24.5190\n",
            "8612/10000 = 86.1%, SPS: 3.9, ELAP: 32:40, ETA: 5:57 - loss: 23.7732\n",
            "8613/10000 = 86.1%, SPS: 3.9, ELAP: 32:41, ETA: 5:57 - loss: 24.3331\n",
            "8614/10000 = 86.1%, SPS: 3.9, ELAP: 32:41, ETA: 5:57 - loss: 24.1136\n",
            "8615/10000 = 86.2%, SPS: 3.9, ELAP: 32:41, ETA: 5:57 - loss: 23.8763\n",
            "8616/10000 = 86.2%, SPS: 3.9, ELAP: 32:41, ETA: 5:56 - loss: 23.9619\n",
            "8617/10000 = 86.2%, SPS: 3.9, ELAP: 32:42, ETA: 5:56 - loss: 23.9024\n",
            "8618/10000 = 86.2%, SPS: 3.9, ELAP: 32:42, ETA: 5:56 - loss: 24.2335\n",
            "8619/10000 = 86.2%, SPS: 3.9, ELAP: 32:42, ETA: 5:56 - loss: 23.8399\n",
            "8620/10000 = 86.2%, SPS: 3.9, ELAP: 32:42, ETA: 5:55 - loss: 24.2831\n",
            "8621/10000 = 86.2%, SPS: 3.9, ELAP: 32:43, ETA: 5:55 - loss: 23.6942\n",
            "8622/10000 = 86.2%, SPS: 3.9, ELAP: 32:43, ETA: 5:55 - loss: 24.5282\n",
            "8623/10000 = 86.2%, SPS: 3.9, ELAP: 32:43, ETA: 5:55 - loss: 24.1616\n",
            "8624/10000 = 86.2%, SPS: 3.9, ELAP: 32:43, ETA: 5:54 - loss: 24.0411\n",
            "8625/10000 = 86.2%, SPS: 3.9, ELAP: 32:43, ETA: 5:54 - loss: 23.9446\n",
            "8626/10000 = 86.3%, SPS: 3.9, ELAP: 32:44, ETA: 5:54 - loss: 23.6724\n",
            "8627/10000 = 86.3%, SPS: 3.9, ELAP: 32:44, ETA: 5:54 - loss: 24.0176\n",
            "8628/10000 = 86.3%, SPS: 3.9, ELAP: 32:44, ETA: 5:53 - loss: 24.0799\n",
            "8629/10000 = 86.3%, SPS: 3.9, ELAP: 32:44, ETA: 5:53 - loss: 23.3075\n",
            "8630/10000 = 86.3%, SPS: 3.9, ELAP: 32:45, ETA: 5:53 - loss: 23.2749\n",
            "8631/10000 = 86.3%, SPS: 3.9, ELAP: 32:45, ETA: 5:52 - loss: 24.0145\n",
            "8632/10000 = 86.3%, SPS: 3.9, ELAP: 32:45, ETA: 5:52 - loss: 23.8074\n",
            "8633/10000 = 86.3%, SPS: 3.9, ELAP: 32:45, ETA: 5:52 - loss: 23.8353\n",
            "8634/10000 = 86.3%, SPS: 3.9, ELAP: 32:46, ETA: 5:52 - loss: 23.5344\n",
            "8635/10000 = 86.3%, SPS: 3.9, ELAP: 32:46, ETA: 5:51 - loss: 23.7907\n",
            "8636/10000 = 86.4%, SPS: 3.9, ELAP: 32:46, ETA: 5:51 - loss: 23.6560\n",
            "8637/10000 = 86.4%, SPS: 3.9, ELAP: 32:46, ETA: 5:51 - loss: 23.9089\n",
            "8638/10000 = 86.4%, SPS: 3.9, ELAP: 32:46, ETA: 5:51 - loss: 24.2152\n",
            "8639/10000 = 86.4%, SPS: 3.9, ELAP: 32:47, ETA: 5:50 - loss: 24.6252\n",
            "8640/10000 = 86.4%, SPS: 3.9, ELAP: 32:47, ETA: 5:50 - loss: 24.0189\n",
            "8641/10000 = 86.4%, SPS: 3.9, ELAP: 32:47, ETA: 5:50 - loss: 23.7610\n",
            "8642/10000 = 86.4%, SPS: 3.9, ELAP: 32:47, ETA: 5:50 - loss: 22.7253\n",
            "8643/10000 = 86.4%, SPS: 3.9, ELAP: 32:48, ETA: 5:49 - loss: 24.4553\n",
            "8644/10000 = 86.4%, SPS: 3.9, ELAP: 32:48, ETA: 5:49 - loss: 23.7378\n",
            "8645/10000 = 86.5%, SPS: 3.9, ELAP: 32:48, ETA: 5:49 - loss: 24.1568\n",
            "8646/10000 = 86.5%, SPS: 3.9, ELAP: 32:48, ETA: 5:49 - loss: 23.8419\n",
            "8647/10000 = 86.5%, SPS: 3.9, ELAP: 32:49, ETA: 5:48 - loss: 24.2210\n",
            "8648/10000 = 86.5%, SPS: 3.9, ELAP: 32:49, ETA: 5:48 - loss: 23.9280\n",
            "8649/10000 = 86.5%, SPS: 3.9, ELAP: 32:49, ETA: 5:48 - loss: 24.2463\n",
            "8650/10000 = 86.5%, SPS: 3.9, ELAP: 32:49, ETA: 5:48 - loss: 23.8609\n",
            "8651/10000 = 86.5%, SPS: 3.9, ELAP: 32:50, ETA: 5:47 - loss: 23.9827\n",
            "8652/10000 = 86.5%, SPS: 3.9, ELAP: 32:50, ETA: 5:47 - loss: 23.2084\n",
            "8653/10000 = 86.5%, SPS: 3.9, ELAP: 32:50, ETA: 5:47 - loss: 23.8687\n",
            "8654/10000 = 86.5%, SPS: 3.9, ELAP: 32:50, ETA: 5:46 - loss: 23.8239\n",
            "8655/10000 = 86.5%, SPS: 3.9, ELAP: 32:50, ETA: 5:46 - loss: 24.5949\n",
            "8656/10000 = 86.6%, SPS: 3.9, ELAP: 32:51, ETA: 5:46 - loss: 23.1965\n",
            "8657/10000 = 86.6%, SPS: 3.9, ELAP: 32:51, ETA: 5:46 - loss: 24.4998\n",
            "8658/10000 = 86.6%, SPS: 3.9, ELAP: 32:51, ETA: 5:45 - loss: 23.3442\n",
            "8659/10000 = 86.6%, SPS: 3.9, ELAP: 32:51, ETA: 5:45 - loss: 24.2816\n",
            "8660/10000 = 86.6%, SPS: 3.9, ELAP: 32:52, ETA: 5:45 - loss: 24.1495\n",
            "8661/10000 = 86.6%, SPS: 3.9, ELAP: 32:52, ETA: 5:45 - loss: 24.1474\n",
            "8662/10000 = 86.6%, SPS: 3.9, ELAP: 32:52, ETA: 5:44 - loss: 24.2627\n",
            "8663/10000 = 86.6%, SPS: 3.9, ELAP: 32:52, ETA: 5:44 - loss: 23.5627\n",
            "8664/10000 = 86.6%, SPS: 3.9, ELAP: 32:53, ETA: 5:44 - loss: 23.8765\n",
            "8665/10000 = 86.7%, SPS: 3.9, ELAP: 32:53, ETA: 5:44 - loss: 23.5413\n",
            "8666/10000 = 86.7%, SPS: 3.9, ELAP: 32:53, ETA: 5:43 - loss: 23.3065\n",
            "8667/10000 = 86.7%, SPS: 3.9, ELAP: 32:53, ETA: 5:43 - loss: 24.2213\n",
            "8668/10000 = 86.7%, SPS: 3.9, ELAP: 32:53, ETA: 5:43 - loss: 24.1095\n",
            "8669/10000 = 86.7%, SPS: 3.9, ELAP: 32:54, ETA: 5:43 - loss: 24.2110\n",
            "8670/10000 = 86.7%, SPS: 3.9, ELAP: 32:54, ETA: 5:42 - loss: 23.7229\n",
            "8671/10000 = 86.7%, SPS: 3.9, ELAP: 32:54, ETA: 5:42 - loss: 24.4235\n",
            "8672/10000 = 86.7%, SPS: 3.9, ELAP: 32:54, ETA: 5:42 - loss: 24.0346\n",
            "8673/10000 = 86.7%, SPS: 3.9, ELAP: 32:55, ETA: 5:42 - loss: 23.8451\n",
            "8674/10000 = 86.7%, SPS: 3.9, ELAP: 32:55, ETA: 5:41 - loss: 24.1713\n",
            "8675/10000 = 86.8%, SPS: 3.9, ELAP: 32:55, ETA: 5:41 - loss: 24.0304\n",
            "8676/10000 = 86.8%, SPS: 3.9, ELAP: 32:55, ETA: 5:41 - loss: 24.7406\n",
            "8677/10000 = 86.8%, SPS: 3.9, ELAP: 32:56, ETA: 5:40 - loss: 23.6096\n",
            "8678/10000 = 86.8%, SPS: 3.9, ELAP: 32:56, ETA: 5:40 - loss: 23.8954\n",
            "8679/10000 = 86.8%, SPS: 3.9, ELAP: 32:56, ETA: 5:40 - loss: 24.1361\n",
            "8680/10000 = 86.8%, SPS: 3.9, ELAP: 32:56, ETA: 5:40 - loss: 24.1433\n",
            "8681/10000 = 86.8%, SPS: 3.9, ELAP: 32:56, ETA: 5:39 - loss: 24.1478\n",
            "8682/10000 = 86.8%, SPS: 3.9, ELAP: 32:57, ETA: 5:39 - loss: 23.4882\n",
            "8683/10000 = 86.8%, SPS: 3.9, ELAP: 32:57, ETA: 5:39 - loss: 23.3917\n",
            "8684/10000 = 86.8%, SPS: 3.9, ELAP: 32:57, ETA: 5:39 - loss: 23.5642\n",
            "8685/10000 = 86.8%, SPS: 3.9, ELAP: 32:57, ETA: 5:38 - loss: 23.8368\n",
            "8686/10000 = 86.9%, SPS: 3.9, ELAP: 32:58, ETA: 5:38 - loss: 24.0171\n",
            "8687/10000 = 86.9%, SPS: 3.9, ELAP: 32:58, ETA: 5:38 - loss: 23.5364\n",
            "8688/10000 = 86.9%, SPS: 3.9, ELAP: 32:58, ETA: 5:38 - loss: 23.8185\n",
            "8689/10000 = 86.9%, SPS: 3.9, ELAP: 32:58, ETA: 5:37 - loss: 23.7257\n",
            "8690/10000 = 86.9%, SPS: 3.9, ELAP: 32:59, ETA: 5:37 - loss: 23.2760\n",
            "8691/10000 = 86.9%, SPS: 3.9, ELAP: 32:59, ETA: 5:37 - loss: 23.3777\n",
            "8692/10000 = 86.9%, SPS: 3.9, ELAP: 32:59, ETA: 5:37 - loss: 24.0620\n",
            "8693/10000 = 86.9%, SPS: 3.9, ELAP: 32:59, ETA: 5:36 - loss: 23.4791\n",
            "8694/10000 = 86.9%, SPS: 3.9, ELAP: 32:59, ETA: 5:36 - loss: 24.5377\n",
            "8695/10000 = 87.0%, SPS: 3.9, ELAP: 33:00, ETA: 5:36 - loss: 23.6484\n",
            "8696/10000 = 87.0%, SPS: 3.9, ELAP: 33:00, ETA: 5:35 - loss: 23.9042\n",
            "8697/10000 = 87.0%, SPS: 3.9, ELAP: 33:00, ETA: 5:35 - loss: 24.0323\n",
            "8698/10000 = 87.0%, SPS: 3.9, ELAP: 33:00, ETA: 5:35 - loss: 23.3841\n",
            "8699/10000 = 87.0%, SPS: 3.9, ELAP: 33:01, ETA: 5:35 - loss: 23.7984\n",
            "8700/10000 = 87.0%, SPS: 3.9, ELAP: 33:03, ETA: 5:35 - loss: 24.2474\n",
            "8701/10000 = 87.0%, SPS: 3.9, ELAP: 33:03, ETA: 5:35 - loss: 24.3997\n",
            "8702/10000 = 87.0%, SPS: 3.9, ELAP: 33:03, ETA: 5:34 - loss: 23.7392\n",
            "8703/10000 = 87.0%, SPS: 3.9, ELAP: 33:04, ETA: 5:34 - loss: 24.3771\n",
            "8704/10000 = 87.0%, SPS: 3.9, ELAP: 33:04, ETA: 5:34 - loss: 23.7927\n",
            "8705/10000 = 87.0%, SPS: 3.9, ELAP: 33:04, ETA: 5:33 - loss: 23.9662\n",
            "8706/10000 = 87.1%, SPS: 3.9, ELAP: 33:04, ETA: 5:33 - loss: 23.8486\n",
            "8707/10000 = 87.1%, SPS: 3.9, ELAP: 33:05, ETA: 5:33 - loss: 24.6696\n",
            "8708/10000 = 87.1%, SPS: 3.9, ELAP: 33:05, ETA: 5:33 - loss: 24.1491\n",
            "8709/10000 = 87.1%, SPS: 3.9, ELAP: 33:05, ETA: 5:32 - loss: 23.9861\n",
            "8710/10000 = 87.1%, SPS: 3.9, ELAP: 33:05, ETA: 5:32 - loss: 23.9022\n",
            "8711/10000 = 87.1%, SPS: 3.9, ELAP: 33:06, ETA: 5:32 - loss: 23.6018\n",
            "8712/10000 = 87.1%, SPS: 3.9, ELAP: 33:06, ETA: 5:32 - loss: 23.7543\n",
            "8713/10000 = 87.1%, SPS: 3.9, ELAP: 33:06, ETA: 5:31 - loss: 23.8545\n",
            "8714/10000 = 87.1%, SPS: 3.9, ELAP: 33:06, ETA: 5:31 - loss: 23.7567\n",
            "8715/10000 = 87.2%, SPS: 3.9, ELAP: 33:06, ETA: 5:31 - loss: 23.2197\n",
            "8716/10000 = 87.2%, SPS: 3.9, ELAP: 33:07, ETA: 5:31 - loss: 24.1995\n",
            "8717/10000 = 87.2%, SPS: 3.9, ELAP: 33:07, ETA: 5:30 - loss: 23.9657\n",
            "8718/10000 = 87.2%, SPS: 3.9, ELAP: 33:07, ETA: 5:30 - loss: 23.5261\n",
            "8719/10000 = 87.2%, SPS: 3.9, ELAP: 33:07, ETA: 5:30 - loss: 23.9631\n",
            "8720/10000 = 87.2%, SPS: 3.9, ELAP: 33:08, ETA: 5:30 - loss: 23.7857\n",
            "8721/10000 = 87.2%, SPS: 3.9, ELAP: 33:08, ETA: 5:29 - loss: 23.9882\n",
            "8722/10000 = 87.2%, SPS: 3.9, ELAP: 33:08, ETA: 5:29 - loss: 24.2395\n",
            "8723/10000 = 87.2%, SPS: 3.9, ELAP: 33:08, ETA: 5:29 - loss: 24.1530\n",
            "8724/10000 = 87.2%, SPS: 3.9, ELAP: 33:09, ETA: 5:28 - loss: 24.0387\n",
            "8725/10000 = 87.2%, SPS: 3.9, ELAP: 33:09, ETA: 5:28 - loss: 23.9490\n",
            "8726/10000 = 87.3%, SPS: 3.9, ELAP: 33:09, ETA: 5:28 - loss: 23.6175\n",
            "8727/10000 = 87.3%, SPS: 3.9, ELAP: 33:09, ETA: 5:28 - loss: 24.0714\n",
            "8728/10000 = 87.3%, SPS: 3.9, ELAP: 33:09, ETA: 5:27 - loss: 24.0544\n",
            "8729/10000 = 87.3%, SPS: 3.9, ELAP: 33:10, ETA: 5:27 - loss: 23.9798\n",
            "8730/10000 = 87.3%, SPS: 3.9, ELAP: 33:10, ETA: 5:27 - loss: 23.8446\n",
            "8731/10000 = 87.3%, SPS: 3.9, ELAP: 33:10, ETA: 5:27 - loss: 23.6223\n",
            "8732/10000 = 87.3%, SPS: 3.9, ELAP: 33:10, ETA: 5:26 - loss: 23.5106\n",
            "8733/10000 = 87.3%, SPS: 3.9, ELAP: 33:11, ETA: 5:26 - loss: 24.0771\n",
            "8734/10000 = 87.3%, SPS: 3.9, ELAP: 33:11, ETA: 5:26 - loss: 24.1757\n",
            "8735/10000 = 87.3%, SPS: 3.9, ELAP: 33:11, ETA: 5:26 - loss: 24.2807\n",
            "8736/10000 = 87.4%, SPS: 3.9, ELAP: 33:11, ETA: 5:25 - loss: 24.1189\n",
            "8737/10000 = 87.4%, SPS: 3.9, ELAP: 33:12, ETA: 5:25 - loss: 23.7403\n",
            "8738/10000 = 87.4%, SPS: 3.9, ELAP: 33:12, ETA: 5:25 - loss: 24.5038\n",
            "8739/10000 = 87.4%, SPS: 3.9, ELAP: 33:12, ETA: 5:25 - loss: 23.8960\n",
            "8740/10000 = 87.4%, SPS: 3.9, ELAP: 33:12, ETA: 5:24 - loss: 23.8232\n",
            "8741/10000 = 87.4%, SPS: 3.9, ELAP: 33:13, ETA: 5:24 - loss: 23.4626\n",
            "8742/10000 = 87.4%, SPS: 3.9, ELAP: 33:13, ETA: 5:24 - loss: 24.0571\n",
            "8743/10000 = 87.4%, SPS: 3.9, ELAP: 33:13, ETA: 5:24 - loss: 23.9528\n",
            "8744/10000 = 87.4%, SPS: 3.9, ELAP: 33:13, ETA: 5:23 - loss: 23.7653\n",
            "8745/10000 = 87.5%, SPS: 3.9, ELAP: 33:14, ETA: 5:23 - loss: 23.4984\n",
            "8746/10000 = 87.5%, SPS: 3.9, ELAP: 33:14, ETA: 5:23 - loss: 23.5533\n",
            "8747/10000 = 87.5%, SPS: 3.9, ELAP: 33:14, ETA: 5:23 - loss: 24.0165\n",
            "8748/10000 = 87.5%, SPS: 3.9, ELAP: 33:14, ETA: 5:22 - loss: 23.6464\n",
            "8749/10000 = 87.5%, SPS: 3.9, ELAP: 33:15, ETA: 5:22 - loss: 24.1563\n",
            "8750/10000 = 87.5%, SPS: 3.9, ELAP: 33:15, ETA: 5:22 - loss: 23.9503\n",
            "8751/10000 = 87.5%, SPS: 3.9, ELAP: 33:15, ETA: 5:21 - loss: 24.0757\n",
            "8752/10000 = 87.5%, SPS: 3.9, ELAP: 33:15, ETA: 5:21 - loss: 23.4182\n",
            "8753/10000 = 87.5%, SPS: 3.9, ELAP: 33:16, ETA: 5:21 - loss: 24.2440\n",
            "8754/10000 = 87.5%, SPS: 3.9, ELAP: 33:16, ETA: 5:21 - loss: 23.6251\n",
            "8755/10000 = 87.5%, SPS: 3.9, ELAP: 33:16, ETA: 5:20 - loss: 23.9970\n",
            "8756/10000 = 87.6%, SPS: 3.9, ELAP: 33:16, ETA: 5:20 - loss: 24.1120\n",
            "8757/10000 = 87.6%, SPS: 3.9, ELAP: 33:16, ETA: 5:20 - loss: 24.3900\n",
            "8758/10000 = 87.6%, SPS: 3.9, ELAP: 33:17, ETA: 5:20 - loss: 23.6119\n",
            "8759/10000 = 87.6%, SPS: 3.9, ELAP: 33:17, ETA: 5:19 - loss: 23.9778\n",
            "8760/10000 = 87.6%, SPS: 3.9, ELAP: 33:17, ETA: 5:19 - loss: 24.0007\n",
            "8761/10000 = 87.6%, SPS: 3.9, ELAP: 33:17, ETA: 5:19 - loss: 24.4292\n",
            "8762/10000 = 87.6%, SPS: 3.9, ELAP: 33:18, ETA: 5:19 - loss: 24.1396\n",
            "8763/10000 = 87.6%, SPS: 3.9, ELAP: 33:18, ETA: 5:18 - loss: 24.0187\n",
            "8764/10000 = 87.6%, SPS: 3.9, ELAP: 33:18, ETA: 5:18 - loss: 23.6163\n",
            "8765/10000 = 87.7%, SPS: 3.9, ELAP: 33:18, ETA: 5:18 - loss: 23.4029\n",
            "8766/10000 = 87.7%, SPS: 3.9, ELAP: 33:19, ETA: 5:18 - loss: 24.0521\n",
            "8767/10000 = 87.7%, SPS: 3.9, ELAP: 33:19, ETA: 5:17 - loss: 24.1322\n",
            "8768/10000 = 87.7%, SPS: 3.9, ELAP: 33:19, ETA: 5:17 - loss: 23.3926\n",
            "8769/10000 = 87.7%, SPS: 3.9, ELAP: 33:19, ETA: 5:17 - loss: 24.5257\n",
            "8770/10000 = 87.7%, SPS: 3.9, ELAP: 33:19, ETA: 5:17 - loss: 23.9709\n",
            "8771/10000 = 87.7%, SPS: 3.9, ELAP: 33:20, ETA: 5:16 - loss: 23.9323\n",
            "8772/10000 = 87.7%, SPS: 3.9, ELAP: 33:20, ETA: 5:16 - loss: 23.5256\n",
            "8773/10000 = 87.7%, SPS: 3.9, ELAP: 33:20, ETA: 5:16 - loss: 24.5316\n",
            "8774/10000 = 87.7%, SPS: 3.9, ELAP: 33:20, ETA: 5:15 - loss: 23.7801\n",
            "8775/10000 = 87.8%, SPS: 3.9, ELAP: 33:21, ETA: 5:15 - loss: 23.6087\n",
            "8776/10000 = 87.8%, SPS: 3.9, ELAP: 33:21, ETA: 5:15 - loss: 24.3183\n",
            "8777/10000 = 87.8%, SPS: 3.9, ELAP: 33:21, ETA: 5:15 - loss: 23.8835\n",
            "8778/10000 = 87.8%, SPS: 3.9, ELAP: 33:21, ETA: 5:14 - loss: 23.2193\n",
            "8779/10000 = 87.8%, SPS: 3.9, ELAP: 33:21, ETA: 5:14 - loss: 23.9947\n",
            "8780/10000 = 87.8%, SPS: 3.9, ELAP: 33:22, ETA: 5:14 - loss: 24.0001\n",
            "8781/10000 = 87.8%, SPS: 3.9, ELAP: 33:22, ETA: 5:14 - loss: 24.0916\n",
            "8782/10000 = 87.8%, SPS: 3.9, ELAP: 33:22, ETA: 5:13 - loss: 24.1687\n",
            "8783/10000 = 87.8%, SPS: 3.9, ELAP: 33:22, ETA: 5:13 - loss: 23.8232\n",
            "8784/10000 = 87.8%, SPS: 3.9, ELAP: 33:23, ETA: 5:13 - loss: 23.7162\n",
            "8785/10000 = 87.8%, SPS: 3.9, ELAP: 33:23, ETA: 5:13 - loss: 23.3542\n",
            "8786/10000 = 87.9%, SPS: 3.9, ELAP: 33:23, ETA: 5:12 - loss: 23.8563\n",
            "8787/10000 = 87.9%, SPS: 3.9, ELAP: 33:23, ETA: 5:12 - loss: 23.4698\n",
            "8788/10000 = 87.9%, SPS: 3.9, ELAP: 33:24, ETA: 5:12 - loss: 23.5911\n",
            "8789/10000 = 87.9%, SPS: 3.9, ELAP: 33:24, ETA: 5:12 - loss: 24.4932\n",
            "8790/10000 = 87.9%, SPS: 3.9, ELAP: 33:24, ETA: 5:11 - loss: 24.5574\n",
            "8791/10000 = 87.9%, SPS: 3.9, ELAP: 33:24, ETA: 5:11 - loss: 23.7658\n",
            "8792/10000 = 87.9%, SPS: 3.9, ELAP: 33:24, ETA: 5:11 - loss: 24.1361\n",
            "8793/10000 = 87.9%, SPS: 3.9, ELAP: 33:25, ETA: 5:10 - loss: 23.9476\n",
            "8794/10000 = 87.9%, SPS: 3.9, ELAP: 33:25, ETA: 5:10 - loss: 23.6418\n",
            "8795/10000 = 88.0%, SPS: 3.9, ELAP: 33:25, ETA: 5:10 - loss: 24.3981\n",
            "8796/10000 = 88.0%, SPS: 3.9, ELAP: 33:25, ETA: 5:10 - loss: 23.8501\n",
            "8797/10000 = 88.0%, SPS: 3.9, ELAP: 33:26, ETA: 5:09 - loss: 24.7669\n",
            "8798/10000 = 88.0%, SPS: 3.9, ELAP: 33:26, ETA: 5:09 - loss: 23.4625\n",
            "8799/10000 = 88.0%, SPS: 3.9, ELAP: 33:26, ETA: 5:09 - loss: 24.0812\n",
            "8800/10000 = 88.0%, SPS: 3.9, ELAP: 33:28, ETA: 5:09 - loss: 23.9982\n",
            "8801/10000 = 88.0%, SPS: 3.9, ELAP: 33:29, ETA: 5:09 - loss: 23.3063\n",
            "8802/10000 = 88.0%, SPS: 3.9, ELAP: 33:29, ETA: 5:08 - loss: 23.6550\n",
            "8803/10000 = 88.0%, SPS: 3.9, ELAP: 33:29, ETA: 5:08 - loss: 23.9943\n",
            "8804/10000 = 88.0%, SPS: 3.9, ELAP: 33:29, ETA: 5:08 - loss: 24.1111\n",
            "8805/10000 = 88.0%, SPS: 3.9, ELAP: 33:30, ETA: 5:08 - loss: 23.4208\n",
            "8806/10000 = 88.1%, SPS: 3.9, ELAP: 33:30, ETA: 5:07 - loss: 24.2114\n",
            "8807/10000 = 88.1%, SPS: 3.9, ELAP: 33:30, ETA: 5:07 - loss: 23.3540\n",
            "8808/10000 = 88.1%, SPS: 3.9, ELAP: 33:30, ETA: 5:07 - loss: 23.5106\n",
            "8809/10000 = 88.1%, SPS: 3.9, ELAP: 33:31, ETA: 5:07 - loss: 23.5385\n",
            "8810/10000 = 88.1%, SPS: 3.9, ELAP: 33:31, ETA: 5:06 - loss: 23.9215\n",
            "8811/10000 = 88.1%, SPS: 3.9, ELAP: 33:31, ETA: 5:06 - loss: 24.0758\n",
            "8812/10000 = 88.1%, SPS: 3.9, ELAP: 33:31, ETA: 5:06 - loss: 24.1721\n",
            "8813/10000 = 88.1%, SPS: 3.9, ELAP: 33:31, ETA: 5:06 - loss: 23.8470\n",
            "8814/10000 = 88.1%, SPS: 3.9, ELAP: 33:32, ETA: 5:05 - loss: 23.7747\n",
            "8815/10000 = 88.2%, SPS: 3.9, ELAP: 33:32, ETA: 5:05 - loss: 23.9920\n",
            "8816/10000 = 88.2%, SPS: 3.9, ELAP: 33:32, ETA: 5:05 - loss: 23.2718\n",
            "8817/10000 = 88.2%, SPS: 3.9, ELAP: 33:32, ETA: 5:05 - loss: 24.0943\n",
            "8818/10000 = 88.2%, SPS: 3.9, ELAP: 33:33, ETA: 5:04 - loss: 23.9258\n",
            "8819/10000 = 88.2%, SPS: 3.9, ELAP: 33:33, ETA: 5:04 - loss: 23.7895\n",
            "8820/10000 = 88.2%, SPS: 3.9, ELAP: 33:33, ETA: 5:04 - loss: 24.2943\n",
            "8821/10000 = 88.2%, SPS: 3.9, ELAP: 33:33, ETA: 5:03 - loss: 22.9859\n",
            "8822/10000 = 88.2%, SPS: 3.9, ELAP: 33:33, ETA: 5:03 - loss: 24.2052\n",
            "8823/10000 = 88.2%, SPS: 3.9, ELAP: 33:34, ETA: 5:03 - loss: 24.2437\n",
            "8824/10000 = 88.2%, SPS: 3.9, ELAP: 33:34, ETA: 5:03 - loss: 23.8838\n",
            "8825/10000 = 88.2%, SPS: 3.9, ELAP: 33:34, ETA: 5:02 - loss: 24.0029\n",
            "8826/10000 = 88.3%, SPS: 3.9, ELAP: 33:34, ETA: 5:02 - loss: 23.9706\n",
            "8827/10000 = 88.3%, SPS: 3.9, ELAP: 33:35, ETA: 5:02 - loss: 23.4341\n",
            "8828/10000 = 88.3%, SPS: 3.9, ELAP: 33:35, ETA: 5:02 - loss: 23.3102\n",
            "8829/10000 = 88.3%, SPS: 3.9, ELAP: 33:35, ETA: 5:01 - loss: 23.9403\n",
            "8830/10000 = 88.3%, SPS: 3.9, ELAP: 33:35, ETA: 5:01 - loss: 24.0280\n",
            "8831/10000 = 88.3%, SPS: 3.9, ELAP: 33:36, ETA: 5:01 - loss: 23.8255\n",
            "8832/10000 = 88.3%, SPS: 3.9, ELAP: 33:36, ETA: 5:01 - loss: 23.3016\n",
            "8833/10000 = 88.3%, SPS: 3.9, ELAP: 33:36, ETA: 5:00 - loss: 23.7463\n",
            "8834/10000 = 88.3%, SPS: 3.9, ELAP: 33:36, ETA: 5:00 - loss: 24.0561\n",
            "8835/10000 = 88.3%, SPS: 3.9, ELAP: 33:37, ETA: 5:00 - loss: 23.9316\n",
            "8836/10000 = 88.4%, SPS: 3.9, ELAP: 33:37, ETA: 5:00 - loss: 24.1447\n",
            "8837/10000 = 88.4%, SPS: 3.9, ELAP: 33:37, ETA: 4:59 - loss: 23.6488\n",
            "8838/10000 = 88.4%, SPS: 3.9, ELAP: 33:37, ETA: 4:59 - loss: 23.6821\n",
            "8839/10000 = 88.4%, SPS: 3.9, ELAP: 33:38, ETA: 4:59 - loss: 23.7403\n",
            "8840/10000 = 88.4%, SPS: 3.9, ELAP: 33:38, ETA: 4:59 - loss: 24.2218\n",
            "8841/10000 = 88.4%, SPS: 3.9, ELAP: 33:38, ETA: 4:58 - loss: 23.8427\n",
            "8842/10000 = 88.4%, SPS: 3.9, ELAP: 33:38, ETA: 4:58 - loss: 23.5986\n",
            "8843/10000 = 88.4%, SPS: 3.9, ELAP: 33:39, ETA: 4:58 - loss: 24.1631\n",
            "8844/10000 = 88.4%, SPS: 3.9, ELAP: 33:39, ETA: 4:58 - loss: 23.6180\n",
            "8845/10000 = 88.5%, SPS: 3.9, ELAP: 33:39, ETA: 4:57 - loss: 23.2133\n",
            "8846/10000 = 88.5%, SPS: 3.9, ELAP: 33:39, ETA: 4:57 - loss: 24.1806\n",
            "8847/10000 = 88.5%, SPS: 3.9, ELAP: 33:40, ETA: 4:57 - loss: 23.3871\n",
            "8848/10000 = 88.5%, SPS: 3.9, ELAP: 33:40, ETA: 4:56 - loss: 23.8785\n",
            "8849/10000 = 88.5%, SPS: 3.9, ELAP: 33:40, ETA: 4:56 - loss: 23.6113\n",
            "8850/10000 = 88.5%, SPS: 3.9, ELAP: 33:40, ETA: 4:56 - loss: 23.8260\n",
            "8851/10000 = 88.5%, SPS: 3.9, ELAP: 33:40, ETA: 4:56 - loss: 24.1459\n",
            "8852/10000 = 88.5%, SPS: 3.9, ELAP: 33:41, ETA: 4:55 - loss: 23.9095\n",
            "8853/10000 = 88.5%, SPS: 3.9, ELAP: 33:41, ETA: 4:55 - loss: 23.6115\n",
            "8854/10000 = 88.5%, SPS: 3.9, ELAP: 33:41, ETA: 4:55 - loss: 23.8058\n",
            "8855/10000 = 88.5%, SPS: 3.9, ELAP: 33:41, ETA: 4:55 - loss: 24.1784\n",
            "8856/10000 = 88.6%, SPS: 3.9, ELAP: 33:42, ETA: 4:54 - loss: 23.7778\n",
            "8857/10000 = 88.6%, SPS: 3.9, ELAP: 33:42, ETA: 4:54 - loss: 23.6869\n",
            "8858/10000 = 88.6%, SPS: 3.9, ELAP: 33:42, ETA: 4:54 - loss: 23.8190\n",
            "8859/10000 = 88.6%, SPS: 3.9, ELAP: 33:42, ETA: 4:54 - loss: 23.6470\n",
            "8860/10000 = 88.6%, SPS: 3.9, ELAP: 33:43, ETA: 4:53 - loss: 23.9372\n",
            "8861/10000 = 88.6%, SPS: 3.9, ELAP: 33:43, ETA: 4:53 - loss: 23.4689\n",
            "8862/10000 = 88.6%, SPS: 3.9, ELAP: 33:43, ETA: 4:53 - loss: 24.0982\n",
            "8863/10000 = 88.6%, SPS: 3.9, ELAP: 33:43, ETA: 4:53 - loss: 24.0558\n",
            "8864/10000 = 88.6%, SPS: 3.9, ELAP: 33:43, ETA: 4:52 - loss: 23.7166\n",
            "8865/10000 = 88.7%, SPS: 3.9, ELAP: 33:44, ETA: 4:52 - loss: 24.0949\n",
            "8866/10000 = 88.7%, SPS: 3.9, ELAP: 33:44, ETA: 4:52 - loss: 23.2193\n",
            "8867/10000 = 88.7%, SPS: 3.9, ELAP: 33:44, ETA: 4:52 - loss: 23.4643\n",
            "8868/10000 = 88.7%, SPS: 3.9, ELAP: 33:44, ETA: 4:51 - loss: 24.3543\n",
            "8869/10000 = 88.7%, SPS: 3.9, ELAP: 33:45, ETA: 4:51 - loss: 23.7654\n",
            "8870/10000 = 88.7%, SPS: 3.9, ELAP: 33:45, ETA: 4:51 - loss: 23.5634\n",
            "8871/10000 = 88.7%, SPS: 3.9, ELAP: 33:45, ETA: 4:50 - loss: 23.6050\n",
            "8872/10000 = 88.7%, SPS: 3.9, ELAP: 33:45, ETA: 4:50 - loss: 23.0946\n",
            "8873/10000 = 88.7%, SPS: 3.9, ELAP: 33:46, ETA: 4:50 - loss: 23.1353\n",
            "8874/10000 = 88.7%, SPS: 3.9, ELAP: 33:46, ETA: 4:50 - loss: 24.1010\n",
            "8875/10000 = 88.8%, SPS: 3.9, ELAP: 33:46, ETA: 4:49 - loss: 23.6525\n",
            "8876/10000 = 88.8%, SPS: 3.9, ELAP: 33:46, ETA: 4:49 - loss: 24.1155\n",
            "8877/10000 = 88.8%, SPS: 3.9, ELAP: 33:46, ETA: 4:49 - loss: 23.7205\n",
            "8878/10000 = 88.8%, SPS: 3.9, ELAP: 33:47, ETA: 4:49 - loss: 23.9416\n",
            "8879/10000 = 88.8%, SPS: 3.9, ELAP: 33:47, ETA: 4:48 - loss: 23.9137\n",
            "8880/10000 = 88.8%, SPS: 3.9, ELAP: 33:47, ETA: 4:48 - loss: 23.6056\n",
            "8881/10000 = 88.8%, SPS: 3.9, ELAP: 33:47, ETA: 4:48 - loss: 24.1188\n",
            "8882/10000 = 88.8%, SPS: 3.9, ELAP: 33:48, ETA: 4:48 - loss: 23.7121\n",
            "8883/10000 = 88.8%, SPS: 3.9, ELAP: 33:48, ETA: 4:47 - loss: 23.7345\n",
            "8884/10000 = 88.8%, SPS: 3.9, ELAP: 33:48, ETA: 4:47 - loss: 24.3407\n",
            "8885/10000 = 88.8%, SPS: 3.9, ELAP: 33:48, ETA: 4:47 - loss: 23.4511\n",
            "8886/10000 = 88.9%, SPS: 3.9, ELAP: 33:49, ETA: 4:47 - loss: 23.9781\n",
            "8887/10000 = 88.9%, SPS: 3.9, ELAP: 33:49, ETA: 4:46 - loss: 24.1332\n",
            "8888/10000 = 88.9%, SPS: 3.9, ELAP: 33:49, ETA: 4:46 - loss: 24.0236\n",
            "8889/10000 = 88.9%, SPS: 3.9, ELAP: 33:49, ETA: 4:46 - loss: 24.2651\n",
            "8890/10000 = 88.9%, SPS: 3.9, ELAP: 33:49, ETA: 4:46 - loss: 23.6780\n",
            "8891/10000 = 88.9%, SPS: 3.9, ELAP: 33:50, ETA: 4:45 - loss: 24.0379\n",
            "8892/10000 = 88.9%, SPS: 3.9, ELAP: 33:50, ETA: 4:45 - loss: 24.2102\n",
            "8893/10000 = 88.9%, SPS: 3.9, ELAP: 33:50, ETA: 4:45 - loss: 23.8534\n",
            "8894/10000 = 88.9%, SPS: 3.9, ELAP: 33:50, ETA: 4:44 - loss: 24.1372\n",
            "8895/10000 = 89.0%, SPS: 3.9, ELAP: 33:51, ETA: 4:44 - loss: 23.7322\n",
            "8896/10000 = 89.0%, SPS: 3.9, ELAP: 33:51, ETA: 4:44 - loss: 23.2381\n",
            "8897/10000 = 89.0%, SPS: 3.9, ELAP: 33:51, ETA: 4:44 - loss: 24.0028\n",
            "8898/10000 = 89.0%, SPS: 3.9, ELAP: 33:51, ETA: 4:43 - loss: 23.6164\n",
            "8899/10000 = 89.0%, SPS: 3.9, ELAP: 33:52, ETA: 4:43 - loss: 23.7733\n",
            "8900/10000 = 89.0%, SPS: 3.9, ELAP: 33:54, ETA: 4:43 - loss: 23.8055\n",
            "8901/10000 = 89.0%, SPS: 3.9, ELAP: 33:54, ETA: 4:43 - loss: 23.8853\n",
            "8902/10000 = 89.0%, SPS: 3.9, ELAP: 33:54, ETA: 4:43 - loss: 24.3065\n",
            "8903/10000 = 89.0%, SPS: 3.9, ELAP: 33:55, ETA: 4:42 - loss: 24.0517\n",
            "8904/10000 = 89.0%, SPS: 3.9, ELAP: 33:55, ETA: 4:42 - loss: 23.7119\n",
            "8905/10000 = 89.0%, SPS: 3.9, ELAP: 33:55, ETA: 4:42 - loss: 23.5919\n",
            "8906/10000 = 89.1%, SPS: 3.9, ELAP: 33:55, ETA: 4:42 - loss: 24.0612\n",
            "8907/10000 = 89.1%, SPS: 3.9, ELAP: 33:56, ETA: 4:41 - loss: 23.9908\n",
            "8908/10000 = 89.1%, SPS: 3.9, ELAP: 33:56, ETA: 4:41 - loss: 24.5916\n",
            "8909/10000 = 89.1%, SPS: 3.9, ELAP: 33:56, ETA: 4:41 - loss: 24.4154\n",
            "8910/10000 = 89.1%, SPS: 3.9, ELAP: 33:56, ETA: 4:41 - loss: 24.0441\n",
            "8911/10000 = 89.1%, SPS: 3.9, ELAP: 33:56, ETA: 4:40 - loss: 23.9818\n",
            "8912/10000 = 89.1%, SPS: 3.9, ELAP: 33:57, ETA: 4:40 - loss: 24.1991\n",
            "8913/10000 = 89.1%, SPS: 3.9, ELAP: 33:57, ETA: 4:40 - loss: 23.5074\n",
            "8914/10000 = 89.1%, SPS: 3.9, ELAP: 33:57, ETA: 4:40 - loss: 24.1241\n",
            "8915/10000 = 89.2%, SPS: 3.9, ELAP: 33:57, ETA: 4:39 - loss: 23.6659\n",
            "8916/10000 = 89.2%, SPS: 3.9, ELAP: 33:58, ETA: 4:39 - loss: 23.5538\n",
            "8917/10000 = 89.2%, SPS: 3.9, ELAP: 33:58, ETA: 4:39 - loss: 24.2099\n",
            "8918/10000 = 89.2%, SPS: 3.9, ELAP: 33:58, ETA: 4:38 - loss: 24.2125\n",
            "8919/10000 = 89.2%, SPS: 3.9, ELAP: 33:58, ETA: 4:38 - loss: 23.3343\n",
            "8920/10000 = 89.2%, SPS: 3.9, ELAP: 33:58, ETA: 4:38 - loss: 24.2943\n",
            "8921/10000 = 89.2%, SPS: 3.9, ELAP: 33:59, ETA: 4:38 - loss: 23.3133\n",
            "8922/10000 = 89.2%, SPS: 3.9, ELAP: 33:59, ETA: 4:37 - loss: 24.1305\n",
            "8923/10000 = 89.2%, SPS: 3.9, ELAP: 33:59, ETA: 4:37 - loss: 24.2286\n",
            "8924/10000 = 89.2%, SPS: 3.9, ELAP: 33:59, ETA: 4:37 - loss: 23.8315\n",
            "8925/10000 = 89.2%, SPS: 3.9, ELAP: 34:00, ETA: 4:37 - loss: 24.1269\n",
            "8926/10000 = 89.3%, SPS: 3.9, ELAP: 34:00, ETA: 4:36 - loss: 23.9866\n",
            "8927/10000 = 89.3%, SPS: 3.9, ELAP: 34:00, ETA: 4:36 - loss: 23.8451\n",
            "8928/10000 = 89.3%, SPS: 3.9, ELAP: 34:00, ETA: 4:36 - loss: 23.3576\n",
            "8929/10000 = 89.3%, SPS: 3.9, ELAP: 34:01, ETA: 4:36 - loss: 24.0153\n",
            "8930/10000 = 89.3%, SPS: 3.9, ELAP: 34:01, ETA: 4:35 - loss: 23.6615\n",
            "8931/10000 = 89.3%, SPS: 3.9, ELAP: 34:01, ETA: 4:35 - loss: 23.8032\n",
            "8932/10000 = 89.3%, SPS: 3.9, ELAP: 34:01, ETA: 4:35 - loss: 23.7986\n",
            "8933/10000 = 89.3%, SPS: 3.9, ELAP: 34:02, ETA: 4:35 - loss: 23.7000\n",
            "8934/10000 = 89.3%, SPS: 3.9, ELAP: 34:02, ETA: 4:34 - loss: 23.9992\n",
            "8935/10000 = 89.3%, SPS: 3.9, ELAP: 34:02, ETA: 4:34 - loss: 24.0146\n",
            "8936/10000 = 89.4%, SPS: 3.9, ELAP: 34:02, ETA: 4:34 - loss: 23.1706\n",
            "8937/10000 = 89.4%, SPS: 3.9, ELAP: 34:03, ETA: 4:34 - loss: 23.4382\n",
            "8938/10000 = 89.4%, SPS: 3.9, ELAP: 34:03, ETA: 4:33 - loss: 23.9975\n",
            "8939/10000 = 89.4%, SPS: 3.9, ELAP: 34:03, ETA: 4:33 - loss: 23.7548\n",
            "8940/10000 = 89.4%, SPS: 3.9, ELAP: 34:03, ETA: 4:33 - loss: 23.6193\n",
            "8941/10000 = 89.4%, SPS: 3.9, ELAP: 34:04, ETA: 4:33 - loss: 24.0312\n",
            "8942/10000 = 89.4%, SPS: 3.9, ELAP: 34:04, ETA: 4:32 - loss: 23.7765\n",
            "8943/10000 = 89.4%, SPS: 3.9, ELAP: 34:04, ETA: 4:32 - loss: 24.0597\n",
            "8944/10000 = 89.4%, SPS: 3.9, ELAP: 34:04, ETA: 4:32 - loss: 23.5017\n",
            "8945/10000 = 89.5%, SPS: 3.9, ELAP: 34:05, ETA: 4:31 - loss: 24.1451\n",
            "8946/10000 = 89.5%, SPS: 3.9, ELAP: 34:05, ETA: 4:31 - loss: 23.3163\n",
            "8947/10000 = 89.5%, SPS: 3.9, ELAP: 34:05, ETA: 4:31 - loss: 23.3001\n",
            "8948/10000 = 89.5%, SPS: 3.9, ELAP: 34:05, ETA: 4:31 - loss: 23.9279\n",
            "8949/10000 = 89.5%, SPS: 3.9, ELAP: 34:05, ETA: 4:30 - loss: 23.8544\n",
            "8950/10000 = 89.5%, SPS: 3.9, ELAP: 34:06, ETA: 4:30 - loss: 24.0700\n",
            "8951/10000 = 89.5%, SPS: 3.9, ELAP: 34:06, ETA: 4:30 - loss: 23.8608\n",
            "8952/10000 = 89.5%, SPS: 3.9, ELAP: 34:06, ETA: 4:30 - loss: 24.3826\n",
            "8953/10000 = 89.5%, SPS: 3.9, ELAP: 34:06, ETA: 4:29 - loss: 23.6091\n",
            "8954/10000 = 89.5%, SPS: 3.9, ELAP: 34:07, ETA: 4:29 - loss: 24.0659\n",
            "8955/10000 = 89.5%, SPS: 3.9, ELAP: 34:07, ETA: 4:29 - loss: 24.0231\n",
            "8956/10000 = 89.6%, SPS: 3.9, ELAP: 34:07, ETA: 4:29 - loss: 24.4650\n",
            "8957/10000 = 89.6%, SPS: 3.9, ELAP: 34:07, ETA: 4:28 - loss: 24.1842\n",
            "8958/10000 = 89.6%, SPS: 3.9, ELAP: 34:08, ETA: 4:28 - loss: 24.1054\n",
            "8959/10000 = 89.6%, SPS: 3.9, ELAP: 34:08, ETA: 4:28 - loss: 23.9047\n",
            "8960/10000 = 89.6%, SPS: 3.9, ELAP: 34:08, ETA: 4:28 - loss: 24.4985\n",
            "8961/10000 = 89.6%, SPS: 3.9, ELAP: 34:08, ETA: 4:27 - loss: 23.9101\n",
            "8962/10000 = 89.6%, SPS: 3.9, ELAP: 34:08, ETA: 4:27 - loss: 23.5395\n",
            "8963/10000 = 89.6%, SPS: 3.9, ELAP: 34:09, ETA: 4:27 - loss: 23.9693\n",
            "8964/10000 = 89.6%, SPS: 3.9, ELAP: 34:09, ETA: 4:27 - loss: 23.5609\n",
            "8965/10000 = 89.7%, SPS: 3.9, ELAP: 34:09, ETA: 4:26 - loss: 24.1937\n",
            "8966/10000 = 89.7%, SPS: 3.9, ELAP: 34:09, ETA: 4:26 - loss: 24.0189\n",
            "8967/10000 = 89.7%, SPS: 3.9, ELAP: 34:10, ETA: 4:26 - loss: 23.8541\n",
            "8968/10000 = 89.7%, SPS: 3.9, ELAP: 34:10, ETA: 4:25 - loss: 23.7042\n",
            "8969/10000 = 89.7%, SPS: 3.9, ELAP: 34:10, ETA: 4:25 - loss: 24.2210\n",
            "8970/10000 = 89.7%, SPS: 3.9, ELAP: 34:10, ETA: 4:25 - loss: 23.9305\n",
            "8971/10000 = 89.7%, SPS: 3.9, ELAP: 34:10, ETA: 4:25 - loss: 23.1797\n",
            "8972/10000 = 89.7%, SPS: 3.9, ELAP: 34:11, ETA: 4:24 - loss: 24.0598\n",
            "8973/10000 = 89.7%, SPS: 3.9, ELAP: 34:11, ETA: 4:24 - loss: 24.3910\n",
            "8974/10000 = 89.7%, SPS: 3.9, ELAP: 34:11, ETA: 4:24 - loss: 23.7515\n",
            "8975/10000 = 89.8%, SPS: 3.9, ELAP: 34:11, ETA: 4:24 - loss: 23.6083\n",
            "8976/10000 = 89.8%, SPS: 3.9, ELAP: 34:12, ETA: 4:23 - loss: 24.1326\n",
            "8977/10000 = 89.8%, SPS: 3.9, ELAP: 34:12, ETA: 4:23 - loss: 23.1753\n",
            "8978/10000 = 89.8%, SPS: 3.9, ELAP: 34:12, ETA: 4:23 - loss: 23.8600\n",
            "8979/10000 = 89.8%, SPS: 3.9, ELAP: 34:12, ETA: 4:23 - loss: 24.2073\n",
            "8980/10000 = 89.8%, SPS: 3.9, ELAP: 34:13, ETA: 4:22 - loss: 23.7012\n",
            "8981/10000 = 89.8%, SPS: 3.9, ELAP: 34:13, ETA: 4:22 - loss: 24.1390\n",
            "8982/10000 = 89.8%, SPS: 3.9, ELAP: 34:13, ETA: 4:22 - loss: 23.4545\n",
            "8983/10000 = 89.8%, SPS: 3.9, ELAP: 34:13, ETA: 4:22 - loss: 23.7282\n",
            "8984/10000 = 89.8%, SPS: 3.9, ELAP: 34:13, ETA: 4:21 - loss: 24.1863\n",
            "8985/10000 = 89.8%, SPS: 3.9, ELAP: 34:14, ETA: 4:21 - loss: 23.4933\n",
            "8986/10000 = 89.9%, SPS: 3.9, ELAP: 34:14, ETA: 4:21 - loss: 23.6148\n",
            "8987/10000 = 89.9%, SPS: 3.9, ELAP: 34:14, ETA: 4:21 - loss: 24.1830\n",
            "8988/10000 = 89.9%, SPS: 3.9, ELAP: 34:14, ETA: 4:20 - loss: 23.8502\n",
            "8989/10000 = 89.9%, SPS: 3.9, ELAP: 34:15, ETA: 4:20 - loss: 24.1086\n",
            "8990/10000 = 89.9%, SPS: 3.9, ELAP: 34:15, ETA: 4:20 - loss: 24.0775\n",
            "8991/10000 = 89.9%, SPS: 3.9, ELAP: 34:15, ETA: 4:19 - loss: 23.8886\n",
            "8992/10000 = 89.9%, SPS: 3.9, ELAP: 34:15, ETA: 4:19 - loss: 24.0962\n",
            "8993/10000 = 89.9%, SPS: 3.9, ELAP: 34:15, ETA: 4:19 - loss: 23.4640\n",
            "8994/10000 = 89.9%, SPS: 3.9, ELAP: 34:16, ETA: 4:19 - loss: 23.5420\n",
            "8995/10000 = 90.0%, SPS: 3.9, ELAP: 34:16, ETA: 4:18 - loss: 22.9139\n",
            "8996/10000 = 90.0%, SPS: 3.9, ELAP: 34:16, ETA: 4:18 - loss: 24.2345\n",
            "8997/10000 = 90.0%, SPS: 3.9, ELAP: 34:16, ETA: 4:18 - loss: 24.3438\n",
            "8998/10000 = 90.0%, SPS: 3.9, ELAP: 34:17, ETA: 4:18 - loss: 23.6604\n",
            "8999/10000 = 90.0%, SPS: 3.9, ELAP: 34:17, ETA: 4:17 - loss: 23.2951\n",
            "9000/10000 = 90.0%, SPS: 3.9, ELAP: 34:19, ETA: 4:17 - loss: 23.6026\n",
            "9001/10000 = 90.0%, SPS: 3.9, ELAP: 34:19, ETA: 4:17 - loss: 24.0152\n",
            "9002/10000 = 90.0%, SPS: 3.9, ELAP: 34:20, ETA: 4:17 - loss: 23.9265\n",
            "9003/10000 = 90.0%, SPS: 3.9, ELAP: 34:20, ETA: 4:17 - loss: 24.0072\n",
            "9004/10000 = 90.0%, SPS: 3.9, ELAP: 34:20, ETA: 4:16 - loss: 23.3260\n",
            "9005/10000 = 90.0%, SPS: 3.9, ELAP: 34:20, ETA: 4:16 - loss: 24.0623\n",
            "9006/10000 = 90.1%, SPS: 3.9, ELAP: 34:21, ETA: 4:16 - loss: 23.8950\n",
            "9007/10000 = 90.1%, SPS: 3.9, ELAP: 34:21, ETA: 4:16 - loss: 23.6160\n",
            "9008/10000 = 90.1%, SPS: 3.9, ELAP: 34:21, ETA: 4:15 - loss: 23.7463\n",
            "9009/10000 = 90.1%, SPS: 3.9, ELAP: 34:21, ETA: 4:15 - loss: 23.7042\n",
            "9010/10000 = 90.1%, SPS: 3.9, ELAP: 34:21, ETA: 4:15 - loss: 24.1389\n",
            "9011/10000 = 90.1%, SPS: 3.9, ELAP: 34:22, ETA: 4:15 - loss: 24.1895\n",
            "9012/10000 = 90.1%, SPS: 3.9, ELAP: 34:22, ETA: 4:14 - loss: 24.1627\n",
            "9013/10000 = 90.1%, SPS: 3.9, ELAP: 34:22, ETA: 4:14 - loss: 24.1518\n",
            "9014/10000 = 90.1%, SPS: 3.9, ELAP: 34:22, ETA: 4:14 - loss: 23.6964\n",
            "9015/10000 = 90.2%, SPS: 3.9, ELAP: 34:23, ETA: 4:13 - loss: 23.9314\n",
            "9016/10000 = 90.2%, SPS: 3.9, ELAP: 34:23, ETA: 4:13 - loss: 24.1196\n",
            "9017/10000 = 90.2%, SPS: 3.9, ELAP: 34:23, ETA: 4:13 - loss: 24.6248\n",
            "9018/10000 = 90.2%, SPS: 3.9, ELAP: 34:23, ETA: 4:13 - loss: 23.9842\n",
            "9019/10000 = 90.2%, SPS: 3.9, ELAP: 34:24, ETA: 4:12 - loss: 22.8133\n",
            "9020/10000 = 90.2%, SPS: 3.9, ELAP: 34:24, ETA: 4:12 - loss: 23.6469\n",
            "9021/10000 = 90.2%, SPS: 3.9, ELAP: 34:24, ETA: 4:12 - loss: 24.2719\n",
            "9022/10000 = 90.2%, SPS: 3.9, ELAP: 34:24, ETA: 4:12 - loss: 24.6152\n",
            "9023/10000 = 90.2%, SPS: 3.9, ELAP: 34:24, ETA: 4:11 - loss: 24.1220\n",
            "9024/10000 = 90.2%, SPS: 3.9, ELAP: 34:25, ETA: 4:11 - loss: 23.7246\n",
            "9025/10000 = 90.2%, SPS: 3.9, ELAP: 34:25, ETA: 4:11 - loss: 23.4528\n",
            "9026/10000 = 90.3%, SPS: 3.9, ELAP: 34:25, ETA: 4:11 - loss: 23.9553\n",
            "9027/10000 = 90.3%, SPS: 3.9, ELAP: 34:25, ETA: 4:10 - loss: 24.2210\n",
            "9028/10000 = 90.3%, SPS: 3.9, ELAP: 34:26, ETA: 4:10 - loss: 23.4109\n",
            "9029/10000 = 90.3%, SPS: 3.9, ELAP: 34:26, ETA: 4:10 - loss: 23.6457\n",
            "9030/10000 = 90.3%, SPS: 3.9, ELAP: 34:26, ETA: 4:10 - loss: 23.8229\n",
            "9031/10000 = 90.3%, SPS: 3.9, ELAP: 34:26, ETA: 4:09 - loss: 23.4587\n",
            "9032/10000 = 90.3%, SPS: 3.9, ELAP: 34:27, ETA: 4:09 - loss: 23.8527\n",
            "9033/10000 = 90.3%, SPS: 3.9, ELAP: 34:27, ETA: 4:09 - loss: 24.0853\n",
            "9034/10000 = 90.3%, SPS: 3.9, ELAP: 34:27, ETA: 4:09 - loss: 23.5926\n",
            "9035/10000 = 90.3%, SPS: 3.9, ELAP: 34:27, ETA: 4:08 - loss: 23.3587\n",
            "9036/10000 = 90.4%, SPS: 3.9, ELAP: 34:28, ETA: 4:08 - loss: 24.5054\n",
            "9037/10000 = 90.4%, SPS: 3.9, ELAP: 34:28, ETA: 4:08 - loss: 23.4149\n",
            "9038/10000 = 90.4%, SPS: 3.9, ELAP: 34:28, ETA: 4:08 - loss: 23.9885\n",
            "9039/10000 = 90.4%, SPS: 3.9, ELAP: 34:28, ETA: 4:07 - loss: 24.4801\n",
            "9040/10000 = 90.4%, SPS: 3.9, ELAP: 34:29, ETA: 4:07 - loss: 24.1571\n",
            "9041/10000 = 90.4%, SPS: 3.9, ELAP: 34:29, ETA: 4:07 - loss: 23.3296\n",
            "9042/10000 = 90.4%, SPS: 3.9, ELAP: 34:29, ETA: 4:06 - loss: 24.2135\n",
            "9043/10000 = 90.4%, SPS: 3.9, ELAP: 34:29, ETA: 4:06 - loss: 23.3741\n",
            "9044/10000 = 90.4%, SPS: 3.9, ELAP: 34:30, ETA: 4:06 - loss: 23.2096\n",
            "9045/10000 = 90.5%, SPS: 3.9, ELAP: 34:30, ETA: 4:06 - loss: 23.6508\n",
            "9046/10000 = 90.5%, SPS: 3.9, ELAP: 34:30, ETA: 4:05 - loss: 23.4477\n",
            "9047/10000 = 90.5%, SPS: 3.9, ELAP: 34:30, ETA: 4:05 - loss: 23.0516\n",
            "9048/10000 = 90.5%, SPS: 3.9, ELAP: 34:30, ETA: 4:05 - loss: 23.7889\n",
            "9049/10000 = 90.5%, SPS: 3.9, ELAP: 34:31, ETA: 4:05 - loss: 23.1774\n",
            "9050/10000 = 90.5%, SPS: 3.9, ELAP: 34:31, ETA: 4:04 - loss: 24.0807\n",
            "9051/10000 = 90.5%, SPS: 3.9, ELAP: 34:31, ETA: 4:04 - loss: 23.5499\n",
            "9052/10000 = 90.5%, SPS: 3.9, ELAP: 34:31, ETA: 4:04 - loss: 22.9044\n",
            "9053/10000 = 90.5%, SPS: 3.9, ELAP: 34:32, ETA: 4:04 - loss: 24.0013\n",
            "9054/10000 = 90.5%, SPS: 3.9, ELAP: 34:32, ETA: 4:03 - loss: 23.5604\n",
            "9055/10000 = 90.5%, SPS: 3.9, ELAP: 34:32, ETA: 4:03 - loss: 23.8037\n",
            "9056/10000 = 90.6%, SPS: 3.9, ELAP: 34:32, ETA: 4:03 - loss: 23.8107\n",
            "9057/10000 = 90.6%, SPS: 3.9, ELAP: 34:33, ETA: 4:03 - loss: 23.9820\n",
            "9058/10000 = 90.6%, SPS: 3.9, ELAP: 34:33, ETA: 4:02 - loss: 24.5779\n",
            "9059/10000 = 90.6%, SPS: 3.9, ELAP: 34:33, ETA: 4:02 - loss: 23.7055\n",
            "9060/10000 = 90.6%, SPS: 3.9, ELAP: 34:33, ETA: 4:02 - loss: 23.8562\n",
            "9061/10000 = 90.6%, SPS: 3.9, ELAP: 34:33, ETA: 4:02 - loss: 23.5012\n",
            "9062/10000 = 90.6%, SPS: 3.9, ELAP: 34:34, ETA: 4:01 - loss: 23.9282\n",
            "9063/10000 = 90.6%, SPS: 3.9, ELAP: 34:34, ETA: 4:01 - loss: 24.0864\n",
            "9064/10000 = 90.6%, SPS: 3.9, ELAP: 34:34, ETA: 4:01 - loss: 24.1661\n",
            "9065/10000 = 90.7%, SPS: 3.9, ELAP: 34:34, ETA: 4:00 - loss: 24.0923\n",
            "9066/10000 = 90.7%, SPS: 3.9, ELAP: 34:35, ETA: 4:00 - loss: 24.2096\n",
            "9067/10000 = 90.7%, SPS: 3.9, ELAP: 34:35, ETA: 4:00 - loss: 23.4497\n",
            "9068/10000 = 90.7%, SPS: 3.9, ELAP: 34:35, ETA: 4:00 - loss: 23.7335\n",
            "9069/10000 = 90.7%, SPS: 3.9, ELAP: 34:35, ETA: 3:59 - loss: 23.4133\n",
            "9070/10000 = 90.7%, SPS: 3.9, ELAP: 34:35, ETA: 3:59 - loss: 23.8300\n",
            "9071/10000 = 90.7%, SPS: 3.9, ELAP: 34:36, ETA: 3:59 - loss: 24.4671\n",
            "9072/10000 = 90.7%, SPS: 3.9, ELAP: 34:36, ETA: 3:59 - loss: 23.6985\n",
            "9073/10000 = 90.7%, SPS: 3.9, ELAP: 34:36, ETA: 3:58 - loss: 23.6075\n",
            "9074/10000 = 90.7%, SPS: 3.9, ELAP: 34:36, ETA: 3:58 - loss: 24.2779\n",
            "9075/10000 = 90.8%, SPS: 3.9, ELAP: 34:37, ETA: 3:58 - loss: 23.8771\n",
            "9076/10000 = 90.8%, SPS: 3.9, ELAP: 34:37, ETA: 3:58 - loss: 23.2486\n",
            "9077/10000 = 90.8%, SPS: 3.9, ELAP: 34:37, ETA: 3:57 - loss: 23.5598\n",
            "9078/10000 = 90.8%, SPS: 3.9, ELAP: 34:37, ETA: 3:57 - loss: 24.1852\n",
            "9079/10000 = 90.8%, SPS: 3.9, ELAP: 34:38, ETA: 3:57 - loss: 24.1885\n",
            "9080/10000 = 90.8%, SPS: 3.9, ELAP: 34:38, ETA: 3:57 - loss: 23.6198\n",
            "9081/10000 = 90.8%, SPS: 3.9, ELAP: 34:38, ETA: 3:56 - loss: 23.9233\n",
            "9082/10000 = 90.8%, SPS: 3.9, ELAP: 34:38, ETA: 3:56 - loss: 23.4631\n",
            "9083/10000 = 90.8%, SPS: 3.9, ELAP: 34:38, ETA: 3:56 - loss: 24.0665\n",
            "9084/10000 = 90.8%, SPS: 3.9, ELAP: 34:39, ETA: 3:56 - loss: 23.9379\n",
            "9085/10000 = 90.8%, SPS: 3.9, ELAP: 34:39, ETA: 3:55 - loss: 23.5197\n",
            "9086/10000 = 90.9%, SPS: 3.9, ELAP: 34:39, ETA: 3:55 - loss: 23.8027\n",
            "9087/10000 = 90.9%, SPS: 3.9, ELAP: 34:39, ETA: 3:55 - loss: 24.2620\n",
            "9088/10000 = 90.9%, SPS: 3.9, ELAP: 34:40, ETA: 3:54 - loss: 23.8478\n",
            "9089/10000 = 90.9%, SPS: 3.9, ELAP: 34:40, ETA: 3:54 - loss: 23.7125\n",
            "9090/10000 = 90.9%, SPS: 3.9, ELAP: 34:40, ETA: 3:54 - loss: 24.0196\n",
            "9091/10000 = 90.9%, SPS: 3.9, ELAP: 34:40, ETA: 3:54 - loss: 24.1405\n",
            "9092/10000 = 90.9%, SPS: 3.9, ELAP: 34:40, ETA: 3:53 - loss: 23.5528\n",
            "9093/10000 = 90.9%, SPS: 3.9, ELAP: 34:41, ETA: 3:53 - loss: 23.9663\n",
            "9094/10000 = 90.9%, SPS: 3.9, ELAP: 34:41, ETA: 3:53 - loss: 24.0009\n",
            "9095/10000 = 91.0%, SPS: 3.9, ELAP: 34:41, ETA: 3:53 - loss: 23.9623\n",
            "9096/10000 = 91.0%, SPS: 3.9, ELAP: 34:41, ETA: 3:52 - loss: 24.2068\n",
            "9097/10000 = 91.0%, SPS: 3.9, ELAP: 34:42, ETA: 3:52 - loss: 23.8625\n",
            "9098/10000 = 91.0%, SPS: 3.9, ELAP: 34:42, ETA: 3:52 - loss: 23.3365\n",
            "9099/10000 = 91.0%, SPS: 3.9, ELAP: 34:42, ETA: 3:52 - loss: 24.1319\n",
            "9100/10000 = 91.0%, SPS: 3.9, ELAP: 34:44, ETA: 3:52 - loss: 23.8079\n",
            "9101/10000 = 91.0%, SPS: 3.9, ELAP: 34:45, ETA: 3:51 - loss: 23.4956\n",
            "9102/10000 = 91.0%, SPS: 3.9, ELAP: 34:45, ETA: 3:51 - loss: 23.9667\n",
            "9103/10000 = 91.0%, SPS: 3.9, ELAP: 34:45, ETA: 3:51 - loss: 23.4846\n",
            "9104/10000 = 91.0%, SPS: 3.9, ELAP: 34:45, ETA: 3:51 - loss: 24.5725\n",
            "9105/10000 = 91.0%, SPS: 3.9, ELAP: 34:46, ETA: 3:50 - loss: 24.3196\n",
            "9106/10000 = 91.1%, SPS: 3.9, ELAP: 34:46, ETA: 3:50 - loss: 23.8751\n",
            "9107/10000 = 91.1%, SPS: 3.9, ELAP: 34:46, ETA: 3:50 - loss: 23.7392\n",
            "9108/10000 = 91.1%, SPS: 3.9, ELAP: 34:46, ETA: 3:50 - loss: 23.2154\n",
            "9109/10000 = 91.1%, SPS: 3.9, ELAP: 34:47, ETA: 3:49 - loss: 24.4089\n",
            "9110/10000 = 91.1%, SPS: 3.9, ELAP: 34:47, ETA: 3:49 - loss: 23.6067\n",
            "9111/10000 = 91.1%, SPS: 3.9, ELAP: 34:47, ETA: 3:49 - loss: 24.5200\n",
            "9112/10000 = 91.1%, SPS: 3.9, ELAP: 34:47, ETA: 3:48 - loss: 24.3074\n",
            "9113/10000 = 91.1%, SPS: 3.9, ELAP: 34:47, ETA: 3:48 - loss: 24.1297\n",
            "9114/10000 = 91.1%, SPS: 3.9, ELAP: 34:48, ETA: 3:48 - loss: 23.1434\n",
            "9115/10000 = 91.2%, SPS: 3.9, ELAP: 34:48, ETA: 3:48 - loss: 24.9435\n",
            "9116/10000 = 91.2%, SPS: 3.9, ELAP: 34:48, ETA: 3:47 - loss: 23.6790\n",
            "9117/10000 = 91.2%, SPS: 3.9, ELAP: 34:48, ETA: 3:47 - loss: 24.6635\n",
            "9118/10000 = 91.2%, SPS: 3.9, ELAP: 34:49, ETA: 3:47 - loss: 23.8691\n",
            "9119/10000 = 91.2%, SPS: 3.9, ELAP: 34:49, ETA: 3:47 - loss: 22.8920\n",
            "9120/10000 = 91.2%, SPS: 3.9, ELAP: 34:49, ETA: 3:46 - loss: 24.4167\n",
            "9121/10000 = 91.2%, SPS: 3.9, ELAP: 34:49, ETA: 3:46 - loss: 24.0382\n",
            "9122/10000 = 91.2%, SPS: 3.9, ELAP: 34:50, ETA: 3:46 - loss: 24.1939\n",
            "9123/10000 = 91.2%, SPS: 3.9, ELAP: 34:50, ETA: 3:46 - loss: 24.5406\n",
            "9124/10000 = 91.2%, SPS: 3.9, ELAP: 34:50, ETA: 3:45 - loss: 24.3439\n",
            "9125/10000 = 91.2%, SPS: 3.9, ELAP: 34:50, ETA: 3:45 - loss: 23.5848\n",
            "9126/10000 = 91.3%, SPS: 3.9, ELAP: 34:50, ETA: 3:45 - loss: 23.5547\n",
            "9127/10000 = 91.3%, SPS: 3.9, ELAP: 34:51, ETA: 3:45 - loss: 23.8616\n",
            "9128/10000 = 91.3%, SPS: 3.9, ELAP: 34:51, ETA: 3:44 - loss: 23.2973\n",
            "9129/10000 = 91.3%, SPS: 3.9, ELAP: 34:51, ETA: 3:44 - loss: 23.7098\n",
            "9130/10000 = 91.3%, SPS: 3.9, ELAP: 34:51, ETA: 3:44 - loss: 24.0240\n",
            "9131/10000 = 91.3%, SPS: 3.9, ELAP: 34:52, ETA: 3:44 - loss: 23.5846\n",
            "9132/10000 = 91.3%, SPS: 3.9, ELAP: 34:52, ETA: 3:43 - loss: 22.9162\n",
            "9133/10000 = 91.3%, SPS: 3.9, ELAP: 34:52, ETA: 3:43 - loss: 23.3519\n",
            "9134/10000 = 91.3%, SPS: 3.9, ELAP: 34:52, ETA: 3:43 - loss: 23.5797\n",
            "9135/10000 = 91.3%, SPS: 3.9, ELAP: 34:53, ETA: 3:43 - loss: 23.2371\n",
            "9136/10000 = 91.4%, SPS: 3.9, ELAP: 34:53, ETA: 3:42 - loss: 23.7335\n",
            "9137/10000 = 91.4%, SPS: 3.9, ELAP: 34:53, ETA: 3:42 - loss: 24.3141\n",
            "9138/10000 = 91.4%, SPS: 3.9, ELAP: 34:53, ETA: 3:42 - loss: 24.1713\n",
            "9139/10000 = 91.4%, SPS: 3.9, ELAP: 34:54, ETA: 3:41 - loss: 23.4978\n",
            "9140/10000 = 91.4%, SPS: 3.9, ELAP: 34:54, ETA: 3:41 - loss: 23.7718\n",
            "9141/10000 = 91.4%, SPS: 3.9, ELAP: 34:54, ETA: 3:41 - loss: 24.0599\n",
            "9142/10000 = 91.4%, SPS: 3.9, ELAP: 34:54, ETA: 3:41 - loss: 23.8239\n",
            "9143/10000 = 91.4%, SPS: 3.9, ELAP: 34:55, ETA: 3:40 - loss: 23.8924\n",
            "9144/10000 = 91.4%, SPS: 3.9, ELAP: 34:55, ETA: 3:40 - loss: 24.0304\n",
            "9145/10000 = 91.5%, SPS: 3.9, ELAP: 34:55, ETA: 3:40 - loss: 24.3105\n",
            "9146/10000 = 91.5%, SPS: 3.9, ELAP: 34:55, ETA: 3:40 - loss: 23.7183\n",
            "9147/10000 = 91.5%, SPS: 3.9, ELAP: 34:56, ETA: 3:39 - loss: 24.0020\n",
            "9148/10000 = 91.5%, SPS: 3.9, ELAP: 34:56, ETA: 3:39 - loss: 24.2923\n",
            "9149/10000 = 91.5%, SPS: 3.9, ELAP: 34:56, ETA: 3:39 - loss: 24.5409\n",
            "9150/10000 = 91.5%, SPS: 3.9, ELAP: 34:56, ETA: 3:39 - loss: 23.9047\n",
            "9151/10000 = 91.5%, SPS: 3.9, ELAP: 34:56, ETA: 3:38 - loss: 24.2352\n",
            "9152/10000 = 91.5%, SPS: 3.9, ELAP: 34:57, ETA: 3:38 - loss: 23.8722\n",
            "9153/10000 = 91.5%, SPS: 3.9, ELAP: 34:57, ETA: 3:38 - loss: 23.9805\n",
            "9154/10000 = 91.5%, SPS: 3.9, ELAP: 34:57, ETA: 3:38 - loss: 23.2182\n",
            "9155/10000 = 91.5%, SPS: 3.9, ELAP: 34:57, ETA: 3:37 - loss: 23.7936\n",
            "9156/10000 = 91.6%, SPS: 3.9, ELAP: 34:58, ETA: 3:37 - loss: 23.2925\n",
            "9157/10000 = 91.6%, SPS: 3.9, ELAP: 34:58, ETA: 3:37 - loss: 23.7891\n",
            "9158/10000 = 91.6%, SPS: 3.9, ELAP: 34:58, ETA: 3:37 - loss: 23.9554\n",
            "9159/10000 = 91.6%, SPS: 3.9, ELAP: 34:58, ETA: 3:36 - loss: 23.4278\n",
            "9160/10000 = 91.6%, SPS: 3.9, ELAP: 34:58, ETA: 3:36 - loss: 24.0617\n",
            "9161/10000 = 91.6%, SPS: 3.9, ELAP: 34:59, ETA: 3:36 - loss: 23.6626\n",
            "9162/10000 = 91.6%, SPS: 3.9, ELAP: 34:59, ETA: 3:35 - loss: 23.9803\n",
            "9163/10000 = 91.6%, SPS: 3.9, ELAP: 34:59, ETA: 3:35 - loss: 24.4006\n",
            "9164/10000 = 91.6%, SPS: 3.9, ELAP: 34:59, ETA: 3:35 - loss: 23.8797\n",
            "9165/10000 = 91.7%, SPS: 3.9, ELAP: 35:00, ETA: 3:35 - loss: 24.3372\n",
            "9166/10000 = 91.7%, SPS: 3.9, ELAP: 35:00, ETA: 3:34 - loss: 24.1029\n",
            "9167/10000 = 91.7%, SPS: 3.9, ELAP: 35:00, ETA: 3:34 - loss: 23.2185\n",
            "9168/10000 = 91.7%, SPS: 3.9, ELAP: 35:00, ETA: 3:34 - loss: 23.8632\n",
            "9169/10000 = 91.7%, SPS: 3.9, ELAP: 35:01, ETA: 3:34 - loss: 23.6040\n",
            "9170/10000 = 91.7%, SPS: 3.9, ELAP: 35:01, ETA: 3:33 - loss: 24.7072\n",
            "9171/10000 = 91.7%, SPS: 3.9, ELAP: 35:01, ETA: 3:33 - loss: 23.6549\n",
            "9172/10000 = 91.7%, SPS: 3.9, ELAP: 35:01, ETA: 3:33 - loss: 23.7909\n",
            "9173/10000 = 91.7%, SPS: 3.9, ELAP: 35:01, ETA: 3:33 - loss: 23.4985\n",
            "9174/10000 = 91.7%, SPS: 3.9, ELAP: 35:02, ETA: 3:32 - loss: 23.4128\n",
            "9175/10000 = 91.8%, SPS: 3.9, ELAP: 35:02, ETA: 3:32 - loss: 23.3934\n",
            "9176/10000 = 91.8%, SPS: 3.9, ELAP: 35:02, ETA: 3:32 - loss: 24.0089\n",
            "9177/10000 = 91.8%, SPS: 3.9, ELAP: 35:02, ETA: 3:32 - loss: 23.9843\n",
            "9178/10000 = 91.8%, SPS: 3.9, ELAP: 35:03, ETA: 3:31 - loss: 23.5205\n",
            "9179/10000 = 91.8%, SPS: 3.9, ELAP: 35:03, ETA: 3:31 - loss: 23.9124\n",
            "9180/10000 = 91.8%, SPS: 3.9, ELAP: 35:03, ETA: 3:31 - loss: 24.2704\n",
            "9181/10000 = 91.8%, SPS: 3.9, ELAP: 35:03, ETA: 3:31 - loss: 24.1557\n",
            "9182/10000 = 91.8%, SPS: 3.9, ELAP: 35:03, ETA: 3:30 - loss: 24.5360\n",
            "9183/10000 = 91.8%, SPS: 3.9, ELAP: 35:04, ETA: 3:30 - loss: 23.4955\n",
            "9184/10000 = 91.8%, SPS: 3.9, ELAP: 35:04, ETA: 3:30 - loss: 24.2369\n",
            "9185/10000 = 91.8%, SPS: 3.9, ELAP: 35:04, ETA: 3:30 - loss: 23.2842\n",
            "9186/10000 = 91.9%, SPS: 3.9, ELAP: 35:04, ETA: 3:29 - loss: 23.6923\n",
            "9187/10000 = 91.9%, SPS: 3.9, ELAP: 35:05, ETA: 3:29 - loss: 23.9113\n",
            "9188/10000 = 91.9%, SPS: 3.9, ELAP: 35:05, ETA: 3:29 - loss: 24.6735\n",
            "9189/10000 = 91.9%, SPS: 3.9, ELAP: 35:05, ETA: 3:28 - loss: 24.1282\n",
            "9190/10000 = 91.9%, SPS: 3.9, ELAP: 35:05, ETA: 3:28 - loss: 24.1195\n",
            "9191/10000 = 91.9%, SPS: 3.9, ELAP: 35:06, ETA: 3:28 - loss: 24.3937\n",
            "9192/10000 = 91.9%, SPS: 3.9, ELAP: 35:06, ETA: 3:28 - loss: 23.8195\n",
            "9193/10000 = 91.9%, SPS: 3.9, ELAP: 35:06, ETA: 3:27 - loss: 24.0694\n",
            "9194/10000 = 91.9%, SPS: 3.9, ELAP: 35:06, ETA: 3:27 - loss: 23.4407\n",
            "9195/10000 = 92.0%, SPS: 3.9, ELAP: 35:06, ETA: 3:27 - loss: 23.8753\n",
            "9196/10000 = 92.0%, SPS: 3.9, ELAP: 35:07, ETA: 3:27 - loss: 23.6874\n",
            "9197/10000 = 92.0%, SPS: 3.9, ELAP: 35:07, ETA: 3:26 - loss: 24.1786\n",
            "9198/10000 = 92.0%, SPS: 3.9, ELAP: 35:07, ETA: 3:26 - loss: 23.8976\n",
            "9199/10000 = 92.0%, SPS: 3.9, ELAP: 35:07, ETA: 3:26 - loss: 24.3409\n",
            "9200/10000 = 92.0%, SPS: 3.9, ELAP: 35:10, ETA: 3:26 - loss: 24.0268\n",
            "9201/10000 = 92.0%, SPS: 3.9, ELAP: 35:10, ETA: 3:26 - loss: 23.9402\n",
            "9202/10000 = 92.0%, SPS: 3.9, ELAP: 35:10, ETA: 3:25 - loss: 24.1749\n",
            "9203/10000 = 92.0%, SPS: 3.9, ELAP: 35:10, ETA: 3:25 - loss: 23.5679\n",
            "9204/10000 = 92.0%, SPS: 3.9, ELAP: 35:11, ETA: 3:25 - loss: 23.4995\n",
            "9205/10000 = 92.0%, SPS: 3.9, ELAP: 35:11, ETA: 3:25 - loss: 24.0603\n",
            "9206/10000 = 92.1%, SPS: 3.9, ELAP: 35:11, ETA: 3:24 - loss: 23.9164\n",
            "9207/10000 = 92.1%, SPS: 3.9, ELAP: 35:11, ETA: 3:24 - loss: 24.2418\n",
            "9208/10000 = 92.1%, SPS: 3.9, ELAP: 35:12, ETA: 3:24 - loss: 23.5195\n",
            "9209/10000 = 92.1%, SPS: 3.9, ELAP: 35:12, ETA: 3:23 - loss: 23.8640\n",
            "9210/10000 = 92.1%, SPS: 3.9, ELAP: 35:12, ETA: 3:23 - loss: 24.0193\n",
            "9211/10000 = 92.1%, SPS: 3.9, ELAP: 35:12, ETA: 3:23 - loss: 24.1226\n",
            "9212/10000 = 92.1%, SPS: 3.9, ELAP: 35:12, ETA: 3:23 - loss: 23.4824\n",
            "9213/10000 = 92.1%, SPS: 3.9, ELAP: 35:13, ETA: 3:22 - loss: 24.1059\n",
            "9214/10000 = 92.1%, SPS: 3.9, ELAP: 35:13, ETA: 3:22 - loss: 23.8633\n",
            "9215/10000 = 92.2%, SPS: 3.9, ELAP: 35:13, ETA: 3:22 - loss: 23.6903\n",
            "9216/10000 = 92.2%, SPS: 3.9, ELAP: 35:13, ETA: 3:22 - loss: 23.7354\n",
            "9217/10000 = 92.2%, SPS: 3.9, ELAP: 35:14, ETA: 3:21 - loss: 24.3342\n",
            "9218/10000 = 92.2%, SPS: 3.9, ELAP: 35:14, ETA: 3:21 - loss: 24.6860\n",
            "9219/10000 = 92.2%, SPS: 3.9, ELAP: 35:14, ETA: 3:21 - loss: 24.2630\n",
            "9220/10000 = 92.2%, SPS: 3.9, ELAP: 35:14, ETA: 3:21 - loss: 23.6812\n",
            "9221/10000 = 92.2%, SPS: 3.9, ELAP: 35:14, ETA: 3:20 - loss: 23.9981\n",
            "9222/10000 = 92.2%, SPS: 3.9, ELAP: 35:15, ETA: 3:20 - loss: 24.1147\n",
            "9223/10000 = 92.2%, SPS: 3.9, ELAP: 35:15, ETA: 3:20 - loss: 24.3738\n",
            "9224/10000 = 92.2%, SPS: 3.9, ELAP: 35:15, ETA: 3:20 - loss: 24.4213\n",
            "9225/10000 = 92.2%, SPS: 3.9, ELAP: 35:15, ETA: 3:19 - loss: 23.8760\n",
            "9226/10000 = 92.3%, SPS: 3.9, ELAP: 35:16, ETA: 3:19 - loss: 23.8559\n",
            "9227/10000 = 92.3%, SPS: 3.9, ELAP: 35:16, ETA: 3:19 - loss: 23.9310\n",
            "9228/10000 = 92.3%, SPS: 3.9, ELAP: 35:16, ETA: 3:19 - loss: 24.1094\n",
            "9229/10000 = 92.3%, SPS: 3.9, ELAP: 35:16, ETA: 3:18 - loss: 23.8589\n",
            "9230/10000 = 92.3%, SPS: 3.9, ELAP: 35:17, ETA: 3:18 - loss: 23.6221\n",
            "9231/10000 = 92.3%, SPS: 3.9, ELAP: 35:17, ETA: 3:18 - loss: 23.6429\n",
            "9232/10000 = 92.3%, SPS: 3.9, ELAP: 35:17, ETA: 3:18 - loss: 23.9909\n",
            "9233/10000 = 92.3%, SPS: 3.9, ELAP: 35:17, ETA: 3:17 - loss: 23.7096\n",
            "9234/10000 = 92.3%, SPS: 3.9, ELAP: 35:18, ETA: 3:17 - loss: 23.6774\n",
            "9235/10000 = 92.3%, SPS: 3.9, ELAP: 35:18, ETA: 3:17 - loss: 23.9538\n",
            "9236/10000 = 92.4%, SPS: 3.9, ELAP: 35:18, ETA: 3:16 - loss: 23.9379\n",
            "9237/10000 = 92.4%, SPS: 3.9, ELAP: 35:18, ETA: 3:16 - loss: 23.3952\n",
            "9238/10000 = 92.4%, SPS: 3.9, ELAP: 35:19, ETA: 3:16 - loss: 24.5093\n",
            "9239/10000 = 92.4%, SPS: 3.9, ELAP: 35:19, ETA: 3:16 - loss: 24.2666\n",
            "9240/10000 = 92.4%, SPS: 3.9, ELAP: 35:19, ETA: 3:15 - loss: 23.9817\n",
            "9241/10000 = 92.4%, SPS: 3.9, ELAP: 35:19, ETA: 3:15 - loss: 24.0867\n",
            "9242/10000 = 92.4%, SPS: 3.9, ELAP: 35:20, ETA: 3:15 - loss: 23.4836\n",
            "9243/10000 = 92.4%, SPS: 3.9, ELAP: 35:20, ETA: 3:15 - loss: 24.3319\n",
            "9244/10000 = 92.4%, SPS: 3.9, ELAP: 35:20, ETA: 3:14 - loss: 23.7269\n",
            "9245/10000 = 92.5%, SPS: 3.9, ELAP: 35:20, ETA: 3:14 - loss: 23.9297\n",
            "9246/10000 = 92.5%, SPS: 3.9, ELAP: 35:20, ETA: 3:14 - loss: 23.8962\n",
            "9247/10000 = 92.5%, SPS: 3.9, ELAP: 35:21, ETA: 3:14 - loss: 24.0873\n",
            "9248/10000 = 92.5%, SPS: 3.9, ELAP: 35:21, ETA: 3:13 - loss: 23.4894\n",
            "9249/10000 = 92.5%, SPS: 3.9, ELAP: 35:21, ETA: 3:13 - loss: 23.8485\n",
            "9250/10000 = 92.5%, SPS: 3.9, ELAP: 35:21, ETA: 3:13 - loss: 23.7909\n",
            "9251/10000 = 92.5%, SPS: 3.9, ELAP: 35:22, ETA: 3:13 - loss: 24.7626\n",
            "9252/10000 = 92.5%, SPS: 3.9, ELAP: 35:22, ETA: 3:12 - loss: 23.6870\n",
            "9253/10000 = 92.5%, SPS: 3.9, ELAP: 35:22, ETA: 3:12 - loss: 23.3880\n",
            "9254/10000 = 92.5%, SPS: 3.9, ELAP: 35:22, ETA: 3:12 - loss: 24.2266\n",
            "9255/10000 = 92.5%, SPS: 3.9, ELAP: 35:23, ETA: 3:12 - loss: 23.3475\n",
            "9256/10000 = 92.6%, SPS: 3.9, ELAP: 35:23, ETA: 3:11 - loss: 23.7369\n",
            "9257/10000 = 92.6%, SPS: 3.9, ELAP: 35:23, ETA: 3:11 - loss: 23.7880\n",
            "9258/10000 = 92.6%, SPS: 3.9, ELAP: 35:23, ETA: 3:11 - loss: 23.2150\n",
            "9259/10000 = 92.6%, SPS: 3.9, ELAP: 35:23, ETA: 3:11 - loss: 23.8469\n",
            "9260/10000 = 92.6%, SPS: 3.9, ELAP: 35:24, ETA: 3:10 - loss: 23.7394\n",
            "9261/10000 = 92.6%, SPS: 3.9, ELAP: 35:24, ETA: 3:10 - loss: 24.4946\n",
            "9262/10000 = 92.6%, SPS: 3.9, ELAP: 35:24, ETA: 3:10 - loss: 24.4232\n",
            "9263/10000 = 92.6%, SPS: 3.9, ELAP: 35:24, ETA: 3:09 - loss: 23.5806\n",
            "9264/10000 = 92.6%, SPS: 3.9, ELAP: 35:25, ETA: 3:09 - loss: 23.6775\n",
            "9265/10000 = 92.7%, SPS: 3.9, ELAP: 35:25, ETA: 3:09 - loss: 23.3381\n",
            "9266/10000 = 92.7%, SPS: 3.9, ELAP: 35:25, ETA: 3:09 - loss: 23.5500\n",
            "9267/10000 = 92.7%, SPS: 3.9, ELAP: 35:25, ETA: 3:08 - loss: 24.1299\n",
            "9268/10000 = 92.7%, SPS: 3.9, ELAP: 35:26, ETA: 3:08 - loss: 23.7171\n",
            "9269/10000 = 92.7%, SPS: 3.9, ELAP: 35:26, ETA: 3:08 - loss: 23.8352\n",
            "9270/10000 = 92.7%, SPS: 3.9, ELAP: 35:26, ETA: 3:08 - loss: 23.1571\n",
            "9271/10000 = 92.7%, SPS: 3.9, ELAP: 35:26, ETA: 3:07 - loss: 24.5617\n",
            "9272/10000 = 92.7%, SPS: 3.9, ELAP: 35:26, ETA: 3:07 - loss: 23.7909\n",
            "9273/10000 = 92.7%, SPS: 3.9, ELAP: 35:27, ETA: 3:07 - loss: 23.9744\n",
            "9274/10000 = 92.7%, SPS: 3.9, ELAP: 35:27, ETA: 3:07 - loss: 23.8425\n",
            "9275/10000 = 92.8%, SPS: 3.9, ELAP: 35:27, ETA: 3:06 - loss: 23.8885\n",
            "9276/10000 = 92.8%, SPS: 3.9, ELAP: 35:27, ETA: 3:06 - loss: 23.4855\n",
            "9277/10000 = 92.8%, SPS: 3.9, ELAP: 35:28, ETA: 3:06 - loss: 23.9156\n",
            "9278/10000 = 92.8%, SPS: 3.9, ELAP: 35:28, ETA: 3:06 - loss: 24.3140\n",
            "9279/10000 = 92.8%, SPS: 3.9, ELAP: 35:28, ETA: 3:05 - loss: 24.0684\n",
            "9280/10000 = 92.8%, SPS: 3.9, ELAP: 35:28, ETA: 3:05 - loss: 24.5217\n",
            "9281/10000 = 92.8%, SPS: 3.9, ELAP: 35:29, ETA: 3:05 - loss: 23.7111\n",
            "9282/10000 = 92.8%, SPS: 3.9, ELAP: 35:29, ETA: 3:05 - loss: 23.7436\n",
            "9283/10000 = 92.8%, SPS: 3.9, ELAP: 35:29, ETA: 3:04 - loss: 23.2653\n",
            "9284/10000 = 92.8%, SPS: 3.9, ELAP: 35:29, ETA: 3:04 - loss: 23.7230\n",
            "9285/10000 = 92.8%, SPS: 3.9, ELAP: 35:29, ETA: 3:04 - loss: 24.3555\n",
            "9286/10000 = 92.9%, SPS: 3.9, ELAP: 35:30, ETA: 3:04 - loss: 23.7212\n",
            "9287/10000 = 92.9%, SPS: 3.9, ELAP: 35:30, ETA: 3:03 - loss: 23.8788\n",
            "9288/10000 = 92.9%, SPS: 3.9, ELAP: 35:30, ETA: 3:03 - loss: 24.0984\n",
            "9289/10000 = 92.9%, SPS: 3.9, ELAP: 35:30, ETA: 3:03 - loss: 24.1349\n",
            "9290/10000 = 92.9%, SPS: 3.9, ELAP: 35:31, ETA: 3:02 - loss: 23.8324\n",
            "9291/10000 = 92.9%, SPS: 3.9, ELAP: 35:31, ETA: 3:02 - loss: 24.3138\n",
            "9292/10000 = 92.9%, SPS: 3.9, ELAP: 35:31, ETA: 3:02 - loss: 24.4712\n",
            "9293/10000 = 92.9%, SPS: 3.9, ELAP: 35:31, ETA: 3:02 - loss: 23.8160\n",
            "9294/10000 = 92.9%, SPS: 3.9, ELAP: 35:31, ETA: 3:01 - loss: 24.3713\n",
            "9295/10000 = 93.0%, SPS: 3.9, ELAP: 35:32, ETA: 3:01 - loss: 24.0128\n",
            "9296/10000 = 93.0%, SPS: 3.9, ELAP: 35:32, ETA: 3:01 - loss: 24.0411\n",
            "9297/10000 = 93.0%, SPS: 3.9, ELAP: 35:32, ETA: 3:01 - loss: 23.2084\n",
            "9298/10000 = 93.0%, SPS: 3.9, ELAP: 35:32, ETA: 3:00 - loss: 24.0752\n",
            "9299/10000 = 93.0%, SPS: 3.9, ELAP: 35:33, ETA: 3:00 - loss: 23.9619\n",
            "9300/10000 = 93.0%, SPS: 3.9, ELAP: 35:35, ETA: 3:00 - loss: 23.5953\n",
            "9301/10000 = 93.0%, SPS: 3.9, ELAP: 35:35, ETA: 3:00 - loss: 24.1147\n",
            "9302/10000 = 93.0%, SPS: 3.9, ELAP: 35:35, ETA: 3:00 - loss: 24.1502\n",
            "9303/10000 = 93.0%, SPS: 3.9, ELAP: 35:36, ETA: 2:59 - loss: 24.4432\n",
            "9304/10000 = 93.0%, SPS: 3.9, ELAP: 35:36, ETA: 2:59 - loss: 23.8185\n",
            "9305/10000 = 93.0%, SPS: 3.9, ELAP: 35:36, ETA: 2:59 - loss: 23.3716\n",
            "9306/10000 = 93.1%, SPS: 3.9, ELAP: 35:36, ETA: 2:59 - loss: 23.9308\n",
            "9307/10000 = 93.1%, SPS: 3.9, ELAP: 35:37, ETA: 2:58 - loss: 24.1070\n",
            "9308/10000 = 93.1%, SPS: 3.9, ELAP: 35:37, ETA: 2:58 - loss: 23.7938\n",
            "9309/10000 = 93.1%, SPS: 3.9, ELAP: 35:37, ETA: 2:58 - loss: 23.7987\n",
            "9310/10000 = 93.1%, SPS: 3.9, ELAP: 35:37, ETA: 2:57 - loss: 24.6498\n",
            "9311/10000 = 93.1%, SPS: 3.9, ELAP: 35:38, ETA: 2:57 - loss: 23.4550\n",
            "9312/10000 = 93.1%, SPS: 3.9, ELAP: 35:38, ETA: 2:57 - loss: 23.8238\n",
            "9313/10000 = 93.1%, SPS: 3.9, ELAP: 35:38, ETA: 2:57 - loss: 23.9047\n",
            "9314/10000 = 93.1%, SPS: 3.9, ELAP: 35:38, ETA: 2:56 - loss: 24.3776\n",
            "9315/10000 = 93.2%, SPS: 3.9, ELAP: 35:38, ETA: 2:56 - loss: 23.8785\n",
            "9316/10000 = 93.2%, SPS: 3.9, ELAP: 35:39, ETA: 2:56 - loss: 23.8417\n",
            "9317/10000 = 93.2%, SPS: 3.9, ELAP: 35:39, ETA: 2:56 - loss: 23.9338\n",
            "9318/10000 = 93.2%, SPS: 3.9, ELAP: 35:39, ETA: 2:55 - loss: 23.7742\n",
            "9319/10000 = 93.2%, SPS: 3.9, ELAP: 35:39, ETA: 2:55 - loss: 23.8419\n",
            "9320/10000 = 93.2%, SPS: 3.9, ELAP: 35:40, ETA: 2:55 - loss: 23.1132\n",
            "9321/10000 = 93.2%, SPS: 3.9, ELAP: 35:40, ETA: 2:55 - loss: 24.4898\n",
            "9322/10000 = 93.2%, SPS: 3.9, ELAP: 35:40, ETA: 2:54 - loss: 23.0410\n",
            "9323/10000 = 93.2%, SPS: 3.9, ELAP: 35:40, ETA: 2:54 - loss: 24.6654\n",
            "9324/10000 = 93.2%, SPS: 3.9, ELAP: 35:41, ETA: 2:54 - loss: 23.9164\n",
            "9325/10000 = 93.2%, SPS: 3.9, ELAP: 35:41, ETA: 2:54 - loss: 23.2213\n",
            "9326/10000 = 93.3%, SPS: 3.9, ELAP: 35:41, ETA: 2:53 - loss: 23.5413\n",
            "9327/10000 = 93.3%, SPS: 3.9, ELAP: 35:41, ETA: 2:53 - loss: 23.4916\n",
            "9328/10000 = 93.3%, SPS: 3.9, ELAP: 35:42, ETA: 2:53 - loss: 23.9504\n",
            "9329/10000 = 93.3%, SPS: 3.9, ELAP: 35:42, ETA: 2:53 - loss: 23.4779\n",
            "9330/10000 = 93.3%, SPS: 3.9, ELAP: 35:42, ETA: 2:52 - loss: 23.3700\n",
            "9331/10000 = 93.3%, SPS: 3.9, ELAP: 35:42, ETA: 2:52 - loss: 24.6601\n",
            "9332/10000 = 93.3%, SPS: 3.9, ELAP: 35:43, ETA: 2:52 - loss: 24.2213\n",
            "9333/10000 = 93.3%, SPS: 3.9, ELAP: 35:43, ETA: 2:52 - loss: 24.2792\n",
            "9334/10000 = 93.3%, SPS: 3.9, ELAP: 35:43, ETA: 2:51 - loss: 23.8761\n",
            "9335/10000 = 93.3%, SPS: 3.9, ELAP: 35:43, ETA: 2:51 - loss: 23.5461\n",
            "9336/10000 = 93.4%, SPS: 3.9, ELAP: 35:44, ETA: 2:51 - loss: 23.7175\n",
            "9337/10000 = 93.4%, SPS: 3.9, ELAP: 35:44, ETA: 2:50 - loss: 23.1242\n",
            "9338/10000 = 93.4%, SPS: 3.9, ELAP: 35:44, ETA: 2:50 - loss: 23.9283\n",
            "9339/10000 = 93.4%, SPS: 3.9, ELAP: 35:44, ETA: 2:50 - loss: 23.3489\n",
            "9340/10000 = 93.4%, SPS: 3.9, ELAP: 35:44, ETA: 2:50 - loss: 24.2828\n",
            "9341/10000 = 93.4%, SPS: 3.9, ELAP: 35:45, ETA: 2:49 - loss: 23.9879\n",
            "9342/10000 = 93.4%, SPS: 3.9, ELAP: 35:45, ETA: 2:49 - loss: 23.6663\n",
            "9343/10000 = 93.4%, SPS: 3.9, ELAP: 35:45, ETA: 2:49 - loss: 23.1852\n",
            "9344/10000 = 93.4%, SPS: 3.9, ELAP: 35:45, ETA: 2:49 - loss: 24.8138\n",
            "9345/10000 = 93.5%, SPS: 3.9, ELAP: 35:46, ETA: 2:48 - loss: 24.0437\n",
            "9346/10000 = 93.5%, SPS: 3.9, ELAP: 35:46, ETA: 2:48 - loss: 23.6929\n",
            "9347/10000 = 93.5%, SPS: 3.9, ELAP: 35:46, ETA: 2:48 - loss: 23.0831\n",
            "9348/10000 = 93.5%, SPS: 3.9, ELAP: 35:46, ETA: 2:48 - loss: 23.4582\n",
            "9349/10000 = 93.5%, SPS: 3.9, ELAP: 35:47, ETA: 2:47 - loss: 24.4377\n",
            "9350/10000 = 93.5%, SPS: 3.9, ELAP: 35:47, ETA: 2:47 - loss: 24.1562\n",
            "9351/10000 = 93.5%, SPS: 3.9, ELAP: 35:47, ETA: 2:47 - loss: 23.7632\n",
            "9352/10000 = 93.5%, SPS: 3.9, ELAP: 35:47, ETA: 2:47 - loss: 23.9995\n",
            "9353/10000 = 93.5%, SPS: 3.9, ELAP: 35:47, ETA: 2:46 - loss: 23.9590\n",
            "9354/10000 = 93.5%, SPS: 3.9, ELAP: 35:48, ETA: 2:46 - loss: 23.9228\n",
            "9355/10000 = 93.5%, SPS: 3.9, ELAP: 35:48, ETA: 2:46 - loss: 23.8027\n",
            "9356/10000 = 93.6%, SPS: 3.9, ELAP: 35:48, ETA: 2:46 - loss: 23.9321\n",
            "9357/10000 = 93.6%, SPS: 3.9, ELAP: 35:48, ETA: 2:45 - loss: 23.6575\n",
            "9358/10000 = 93.6%, SPS: 3.9, ELAP: 35:49, ETA: 2:45 - loss: 23.6424\n",
            "9359/10000 = 93.6%, SPS: 3.9, ELAP: 35:49, ETA: 2:45 - loss: 23.8426\n",
            "9360/10000 = 93.6%, SPS: 3.9, ELAP: 35:49, ETA: 2:45 - loss: 23.8993\n",
            "9361/10000 = 93.6%, SPS: 3.9, ELAP: 35:49, ETA: 2:44 - loss: 23.9003\n",
            "9362/10000 = 93.6%, SPS: 3.9, ELAP: 35:50, ETA: 2:44 - loss: 24.1378\n",
            "9363/10000 = 93.6%, SPS: 3.9, ELAP: 35:50, ETA: 2:44 - loss: 24.4659\n",
            "9364/10000 = 93.6%, SPS: 3.9, ELAP: 35:50, ETA: 2:43 - loss: 23.4346\n",
            "9365/10000 = 93.7%, SPS: 3.9, ELAP: 35:50, ETA: 2:43 - loss: 23.6748\n",
            "9366/10000 = 93.7%, SPS: 3.9, ELAP: 35:50, ETA: 2:43 - loss: 24.0070\n",
            "9367/10000 = 93.7%, SPS: 3.9, ELAP: 35:51, ETA: 2:43 - loss: 23.7954\n",
            "9368/10000 = 93.7%, SPS: 3.9, ELAP: 35:51, ETA: 2:42 - loss: 24.0405\n",
            "9369/10000 = 93.7%, SPS: 3.9, ELAP: 35:51, ETA: 2:42 - loss: 23.8140\n",
            "9370/10000 = 93.7%, SPS: 3.9, ELAP: 35:51, ETA: 2:42 - loss: 23.9785\n",
            "9371/10000 = 93.7%, SPS: 3.9, ELAP: 35:52, ETA: 2:42 - loss: 24.0251\n",
            "9372/10000 = 93.7%, SPS: 3.9, ELAP: 35:52, ETA: 2:41 - loss: 23.4229\n",
            "9373/10000 = 93.7%, SPS: 3.9, ELAP: 35:52, ETA: 2:41 - loss: 24.0666\n",
            "9374/10000 = 93.7%, SPS: 3.9, ELAP: 35:52, ETA: 2:41 - loss: 23.8470\n",
            "9375/10000 = 93.8%, SPS: 3.9, ELAP: 35:53, ETA: 2:41 - loss: 24.3127\n",
            "9376/10000 = 93.8%, SPS: 3.9, ELAP: 35:53, ETA: 2:40 - loss: 24.0258\n",
            "9377/10000 = 93.8%, SPS: 3.9, ELAP: 35:53, ETA: 2:40 - loss: 24.4506\n",
            "9378/10000 = 93.8%, SPS: 3.9, ELAP: 35:53, ETA: 2:40 - loss: 23.5901\n",
            "9379/10000 = 93.8%, SPS: 3.9, ELAP: 35:53, ETA: 2:40 - loss: 23.4358\n",
            "9380/10000 = 93.8%, SPS: 3.9, ELAP: 35:54, ETA: 2:39 - loss: 23.6975\n",
            "9381/10000 = 93.8%, SPS: 3.9, ELAP: 35:54, ETA: 2:39 - loss: 23.8099\n",
            "9382/10000 = 93.8%, SPS: 3.9, ELAP: 35:54, ETA: 2:39 - loss: 24.2784\n",
            "9383/10000 = 93.8%, SPS: 3.9, ELAP: 35:54, ETA: 2:39 - loss: 24.4442\n",
            "9384/10000 = 93.8%, SPS: 3.9, ELAP: 35:55, ETA: 2:38 - loss: 23.9637\n",
            "9385/10000 = 93.8%, SPS: 3.9, ELAP: 35:55, ETA: 2:38 - loss: 24.5609\n",
            "9386/10000 = 93.9%, SPS: 3.9, ELAP: 35:55, ETA: 2:38 - loss: 23.8672\n",
            "9387/10000 = 93.9%, SPS: 3.9, ELAP: 35:55, ETA: 2:38 - loss: 23.3211\n",
            "9388/10000 = 93.9%, SPS: 3.9, ELAP: 35:56, ETA: 2:37 - loss: 24.3240\n",
            "9389/10000 = 93.9%, SPS: 3.9, ELAP: 35:56, ETA: 2:37 - loss: 23.7886\n",
            "9390/10000 = 93.9%, SPS: 3.9, ELAP: 35:56, ETA: 2:37 - loss: 23.0561\n",
            "9391/10000 = 93.9%, SPS: 3.9, ELAP: 35:56, ETA: 2:36 - loss: 23.6761\n",
            "9392/10000 = 93.9%, SPS: 3.9, ELAP: 35:56, ETA: 2:36 - loss: 23.8942\n",
            "9393/10000 = 93.9%, SPS: 3.9, ELAP: 35:57, ETA: 2:36 - loss: 24.0233\n",
            "9394/10000 = 93.9%, SPS: 3.9, ELAP: 35:57, ETA: 2:36 - loss: 23.4596\n",
            "9395/10000 = 94.0%, SPS: 3.9, ELAP: 35:57, ETA: 2:35 - loss: 24.1014\n",
            "9396/10000 = 94.0%, SPS: 3.9, ELAP: 35:57, ETA: 2:35 - loss: 23.2689\n",
            "9397/10000 = 94.0%, SPS: 3.9, ELAP: 35:58, ETA: 2:35 - loss: 23.7234\n",
            "9398/10000 = 94.0%, SPS: 3.9, ELAP: 35:58, ETA: 2:35 - loss: 23.3995\n",
            "9399/10000 = 94.0%, SPS: 3.9, ELAP: 35:58, ETA: 2:34 - loss: 24.0961\n",
            "9400/10000 = 94.0%, SPS: 3.9, ELAP: 36:00, ETA: 2:34 - loss: 24.4595\n",
            "9401/10000 = 94.0%, SPS: 3.9, ELAP: 36:01, ETA: 2:34 - loss: 23.8626\n",
            "9402/10000 = 94.0%, SPS: 3.9, ELAP: 36:01, ETA: 2:34 - loss: 24.1643\n",
            "9403/10000 = 94.0%, SPS: 3.9, ELAP: 36:01, ETA: 2:34 - loss: 24.0095\n",
            "9404/10000 = 94.0%, SPS: 3.9, ELAP: 36:01, ETA: 2:33 - loss: 24.0887\n",
            "9405/10000 = 94.0%, SPS: 3.9, ELAP: 36:02, ETA: 2:33 - loss: 24.2063\n",
            "9406/10000 = 94.1%, SPS: 3.9, ELAP: 36:02, ETA: 2:33 - loss: 23.7652\n",
            "9407/10000 = 94.1%, SPS: 3.9, ELAP: 36:02, ETA: 2:32 - loss: 23.6903\n",
            "9408/10000 = 94.1%, SPS: 3.9, ELAP: 36:02, ETA: 2:32 - loss: 24.6963\n",
            "9409/10000 = 94.1%, SPS: 3.9, ELAP: 36:02, ETA: 2:32 - loss: 23.7530\n",
            "9410/10000 = 94.1%, SPS: 3.9, ELAP: 36:03, ETA: 2:32 - loss: 23.5828\n",
            "9411/10000 = 94.1%, SPS: 3.9, ELAP: 36:03, ETA: 2:31 - loss: 23.3530\n",
            "9412/10000 = 94.1%, SPS: 3.9, ELAP: 36:03, ETA: 2:31 - loss: 23.3410\n",
            "9413/10000 = 94.1%, SPS: 3.9, ELAP: 36:03, ETA: 2:31 - loss: 23.8741\n",
            "9414/10000 = 94.1%, SPS: 3.9, ELAP: 36:04, ETA: 2:31 - loss: 23.9352\n",
            "9415/10000 = 94.2%, SPS: 3.9, ELAP: 36:04, ETA: 2:30 - loss: 23.7040\n",
            "9416/10000 = 94.2%, SPS: 3.9, ELAP: 36:04, ETA: 2:30 - loss: 23.7784\n",
            "9417/10000 = 94.2%, SPS: 3.9, ELAP: 36:04, ETA: 2:30 - loss: 24.0835\n",
            "9418/10000 = 94.2%, SPS: 3.9, ELAP: 36:05, ETA: 2:30 - loss: 23.8457\n",
            "9419/10000 = 94.2%, SPS: 3.9, ELAP: 36:05, ETA: 2:29 - loss: 24.0216\n",
            "9420/10000 = 94.2%, SPS: 3.9, ELAP: 36:05, ETA: 2:29 - loss: 23.8445\n",
            "9421/10000 = 94.2%, SPS: 3.9, ELAP: 36:05, ETA: 2:29 - loss: 24.0625\n",
            "9422/10000 = 94.2%, SPS: 3.9, ELAP: 36:05, ETA: 2:29 - loss: 23.3777\n",
            "9423/10000 = 94.2%, SPS: 3.9, ELAP: 36:06, ETA: 2:28 - loss: 24.4437\n",
            "9424/10000 = 94.2%, SPS: 3.9, ELAP: 36:06, ETA: 2:28 - loss: 24.0607\n",
            "9425/10000 = 94.2%, SPS: 3.9, ELAP: 36:06, ETA: 2:28 - loss: 23.7842\n",
            "9426/10000 = 94.3%, SPS: 3.9, ELAP: 36:06, ETA: 2:28 - loss: 24.4506\n",
            "9427/10000 = 94.3%, SPS: 3.9, ELAP: 36:07, ETA: 2:27 - loss: 23.3050\n",
            "9428/10000 = 94.3%, SPS: 3.9, ELAP: 36:07, ETA: 2:27 - loss: 23.5981\n",
            "9429/10000 = 94.3%, SPS: 3.9, ELAP: 36:07, ETA: 2:27 - loss: 23.5367\n",
            "9430/10000 = 94.3%, SPS: 3.9, ELAP: 36:07, ETA: 2:27 - loss: 23.8047\n",
            "9431/10000 = 94.3%, SPS: 3.9, ELAP: 36:08, ETA: 2:26 - loss: 24.0944\n",
            "9432/10000 = 94.3%, SPS: 3.9, ELAP: 36:08, ETA: 2:26 - loss: 23.7985\n",
            "9433/10000 = 94.3%, SPS: 3.9, ELAP: 36:08, ETA: 2:26 - loss: 23.9746\n",
            "9434/10000 = 94.3%, SPS: 3.9, ELAP: 36:08, ETA: 2:26 - loss: 23.2837\n",
            "9435/10000 = 94.3%, SPS: 3.9, ELAP: 36:09, ETA: 2:25 - loss: 24.2022\n",
            "9436/10000 = 94.4%, SPS: 3.9, ELAP: 36:09, ETA: 2:25 - loss: 24.1833\n",
            "9437/10000 = 94.4%, SPS: 3.9, ELAP: 36:09, ETA: 2:25 - loss: 23.9163\n",
            "9438/10000 = 94.4%, SPS: 3.9, ELAP: 36:09, ETA: 2:24 - loss: 23.5729\n",
            "9439/10000 = 94.4%, SPS: 3.9, ELAP: 36:10, ETA: 2:24 - loss: 24.1226\n",
            "9440/10000 = 94.4%, SPS: 3.9, ELAP: 36:10, ETA: 2:24 - loss: 24.0520\n",
            "9441/10000 = 94.4%, SPS: 3.9, ELAP: 36:10, ETA: 2:24 - loss: 24.2191\n",
            "9442/10000 = 94.4%, SPS: 3.9, ELAP: 36:10, ETA: 2:23 - loss: 23.7262\n",
            "9443/10000 = 94.4%, SPS: 3.9, ELAP: 36:10, ETA: 2:23 - loss: 24.2264\n",
            "9444/10000 = 94.4%, SPS: 3.9, ELAP: 36:11, ETA: 2:23 - loss: 23.5519\n",
            "9445/10000 = 94.5%, SPS: 3.9, ELAP: 36:11, ETA: 2:23 - loss: 23.8220\n",
            "9446/10000 = 94.5%, SPS: 3.9, ELAP: 36:11, ETA: 2:22 - loss: 24.2060\n",
            "9447/10000 = 94.5%, SPS: 3.9, ELAP: 36:11, ETA: 2:22 - loss: 23.7810\n",
            "9448/10000 = 94.5%, SPS: 3.9, ELAP: 36:12, ETA: 2:22 - loss: 22.7257\n",
            "9449/10000 = 94.5%, SPS: 3.9, ELAP: 36:12, ETA: 2:22 - loss: 23.5735\n",
            "9450/10000 = 94.5%, SPS: 3.9, ELAP: 36:12, ETA: 2:21 - loss: 24.0634\n",
            "9451/10000 = 94.5%, SPS: 3.9, ELAP: 36:12, ETA: 2:21 - loss: 23.5943\n",
            "9452/10000 = 94.5%, SPS: 3.9, ELAP: 36:13, ETA: 2:21 - loss: 24.1171\n",
            "9453/10000 = 94.5%, SPS: 3.9, ELAP: 36:13, ETA: 2:21 - loss: 23.8394\n",
            "9454/10000 = 94.5%, SPS: 3.9, ELAP: 36:13, ETA: 2:20 - loss: 24.0368\n",
            "9455/10000 = 94.5%, SPS: 3.9, ELAP: 36:13, ETA: 2:20 - loss: 23.8661\n",
            "9456/10000 = 94.6%, SPS: 3.9, ELAP: 36:13, ETA: 2:20 - loss: 23.7581\n",
            "9457/10000 = 94.6%, SPS: 3.9, ELAP: 36:14, ETA: 2:20 - loss: 23.3873\n",
            "9458/10000 = 94.6%, SPS: 3.9, ELAP: 36:14, ETA: 2:19 - loss: 23.7674\n",
            "9459/10000 = 94.6%, SPS: 3.9, ELAP: 36:14, ETA: 2:19 - loss: 23.5739\n",
            "9460/10000 = 94.6%, SPS: 3.9, ELAP: 36:14, ETA: 2:19 - loss: 23.3762\n",
            "9461/10000 = 94.6%, SPS: 3.9, ELAP: 36:15, ETA: 2:19 - loss: 24.0861\n",
            "9462/10000 = 94.6%, SPS: 3.9, ELAP: 36:15, ETA: 2:18 - loss: 23.9604\n",
            "9463/10000 = 94.6%, SPS: 3.9, ELAP: 36:15, ETA: 2:18 - loss: 22.9455\n",
            "9464/10000 = 94.6%, SPS: 3.9, ELAP: 36:15, ETA: 2:18 - loss: 23.6622\n",
            "9465/10000 = 94.7%, SPS: 3.9, ELAP: 36:16, ETA: 2:17 - loss: 23.5801\n",
            "9466/10000 = 94.7%, SPS: 3.9, ELAP: 36:16, ETA: 2:17 - loss: 23.2770\n",
            "9467/10000 = 94.7%, SPS: 3.9, ELAP: 36:16, ETA: 2:17 - loss: 24.0098\n",
            "9468/10000 = 94.7%, SPS: 3.9, ELAP: 36:16, ETA: 2:17 - loss: 23.9496\n",
            "9469/10000 = 94.7%, SPS: 3.9, ELAP: 36:16, ETA: 2:16 - loss: 24.0852\n",
            "9470/10000 = 94.7%, SPS: 3.9, ELAP: 36:17, ETA: 2:16 - loss: 23.4092\n",
            "9471/10000 = 94.7%, SPS: 3.9, ELAP: 36:17, ETA: 2:16 - loss: 23.9916\n",
            "9472/10000 = 94.7%, SPS: 3.9, ELAP: 36:17, ETA: 2:16 - loss: 23.6862\n",
            "9473/10000 = 94.7%, SPS: 3.9, ELAP: 36:17, ETA: 2:15 - loss: 23.7518\n",
            "9474/10000 = 94.7%, SPS: 3.9, ELAP: 36:18, ETA: 2:15 - loss: 23.9996\n",
            "9475/10000 = 94.8%, SPS: 3.9, ELAP: 36:18, ETA: 2:15 - loss: 24.1557\n",
            "9476/10000 = 94.8%, SPS: 3.9, ELAP: 36:18, ETA: 2:15 - loss: 24.2956\n",
            "9477/10000 = 94.8%, SPS: 3.9, ELAP: 36:18, ETA: 2:14 - loss: 22.7205\n",
            "9478/10000 = 94.8%, SPS: 3.9, ELAP: 36:19, ETA: 2:14 - loss: 23.8272\n",
            "9479/10000 = 94.8%, SPS: 3.9, ELAP: 36:19, ETA: 2:14 - loss: 23.7822\n",
            "9480/10000 = 94.8%, SPS: 3.9, ELAP: 36:19, ETA: 2:14 - loss: 23.2971\n",
            "9481/10000 = 94.8%, SPS: 3.9, ELAP: 36:19, ETA: 2:13 - loss: 23.8055\n",
            "9482/10000 = 94.8%, SPS: 3.9, ELAP: 36:19, ETA: 2:13 - loss: 23.8782\n",
            "9483/10000 = 94.8%, SPS: 3.9, ELAP: 36:20, ETA: 2:13 - loss: 23.7473\n",
            "9484/10000 = 94.8%, SPS: 3.9, ELAP: 36:20, ETA: 2:13 - loss: 24.2893\n",
            "9485/10000 = 94.8%, SPS: 3.9, ELAP: 36:20, ETA: 2:12 - loss: 23.3415\n",
            "9486/10000 = 94.9%, SPS: 3.9, ELAP: 36:20, ETA: 2:12 - loss: 23.8764\n",
            "9487/10000 = 94.9%, SPS: 3.9, ELAP: 36:21, ETA: 2:12 - loss: 24.3675\n",
            "9488/10000 = 94.9%, SPS: 3.9, ELAP: 36:21, ETA: 2:12 - loss: 23.8626\n",
            "9489/10000 = 94.9%, SPS: 3.9, ELAP: 36:21, ETA: 2:11 - loss: 23.8423\n",
            "9490/10000 = 94.9%, SPS: 3.9, ELAP: 36:21, ETA: 2:11 - loss: 23.5592\n",
            "9491/10000 = 94.9%, SPS: 3.9, ELAP: 36:22, ETA: 2:11 - loss: 23.7835\n",
            "9492/10000 = 94.9%, SPS: 3.9, ELAP: 36:22, ETA: 2:11 - loss: 24.0605\n",
            "9493/10000 = 94.9%, SPS: 3.9, ELAP: 36:22, ETA: 2:10 - loss: 23.6776\n",
            "9494/10000 = 94.9%, SPS: 3.9, ELAP: 36:22, ETA: 2:10 - loss: 23.7881\n",
            "9495/10000 = 95.0%, SPS: 3.9, ELAP: 36:22, ETA: 2:10 - loss: 24.4778\n",
            "9496/10000 = 95.0%, SPS: 3.9, ELAP: 36:23, ETA: 2:09 - loss: 23.5733\n",
            "9497/10000 = 95.0%, SPS: 3.9, ELAP: 36:23, ETA: 2:09 - loss: 23.8756\n",
            "9498/10000 = 95.0%, SPS: 3.9, ELAP: 36:23, ETA: 2:09 - loss: 23.9272\n",
            "9499/10000 = 95.0%, SPS: 3.9, ELAP: 36:23, ETA: 2:09 - loss: 23.4426\n",
            "9500/10000 = 95.0%, SPS: 3.9, ELAP: 36:26, ETA: 2:09 - loss: 23.9902\n",
            "9501/10000 = 95.0%, SPS: 3.9, ELAP: 36:26, ETA: 2:08 - loss: 23.2005\n",
            "9502/10000 = 95.0%, SPS: 3.9, ELAP: 36:26, ETA: 2:08 - loss: 23.7577\n",
            "9503/10000 = 95.0%, SPS: 3.9, ELAP: 36:26, ETA: 2:08 - loss: 23.6479\n",
            "9504/10000 = 95.0%, SPS: 3.9, ELAP: 36:27, ETA: 2:08 - loss: 23.9677\n",
            "9505/10000 = 95.0%, SPS: 3.9, ELAP: 36:27, ETA: 2:07 - loss: 22.8727\n",
            "9506/10000 = 95.1%, SPS: 3.9, ELAP: 36:27, ETA: 2:07 - loss: 23.5071\n",
            "9507/10000 = 95.1%, SPS: 3.9, ELAP: 36:27, ETA: 2:07 - loss: 24.1680\n",
            "9508/10000 = 95.1%, SPS: 3.9, ELAP: 36:28, ETA: 2:07 - loss: 23.5123\n",
            "9509/10000 = 95.1%, SPS: 3.9, ELAP: 36:28, ETA: 2:06 - loss: 23.9148\n",
            "9510/10000 = 95.1%, SPS: 3.9, ELAP: 36:28, ETA: 2:06 - loss: 24.5166\n",
            "9511/10000 = 95.1%, SPS: 3.9, ELAP: 36:28, ETA: 2:06 - loss: 24.2477\n",
            "9512/10000 = 95.1%, SPS: 3.9, ELAP: 36:29, ETA: 2:05 - loss: 23.4730\n",
            "9513/10000 = 95.1%, SPS: 3.9, ELAP: 36:29, ETA: 2:05 - loss: 23.5682\n",
            "9514/10000 = 95.1%, SPS: 3.9, ELAP: 36:29, ETA: 2:05 - loss: 24.4173\n",
            "9515/10000 = 95.2%, SPS: 3.9, ELAP: 36:29, ETA: 2:05 - loss: 23.6643\n",
            "9516/10000 = 95.2%, SPS: 3.9, ELAP: 36:29, ETA: 2:04 - loss: 23.7343\n",
            "9517/10000 = 95.2%, SPS: 3.9, ELAP: 36:30, ETA: 2:04 - loss: 23.8158\n",
            "9518/10000 = 95.2%, SPS: 3.9, ELAP: 36:30, ETA: 2:04 - loss: 24.3169\n",
            "9519/10000 = 95.2%, SPS: 3.9, ELAP: 36:30, ETA: 2:04 - loss: 24.2200\n",
            "9520/10000 = 95.2%, SPS: 3.9, ELAP: 36:30, ETA: 2:03 - loss: 23.5090\n",
            "9521/10000 = 95.2%, SPS: 3.9, ELAP: 36:31, ETA: 2:03 - loss: 23.4342\n",
            "9522/10000 = 95.2%, SPS: 3.9, ELAP: 36:31, ETA: 2:03 - loss: 23.8639\n",
            "9523/10000 = 95.2%, SPS: 3.9, ELAP: 36:31, ETA: 2:03 - loss: 23.9093\n",
            "9524/10000 = 95.2%, SPS: 3.9, ELAP: 36:31, ETA: 2:02 - loss: 23.8111\n",
            "9525/10000 = 95.2%, SPS: 3.9, ELAP: 36:32, ETA: 2:02 - loss: 23.6393\n",
            "9526/10000 = 95.3%, SPS: 3.9, ELAP: 36:32, ETA: 2:02 - loss: 24.0994\n",
            "9527/10000 = 95.3%, SPS: 3.9, ELAP: 36:32, ETA: 2:02 - loss: 24.1027\n",
            "9528/10000 = 95.3%, SPS: 3.9, ELAP: 36:32, ETA: 2:01 - loss: 23.9009\n",
            "9529/10000 = 95.3%, SPS: 3.9, ELAP: 36:33, ETA: 2:01 - loss: 23.7997\n",
            "9530/10000 = 95.3%, SPS: 3.9, ELAP: 36:33, ETA: 2:01 - loss: 23.8413\n",
            "9531/10000 = 95.3%, SPS: 3.9, ELAP: 36:33, ETA: 2:01 - loss: 23.5564\n",
            "9532/10000 = 95.3%, SPS: 3.9, ELAP: 36:33, ETA: 2:00 - loss: 23.8299\n",
            "9533/10000 = 95.3%, SPS: 3.9, ELAP: 36:34, ETA: 2:00 - loss: 23.2951\n",
            "9534/10000 = 95.3%, SPS: 3.9, ELAP: 36:34, ETA: 2:00 - loss: 23.9779\n",
            "9535/10000 = 95.3%, SPS: 3.9, ELAP: 36:34, ETA: 2:00 - loss: 23.1203\n",
            "9536/10000 = 95.4%, SPS: 3.9, ELAP: 36:34, ETA: 1:59 - loss: 23.3460\n",
            "9537/10000 = 95.4%, SPS: 3.9, ELAP: 36:35, ETA: 1:59 - loss: 24.1361\n",
            "9538/10000 = 95.4%, SPS: 3.9, ELAP: 36:35, ETA: 1:59 - loss: 23.9249\n",
            "9539/10000 = 95.4%, SPS: 3.9, ELAP: 36:35, ETA: 1:59 - loss: 24.6341\n",
            "9540/10000 = 95.4%, SPS: 3.9, ELAP: 36:35, ETA: 1:58 - loss: 23.8579\n",
            "9541/10000 = 95.4%, SPS: 3.9, ELAP: 36:36, ETA: 1:58 - loss: 23.7131\n",
            "9542/10000 = 95.4%, SPS: 3.9, ELAP: 36:36, ETA: 1:58 - loss: 23.8266\n",
            "9543/10000 = 95.4%, SPS: 3.9, ELAP: 36:36, ETA: 1:57 - loss: 24.4265\n",
            "9544/10000 = 95.4%, SPS: 3.9, ELAP: 36:36, ETA: 1:57 - loss: 23.6415\n",
            "9545/10000 = 95.5%, SPS: 3.9, ELAP: 36:36, ETA: 1:57 - loss: 24.0641\n",
            "9546/10000 = 95.5%, SPS: 3.9, ELAP: 36:37, ETA: 1:57 - loss: 24.2157\n",
            "9547/10000 = 95.5%, SPS: 3.9, ELAP: 36:37, ETA: 1:56 - loss: 24.3506\n",
            "9548/10000 = 95.5%, SPS: 3.9, ELAP: 36:37, ETA: 1:56 - loss: 24.5831\n",
            "9549/10000 = 95.5%, SPS: 3.9, ELAP: 36:37, ETA: 1:56 - loss: 22.5782\n",
            "9550/10000 = 95.5%, SPS: 3.9, ELAP: 36:38, ETA: 1:56 - loss: 23.3227\n",
            "9551/10000 = 95.5%, SPS: 3.9, ELAP: 36:38, ETA: 1:55 - loss: 23.6821\n",
            "9552/10000 = 95.5%, SPS: 3.9, ELAP: 36:38, ETA: 1:55 - loss: 23.5989\n",
            "9553/10000 = 95.5%, SPS: 3.9, ELAP: 36:38, ETA: 1:55 - loss: 24.0183\n",
            "9554/10000 = 95.5%, SPS: 3.9, ELAP: 36:39, ETA: 1:55 - loss: 23.3573\n",
            "9555/10000 = 95.5%, SPS: 3.9, ELAP: 36:39, ETA: 1:54 - loss: 24.1525\n",
            "9556/10000 = 95.6%, SPS: 3.9, ELAP: 36:39, ETA: 1:54 - loss: 23.8499\n",
            "9557/10000 = 95.6%, SPS: 3.9, ELAP: 36:39, ETA: 1:54 - loss: 24.0668\n",
            "9558/10000 = 95.6%, SPS: 3.9, ELAP: 36:40, ETA: 1:54 - loss: 23.0358\n",
            "9559/10000 = 95.6%, SPS: 3.9, ELAP: 36:40, ETA: 1:53 - loss: 23.5998\n",
            "9560/10000 = 95.6%, SPS: 3.9, ELAP: 36:40, ETA: 1:53 - loss: 23.9916\n",
            "9561/10000 = 95.6%, SPS: 3.9, ELAP: 36:40, ETA: 1:53 - loss: 23.1522\n",
            "9562/10000 = 95.6%, SPS: 3.9, ELAP: 36:40, ETA: 1:53 - loss: 23.1978\n",
            "9563/10000 = 95.6%, SPS: 3.9, ELAP: 36:41, ETA: 1:52 - loss: 23.7524\n",
            "9564/10000 = 95.6%, SPS: 3.9, ELAP: 36:41, ETA: 1:52 - loss: 23.8199\n",
            "9565/10000 = 95.7%, SPS: 3.9, ELAP: 36:41, ETA: 1:52 - loss: 23.4783\n",
            "9566/10000 = 95.7%, SPS: 3.9, ELAP: 36:41, ETA: 1:52 - loss: 23.8548\n",
            "9567/10000 = 95.7%, SPS: 3.9, ELAP: 36:42, ETA: 1:51 - loss: 24.2430\n",
            "9568/10000 = 95.7%, SPS: 3.9, ELAP: 36:42, ETA: 1:51 - loss: 23.6811\n",
            "9569/10000 = 95.7%, SPS: 3.9, ELAP: 36:42, ETA: 1:51 - loss: 24.2417\n",
            "9570/10000 = 95.7%, SPS: 3.9, ELAP: 36:42, ETA: 1:51 - loss: 23.9788\n",
            "9571/10000 = 95.7%, SPS: 3.9, ELAP: 36:43, ETA: 1:50 - loss: 24.0216\n",
            "9572/10000 = 95.7%, SPS: 3.9, ELAP: 36:43, ETA: 1:50 - loss: 22.9879\n",
            "9573/10000 = 95.7%, SPS: 3.9, ELAP: 36:43, ETA: 1:50 - loss: 23.5297\n",
            "9574/10000 = 95.7%, SPS: 3.9, ELAP: 36:43, ETA: 1:49 - loss: 23.1656\n",
            "9575/10000 = 95.8%, SPS: 3.9, ELAP: 36:43, ETA: 1:49 - loss: 24.5586\n",
            "9576/10000 = 95.8%, SPS: 3.9, ELAP: 36:44, ETA: 1:49 - loss: 23.6829\n",
            "9577/10000 = 95.8%, SPS: 3.9, ELAP: 36:44, ETA: 1:49 - loss: 24.2911\n",
            "9578/10000 = 95.8%, SPS: 3.9, ELAP: 36:44, ETA: 1:48 - loss: 23.4420\n",
            "9579/10000 = 95.8%, SPS: 3.9, ELAP: 36:44, ETA: 1:48 - loss: 23.6019\n",
            "9580/10000 = 95.8%, SPS: 3.9, ELAP: 36:45, ETA: 1:48 - loss: 23.9247\n",
            "9581/10000 = 95.8%, SPS: 3.9, ELAP: 36:45, ETA: 1:48 - loss: 24.7748\n",
            "9582/10000 = 95.8%, SPS: 3.9, ELAP: 36:45, ETA: 1:47 - loss: 23.9608\n",
            "9583/10000 = 95.8%, SPS: 3.9, ELAP: 36:45, ETA: 1:47 - loss: 23.9706\n",
            "9584/10000 = 95.8%, SPS: 3.9, ELAP: 36:46, ETA: 1:47 - loss: 23.7638\n",
            "9585/10000 = 95.8%, SPS: 3.9, ELAP: 36:46, ETA: 1:47 - loss: 23.5869\n",
            "9586/10000 = 95.9%, SPS: 3.9, ELAP: 36:46, ETA: 1:46 - loss: 24.2733\n",
            "9587/10000 = 95.9%, SPS: 3.9, ELAP: 36:46, ETA: 1:46 - loss: 24.0346\n",
            "9588/10000 = 95.9%, SPS: 3.9, ELAP: 36:46, ETA: 1:46 - loss: 23.6814\n",
            "9589/10000 = 95.9%, SPS: 3.9, ELAP: 36:47, ETA: 1:46 - loss: 23.8869\n",
            "9590/10000 = 95.9%, SPS: 3.9, ELAP: 36:47, ETA: 1:45 - loss: 23.8317\n",
            "9591/10000 = 95.9%, SPS: 3.9, ELAP: 36:47, ETA: 1:45 - loss: 23.5672\n",
            "9592/10000 = 95.9%, SPS: 3.9, ELAP: 36:47, ETA: 1:45 - loss: 24.3334\n",
            "9593/10000 = 95.9%, SPS: 3.9, ELAP: 36:48, ETA: 1:45 - loss: 23.1535\n",
            "9594/10000 = 95.9%, SPS: 3.9, ELAP: 36:48, ETA: 1:44 - loss: 23.2704\n",
            "9595/10000 = 96.0%, SPS: 3.9, ELAP: 36:48, ETA: 1:44 - loss: 24.8267\n",
            "9596/10000 = 96.0%, SPS: 3.9, ELAP: 36:48, ETA: 1:44 - loss: 23.6522\n",
            "9597/10000 = 96.0%, SPS: 3.9, ELAP: 36:48, ETA: 1:44 - loss: 24.0014\n",
            "9598/10000 = 96.0%, SPS: 3.9, ELAP: 36:49, ETA: 1:43 - loss: 23.6732\n",
            "9599/10000 = 96.0%, SPS: 3.9, ELAP: 36:49, ETA: 1:43 - loss: 23.7412\n",
            "9600/10000 = 96.0%, SPS: 3.9, ELAP: 36:51, ETA: 1:43 - loss: 23.5284\n",
            "9601/10000 = 96.0%, SPS: 3.9, ELAP: 36:52, ETA: 1:43 - loss: 23.7224\n",
            "9602/10000 = 96.0%, SPS: 3.9, ELAP: 36:52, ETA: 1:42 - loss: 24.1643\n",
            "9603/10000 = 96.0%, SPS: 3.9, ELAP: 36:52, ETA: 1:42 - loss: 23.4928\n",
            "9604/10000 = 96.0%, SPS: 3.9, ELAP: 36:52, ETA: 1:42 - loss: 23.3567\n",
            "9605/10000 = 96.0%, SPS: 3.9, ELAP: 36:52, ETA: 1:42 - loss: 23.0917\n",
            "9606/10000 = 96.1%, SPS: 3.9, ELAP: 36:53, ETA: 1:41 - loss: 23.5964\n",
            "9607/10000 = 96.1%, SPS: 3.9, ELAP: 36:53, ETA: 1:41 - loss: 24.3906\n",
            "9608/10000 = 96.1%, SPS: 3.9, ELAP: 36:53, ETA: 1:41 - loss: 23.3392\n",
            "9609/10000 = 96.1%, SPS: 3.9, ELAP: 36:53, ETA: 1:41 - loss: 23.9330\n",
            "9610/10000 = 96.1%, SPS: 3.9, ELAP: 36:54, ETA: 1:40 - loss: 23.3073\n",
            "9611/10000 = 96.1%, SPS: 3.9, ELAP: 36:54, ETA: 1:40 - loss: 23.6338\n",
            "9612/10000 = 96.1%, SPS: 3.9, ELAP: 36:54, ETA: 1:40 - loss: 23.9420\n",
            "9613/10000 = 96.1%, SPS: 3.9, ELAP: 36:54, ETA: 1:39 - loss: 23.4488\n",
            "9614/10000 = 96.1%, SPS: 3.9, ELAP: 36:55, ETA: 1:39 - loss: 23.7352\n",
            "9615/10000 = 96.2%, SPS: 3.9, ELAP: 36:55, ETA: 1:39 - loss: 23.4005\n",
            "9616/10000 = 96.2%, SPS: 3.9, ELAP: 36:55, ETA: 1:39 - loss: 23.6512\n",
            "9617/10000 = 96.2%, SPS: 3.9, ELAP: 36:55, ETA: 1:38 - loss: 23.3588\n",
            "9618/10000 = 96.2%, SPS: 3.9, ELAP: 36:55, ETA: 1:38 - loss: 24.1007\n",
            "9619/10000 = 96.2%, SPS: 3.9, ELAP: 36:56, ETA: 1:38 - loss: 23.7712\n",
            "9620/10000 = 96.2%, SPS: 3.9, ELAP: 36:56, ETA: 1:38 - loss: 23.8426\n",
            "9621/10000 = 96.2%, SPS: 3.9, ELAP: 36:56, ETA: 1:37 - loss: 24.2572\n",
            "9622/10000 = 96.2%, SPS: 3.9, ELAP: 36:56, ETA: 1:37 - loss: 24.3564\n",
            "9623/10000 = 96.2%, SPS: 3.9, ELAP: 36:57, ETA: 1:37 - loss: 23.8502\n",
            "9624/10000 = 96.2%, SPS: 3.9, ELAP: 36:57, ETA: 1:37 - loss: 24.0075\n",
            "9625/10000 = 96.2%, SPS: 3.9, ELAP: 36:57, ETA: 1:36 - loss: 23.7005\n",
            "9626/10000 = 96.3%, SPS: 3.9, ELAP: 36:57, ETA: 1:36 - loss: 24.0608\n",
            "9627/10000 = 96.3%, SPS: 3.9, ELAP: 36:58, ETA: 1:36 - loss: 24.0996\n",
            "9628/10000 = 96.3%, SPS: 3.9, ELAP: 36:58, ETA: 1:36 - loss: 23.2926\n",
            "9629/10000 = 96.3%, SPS: 3.9, ELAP: 36:58, ETA: 1:35 - loss: 23.3508\n",
            "9630/10000 = 96.3%, SPS: 3.9, ELAP: 36:58, ETA: 1:35 - loss: 23.6447\n",
            "9631/10000 = 96.3%, SPS: 3.9, ELAP: 36:59, ETA: 1:35 - loss: 23.5300\n",
            "9632/10000 = 96.3%, SPS: 3.9, ELAP: 36:59, ETA: 1:35 - loss: 23.9006\n",
            "9633/10000 = 96.3%, SPS: 3.9, ELAP: 36:59, ETA: 1:34 - loss: 24.2433\n",
            "9634/10000 = 96.3%, SPS: 3.9, ELAP: 36:59, ETA: 1:34 - loss: 23.6553\n",
            "9635/10000 = 96.3%, SPS: 3.9, ELAP: 37:00, ETA: 1:34 - loss: 24.2609\n",
            "9636/10000 = 96.4%, SPS: 3.9, ELAP: 37:00, ETA: 1:34 - loss: 24.0824\n",
            "9637/10000 = 96.4%, SPS: 3.9, ELAP: 37:00, ETA: 1:33 - loss: 23.8674\n",
            "9638/10000 = 96.4%, SPS: 3.9, ELAP: 37:00, ETA: 1:33 - loss: 23.7750\n",
            "9639/10000 = 96.4%, SPS: 3.9, ELAP: 37:00, ETA: 1:33 - loss: 23.3699\n",
            "9640/10000 = 96.4%, SPS: 3.9, ELAP: 37:01, ETA: 1:33 - loss: 23.6372\n",
            "9641/10000 = 96.4%, SPS: 3.9, ELAP: 37:01, ETA: 1:32 - loss: 23.8241\n",
            "9642/10000 = 96.4%, SPS: 3.9, ELAP: 37:01, ETA: 1:32 - loss: 23.8777\n",
            "9643/10000 = 96.4%, SPS: 3.9, ELAP: 37:01, ETA: 1:32 - loss: 24.1864\n",
            "9644/10000 = 96.4%, SPS: 3.9, ELAP: 37:02, ETA: 1:31 - loss: 23.7608\n",
            "9645/10000 = 96.5%, SPS: 3.9, ELAP: 37:02, ETA: 1:31 - loss: 23.9581\n",
            "9646/10000 = 96.5%, SPS: 3.9, ELAP: 37:02, ETA: 1:31 - loss: 23.7488\n",
            "9647/10000 = 96.5%, SPS: 3.9, ELAP: 37:02, ETA: 1:31 - loss: 24.0494\n",
            "9648/10000 = 96.5%, SPS: 3.9, ELAP: 37:03, ETA: 1:30 - loss: 24.4561\n",
            "9649/10000 = 96.5%, SPS: 3.9, ELAP: 37:03, ETA: 1:30 - loss: 23.8100\n",
            "9650/10000 = 96.5%, SPS: 3.9, ELAP: 37:03, ETA: 1:30 - loss: 23.9397\n",
            "9651/10000 = 96.5%, SPS: 3.9, ELAP: 37:03, ETA: 1:30 - loss: 23.3187\n",
            "9652/10000 = 96.5%, SPS: 3.9, ELAP: 37:03, ETA: 1:29 - loss: 24.3602\n",
            "9653/10000 = 96.5%, SPS: 3.9, ELAP: 37:04, ETA: 1:29 - loss: 23.7291\n",
            "9654/10000 = 96.5%, SPS: 3.9, ELAP: 37:04, ETA: 1:29 - loss: 24.1600\n",
            "9655/10000 = 96.5%, SPS: 3.9, ELAP: 37:04, ETA: 1:29 - loss: 23.8472\n",
            "9656/10000 = 96.6%, SPS: 3.9, ELAP: 37:04, ETA: 1:28 - loss: 23.7107\n",
            "9657/10000 = 96.6%, SPS: 3.9, ELAP: 37:05, ETA: 1:28 - loss: 23.7373\n",
            "9658/10000 = 96.6%, SPS: 3.9, ELAP: 37:05, ETA: 1:28 - loss: 24.2047\n",
            "9659/10000 = 96.6%, SPS: 3.9, ELAP: 37:05, ETA: 1:28 - loss: 23.6777\n",
            "9660/10000 = 96.6%, SPS: 3.9, ELAP: 37:05, ETA: 1:27 - loss: 23.6600\n",
            "9661/10000 = 96.6%, SPS: 3.9, ELAP: 37:06, ETA: 1:27 - loss: 24.3335\n",
            "9662/10000 = 96.6%, SPS: 3.9, ELAP: 37:06, ETA: 1:27 - loss: 24.2053\n",
            "9663/10000 = 96.6%, SPS: 3.9, ELAP: 37:06, ETA: 1:27 - loss: 23.1745\n",
            "9664/10000 = 96.6%, SPS: 3.9, ELAP: 37:06, ETA: 1:26 - loss: 24.0638\n",
            "9665/10000 = 96.7%, SPS: 3.9, ELAP: 37:06, ETA: 1:26 - loss: 23.7039\n",
            "9666/10000 = 96.7%, SPS: 3.9, ELAP: 37:07, ETA: 1:26 - loss: 24.2920\n",
            "9667/10000 = 96.7%, SPS: 3.9, ELAP: 37:07, ETA: 1:26 - loss: 23.8859\n",
            "9668/10000 = 96.7%, SPS: 3.9, ELAP: 37:07, ETA: 1:25 - loss: 23.6242\n",
            "9669/10000 = 96.7%, SPS: 3.9, ELAP: 37:07, ETA: 1:25 - loss: 23.4749\n",
            "9670/10000 = 96.7%, SPS: 3.9, ELAP: 37:08, ETA: 1:25 - loss: 24.1645\n",
            "9671/10000 = 96.7%, SPS: 3.9, ELAP: 37:08, ETA: 1:25 - loss: 23.6310\n",
            "9672/10000 = 96.7%, SPS: 3.9, ELAP: 37:08, ETA: 1:24 - loss: 23.7815\n",
            "9673/10000 = 96.7%, SPS: 3.9, ELAP: 37:08, ETA: 1:24 - loss: 23.4481\n",
            "9674/10000 = 96.7%, SPS: 3.9, ELAP: 37:08, ETA: 1:24 - loss: 24.3209\n",
            "9675/10000 = 96.8%, SPS: 3.9, ELAP: 37:09, ETA: 1:23 - loss: 23.5782\n",
            "9676/10000 = 96.8%, SPS: 3.9, ELAP: 37:09, ETA: 1:23 - loss: 24.1195\n",
            "9677/10000 = 96.8%, SPS: 3.9, ELAP: 37:09, ETA: 1:23 - loss: 24.1413\n",
            "9678/10000 = 96.8%, SPS: 3.9, ELAP: 37:09, ETA: 1:23 - loss: 23.3226\n",
            "9679/10000 = 96.8%, SPS: 3.9, ELAP: 37:10, ETA: 1:22 - loss: 23.7172\n",
            "9680/10000 = 96.8%, SPS: 3.9, ELAP: 37:10, ETA: 1:22 - loss: 23.9007\n",
            "9681/10000 = 96.8%, SPS: 3.9, ELAP: 37:10, ETA: 1:22 - loss: 24.0743\n",
            "9682/10000 = 96.8%, SPS: 3.9, ELAP: 37:10, ETA: 1:22 - loss: 24.3197\n",
            "9683/10000 = 96.8%, SPS: 3.9, ELAP: 37:11, ETA: 1:21 - loss: 23.6165\n",
            "9684/10000 = 96.8%, SPS: 3.9, ELAP: 37:11, ETA: 1:21 - loss: 23.7550\n",
            "9685/10000 = 96.8%, SPS: 3.9, ELAP: 37:11, ETA: 1:21 - loss: 23.9175\n",
            "9686/10000 = 96.9%, SPS: 3.9, ELAP: 37:11, ETA: 1:21 - loss: 23.8962\n",
            "9687/10000 = 96.9%, SPS: 3.9, ELAP: 37:11, ETA: 1:20 - loss: 23.7258\n",
            "9688/10000 = 96.9%, SPS: 3.9, ELAP: 37:12, ETA: 1:20 - loss: 23.6576\n",
            "9689/10000 = 96.9%, SPS: 3.9, ELAP: 37:12, ETA: 1:20 - loss: 22.9569\n",
            "9690/10000 = 96.9%, SPS: 3.9, ELAP: 37:12, ETA: 1:20 - loss: 24.2323\n",
            "9691/10000 = 96.9%, SPS: 3.9, ELAP: 37:12, ETA: 1:19 - loss: 23.2842\n",
            "9692/10000 = 96.9%, SPS: 3.9, ELAP: 37:13, ETA: 1:19 - loss: 23.1237\n",
            "9693/10000 = 96.9%, SPS: 3.9, ELAP: 37:13, ETA: 1:19 - loss: 23.9551\n",
            "9694/10000 = 96.9%, SPS: 3.9, ELAP: 37:13, ETA: 1:19 - loss: 22.9823\n",
            "9695/10000 = 97.0%, SPS: 3.9, ELAP: 37:13, ETA: 1:18 - loss: 23.7658\n",
            "9696/10000 = 97.0%, SPS: 3.9, ELAP: 37:14, ETA: 1:18 - loss: 24.1150\n",
            "9697/10000 = 97.0%, SPS: 3.9, ELAP: 37:14, ETA: 1:18 - loss: 23.5743\n",
            "9698/10000 = 97.0%, SPS: 3.9, ELAP: 37:14, ETA: 1:18 - loss: 24.9708\n",
            "9699/10000 = 97.0%, SPS: 3.9, ELAP: 37:14, ETA: 1:17 - loss: 24.0546\n",
            "9700/10000 = 97.0%, SPS: 3.9, ELAP: 37:17, ETA: 1:17 - loss: 23.9477\n",
            "9701/10000 = 97.0%, SPS: 3.9, ELAP: 37:17, ETA: 1:17 - loss: 23.0929\n",
            "9702/10000 = 97.0%, SPS: 3.9, ELAP: 37:17, ETA: 1:17 - loss: 23.9988\n",
            "9703/10000 = 97.0%, SPS: 3.9, ELAP: 37:17, ETA: 1:16 - loss: 23.8713\n",
            "9704/10000 = 97.0%, SPS: 3.9, ELAP: 37:18, ETA: 1:16 - loss: 23.6153\n",
            "9705/10000 = 97.0%, SPS: 3.9, ELAP: 37:18, ETA: 1:16 - loss: 24.4611\n",
            "9706/10000 = 97.1%, SPS: 3.9, ELAP: 37:18, ETA: 1:16 - loss: 23.5388\n",
            "9707/10000 = 97.1%, SPS: 3.9, ELAP: 37:18, ETA: 1:15 - loss: 23.3474\n",
            "9708/10000 = 97.1%, SPS: 3.9, ELAP: 37:18, ETA: 1:15 - loss: 24.4331\n",
            "9709/10000 = 97.1%, SPS: 3.9, ELAP: 37:19, ETA: 1:15 - loss: 24.0322\n",
            "9710/10000 = 97.1%, SPS: 3.9, ELAP: 37:19, ETA: 1:15 - loss: 24.1472\n",
            "9711/10000 = 97.1%, SPS: 3.9, ELAP: 37:19, ETA: 1:14 - loss: 23.7523\n",
            "9712/10000 = 97.1%, SPS: 3.9, ELAP: 37:19, ETA: 1:14 - loss: 24.1510\n",
            "9713/10000 = 97.1%, SPS: 3.9, ELAP: 37:20, ETA: 1:14 - loss: 23.1628\n",
            "9714/10000 = 97.1%, SPS: 3.9, ELAP: 37:20, ETA: 1:14 - loss: 24.1549\n",
            "9715/10000 = 97.2%, SPS: 3.9, ELAP: 37:20, ETA: 1:13 - loss: 23.4973\n",
            "9716/10000 = 97.2%, SPS: 3.9, ELAP: 37:20, ETA: 1:13 - loss: 24.1221\n",
            "9717/10000 = 97.2%, SPS: 3.9, ELAP: 37:21, ETA: 1:13 - loss: 23.4510\n",
            "9718/10000 = 97.2%, SPS: 3.9, ELAP: 37:21, ETA: 1:12 - loss: 24.4470\n",
            "9719/10000 = 97.2%, SPS: 3.9, ELAP: 37:21, ETA: 1:12 - loss: 23.9564\n",
            "9720/10000 = 97.2%, SPS: 3.9, ELAP: 37:21, ETA: 1:12 - loss: 22.5469\n",
            "9721/10000 = 97.2%, SPS: 3.9, ELAP: 37:22, ETA: 1:12 - loss: 23.7454\n",
            "9722/10000 = 97.2%, SPS: 3.9, ELAP: 37:22, ETA: 1:11 - loss: 23.9600\n",
            "9723/10000 = 97.2%, SPS: 3.9, ELAP: 37:22, ETA: 1:11 - loss: 23.7479\n",
            "9724/10000 = 97.2%, SPS: 3.9, ELAP: 37:22, ETA: 1:11 - loss: 23.5232\n",
            "9725/10000 = 97.2%, SPS: 3.9, ELAP: 37:23, ETA: 1:11 - loss: 23.0278\n",
            "9726/10000 = 97.3%, SPS: 3.9, ELAP: 37:23, ETA: 1:10 - loss: 24.0779\n",
            "9727/10000 = 97.3%, SPS: 3.9, ELAP: 37:23, ETA: 1:10 - loss: 23.1322\n",
            "9728/10000 = 97.3%, SPS: 3.9, ELAP: 37:23, ETA: 1:10 - loss: 23.6262\n",
            "9729/10000 = 97.3%, SPS: 3.9, ELAP: 37:23, ETA: 1:10 - loss: 23.2464\n",
            "9730/10000 = 97.3%, SPS: 3.9, ELAP: 37:24, ETA: 1:09 - loss: 23.6582\n",
            "9731/10000 = 97.3%, SPS: 3.9, ELAP: 37:24, ETA: 1:09 - loss: 23.5775\n",
            "9732/10000 = 97.3%, SPS: 3.9, ELAP: 37:24, ETA: 1:09 - loss: 23.7539\n",
            "9733/10000 = 97.3%, SPS: 3.9, ELAP: 37:24, ETA: 1:09 - loss: 24.5615\n",
            "9734/10000 = 97.3%, SPS: 3.9, ELAP: 37:25, ETA: 1:08 - loss: 23.9582\n",
            "9735/10000 = 97.3%, SPS: 3.9, ELAP: 37:25, ETA: 1:08 - loss: 23.5801\n",
            "9736/10000 = 97.4%, SPS: 3.9, ELAP: 37:25, ETA: 1:08 - loss: 23.8599\n",
            "9737/10000 = 97.4%, SPS: 3.9, ELAP: 37:25, ETA: 1:08 - loss: 24.0656\n",
            "9738/10000 = 97.4%, SPS: 3.9, ELAP: 37:26, ETA: 1:07 - loss: 24.0134\n",
            "9739/10000 = 97.4%, SPS: 3.9, ELAP: 37:26, ETA: 1:07 - loss: 24.0866\n",
            "9740/10000 = 97.4%, SPS: 3.9, ELAP: 37:26, ETA: 1:07 - loss: 24.3595\n",
            "9741/10000 = 97.4%, SPS: 3.9, ELAP: 37:26, ETA: 1:07 - loss: 23.8622\n",
            "9742/10000 = 97.4%, SPS: 3.9, ELAP: 37:27, ETA: 1:06 - loss: 23.7743\n",
            "9743/10000 = 97.4%, SPS: 3.9, ELAP: 37:27, ETA: 1:06 - loss: 23.7768\n",
            "9744/10000 = 97.4%, SPS: 3.9, ELAP: 37:27, ETA: 1:06 - loss: 23.2319\n",
            "9745/10000 = 97.5%, SPS: 3.9, ELAP: 37:27, ETA: 1:06 - loss: 24.0645\n",
            "9746/10000 = 97.5%, SPS: 3.9, ELAP: 37:27, ETA: 1:05 - loss: 24.2071\n",
            "9747/10000 = 97.5%, SPS: 3.9, ELAP: 37:28, ETA: 1:05 - loss: 23.6442\n",
            "9748/10000 = 97.5%, SPS: 3.9, ELAP: 37:28, ETA: 1:05 - loss: 23.7191\n",
            "9749/10000 = 97.5%, SPS: 3.9, ELAP: 37:28, ETA: 1:04 - loss: 23.7004\n",
            "9750/10000 = 97.5%, SPS: 3.9, ELAP: 37:28, ETA: 1:04 - loss: 23.2496\n",
            "9751/10000 = 97.5%, SPS: 3.9, ELAP: 37:29, ETA: 1:04 - loss: 23.4582\n",
            "9752/10000 = 97.5%, SPS: 3.9, ELAP: 37:29, ETA: 1:04 - loss: 23.3048\n",
            "9753/10000 = 97.5%, SPS: 3.9, ELAP: 37:29, ETA: 1:03 - loss: 23.3467\n",
            "9754/10000 = 97.5%, SPS: 3.9, ELAP: 37:29, ETA: 1:03 - loss: 23.5061\n",
            "9755/10000 = 97.5%, SPS: 3.9, ELAP: 37:29, ETA: 1:03 - loss: 24.5679\n",
            "9756/10000 = 97.6%, SPS: 3.9, ELAP: 37:30, ETA: 1:03 - loss: 23.2857\n",
            "9757/10000 = 97.6%, SPS: 3.9, ELAP: 37:30, ETA: 1:02 - loss: 24.3110\n",
            "9758/10000 = 97.6%, SPS: 3.9, ELAP: 37:30, ETA: 1:02 - loss: 23.2899\n",
            "9759/10000 = 97.6%, SPS: 3.9, ELAP: 37:30, ETA: 1:02 - loss: 23.5921\n",
            "9760/10000 = 97.6%, SPS: 3.9, ELAP: 37:31, ETA: 1:02 - loss: 24.4955\n",
            "9761/10000 = 97.6%, SPS: 3.9, ELAP: 37:31, ETA: 1:01 - loss: 24.0924\n",
            "9762/10000 = 97.6%, SPS: 3.9, ELAP: 37:31, ETA: 1:01 - loss: 24.2761\n",
            "9763/10000 = 97.6%, SPS: 3.9, ELAP: 37:31, ETA: 1:01 - loss: 24.1976\n",
            "9764/10000 = 97.6%, SPS: 3.9, ELAP: 37:31, ETA: 1:01 - loss: 23.4781\n",
            "9765/10000 = 97.7%, SPS: 3.9, ELAP: 37:32, ETA: 1:00 - loss: 24.5210\n",
            "9766/10000 = 97.7%, SPS: 3.9, ELAP: 37:32, ETA: 1:00 - loss: 23.7505\n",
            "9767/10000 = 97.7%, SPS: 3.9, ELAP: 37:32, ETA: 1:00 - loss: 23.1914\n",
            "9768/10000 = 97.7%, SPS: 3.9, ELAP: 37:32, ETA: 1:00 - loss: 23.4677\n",
            "9769/10000 = 97.7%, SPS: 3.9, ELAP: 37:33, ETA: 59 - loss: 23.7689\n",
            "9770/10000 = 97.7%, SPS: 3.9, ELAP: 37:33, ETA: 59 - loss: 23.6152\n",
            "9771/10000 = 97.7%, SPS: 3.9, ELAP: 37:33, ETA: 59 - loss: 23.8201\n",
            "9772/10000 = 97.7%, SPS: 3.9, ELAP: 37:33, ETA: 59 - loss: 23.5570\n",
            "9773/10000 = 97.7%, SPS: 3.9, ELAP: 37:34, ETA: 58 - loss: 24.1603\n",
            "9774/10000 = 97.7%, SPS: 3.9, ELAP: 37:34, ETA: 58 - loss: 23.8271\n",
            "9775/10000 = 97.8%, SPS: 3.9, ELAP: 37:34, ETA: 58 - loss: 24.1442\n",
            "9776/10000 = 97.8%, SPS: 3.9, ELAP: 37:34, ETA: 58 - loss: 23.4012\n",
            "9777/10000 = 97.8%, SPS: 3.9, ELAP: 37:34, ETA: 57 - loss: 23.9829\n",
            "9778/10000 = 97.8%, SPS: 3.9, ELAP: 37:35, ETA: 57 - loss: 23.3288\n",
            "9779/10000 = 97.8%, SPS: 3.9, ELAP: 37:35, ETA: 57 - loss: 24.1964\n",
            "9780/10000 = 97.8%, SPS: 3.9, ELAP: 37:35, ETA: 57 - loss: 23.2037\n",
            "9781/10000 = 97.8%, SPS: 3.9, ELAP: 37:35, ETA: 56 - loss: 23.4511\n",
            "9782/10000 = 97.8%, SPS: 3.9, ELAP: 37:36, ETA: 56 - loss: 23.9927\n",
            "9783/10000 = 97.8%, SPS: 3.9, ELAP: 37:36, ETA: 56 - loss: 23.6488\n",
            "9784/10000 = 97.8%, SPS: 3.9, ELAP: 37:36, ETA: 55 - loss: 23.9703\n",
            "9785/10000 = 97.8%, SPS: 3.9, ELAP: 37:36, ETA: 55 - loss: 23.8592\n",
            "9786/10000 = 97.9%, SPS: 3.9, ELAP: 37:37, ETA: 55 - loss: 23.0399\n",
            "9787/10000 = 97.9%, SPS: 3.9, ELAP: 37:37, ETA: 55 - loss: 23.5330\n",
            "9788/10000 = 97.9%, SPS: 3.9, ELAP: 37:37, ETA: 54 - loss: 23.5532\n",
            "9789/10000 = 97.9%, SPS: 3.9, ELAP: 37:37, ETA: 54 - loss: 23.4620\n",
            "9790/10000 = 97.9%, SPS: 3.9, ELAP: 37:37, ETA: 54 - loss: 23.9614\n",
            "9791/10000 = 97.9%, SPS: 3.9, ELAP: 37:38, ETA: 54 - loss: 23.7499\n",
            "9792/10000 = 97.9%, SPS: 3.9, ELAP: 37:38, ETA: 53 - loss: 24.2455\n",
            "9793/10000 = 97.9%, SPS: 3.9, ELAP: 37:38, ETA: 53 - loss: 23.9394\n",
            "9794/10000 = 97.9%, SPS: 3.9, ELAP: 37:38, ETA: 53 - loss: 24.0943\n",
            "9795/10000 = 98.0%, SPS: 3.9, ELAP: 37:39, ETA: 53 - loss: 23.3106\n",
            "9796/10000 = 98.0%, SPS: 3.9, ELAP: 37:39, ETA: 52 - loss: 23.2959\n",
            "9797/10000 = 98.0%, SPS: 3.9, ELAP: 37:39, ETA: 52 - loss: 23.9070\n",
            "9798/10000 = 98.0%, SPS: 3.9, ELAP: 37:39, ETA: 52 - loss: 24.3032\n",
            "9799/10000 = 98.0%, SPS: 3.9, ELAP: 37:39, ETA: 52 - loss: 24.6180\n",
            "9800/10000 = 98.0%, SPS: 3.9, ELAP: 37:42, ETA: 51 - loss: 23.9839\n",
            "9801/10000 = 98.0%, SPS: 3.9, ELAP: 37:42, ETA: 51 - loss: 23.8312\n",
            "9802/10000 = 98.0%, SPS: 3.9, ELAP: 37:42, ETA: 51 - loss: 24.1719\n",
            "9803/10000 = 98.0%, SPS: 3.9, ELAP: 37:43, ETA: 51 - loss: 23.7878\n",
            "9804/10000 = 98.0%, SPS: 3.9, ELAP: 37:43, ETA: 50 - loss: 23.6717\n",
            "9805/10000 = 98.0%, SPS: 3.9, ELAP: 37:43, ETA: 50 - loss: 23.9336\n",
            "9806/10000 = 98.1%, SPS: 3.9, ELAP: 37:43, ETA: 50 - loss: 23.8557\n",
            "9807/10000 = 98.1%, SPS: 3.9, ELAP: 37:44, ETA: 50 - loss: 23.3368\n",
            "9808/10000 = 98.1%, SPS: 3.9, ELAP: 37:44, ETA: 49 - loss: 23.5564\n",
            "9809/10000 = 98.1%, SPS: 3.9, ELAP: 37:44, ETA: 49 - loss: 24.1866\n",
            "9810/10000 = 98.1%, SPS: 3.9, ELAP: 37:44, ETA: 49 - loss: 24.1568\n",
            "9811/10000 = 98.1%, SPS: 3.9, ELAP: 37:44, ETA: 49 - loss: 24.2963\n",
            "9812/10000 = 98.1%, SPS: 3.9, ELAP: 37:45, ETA: 48 - loss: 23.8373\n",
            "9813/10000 = 98.1%, SPS: 3.9, ELAP: 37:45, ETA: 48 - loss: 23.4874\n",
            "9814/10000 = 98.1%, SPS: 3.9, ELAP: 37:45, ETA: 48 - loss: 24.0962\n",
            "9815/10000 = 98.2%, SPS: 3.9, ELAP: 37:45, ETA: 48 - loss: 23.6331\n",
            "9816/10000 = 98.2%, SPS: 3.9, ELAP: 37:46, ETA: 47 - loss: 23.6243\n",
            "9817/10000 = 98.2%, SPS: 3.9, ELAP: 37:46, ETA: 47 - loss: 24.2601\n",
            "9818/10000 = 98.2%, SPS: 3.9, ELAP: 37:46, ETA: 47 - loss: 24.0358\n",
            "9819/10000 = 98.2%, SPS: 3.9, ELAP: 37:46, ETA: 47 - loss: 23.7385\n",
            "9820/10000 = 98.2%, SPS: 3.9, ELAP: 37:47, ETA: 46 - loss: 23.6648\n",
            "9821/10000 = 98.2%, SPS: 3.9, ELAP: 37:47, ETA: 46 - loss: 23.1691\n",
            "9822/10000 = 98.2%, SPS: 3.9, ELAP: 37:47, ETA: 46 - loss: 23.2244\n",
            "9823/10000 = 98.2%, SPS: 3.9, ELAP: 37:47, ETA: 45 - loss: 24.3130\n",
            "9824/10000 = 98.2%, SPS: 3.9, ELAP: 37:48, ETA: 45 - loss: 23.3119\n",
            "9825/10000 = 98.2%, SPS: 3.9, ELAP: 37:48, ETA: 45 - loss: 24.4272\n",
            "9826/10000 = 98.3%, SPS: 3.9, ELAP: 37:48, ETA: 45 - loss: 23.7954\n",
            "9827/10000 = 98.3%, SPS: 3.9, ELAP: 37:48, ETA: 44 - loss: 23.9484\n",
            "9828/10000 = 98.3%, SPS: 3.9, ELAP: 37:48, ETA: 44 - loss: 23.7940\n",
            "9829/10000 = 98.3%, SPS: 3.9, ELAP: 37:49, ETA: 44 - loss: 23.4057\n",
            "9830/10000 = 98.3%, SPS: 3.9, ELAP: 37:49, ETA: 44 - loss: 23.4707\n",
            "9831/10000 = 98.3%, SPS: 3.9, ELAP: 37:49, ETA: 43 - loss: 24.5390\n",
            "9832/10000 = 98.3%, SPS: 3.9, ELAP: 37:49, ETA: 43 - loss: 23.6948\n",
            "9833/10000 = 98.3%, SPS: 3.9, ELAP: 37:50, ETA: 43 - loss: 23.9158\n",
            "9834/10000 = 98.3%, SPS: 3.9, ELAP: 37:50, ETA: 43 - loss: 23.9857\n",
            "9835/10000 = 98.3%, SPS: 3.9, ELAP: 37:50, ETA: 42 - loss: 23.9798\n",
            "9836/10000 = 98.4%, SPS: 3.9, ELAP: 37:50, ETA: 42 - loss: 23.4494\n",
            "9837/10000 = 98.4%, SPS: 3.9, ELAP: 37:51, ETA: 42 - loss: 23.5236\n",
            "9838/10000 = 98.4%, SPS: 3.9, ELAP: 37:51, ETA: 42 - loss: 23.8966\n",
            "9839/10000 = 98.4%, SPS: 3.9, ELAP: 37:51, ETA: 41 - loss: 23.8921\n",
            "9840/10000 = 98.4%, SPS: 3.9, ELAP: 37:51, ETA: 41 - loss: 23.0185\n",
            "9841/10000 = 98.4%, SPS: 3.9, ELAP: 37:52, ETA: 41 - loss: 23.9962\n",
            "9842/10000 = 98.4%, SPS: 3.9, ELAP: 37:52, ETA: 41 - loss: 23.5351\n",
            "9843/10000 = 98.4%, SPS: 3.9, ELAP: 37:52, ETA: 40 - loss: 24.0505\n",
            "9844/10000 = 98.4%, SPS: 3.9, ELAP: 37:52, ETA: 40 - loss: 24.2724\n",
            "9845/10000 = 98.5%, SPS: 3.9, ELAP: 37:52, ETA: 40 - loss: 23.5088\n",
            "9846/10000 = 98.5%, SPS: 3.9, ELAP: 37:53, ETA: 40 - loss: 23.6031\n",
            "9847/10000 = 98.5%, SPS: 3.9, ELAP: 37:53, ETA: 39 - loss: 24.2856\n",
            "9848/10000 = 98.5%, SPS: 3.9, ELAP: 37:53, ETA: 39 - loss: 23.6728\n",
            "9849/10000 = 98.5%, SPS: 3.9, ELAP: 37:53, ETA: 39 - loss: 23.3532\n",
            "9850/10000 = 98.5%, SPS: 3.9, ELAP: 37:54, ETA: 39 - loss: 23.9614\n",
            "9851/10000 = 98.5%, SPS: 3.9, ELAP: 37:54, ETA: 38 - loss: 23.7939\n",
            "9852/10000 = 98.5%, SPS: 3.9, ELAP: 37:54, ETA: 38 - loss: 24.4991\n",
            "9853/10000 = 98.5%, SPS: 3.9, ELAP: 37:54, ETA: 38 - loss: 23.5594\n",
            "9854/10000 = 98.5%, SPS: 3.9, ELAP: 37:55, ETA: 38 - loss: 23.4982\n",
            "9855/10000 = 98.5%, SPS: 3.9, ELAP: 37:55, ETA: 37 - loss: 23.4606\n",
            "9856/10000 = 98.6%, SPS: 3.9, ELAP: 37:55, ETA: 37 - loss: 24.3590\n",
            "9857/10000 = 98.6%, SPS: 3.9, ELAP: 37:55, ETA: 37 - loss: 23.8261\n",
            "9858/10000 = 98.6%, SPS: 3.9, ELAP: 37:55, ETA: 36 - loss: 24.3681\n",
            "9859/10000 = 98.6%, SPS: 3.9, ELAP: 37:56, ETA: 36 - loss: 23.8189\n",
            "9860/10000 = 98.6%, SPS: 3.9, ELAP: 37:56, ETA: 36 - loss: 23.7461\n",
            "9861/10000 = 98.6%, SPS: 3.9, ELAP: 37:56, ETA: 36 - loss: 23.8523\n",
            "9862/10000 = 98.6%, SPS: 3.9, ELAP: 37:56, ETA: 35 - loss: 23.7814\n",
            "9863/10000 = 98.6%, SPS: 3.9, ELAP: 37:57, ETA: 35 - loss: 23.7974\n",
            "9864/10000 = 98.6%, SPS: 3.9, ELAP: 37:57, ETA: 35 - loss: 24.2762\n",
            "9865/10000 = 98.7%, SPS: 3.9, ELAP: 37:57, ETA: 35 - loss: 23.2142\n",
            "9866/10000 = 98.7%, SPS: 3.9, ELAP: 37:57, ETA: 34 - loss: 24.2303\n",
            "9867/10000 = 98.7%, SPS: 3.9, ELAP: 37:58, ETA: 34 - loss: 23.0930\n",
            "9868/10000 = 98.7%, SPS: 3.9, ELAP: 37:58, ETA: 34 - loss: 23.8020\n",
            "9869/10000 = 98.7%, SPS: 3.9, ELAP: 37:58, ETA: 34 - loss: 23.3851\n",
            "9870/10000 = 98.7%, SPS: 3.9, ELAP: 37:58, ETA: 33 - loss: 23.0478\n",
            "9871/10000 = 98.7%, SPS: 3.9, ELAP: 37:58, ETA: 33 - loss: 24.1280\n",
            "9872/10000 = 98.7%, SPS: 3.9, ELAP: 37:59, ETA: 33 - loss: 23.5032\n",
            "9873/10000 = 98.7%, SPS: 3.9, ELAP: 37:59, ETA: 33 - loss: 23.8218\n",
            "9874/10000 = 98.7%, SPS: 3.9, ELAP: 37:59, ETA: 32 - loss: 23.6816\n",
            "9875/10000 = 98.8%, SPS: 3.9, ELAP: 37:59, ETA: 32 - loss: 23.9261\n",
            "9876/10000 = 98.8%, SPS: 3.9, ELAP: 38:00, ETA: 32 - loss: 24.2722\n",
            "9877/10000 = 98.8%, SPS: 3.9, ELAP: 38:00, ETA: 32 - loss: 23.5509\n",
            "9878/10000 = 98.8%, SPS: 3.9, ELAP: 38:00, ETA: 31 - loss: 23.7053\n",
            "9879/10000 = 98.8%, SPS: 3.9, ELAP: 38:00, ETA: 31 - loss: 23.7611\n",
            "9880/10000 = 98.8%, SPS: 3.9, ELAP: 38:01, ETA: 31 - loss: 23.8645\n",
            "9881/10000 = 98.8%, SPS: 3.9, ELAP: 38:01, ETA: 31 - loss: 23.5650\n",
            "9882/10000 = 98.8%, SPS: 3.9, ELAP: 38:01, ETA: 30 - loss: 23.6689\n",
            "9883/10000 = 98.8%, SPS: 3.9, ELAP: 38:01, ETA: 30 - loss: 23.4313\n",
            "9884/10000 = 98.8%, SPS: 3.9, ELAP: 38:01, ETA: 30 - loss: 23.5415\n",
            "9885/10000 = 98.8%, SPS: 3.9, ELAP: 38:02, ETA: 30 - loss: 23.8496\n",
            "9886/10000 = 98.9%, SPS: 3.9, ELAP: 38:02, ETA: 29 - loss: 23.7829\n",
            "9887/10000 = 98.9%, SPS: 3.9, ELAP: 38:02, ETA: 29 - loss: 23.9844\n",
            "9888/10000 = 98.9%, SPS: 3.9, ELAP: 38:02, ETA: 29 - loss: 23.7062\n",
            "9889/10000 = 98.9%, SPS: 3.9, ELAP: 38:03, ETA: 29 - loss: 23.7577\n",
            "9890/10000 = 98.9%, SPS: 3.9, ELAP: 38:03, ETA: 28 - loss: 23.8356\n",
            "9891/10000 = 98.9%, SPS: 3.9, ELAP: 38:03, ETA: 28 - loss: 24.2442\n",
            "9892/10000 = 98.9%, SPS: 3.9, ELAP: 38:03, ETA: 28 - loss: 23.1542\n",
            "9893/10000 = 98.9%, SPS: 3.9, ELAP: 38:03, ETA: 27 - loss: 23.5869\n",
            "9894/10000 = 98.9%, SPS: 3.9, ELAP: 38:04, ETA: 27 - loss: 24.0007\n",
            "9895/10000 = 99.0%, SPS: 3.9, ELAP: 38:04, ETA: 27 - loss: 23.5778\n",
            "9896/10000 = 99.0%, SPS: 3.9, ELAP: 38:04, ETA: 27 - loss: 23.8903\n",
            "9897/10000 = 99.0%, SPS: 3.9, ELAP: 38:04, ETA: 26 - loss: 24.0251\n",
            "9898/10000 = 99.0%, SPS: 3.9, ELAP: 38:05, ETA: 26 - loss: 23.8916\n",
            "9899/10000 = 99.0%, SPS: 3.9, ELAP: 38:05, ETA: 26 - loss: 23.9379\n",
            "9900/10000 = 99.0%, SPS: 3.9, ELAP: 38:07, ETA: 26 - loss: 24.2295\n",
            "9901/10000 = 99.0%, SPS: 3.9, ELAP: 38:08, ETA: 25 - loss: 23.3670\n",
            "9902/10000 = 99.0%, SPS: 3.9, ELAP: 38:08, ETA: 25 - loss: 23.8300\n",
            "9903/10000 = 99.0%, SPS: 3.9, ELAP: 38:08, ETA: 25 - loss: 23.2025\n",
            "9904/10000 = 99.0%, SPS: 3.9, ELAP: 38:08, ETA: 25 - loss: 23.7863\n",
            "9905/10000 = 99.0%, SPS: 3.9, ELAP: 38:08, ETA: 24 - loss: 23.8202\n",
            "9906/10000 = 99.1%, SPS: 3.9, ELAP: 38:09, ETA: 24 - loss: 24.0084\n",
            "9907/10000 = 99.1%, SPS: 3.9, ELAP: 38:09, ETA: 24 - loss: 24.3310\n",
            "9908/10000 = 99.1%, SPS: 3.9, ELAP: 38:09, ETA: 24 - loss: 23.8709\n",
            "9909/10000 = 99.1%, SPS: 3.9, ELAP: 38:09, ETA: 23 - loss: 23.7629\n",
            "9910/10000 = 99.1%, SPS: 3.9, ELAP: 38:10, ETA: 23 - loss: 23.0389\n",
            "9911/10000 = 99.1%, SPS: 3.9, ELAP: 38:10, ETA: 23 - loss: 23.2449\n",
            "9912/10000 = 99.1%, SPS: 3.9, ELAP: 38:10, ETA: 23 - loss: 23.6206\n",
            "9913/10000 = 99.1%, SPS: 3.9, ELAP: 38:10, ETA: 22 - loss: 24.1165\n",
            "9914/10000 = 99.1%, SPS: 3.9, ELAP: 38:10, ETA: 22 - loss: 23.8325\n",
            "9915/10000 = 99.2%, SPS: 3.9, ELAP: 38:11, ETA: 22 - loss: 24.0855\n",
            "9916/10000 = 99.2%, SPS: 3.9, ELAP: 38:11, ETA: 22 - loss: 23.8689\n",
            "9917/10000 = 99.2%, SPS: 3.9, ELAP: 38:11, ETA: 21 - loss: 23.9773\n",
            "9918/10000 = 99.2%, SPS: 3.9, ELAP: 38:11, ETA: 21 - loss: 23.8748\n",
            "9919/10000 = 99.2%, SPS: 3.9, ELAP: 38:12, ETA: 21 - loss: 24.0597\n",
            "9920/10000 = 99.2%, SPS: 3.9, ELAP: 38:12, ETA: 21 - loss: 24.3044\n",
            "9921/10000 = 99.2%, SPS: 3.9, ELAP: 38:12, ETA: 20 - loss: 23.5929\n",
            "9922/10000 = 99.2%, SPS: 3.9, ELAP: 38:12, ETA: 20 - loss: 22.4647\n",
            "9923/10000 = 99.2%, SPS: 3.9, ELAP: 38:13, ETA: 20 - loss: 24.3572\n",
            "9924/10000 = 99.2%, SPS: 3.9, ELAP: 38:13, ETA: 20 - loss: 24.5154\n",
            "9925/10000 = 99.2%, SPS: 3.9, ELAP: 38:13, ETA: 19 - loss: 24.0191\n",
            "9926/10000 = 99.3%, SPS: 3.9, ELAP: 38:13, ETA: 19 - loss: 23.7868\n",
            "9927/10000 = 99.3%, SPS: 3.9, ELAP: 38:13, ETA: 19 - loss: 24.0266\n",
            "9928/10000 = 99.3%, SPS: 3.9, ELAP: 38:14, ETA: 18 - loss: 23.9251\n",
            "9929/10000 = 99.3%, SPS: 3.9, ELAP: 38:14, ETA: 18 - loss: 23.9167\n",
            "9930/10000 = 99.3%, SPS: 3.9, ELAP: 38:14, ETA: 18 - loss: 24.3130\n",
            "9931/10000 = 99.3%, SPS: 3.9, ELAP: 38:14, ETA: 18 - loss: 23.6830\n",
            "9932/10000 = 99.3%, SPS: 3.9, ELAP: 38:15, ETA: 17 - loss: 24.4792\n",
            "9933/10000 = 99.3%, SPS: 3.9, ELAP: 38:15, ETA: 17 - loss: 24.0670\n",
            "9934/10000 = 99.3%, SPS: 3.9, ELAP: 38:15, ETA: 17 - loss: 23.0371\n",
            "9935/10000 = 99.3%, SPS: 3.9, ELAP: 38:15, ETA: 17 - loss: 23.0764\n",
            "9936/10000 = 99.4%, SPS: 3.9, ELAP: 38:16, ETA: 16 - loss: 23.7182\n",
            "9937/10000 = 99.4%, SPS: 3.9, ELAP: 38:16, ETA: 16 - loss: 24.0598\n",
            "9938/10000 = 99.4%, SPS: 3.9, ELAP: 38:16, ETA: 16 - loss: 24.3475\n",
            "9939/10000 = 99.4%, SPS: 3.9, ELAP: 38:16, ETA: 16 - loss: 23.3819\n",
            "9940/10000 = 99.4%, SPS: 3.9, ELAP: 38:17, ETA: 15 - loss: 23.7278\n",
            "9941/10000 = 99.4%, SPS: 3.9, ELAP: 38:17, ETA: 15 - loss: 22.8842\n",
            "9942/10000 = 99.4%, SPS: 3.9, ELAP: 38:17, ETA: 15 - loss: 23.5010\n",
            "9943/10000 = 99.4%, SPS: 3.9, ELAP: 38:17, ETA: 15 - loss: 24.3906\n",
            "9944/10000 = 99.4%, SPS: 3.9, ELAP: 38:17, ETA: 14 - loss: 24.3440\n",
            "9945/10000 = 99.5%, SPS: 3.9, ELAP: 38:18, ETA: 14 - loss: 23.7052\n",
            "9946/10000 = 99.5%, SPS: 3.9, ELAP: 38:18, ETA: 14 - loss: 24.3855\n",
            "9947/10000 = 99.5%, SPS: 3.9, ELAP: 38:18, ETA: 14 - loss: 23.3134\n",
            "9948/10000 = 99.5%, SPS: 3.9, ELAP: 38:18, ETA: 13 - loss: 23.6504\n",
            "9949/10000 = 99.5%, SPS: 3.9, ELAP: 38:19, ETA: 13 - loss: 24.0445\n",
            "9950/10000 = 99.5%, SPS: 3.9, ELAP: 38:19, ETA: 13 - loss: 24.0839\n",
            "9951/10000 = 99.5%, SPS: 3.9, ELAP: 38:19, ETA: 13 - loss: 23.7413\n",
            "9952/10000 = 99.5%, SPS: 3.9, ELAP: 38:19, ETA: 12 - loss: 23.9186\n",
            "9953/10000 = 99.5%, SPS: 3.9, ELAP: 38:20, ETA: 12 - loss: 23.9129\n",
            "9954/10000 = 99.5%, SPS: 3.9, ELAP: 38:20, ETA: 12 - loss: 23.6153\n",
            "9955/10000 = 99.5%, SPS: 3.9, ELAP: 38:20, ETA: 12 - loss: 23.1092\n",
            "9956/10000 = 99.6%, SPS: 3.9, ELAP: 38:20, ETA: 11 - loss: 23.8838\n",
            "9957/10000 = 99.6%, SPS: 3.9, ELAP: 38:21, ETA: 11 - loss: 24.4194\n",
            "9958/10000 = 99.6%, SPS: 3.9, ELAP: 38:21, ETA: 11 - loss: 23.8507\n",
            "9959/10000 = 99.6%, SPS: 3.9, ELAP: 38:21, ETA: 11 - loss: 24.5925\n",
            "9960/10000 = 99.6%, SPS: 3.9, ELAP: 38:21, ETA: 10 - loss: 23.2065\n",
            "9961/10000 = 99.6%, SPS: 3.9, ELAP: 38:21, ETA: 10 - loss: 23.5667\n",
            "9962/10000 = 99.6%, SPS: 3.9, ELAP: 38:22, ETA: 10 - loss: 23.3631\n",
            "9963/10000 = 99.6%, SPS: 3.9, ELAP: 38:22, ETA: 10 - loss: 23.7192\n",
            "9964/10000 = 99.6%, SPS: 3.9, ELAP: 38:22, ETA: 9 - loss: 23.9094\n",
            "9965/10000 = 99.7%, SPS: 3.9, ELAP: 38:22, ETA: 9 - loss: 23.3228\n",
            "9966/10000 = 99.7%, SPS: 3.9, ELAP: 38:23, ETA: 9 - loss: 23.6592\n",
            "9967/10000 = 99.7%, SPS: 3.9, ELAP: 38:23, ETA: 8 - loss: 23.3915\n",
            "9968/10000 = 99.7%, SPS: 3.9, ELAP: 38:23, ETA: 8 - loss: 23.1923\n",
            "9969/10000 = 99.7%, SPS: 3.9, ELAP: 38:23, ETA: 8 - loss: 23.5632\n",
            "9970/10000 = 99.7%, SPS: 3.9, ELAP: 38:23, ETA: 8 - loss: 23.3698\n",
            "9971/10000 = 99.7%, SPS: 3.9, ELAP: 38:24, ETA: 7 - loss: 23.3499\n",
            "9972/10000 = 99.7%, SPS: 3.9, ELAP: 38:24, ETA: 7 - loss: 23.9219\n",
            "9973/10000 = 99.7%, SPS: 3.9, ELAP: 38:24, ETA: 7 - loss: 24.3029\n",
            "9974/10000 = 99.7%, SPS: 3.9, ELAP: 38:24, ETA: 7 - loss: 23.5808\n",
            "9975/10000 = 99.8%, SPS: 3.9, ELAP: 38:25, ETA: 6 - loss: 23.9740\n",
            "9976/10000 = 99.8%, SPS: 3.9, ELAP: 38:25, ETA: 6 - loss: 23.5307\n",
            "9977/10000 = 99.8%, SPS: 3.9, ELAP: 38:25, ETA: 6 - loss: 23.5883\n",
            "9978/10000 = 99.8%, SPS: 3.9, ELAP: 38:25, ETA: 6 - loss: 23.4832\n",
            "9979/10000 = 99.8%, SPS: 3.9, ELAP: 38:26, ETA: 5 - loss: 23.6970\n",
            "9980/10000 = 99.8%, SPS: 3.9, ELAP: 38:26, ETA: 5 - loss: 23.8422\n",
            "9981/10000 = 99.8%, SPS: 3.9, ELAP: 38:26, ETA: 5 - loss: 23.8033\n",
            "9982/10000 = 99.8%, SPS: 3.9, ELAP: 38:26, ETA: 5 - loss: 23.7518\n",
            "9983/10000 = 99.8%, SPS: 3.9, ELAP: 38:26, ETA: 4 - loss: 24.3896\n",
            "9984/10000 = 99.8%, SPS: 3.9, ELAP: 38:27, ETA: 4 - loss: 23.1950\n",
            "9985/10000 = 99.8%, SPS: 3.9, ELAP: 38:27, ETA: 4 - loss: 24.0102\n",
            "9986/10000 = 99.9%, SPS: 3.9, ELAP: 38:27, ETA: 4 - loss: 24.3904\n",
            "9987/10000 = 99.9%, SPS: 3.9, ELAP: 38:27, ETA: 3 - loss: 23.7750\n",
            "9988/10000 = 99.9%, SPS: 3.9, ELAP: 38:28, ETA: 3 - loss: 23.3804\n",
            "9989/10000 = 99.9%, SPS: 3.9, ELAP: 38:28, ETA: 3 - loss: 23.7936\n",
            "9990/10000 = 99.9%, SPS: 3.9, ELAP: 38:28, ETA: 3 - loss: 23.7180\n",
            "9991/10000 = 99.9%, SPS: 3.9, ELAP: 38:28, ETA: 2 - loss: 23.6365\n",
            "9992/10000 = 99.9%, SPS: 3.9, ELAP: 38:29, ETA: 2 - loss: 23.1881\n",
            "9993/10000 = 99.9%, SPS: 3.9, ELAP: 38:29, ETA: 2 - loss: 23.6555\n",
            "9994/10000 = 99.9%, SPS: 3.9, ELAP: 38:29, ETA: 2 - loss: 23.9333\n",
            "9995/10000 = 100.0%, SPS: 3.9, ELAP: 38:29, ETA: 1 - loss: 23.8065\n",
            "9996/10000 = 100.0%, SPS: 3.9, ELAP: 38:29, ETA: 1 - loss: 24.5126\n",
            "9997/10000 = 100.0%, SPS: 3.9, ELAP: 38:30, ETA: 1 - loss: 23.9600\n",
            "9998/10000 = 100.0%, SPS: 3.9, ELAP: 38:30, ETA: 1 - loss: 23.7510\n",
            "9999/10000 = 100.0%, SPS: 3.9, ELAP: 38:30, ETA: 0 - loss: 23.2495\n",
            "10000/10000 = 100.0%, SPS: 3.9, ELAP: 38:33, ETA: 0 - loss: 23.9637\n",
            "10000/10000 = 100.0%, SPS: 3.9, ELAP: 38:33, ETA: 0\n",
            "================================================================================\n",
            "Running evaluation\n",
            "================================================================================\n",
            "Model is built!\n",
            "2020-10-13 16:47:20.868527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-13 16:47:20.869206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-10-13 16:47:20.869294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-10-13 16:47:20.869331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-10-13 16:47:20.869371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-10-13 16:47:20.869405: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-10-13 16:47:20.869444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-10-13 16:47:20.869484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-10-13 16:47:20.869523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-10-13 16:47:20.869632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-13 16:47:20.870575: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-13 16:47:20.871234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-10-13 16:47:20.871456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-10-13 16:47:20.871481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-10-13 16:47:20.871494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-10-13 16:47:20.871616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-13 16:47:20.872250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-10-13 16:47:20.872829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "1/100 = 1.0%, SPS: 0.8, ELAP: 1, ETA: 2:08 - loss: 23.5256\n",
            "2/100 = 2.0%, SPS: 1.2, ELAP: 2, ETA: 1:19 - loss: 23.4186\n",
            "3/100 = 3.0%, SPS: 1.6, ELAP: 2, ETA: 1:02 - loss: 23.6912\n",
            "4/100 = 4.0%, SPS: 1.8, ELAP: 2, ETA: 53 - loss: 23.6743\n",
            "5/100 = 5.0%, SPS: 2.0, ELAP: 3, ETA: 48 - loss: 23.6335\n",
            "6/100 = 6.0%, SPS: 2.1, ELAP: 3, ETA: 44 - loss: 23.9032\n",
            "7/100 = 7.0%, SPS: 2.2, ELAP: 3, ETA: 41 - loss: 23.9857\n",
            "8/100 = 8.0%, SPS: 2.3, ELAP: 3, ETA: 39 - loss: 23.7697\n",
            "9/100 = 9.0%, SPS: 2.4, ELAP: 4, ETA: 38 - loss: 23.5870\n",
            "10/100 = 10.0%, SPS: 2.5, ELAP: 4, ETA: 36 - loss: 23.6845\n",
            "11/100 = 11.0%, SPS: 2.5, ELAP: 4, ETA: 35 - loss: 23.6701\n",
            "12/100 = 12.0%, SPS: 2.6, ELAP: 5, ETA: 34 - loss: 23.5937\n",
            "13/100 = 13.0%, SPS: 2.6, ELAP: 5, ETA: 33 - loss: 23.7127\n",
            "14/100 = 14.0%, SPS: 2.7, ELAP: 5, ETA: 32 - loss: 23.8757\n",
            "15/100 = 15.0%, SPS: 2.7, ELAP: 6, ETA: 31 - loss: 23.4699\n",
            "16/100 = 16.0%, SPS: 2.7, ELAP: 6, ETA: 31 - loss: 23.7551\n",
            "17/100 = 17.0%, SPS: 2.8, ELAP: 6, ETA: 30 - loss: 23.5044\n",
            "18/100 = 18.0%, SPS: 2.8, ELAP: 6, ETA: 29 - loss: 23.5681\n",
            "19/100 = 19.0%, SPS: 2.8, ELAP: 7, ETA: 29 - loss: 23.7387\n",
            "20/100 = 20.0%, SPS: 2.8, ELAP: 7, ETA: 28 - loss: 23.9608\n",
            "21/100 = 21.0%, SPS: 2.9, ELAP: 7, ETA: 28 - loss: 23.7141\n",
            "22/100 = 22.0%, SPS: 2.9, ELAP: 8, ETA: 27 - loss: 23.4681\n",
            "23/100 = 23.0%, SPS: 2.9, ELAP: 8, ETA: 27 - loss: 23.8785\n",
            "24/100 = 24.0%, SPS: 2.9, ELAP: 8, ETA: 26 - loss: 23.2499\n",
            "25/100 = 25.0%, SPS: 2.9, ELAP: 9, ETA: 26 - loss: 23.7002\n",
            "26/100 = 26.0%, SPS: 2.9, ELAP: 9, ETA: 25 - loss: 23.2986\n",
            "27/100 = 27.0%, SPS: 2.9, ELAP: 9, ETA: 25 - loss: 23.8582\n",
            "28/100 = 28.0%, SPS: 3.0, ELAP: 9, ETA: 24 - loss: 23.5837\n",
            "29/100 = 29.0%, SPS: 3.0, ELAP: 10, ETA: 24 - loss: 23.7669\n",
            "30/100 = 30.0%, SPS: 3.0, ELAP: 10, ETA: 24 - loss: 23.5144\n",
            "31/100 = 31.0%, SPS: 3.0, ELAP: 10, ETA: 23 - loss: 23.6092\n",
            "32/100 = 32.0%, SPS: 3.0, ELAP: 11, ETA: 23 - loss: 23.4642\n",
            "33/100 = 33.0%, SPS: 3.0, ELAP: 11, ETA: 22 - loss: 23.7055\n",
            "34/100 = 34.0%, SPS: 3.0, ELAP: 11, ETA: 22 - loss: 23.5066\n",
            "35/100 = 35.0%, SPS: 3.0, ELAP: 12, ETA: 21 - loss: 23.6747\n",
            "36/100 = 36.0%, SPS: 3.0, ELAP: 12, ETA: 21 - loss: 23.6755\n",
            "37/100 = 37.0%, SPS: 3.0, ELAP: 12, ETA: 21 - loss: 23.7388\n",
            "38/100 = 38.0%, SPS: 3.0, ELAP: 12, ETA: 20 - loss: 23.7028\n",
            "39/100 = 39.0%, SPS: 3.1, ELAP: 13, ETA: 20 - loss: 23.7595\n",
            "40/100 = 40.0%, SPS: 3.1, ELAP: 13, ETA: 20 - loss: 23.7809\n",
            "41/100 = 41.0%, SPS: 3.1, ELAP: 13, ETA: 19 - loss: 23.6809\n",
            "42/100 = 42.0%, SPS: 3.1, ELAP: 14, ETA: 19 - loss: 23.7303\n",
            "43/100 = 43.0%, SPS: 3.1, ELAP: 14, ETA: 19 - loss: 23.7154\n",
            "44/100 = 44.0%, SPS: 3.1, ELAP: 14, ETA: 18 - loss: 23.7808\n",
            "45/100 = 45.0%, SPS: 3.1, ELAP: 15, ETA: 18 - loss: 23.7432\n",
            "46/100 = 46.0%, SPS: 3.1, ELAP: 15, ETA: 17 - loss: 23.5365\n",
            "47/100 = 47.0%, SPS: 3.1, ELAP: 15, ETA: 17 - loss: 23.8754\n",
            "48/100 = 48.0%, SPS: 3.1, ELAP: 16, ETA: 17 - loss: 23.7862\n",
            "49/100 = 49.0%, SPS: 3.1, ELAP: 16, ETA: 16 - loss: 23.9487\n",
            "50/100 = 50.0%, SPS: 3.1, ELAP: 16, ETA: 16 - loss: 23.7349\n",
            "51/100 = 51.0%, SPS: 3.1, ELAP: 16, ETA: 16 - loss: 23.4874\n",
            "52/100 = 52.0%, SPS: 3.1, ELAP: 17, ETA: 15 - loss: 23.6085\n",
            "53/100 = 53.0%, SPS: 3.1, ELAP: 17, ETA: 15 - loss: 23.4713\n",
            "54/100 = 54.0%, SPS: 3.1, ELAP: 17, ETA: 15 - loss: 23.5945\n",
            "55/100 = 55.0%, SPS: 3.1, ELAP: 18, ETA: 14 - loss: 23.3271\n",
            "56/100 = 56.0%, SPS: 3.1, ELAP: 18, ETA: 14 - loss: 23.6686\n",
            "57/100 = 57.0%, SPS: 3.1, ELAP: 18, ETA: 14 - loss: 23.4980\n",
            "58/100 = 58.0%, SPS: 3.1, ELAP: 19, ETA: 13 - loss: 23.7518\n",
            "59/100 = 59.0%, SPS: 3.1, ELAP: 19, ETA: 13 - loss: 23.6983\n",
            "60/100 = 60.0%, SPS: 3.1, ELAP: 19, ETA: 13 - loss: 23.5386\n",
            "61/100 = 61.0%, SPS: 3.1, ELAP: 19, ETA: 12 - loss: 23.8055\n",
            "62/100 = 62.0%, SPS: 3.1, ELAP: 20, ETA: 12 - loss: 23.5213\n",
            "63/100 = 63.0%, SPS: 3.1, ELAP: 20, ETA: 12 - loss: 23.5836\n",
            "64/100 = 64.0%, SPS: 3.1, ELAP: 20, ETA: 11 - loss: 23.4287\n",
            "65/100 = 65.0%, SPS: 3.2, ELAP: 21, ETA: 11 - loss: 23.5888\n",
            "66/100 = 66.0%, SPS: 3.2, ELAP: 21, ETA: 11 - loss: 23.5863\n",
            "67/100 = 67.0%, SPS: 3.2, ELAP: 21, ETA: 10 - loss: 23.6911\n",
            "68/100 = 68.0%, SPS: 3.2, ELAP: 22, ETA: 10 - loss: 23.5315\n",
            "69/100 = 69.0%, SPS: 3.2, ELAP: 22, ETA: 10 - loss: 23.6674\n",
            "70/100 = 70.0%, SPS: 3.2, ELAP: 22, ETA: 9 - loss: 23.4046\n",
            "71/100 = 71.0%, SPS: 3.2, ELAP: 22, ETA: 9 - loss: 23.7086\n",
            "72/100 = 72.0%, SPS: 3.2, ELAP: 23, ETA: 9 - loss: 23.5430\n",
            "73/100 = 73.0%, SPS: 3.2, ELAP: 23, ETA: 9 - loss: 23.3669\n",
            "74/100 = 74.0%, SPS: 3.2, ELAP: 23, ETA: 8 - loss: 23.6805\n",
            "75/100 = 75.0%, SPS: 3.2, ELAP: 24, ETA: 8 - loss: 23.8408\n",
            "76/100 = 76.0%, SPS: 3.2, ELAP: 24, ETA: 8 - loss: 23.7704\n",
            "77/100 = 77.0%, SPS: 3.2, ELAP: 24, ETA: 7 - loss: 23.4449\n",
            "78/100 = 78.0%, SPS: 3.2, ELAP: 25, ETA: 7 - loss: 23.6922\n",
            "79/100 = 79.0%, SPS: 3.2, ELAP: 25, ETA: 7 - loss: 23.4125\n",
            "80/100 = 80.0%, SPS: 3.2, ELAP: 25, ETA: 6 - loss: 23.8205\n",
            "81/100 = 81.0%, SPS: 3.2, ELAP: 25, ETA: 6 - loss: 23.6010\n",
            "82/100 = 82.0%, SPS: 3.2, ELAP: 26, ETA: 6 - loss: 23.2986\n",
            "83/100 = 83.0%, SPS: 3.2, ELAP: 26, ETA: 5 - loss: 23.3289\n",
            "84/100 = 84.0%, SPS: 3.2, ELAP: 26, ETA: 5 - loss: 23.2627\n",
            "85/100 = 85.0%, SPS: 3.2, ELAP: 27, ETA: 5 - loss: 23.6231\n",
            "86/100 = 86.0%, SPS: 3.2, ELAP: 27, ETA: 4 - loss: 23.4504\n",
            "87/100 = 87.0%, SPS: 3.2, ELAP: 27, ETA: 4 - loss: 23.5254\n",
            "88/100 = 88.0%, SPS: 3.2, ELAP: 28, ETA: 4 - loss: 23.5220\n",
            "89/100 = 89.0%, SPS: 3.2, ELAP: 28, ETA: 3 - loss: 23.5840\n",
            "90/100 = 90.0%, SPS: 3.2, ELAP: 28, ETA: 3 - loss: 23.8206\n",
            "91/100 = 91.0%, SPS: 3.2, ELAP: 28, ETA: 3 - loss: 23.5132\n",
            "92/100 = 92.0%, SPS: 3.2, ELAP: 29, ETA: 3 - loss: 23.8976\n",
            "93/100 = 93.0%, SPS: 3.2, ELAP: 29, ETA: 2 - loss: 23.8494\n",
            "94/100 = 94.0%, SPS: 3.2, ELAP: 29, ETA: 2 - loss: 23.7367\n",
            "95/100 = 95.0%, SPS: 3.2, ELAP: 30, ETA: 2 - loss: 23.7916\n",
            "96/100 = 96.0%, SPS: 3.2, ELAP: 30, ETA: 1 - loss: 23.7458\n",
            "97/100 = 97.0%, SPS: 3.2, ELAP: 30, ETA: 1 - loss: 23.7727\n",
            "98/100 = 98.0%, SPS: 3.2, ELAP: 31, ETA: 1 - loss: 23.5717\n",
            "99/100 = 99.0%, SPS: 3.2, ELAP: 31, ETA: 0 - loss: 23.5246\n",
            "100/100 = 100.0%, SPS: 3.2, ELAP: 31, ETA: 0 - loss: 23.3676\n",
            "100/100 = 100.0%, SPS: 3.2, ELAP: 31, ETA: 0\n",
            "  disc_accuracy = 0.8799084\n",
            "  disc_auc = 0.64940655\n",
            "  disc_loss = 0.35064933\n",
            "  disc_precision = 0.76765096\n",
            "  disc_recall = 0.06704284\n",
            "  global_step = 10000\n",
            "  loss = 23.634266\n",
            "  masked_lm_accuracy = 0.18227784\n",
            "  masked_lm_loss = 6.1017737\n",
            "  sampled_masked_lm_accuracy = 0.087187275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU4uE_LxXVOa"
      },
      "source": [
        "## 4. Convert Tensorflow checkpoints to PyTorch format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJfzThSQySKD"
      },
      "source": [
        "Hugging Face has [a tool](https://huggingface.co/transformers/converting_tensorflow_models.html) to convert Tensorflow checkpoints to PyTorch. However, this tool has yet been updated for ELECTRA. Fortunately, I found a GitHub repo by @lonePatient that can help us with this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtnLZBugZTMY",
        "outputId": "6f9bc475-f1bf-4097-f5ba-7d151186372d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/lonePatient/electra_pytorch.git"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'electra_pytorch'...\n",
            "remote: Enumerating objects: 196, done.\u001b[K\n",
            "remote: Counting objects: 100% (196/196), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 196 (delta 89), reused 124 (delta 46), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (196/196), 404.99 KiB | 643.00 KiB/s, done.\n",
            "Resolving deltas: 100% (89/89), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZGvNz-w0fXi"
      },
      "source": [
        "MODEL_DIR = \"/content/drive/My Drive/Colab Notebooks/from_scratch/models/electra_MALS/\"\n",
        "\n",
        "config = {\n",
        "  \"vocab_size\": 30_000,\n",
        "  \"embedding_size\": 128,\n",
        "  \"hidden_size\": 256,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"num_attention_heads\": 4,\n",
        "  \"intermediate_size\": 1024,\n",
        "  \"generator_size\":\"0.25\",\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"max_position_embeddings\": 512,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"initializer_range\": 0.02\n",
        "}\n",
        "\n",
        "with open(MODEL_DIR + \"config.json\", \"w\") as f:\n",
        "    json.dump(config, f)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XazVpTLltlrp",
        "outputId": "b140d07b-736f-4df6-ba0a-a838c57d63a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python electra_pytorch/convert_electra_tf_checkpoint_to_pytorch.py \\\n",
        "    --tf_checkpoint_path=/content/drive/My\\ Drive/Colab\\ Notebooks/from_scratch/models/electra_MALS/ \\\n",
        "    --electra_config_file=/content/drive/My\\ Drive/Colab\\ Notebooks/from_scratch/models/electra_MALS/config.json \\\n",
        "    --pytorch_dump_path=/content/drive/My\\ Drive/Colab\\ Notebooks/from_scratch/models/electra_MALS/pytorch_model.bin"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:model.configuration_utils:loading configuration file /content/drive/My Drive/Colab Notebooks/from_scratch/models/electra_MALS/config.json\n",
            "INFO:model.configuration_utils:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"finetuning_task\": null,\n",
            "  \"generator_size\": \"0.25\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1024,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 4,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "INFO:model.modeling_electra:Converting TensorFlow checkpoint from /content/drive/My Drive/Colab Notebooks/from_scratch/models/electra_MALS\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias with shape [1]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias/adam_m with shape [1]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias/adam_v with shape [1]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel with shape [256, 1]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel/adam_m with shape [256, 1]\n",
            "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel/adam_v with shape [256, 1]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta/adam_m with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta/adam_v with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma/adam_m with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma/adam_v with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/position_embeddings with shape [512, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/position_embeddings/adam_m with shape [512, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/position_embeddings/adam_v with shape [512, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings with shape [2, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings/adam_m with shape [2, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings/adam_v with shape [2, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/word_embeddings with shape [30000, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/word_embeddings/adam_m with shape [30000, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings/word_embeddings/adam_v with shape [30000, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/kernel with shape [128, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/kernel/adam_m with shape [128, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/kernel/adam_v with shape [128, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel/adam_m with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel/adam_v with shape [256, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias/adam_m with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias/adam_v with shape [1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel/adam_m with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel/adam_v with shape [1024, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/kernel with shape [128, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/kernel/adam_m with shape [128, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/kernel/adam_v with shape [128, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel/adam_m with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel/adam_v with shape [64, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias/adam_m with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias/adam_v with shape [256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias/adam_m with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias/adam_v with shape [64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel/adam_m with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel/adam_v with shape [256, 64]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta/adam_m with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta/adam_v with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma/adam_m with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma/adam_v with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/bias with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/bias/adam_m with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/bias/adam_v with shape [128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/kernel with shape [64, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/kernel/adam_m with shape [64, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/kernel/adam_v with shape [64, 128]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/output_bias with shape [30000]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/output_bias/adam_m with shape [30000]\n",
            "INFO:model.modeling_electra:Loading TF weight generator_predictions/output_bias/adam_v with shape [30000]\n",
            "INFO:model.modeling_electra:Loading TF weight global_step with shape []\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['discriminator_predictions', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping discriminator_predictions/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping discriminator_predictions/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['discriminator_predictions', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping discriminator_predictions/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping discriminator_predictions/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['discriminator_predictions', 'classifier', 'bias']\n",
            "INFO:model.modeling_electra:Skipping discriminator_predictions/classifier/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping discriminator_predictions/classifier/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['discriminator_predictions', 'classifier', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping discriminator_predictions/classifier/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping discriminator_predictions/classifier/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'position_embeddings']\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/position_embeddings/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/position_embeddings/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'token_type_embeddings']\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/token_type_embeddings/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/token_type_embeddings/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'word_embeddings']\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/word_embeddings/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings/word_embeddings/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings_project', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings_project/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings_project/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings_project', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings_project/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/embeddings_project/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'embeddings_project', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/embeddings_project/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/embeddings_project/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'embeddings_project', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/embeddings_project/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/embeddings_project/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/key/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/key/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/key/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/key/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/query/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/query/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/query/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/query/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/value/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/value/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/value/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/value/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/intermediate/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/intermediate/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/intermediate/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/intermediate/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'LayerNorm', 'beta']\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/LayerNorm/beta/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/LayerNorm/beta/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'LayerNorm', 'gamma']\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/LayerNorm/gamma/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/LayerNorm/gamma/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'dense', 'bias']\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/dense/bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/dense/bias/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'dense', 'kernel']\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/dense/kernel/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/dense/kernel/adam_v\n",
            "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'output_bias']\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/output_bias/adam_m\n",
            "INFO:model.modeling_electra:Skipping generator_predictions/output_bias/adam_v\n",
            "INFO:model.modeling_electra:Skipping global_step\n",
            "Save PyTorch model to /content/drive/My Drive/Colab Notebooks/from_scratch/models/electra_MALS/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN1HUyV9t_FI"
      },
      "source": [
        "**Use ELECTRA with `transformers`**\n",
        "\n",
        "After converting the model checkpoint to PyTorch format, we can start to use our pre-trained ELECTRA model on downstream tasks with the `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSiRA3Ec3YQA"
      },
      "source": [
        "from transformers import ElectraTokenizer, ElectraModel"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrgjRUew2i_6"
      },
      "source": [
        "import torch\n",
        "from transformers import ElectraForPreTraining, ElectraTokenizerFast\n",
        "\n",
        "discriminator = ElectraForPreTraining.from_pretrained(MODEL_DIR)\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained('/content/drive/My Drive/Colab Notebooks/from_scratch', do_lower_case=False)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfy3IlEq2oDk",
        "outputId": "f6f94b8d-8747-4b09-bb00-95872458c404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "fake_sentence = \"Isso é um exemplo!\"\n",
        "\n",
        "fake_tokens = tokenizer.tokenize(fake_sentence, add_special_tokens=True)\n",
        "fake_inputs = tokenizer.encode(fake_sentence, return_tensors=\"pt\")\n",
        "discriminator_outputs = discriminator(fake_inputs)\n",
        "predictions = discriminator_outputs[0] > 0\n",
        "\n",
        "[print(\"%7s\" % token, end=\"\") for token in fake_tokens]\n",
        "print(\"\\n\")\n",
        "[print(\"%7s\" % int(prediction), end=\"\") for prediction in predictions.tolist()];"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  [CLS]   Isso      e     umexemplo      !  [SEP]\n",
            "\n",
            "      1      0      0      0      0      0      1"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKVKso6O3IRC"
      },
      "source": [
        "Our model was trained for only 100 steps so the predictions are not accurate. The fully-trained ELECTRA-small for Spanish can be loaded as below:\n",
        "\n",
        "```python\n",
        "discriminator = ElectraForPreTraining.from_pretrained(\"skimai/electra-small-spanish\")\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained(\"skimai/electra-small-spanish\", do_lower_case=False)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjJz-7T2_v-x"
      },
      "source": [
        "## 5. Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TxoYuDwAASv"
      },
      "source": [
        "In this article, we have walked through the ELECTRA paper to understand why ELECTRA is the most efficient transformer pre-training approach at the moment. At a small scale, ELECTRA-small can be trained on one GPU for 4 days to outperform GPT on the GLUE benchmark. At a large scale, ELECTRA-large sets a new state-of-the-art for SQuAD 2.0.\n",
        "\n",
        "We then actually train an ELECTRA model on Spanish texts and convert Tensorflow checkpoint to PyTorch and use the model with the `transformers` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aptXQo8lBtnf"
      },
      "source": [
        "## References\n",
        "- [1] [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://openreview.net/pdf?id=r1xMH1BtvB)\n",
        "- [2] [google-research/electra](https://github.com/google-research/electra) - the official GitHub repository of the original paper\n",
        "- [3] [electra_pytorch](https://github.com/lonePatient/electra_pytorch) - a PyTorch implementation of ELECTRA"
      ]
    }
  ]
}