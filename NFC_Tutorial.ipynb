{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NFC - Tutorial.ipynb",
      "provenance": [],
      "mount_file_id": "1XxneANGLdefGQlPfZidp1wWPLsooLawh",
      "authorship_tag": "ABX9TyMKgcC1x8s84K5BqE7wm0Dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/NFC_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6yuuNIdjgQh"
      },
      "source": [
        "# This notebook is a tutorial for the article: \n",
        "\n",
        "[Neural Collaborative Filtering](https://arxiv.org/pdf/1708.05031.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeNhzxp4j1PD"
      },
      "source": [
        "# Explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cmdlkY7Mrbw"
      },
      "source": [
        "### Matrix Factorization\n",
        "> ##### Let’s begin with Matrix Factorization. It decomposes the utility matrix into two sub matrices. During prediction, we multiply the two sub-matrices to reconstruct the predicted utility matrix. The utility matrix is factorized such that the loss between the multiplication of these two and the true utility matrix is minimized. One commonly used loss function is mean-squared error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAeMzsDMf1bP"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ru0sw_iSaDErRpAh549RxupedZDWQY5s\" alt=\"drawing\" width=\"700\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4MZNSiSggWS"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1SrMZxzBCjhHH3RsALg9HzwJiF7emAqJW\" alt=\"drawing\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uSnTjSPfQC_"
      },
      "source": [
        "> #### Essentially, each user and item is projected onto a latent space, represented by a latent vector. The more similar the two latent vectors are, the more related the corresponding users’ preference. Since we factorize the utility matrix into the same latent space, we can measure the similarity of any two latent vectors with cosine-similarity or dot product. In fact, the prediction for each user/ item entry is computed by the dot product of the corresponding latent vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY56ZRuTe2xj"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ppD_wElrYifbxf2336mrtu_eGtIspgQg\" alt=\"drawing\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxbPL18hNFxs"
      },
      "source": [
        "> #### However, the paper argued that dot product limits the expressiveness of user and item latent vectors. Let’s consider the following case. We first focus on the top three rows of this utility matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEtqvGUghCBe"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Mm2Ma6X45JtM9TVP5AgsZorWhjZP0JPt\" alt=\"drawing\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmW2FLHdNOfg"
      },
      "source": [
        "## Neural Collaborative Filtering\n",
        "> #### The paper proposed Neural Collaborative Filtering as shown in the graph below. In the input layer, the user and item are one-hot encoded. Then, they are mapped to the hidden space with embedding layers accordingly. The Neural FC layer can be any kind neuron connections. Multiple layer perceptron, for example, can be placed here. It claims that with the complicated connection and non-linearity in the Neural CF layers, this model is capable of properly estimating the complex interactions between user and item in the latent space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6Cgx9rQhOrN"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1zplOPLxNYY4aj_j4qcj6_xQoNYkLjMlS\" alt=\"drawing\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xfUBf4zNk71"
      },
      "source": [
        "> #### The NCF is a generalization of Matrix Factorization. We first replace Neural CF layer with a multiplication layer, which performs element-wise multiplication on the two inputs. Then, we set the weight from the multiplication layer to the output layer to be a fixed unit matrix (matrix of all ones) of dimension $K \\times 1$ with linear activation function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMbpcjn_N5Ht"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1cLUSxl342LRyldIP7WcsratPE1HY7Y62\" alt=\"drawing\" width=\"700\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9kResAwO1_x"
      },
      "source": [
        "> #### Then, we have the following equations. The prediction of the unobserved interaction $\\hat{y}_{ui}$ denotes the predicted value of the $(u, i)$ entry on the reconstructed utility matrix. $L$ is the linear activation function, while $⊙$ denotes the element-wise multiplication operation. $p_u$ and $q_i$ are the latent vector of user and item, respectively, and $J$ is the unit matrix of dimension $(K,1)$. Since $J$ is a unit matrix, inside the linear function becomes the inner product between latent vector $p_u$ and $q_i$. Furthermore, because the input and output are the same for linear function, it boils down to the last line. The predicted label is the inner product of the corresponding user and item latent vector. This equation is identical to that shown in the Matrix Factorization section. Thus, this proves that Matrix Factorization is a special case of NCF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyLRq9s9hyub"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1bDx93lQXu7DCfOGAXg-6u6-EVy005CZY\" alt=\"drawing\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPFZVqs1Pasd"
      },
      "source": [
        "## NeuMF\n",
        "\n",
        "> #### In order to introduce additional non-linearity, the final model proposed, NeuMF, includes a Mutliple-layer Perceptron (MLP) module apart from the Generalized Matrix Factorization (GMP) layer. The output of GMF and MLP modules are concatenated and connected with the sigmoid activation output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lyMQEXohzaq"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=10GVO_vb3wBnlMz5kQi8bVIJiMJ01z4-j\" alt=\"drawing\" width=\"700\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-__2afuPrMM"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c630M6OhPtc6"
      },
      "source": [
        "def __init__(self, config):\n",
        "    super(NeuMF, self).__init__()\n",
        "\n",
        "    #mf part\n",
        "    self.embedding_user_mf = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mf)\n",
        "    self.embedding_item_mf = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mf)\n",
        "\n",
        "    #mlp part\n",
        "    self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mlp)\n",
        "    self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mlp)\n",
        "\n",
        "    self.fc_layers = torch.nn.ModuleList()\n",
        "    for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "        self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "    self.logits = torch.nn.Linear(in_features=config['layers'][-1] + config['latent_dim_mf']  , out_features=1)\n",
        "    self.sigmoid = torch.nn.Sigmoid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dENKugXNQObq"
      },
      "source": [
        "> #### The paper claimed that using separate embedding layer for MLP and GMF yield better performance. Therefore, we define the designated embedding layer for these two modules. In addition, ModuleList is leveraged for building multiple layers of perceptron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LyFLQD6QQ7B"
      },
      "source": [
        "def forward(self, user_indices, item_indices, titles):\n",
        "    user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
        "    item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
        "    user_embedding_mf = self.embedding_user_mf(user_indices)\n",
        "    item_embedding_mf = self.embedding_item_mf(item_indices)\n",
        "\n",
        "    #### mf part\n",
        "    mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
        "    mf_vector = torch.nn.Dropout(self.config.dropout_rate_mf)(mf_vector)\n",
        "\n",
        "    #### mlp part        \n",
        "    mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
        "\n",
        "    for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "        mlp_vector = self.fc_layers[idx](mlp_vector)\n",
        "        mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
        "    mlp_vector = torch.nn.Dropout(self.config.dropout_rate_mlp)(mlp_vector)\n",
        "\n",
        "    vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
        "    logits = self.logits(vector)\n",
        "    output = self.sigmoid(logits)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naucsxZzPuOc"
      },
      "source": [
        "## Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kukDfOPIhzgl"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1XZtLeBYiNhQ5MUL4RimK5_3lhTzSczuu\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8cCVeVfhznh"
      },
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1FZOZAMB3uYURFF6jTjYyOAzP1xIRt-az\" alt=\"drawing\" width=\"700\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih3YKtswhz0o"
      },
      "source": [
        "# Code and Implementation with MovieLens 1M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuO6qbuWhz72"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from torch.autograd import Variable\n",
        "# from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "650SOTcNqhvh"
      },
      "source": [
        "# Checkpoints\n",
        "def save_checkpoint(model, model_dir):\n",
        "    torch.save(model.state_dict(), model_dir)\n",
        "\n",
        "def resume_checkpoint(model, model_dir, device_id):\n",
        "    state_dict = torch.load(model_dir,\n",
        "                            map_location=lambda storage, loc: storage.cuda(device=device_id))  # ensure all storage are on gpu\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "# Hyper params\n",
        "def use_cuda(enabled, device_id=0):\n",
        "    if enabled:\n",
        "        assert torch.cuda.is_available(), 'CUDA is not available'\n",
        "        torch.cuda.set_device(device_id)\n",
        "\n",
        "def use_optimizer(network, params):\n",
        "    if params['optimizer'] == 'sgd':\n",
        "        optimizer = torch.optim.SGD(network.parameters(),\n",
        "                                    lr=params['sgd_lr'],\n",
        "                                    momentum=params['sgd_momentum'],\n",
        "                                    weight_decay=params['l2_regularization'])\n",
        "    elif params['optimizer'] == 'adam':\n",
        "        optimizer = torch.optim.Adam(network.parameters(), \n",
        "                                                          lr=params['adam_lr'],\n",
        "                                                          weight_decay=params['l2_regularization'])\n",
        "    elif params['optimizer'] == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(network.parameters(),\n",
        "                                        lr=params['rmsprop_lr'],\n",
        "                                        alpha=params['rmsprop_alpha'],\n",
        "                                        momentum=params['rmsprop_momentum'])\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ocD46lSpFv9"
      },
      "source": [
        "class UserItemRatingDataset(Dataset):\n",
        "    \"\"\"Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset\"\"\"\n",
        "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
        "        \"\"\"\n",
        "        args:\n",
        "\n",
        "            target_tensor: torch.Tensor, the corresponding rating for <user, item> pair\n",
        "        \"\"\"\n",
        "        self.user_tensor = user_tensor\n",
        "        self.item_tensor = item_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.user_tensor.size(0)\n",
        "\n",
        "\n",
        "class SampleGenerator(object):\n",
        "    \"\"\"Construct dataset for NCF\"\"\"\n",
        "\n",
        "    def __init__(self, ratings):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
        "        \"\"\"\n",
        "        assert 'userId' in ratings.columns\n",
        "        assert 'itemId' in ratings.columns\n",
        "        assert 'rating' in ratings.columns\n",
        "\n",
        "        self.ratings = ratings\n",
        "        # explicit feedback using _normalize and implicit using _binarize\n",
        "        # self.preprocess_ratings = self._normalize(ratings)\n",
        "        self.preprocess_ratings = self._binarize(ratings)\n",
        "        self.user_pool = set(self.ratings['userId'].unique())\n",
        "        self.item_pool = set(self.ratings['itemId'].unique())\n",
        "        # create negative item samples for NCF learning\n",
        "        self.negatives = self._sample_negative(ratings)\n",
        "        self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings)\n",
        "\n",
        "    def _normalize(self, ratings):\n",
        "        \"\"\"normalize into [0, 1] from [0, max_rating], explicit feedback\"\"\"\n",
        "        ratings = deepcopy(ratings)\n",
        "        max_rating = ratings.rating.max()\n",
        "        ratings['rating'] = ratings.rating * 1.0 / max_rating\n",
        "        return ratings\n",
        "    \n",
        "    def _binarize(self, ratings):\n",
        "        \"\"\"binarize into 0 or 1, imlicit feedback\"\"\"\n",
        "        ratings = deepcopy(ratings)\n",
        "        ratings['rating'][ratings['rating'] > 0] = 1.0\n",
        "        return ratings\n",
        "\n",
        "    def _split_loo(self, ratings):\n",
        "        \"\"\"leave one out train/test split \"\"\"\n",
        "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
        "        test = ratings[ratings['rank_latest'] == 1]\n",
        "        train = ratings[ratings['rank_latest'] > 1]\n",
        "        assert train['userId'].nunique() == test['userId'].nunique()\n",
        "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
        "\n",
        "    def _sample_negative(self, ratings):\n",
        "        \"\"\"return all negative items & 100 sampled negative items\"\"\"\n",
        "        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n",
        "            columns={'itemId': 'interacted_items'})\n",
        "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
        "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n",
        "        return interact_status[['userId', 'negative_items', 'negative_samples']]\n",
        "\n",
        "    def instance_a_train_loader(self, num_negatives, batch_size):\n",
        "        \"\"\"instance train loader for one training epoch\"\"\"\n",
        "        users, items, ratings = [], [], []\n",
        "        train_ratings = pd.merge(self.train_ratings, self.negatives[['userId', 'negative_items']], on='userId')\n",
        "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
        "        for row in train_ratings.itertuples():\n",
        "            users.append(int(row.userId))\n",
        "            items.append(int(row.itemId))\n",
        "            ratings.append(float(row.rating))\n",
        "            for i in range(num_negatives):\n",
        "                users.append(int(row.userId))\n",
        "                items.append(int(row.negatives[i]))\n",
        "                ratings.append(float(0))  # negative samples get 0 rating\n",
        "        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),\n",
        "                                        item_tensor=torch.LongTensor(items),\n",
        "                                        target_tensor=torch.FloatTensor(ratings))\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    @property\n",
        "    def evaluate_data(self):\n",
        "        \"\"\"create evaluate data\"\"\"\n",
        "        test_ratings = pd.merge(self.test_ratings, self.negatives[['userId', 'negative_samples']], on='userId')\n",
        "        test_users, test_items, negative_users, negative_items = [], [], [], []\n",
        "        for row in test_ratings.itertuples():\n",
        "            test_users.append(int(row.userId))\n",
        "            test_items.append(int(row.itemId))\n",
        "            for i in range(len(row.negative_samples)):\n",
        "                negative_users.append(int(row.userId))\n",
        "                negative_items.append(int(row.negative_samples[i]))\n",
        "        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(negative_users),\n",
        "                torch.LongTensor(negative_items)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfCeRC74qP2c"
      },
      "source": [
        "class MetronAtK(object):\n",
        "    def __init__(self, top_k):\n",
        "        self._top_k = top_k\n",
        "        self._subjects = None  # Subjects which we ran evaluation on\n",
        "\n",
        "    @property\n",
        "    def top_k(self):\n",
        "        return self._top_k\n",
        "\n",
        "    @top_k.setter\n",
        "    def top_k(self, top_k):\n",
        "        self._top_k = top_k\n",
        "\n",
        "    @property\n",
        "    def subjects(self):\n",
        "        return self._subjects\n",
        "\n",
        "    @subjects.setter\n",
        "    def subjects(self, subjects):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            subjects: list, [test_users, test_items, test_scores, negative users, negative items, negative scores]\n",
        "        \"\"\"\n",
        "        assert isinstance(subjects, list)\n",
        "        test_users, test_items, test_scores = subjects[0], subjects[1], subjects[2]\n",
        "        neg_users, neg_items, neg_scores = subjects[3], subjects[4], subjects[5]\n",
        "        # the golden set\n",
        "        test = pd.DataFrame({'user': test_users,\n",
        "                             'test_item': test_items,\n",
        "                             'test_score': test_scores})\n",
        "        # the full set\n",
        "        full = pd.DataFrame({'user': neg_users + test_users,\n",
        "                            'item': neg_items + test_items,\n",
        "                            'score': neg_scores + test_scores})\n",
        "        full = pd.merge(full, test, on=['user'], how='left')\n",
        "        # rank the items according to the scores for each user\n",
        "        full['rank'] = full.groupby('user')['score'].rank(method='first', ascending=False)\n",
        "        full.sort_values(['user', 'rank'], inplace=True)\n",
        "        self._subjects = full\n",
        "\n",
        "    def cal_hit_ratio(self):\n",
        "        \"\"\"Hit Ratio @ top_K\"\"\"\n",
        "        full, top_k = self._subjects, self._top_k\n",
        "        top_k = full[full['rank']<=top_k]\n",
        "        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]  # golden items hit in the top_K items\n",
        "        return len(test_in_top_k) * 1.0 / full['user'].nunique()\n",
        "\n",
        "    def cal_ndcg(self):\n",
        "        full, top_k = self._subjects, self._top_k\n",
        "        top_k = full[full['rank']<=top_k]\n",
        "        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]\n",
        "        test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n",
        "        return test_in_top_k['ndcg'].sum() * 1.0 / full['user'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAMKuPRLrOiS"
      },
      "source": [
        "class Engine(object):\n",
        "    \"\"\"Meta Engine for training & evaluating NCF model\n",
        "\n",
        "    Note: Subclass should implement self.model !\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config  # model configuration\n",
        "        self._metron = MetronAtK(top_k=10)\n",
        "        self.opt = use_optimizer(self.model, config)\n",
        "        # explicit feedback\n",
        "        # self.crit = torch.nn.MSELoss()\n",
        "        # implicit feedback\n",
        "        self.crit = torch.nn.BCELoss()\n",
        "\n",
        "    def train_single_batch(self, users, items, ratings):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        if self.config['use_cuda'] is True:\n",
        "            users, items, ratings = users.cuda(), items.cuda(), ratings.cuda()\n",
        "        self.opt.zero_grad()\n",
        "        ratings_pred = self.model(users, items)\n",
        "        loss = self.crit(ratings_pred.view(-1), ratings)\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "        loss = loss.item()\n",
        "        return loss\n",
        "\n",
        "    def train_an_epoch(self, train_loader, epoch_id):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch_id, batch in enumerate(train_loader):\n",
        "            assert isinstance(batch[0], torch.LongTensor)\n",
        "            user, item, rating = batch[0], batch[1], batch[2]\n",
        "            rating = rating.float()\n",
        "            loss = self.train_single_batch(user, item, rating)\n",
        "            if batch_id % 400 == 0:\n",
        "                print('[Training Epoch {}] Batch {}, Loss {}'.format(epoch_id, batch_id, loss))\n",
        "            total_loss += loss\n",
        "        # self._writer.add_scalar('model/loss', total_loss, epoch_id)\n",
        "\n",
        "    def evaluate(self, evaluate_data, epoch_id):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_users, test_items = evaluate_data[0], evaluate_data[1]\n",
        "            negative_users, negative_items = evaluate_data[2], evaluate_data[3]\n",
        "            if self.config['use_cuda'] is True:\n",
        "                test_users = test_users.cuda()\n",
        "                test_items = test_items.cuda()\n",
        "                negative_users = negative_users.cuda()\n",
        "                negative_items = negative_items.cuda()\n",
        "            test_scores = self.model(test_users, test_items)\n",
        "            negative_scores = self.model(negative_users, negative_items)\n",
        "            if self.config['use_cuda'] is True:\n",
        "                test_users = test_users.cpu()\n",
        "                test_items = test_items.cpu()\n",
        "                test_scores = test_scores.cpu()\n",
        "                negative_users = negative_users.cpu()\n",
        "                negative_items = negative_items.cpu()\n",
        "                negative_scores = negative_scores.cpu()\n",
        "            self._metron.subjects = [test_users.data.view(-1).tolist(),\n",
        "                                 test_items.data.view(-1).tolist(),\n",
        "                                 test_scores.data.view(-1).tolist(),\n",
        "                                 negative_users.data.view(-1).tolist(),\n",
        "                                 negative_items.data.view(-1).tolist(),\n",
        "                                 negative_scores.data.view(-1).tolist()]\n",
        "        hit_ratio, ndcg = self._metron.cal_hit_ratio(), self._metron.cal_ndcg()\n",
        "        # self._writer.add_scalar('performance/HR', hit_ratio, epoch_id)\n",
        "        # self._writer.add_scalar('performance/NDCG', ndcg, epoch_id)\n",
        "        print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f}'.format(epoch_id, hit_ratio, ndcg))\n",
        "        return hit_ratio, ndcg\n",
        "\n",
        "    def save(self, alias, epoch_id, hit_ratio, ndcg):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        model_dir = self.config['model_dir'].format(alias, epoch_id, hit_ratio, ndcg)\n",
        "        save_checkpoint(self.model, model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43s9HkCbq1om"
      },
      "source": [
        "class GMF(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GMF, self).__init__()\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim = config['latent_dim']\n",
        "\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=self.latent_dim, out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class GMFEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(GMFEngine, self).__init__(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1v9lFzirHN9"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MLP, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim = config['latent_dim']\n",
        "\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = torch.nn.ReLU()(vector)\n",
        "            # vector = torch.nn.BatchNorm1d()(vector)\n",
        "            # vector = torch.nn.Dropout(p=0.5)(vector)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "    def load_pretrain_weights(self):\n",
        "        \"\"\"Loading weights from trained GMF model\"\"\"\n",
        "        config = self.config\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n",
        "        self.embedding_user.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "\n",
        "class MLPEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = MLP(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(MLPEngine, self).__init__(config)\n",
        "        print(self.model)\n",
        "\n",
        "        if config['pretrain']:\n",
        "            self.model.load_pretrain_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pglY_ABGroNn"
      },
      "source": [
        "class NeuMF(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(NeuMF, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim_mf = config['latent_dim_mf']\n",
        "        self.latent_dim_mlp = config['latent_dim_mlp']\n",
        "\n",
        "        self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_user_mf = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mf)\n",
        "        self.embedding_item_mf = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mf)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1] + config['latent_dim_mf'], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
        "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
        "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
        "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
        "\n",
        "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
        "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
        "\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
        "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
        "\n",
        "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "    def load_pretrain_weights(self):\n",
        "        \"\"\"Loading weights from trained MLP model & GMF model\"\"\"\n",
        "        config = self.config\n",
        "        config['latent_dim'] = config['latent_dim_mlp']\n",
        "        mlp_model = MLP(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            mlp_model.cuda()\n",
        "        resume_checkpoint(mlp_model, model_dir=config['pretrain_mlp'], device_id=config['device_id'])\n",
        "\n",
        "        self.embedding_user_mlp.weight.data = mlp_model.embedding_user.weight.data\n",
        "        self.embedding_item_mlp.weight.data = mlp_model.embedding_item.weight.data\n",
        "        for idx in range(len(self.fc_layers)):\n",
        "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
        "\n",
        "        config['latent_dim'] = config['latent_dim_mf']\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n",
        "        self.embedding_user_mf.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item_mf.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
        "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)\n",
        "\n",
        "\n",
        "class NeuMFEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = NeuMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(NeuMFEngine, self).__init__(config)\n",
        "        print(self.model)\n",
        "\n",
        "        if config['pretrain']:\n",
        "            self.model.load_pretrain_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iC9HGAkr-d4",
        "outputId": "6130f4ae-0b17-4f69-e8b9-e8319084d449"
      },
      "source": [
        "!mkdir checkpoints"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘checkpoints’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC7ix8uirueW",
        "outputId": "4421b373-8686-45b8-cab1-b7810662aa1b"
      },
      "source": [
        "gmf_config = {'alias': 'gmf_factor8neg4-implict',\n",
        "              'num_epoch': 200,\n",
        "              'batch_size': 1024,\n",
        "              # 'optimizer': 'sgd',\n",
        "              # 'sgd_lr': 1e-3,\n",
        "              # 'sgd_momentum': 0.9,\n",
        "              # 'optimizer': 'rmsprop',\n",
        "              # 'rmsprop_lr': 1e-3,\n",
        "              # 'rmsprop_alpha': 0.99,\n",
        "              # 'rmsprop_momentum': 0,\n",
        "              'optimizer': 'adam',\n",
        "              'adam_lr': 1e-3,\n",
        "              'num_users': 6040,\n",
        "              'num_items': 3706,\n",
        "              'latent_dim': 8,\n",
        "              'num_negative': 4,\n",
        "              'l2_regularization': 0, # 0.01\n",
        "              'use_cuda': True,\n",
        "              'device_id': 0,\n",
        "              'model_dir':'/content/checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'\n",
        "              }\n",
        "\n",
        "mlp_config = {'alias': 'mlp_factor8neg4_bz256_166432168_pretrain_reg_0.0000001',\n",
        "              'num_epoch': 200,\n",
        "              'batch_size': 256,  # 1024,\n",
        "              'optimizer': 'adam',\n",
        "              'adam_lr': 1e-3,\n",
        "              'num_users': 6040,\n",
        "              'num_items': 3706,\n",
        "              'latent_dim': 8,\n",
        "              'num_negative': 4,\n",
        "              'layers': [16,64,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
        "              'l2_regularization': 0.0000001,  # MLP model is sensitive to hyper params\n",
        "              'use_cuda': True,\n",
        "              'device_id': 7,\n",
        "              'pretrain': False,\n",
        "            #   'pretrain_mf': '/content/checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
        "              'model_dir':'/content/checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}\n",
        "\n",
        "neumf_config = {'alias': 'pretrain_neumf_factor8neg4',\n",
        "                'num_epoch': 200,\n",
        "                'batch_size': 1024,\n",
        "                'optimizer': 'adam',\n",
        "                'adam_lr': 1e-3,\n",
        "                'num_users': 6040,\n",
        "                'num_items': 3706,\n",
        "                'latent_dim_mf': 8,\n",
        "                'latent_dim_mlp': 8,\n",
        "                'num_negative': 4,\n",
        "                'layers': [16,32,16,8],  # layers[0] is the concat of latent user vector & latent item vector\n",
        "                'l2_regularization': 0.01,\n",
        "                'use_cuda': True,\n",
        "                'device_id': 7,\n",
        "                'pretrain': False,\n",
        "                # 'pretrain_mf': 'checkpoints/{}'.format('gmf_factor8neg4_Epoch100_HR0.6391_NDCG0.2852.model'),\n",
        "                # 'pretrain_mlp': 'checkpoints/{}'.format('mlp_factor8neg4_Epoch100_HR0.5606_NDCG0.2463.model'),\n",
        "                'model_dir':'/content/checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'\n",
        "                }\n",
        "\n",
        "# Load Data\n",
        "ml1m_dir = '/content/drive/MyDrive/Colab Notebooks/RecSys/movielens_datasets/ml-1m/ratings.dat'\n",
        "ml1m_rating = pd.read_csv(ml1m_dir, sep='::', header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')\n",
        "\n",
        "# Reindex\n",
        "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
        "user_id['userId'] = np.arange(len(user_id))\n",
        "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
        "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
        "item_id['itemId'] = np.arange(len(item_id))\n",
        "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
        "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
        "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
        "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))\n",
        "\n",
        "# DataLoader for training\n",
        "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
        "evaluate_data = sample_generator.evaluate_data\n",
        "\n",
        "# Specify the exact model\n",
        "config = gmf_config\n",
        "engine = GMFEngine(config)\n",
        "# config = mlp_config\n",
        "# engine = MLPEngine(config)\n",
        "# config = neumf_config\n",
        "# engine = NeuMFEngine(config)\n",
        "for epoch in range(config['num_epoch']):\n",
        "    print('Epoch {} starts !'.format(epoch))\n",
        "    print('-' * 80)\n",
        "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
        "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
        "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
        "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Range of userId is [0, 6039]\n",
            "Range of itemId is [0, 3705]\n",
            "Epoch 0 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 0] Batch 0, Loss 0.634957492351532\n",
            "[Training Epoch 0] Batch 100, Loss 0.6084944009780884\n",
            "[Training Epoch 0] Batch 200, Loss 0.5835074186325073\n",
            "[Training Epoch 0] Batch 300, Loss 0.5762701034545898\n",
            "[Training Epoch 0] Batch 400, Loss 0.570959746837616\n",
            "[Training Epoch 0] Batch 500, Loss 0.5489323139190674\n",
            "[Training Epoch 0] Batch 600, Loss 0.532582700252533\n",
            "[Training Epoch 0] Batch 700, Loss 0.5126482844352722\n",
            "[Training Epoch 0] Batch 800, Loss 0.5320024490356445\n",
            "[Training Epoch 0] Batch 900, Loss 0.538666844367981\n",
            "[Training Epoch 0] Batch 1000, Loss 0.49481943249702454\n",
            "[Training Epoch 0] Batch 1100, Loss 0.4894544184207916\n",
            "[Training Epoch 0] Batch 1200, Loss 0.48130548000335693\n",
            "[Training Epoch 0] Batch 1300, Loss 0.5132834911346436\n",
            "[Training Epoch 0] Batch 1400, Loss 0.5031135082244873\n",
            "[Training Epoch 0] Batch 1500, Loss 0.5163837671279907\n",
            "[Training Epoch 0] Batch 1600, Loss 0.48637908697128296\n",
            "[Training Epoch 0] Batch 1700, Loss 0.5203533172607422\n",
            "[Training Epoch 0] Batch 1800, Loss 0.512492299079895\n",
            "[Training Epoch 0] Batch 1900, Loss 0.48810654878616333\n",
            "[Training Epoch 0] Batch 2000, Loss 0.4919714331626892\n",
            "[Training Epoch 0] Batch 2100, Loss 0.5018827319145203\n",
            "[Training Epoch 0] Batch 2200, Loss 0.4942091107368469\n",
            "[Training Epoch 0] Batch 2300, Loss 0.49138036370277405\n",
            "[Training Epoch 0] Batch 2400, Loss 0.5062354803085327\n",
            "[Training Epoch 0] Batch 2500, Loss 0.4955497980117798\n",
            "[Training Epoch 0] Batch 2600, Loss 0.4942401051521301\n",
            "[Training Epoch 0] Batch 2700, Loss 0.47791236639022827\n",
            "[Training Epoch 0] Batch 2800, Loss 0.493691086769104\n",
            "[Training Epoch 0] Batch 2900, Loss 0.49014076590538025\n",
            "[Training Epoch 0] Batch 3000, Loss 0.4845649003982544\n",
            "[Training Epoch 0] Batch 3100, Loss 0.5238281488418579\n",
            "[Training Epoch 0] Batch 3200, Loss 0.5089951157569885\n",
            "[Training Epoch 0] Batch 3300, Loss 0.4842826724052429\n",
            "[Training Epoch 0] Batch 3400, Loss 0.47343212366104126\n",
            "[Training Epoch 0] Batch 3500, Loss 0.4898252785205841\n",
            "[Training Epoch 0] Batch 3600, Loss 0.5062601566314697\n",
            "[Training Epoch 0] Batch 3700, Loss 0.5018775463104248\n",
            "[Training Epoch 0] Batch 3800, Loss 0.5047386884689331\n",
            "[Training Epoch 0] Batch 3900, Loss 0.509033739566803\n",
            "[Training Epoch 0] Batch 4000, Loss 0.5169150233268738\n",
            "[Training Epoch 0] Batch 4100, Loss 0.4803113639354706\n",
            "[Training Epoch 0] Batch 4200, Loss 0.48717963695526123\n",
            "[Training Epoch 0] Batch 4300, Loss 0.48857951164245605\n",
            "[Training Epoch 0] Batch 4400, Loss 0.497758686542511\n",
            "[Training Epoch 0] Batch 4500, Loss 0.5129822492599487\n",
            "[Training Epoch 0] Batch 4600, Loss 0.5018301010131836\n",
            "[Training Epoch 0] Batch 4700, Loss 0.49250030517578125\n",
            "[Training Epoch 0] Batch 4800, Loss 0.4913768470287323\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 0] HR = 0.0978, NDCG = 0.0428\n",
            "Epoch 1 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 1] Batch 0, Loss 0.46698641777038574\n",
            "[Training Epoch 1] Batch 100, Loss 0.5061224102973938\n",
            "[Training Epoch 1] Batch 200, Loss 0.5128269195556641\n",
            "[Training Epoch 1] Batch 300, Loss 0.4776607155799866\n",
            "[Training Epoch 1] Batch 400, Loss 0.5141273736953735\n",
            "[Training Epoch 1] Batch 500, Loss 0.49529266357421875\n",
            "[Training Epoch 1] Batch 600, Loss 0.5304645299911499\n",
            "[Training Epoch 1] Batch 700, Loss 0.4937954545021057\n",
            "[Training Epoch 1] Batch 800, Loss 0.5020802021026611\n",
            "[Training Epoch 1] Batch 900, Loss 0.5221403241157532\n",
            "[Training Epoch 1] Batch 1000, Loss 0.5045718550682068\n",
            "[Training Epoch 1] Batch 1100, Loss 0.4829954206943512\n",
            "[Training Epoch 1] Batch 1200, Loss 0.5141868591308594\n",
            "[Training Epoch 1] Batch 1300, Loss 0.502382755279541\n",
            "[Training Epoch 1] Batch 1400, Loss 0.502103328704834\n",
            "[Training Epoch 1] Batch 1500, Loss 0.47194793820381165\n",
            "[Training Epoch 1] Batch 1600, Loss 0.4938942790031433\n",
            "[Training Epoch 1] Batch 1700, Loss 0.5239670276641846\n",
            "[Training Epoch 1] Batch 1800, Loss 0.4955097436904907\n",
            "[Training Epoch 1] Batch 1900, Loss 0.4802895188331604\n",
            "[Training Epoch 1] Batch 2000, Loss 0.47476726770401\n",
            "[Training Epoch 1] Batch 2100, Loss 0.48812422156333923\n",
            "[Training Epoch 1] Batch 2200, Loss 0.4914399981498718\n",
            "[Training Epoch 1] Batch 2300, Loss 0.5381681323051453\n",
            "[Training Epoch 1] Batch 2400, Loss 0.5305891036987305\n",
            "[Training Epoch 1] Batch 2500, Loss 0.49934935569763184\n",
            "[Training Epoch 1] Batch 2600, Loss 0.49280405044555664\n",
            "[Training Epoch 1] Batch 2700, Loss 0.4994889497756958\n",
            "[Training Epoch 1] Batch 2800, Loss 0.5150998830795288\n",
            "[Training Epoch 1] Batch 2900, Loss 0.4926181435585022\n",
            "[Training Epoch 1] Batch 3000, Loss 0.4805949330329895\n",
            "[Training Epoch 1] Batch 3100, Loss 0.5129451751708984\n",
            "[Training Epoch 1] Batch 3200, Loss 0.4494751989841461\n",
            "[Training Epoch 1] Batch 3300, Loss 0.5104631185531616\n",
            "[Training Epoch 1] Batch 3400, Loss 0.5195478200912476\n",
            "[Training Epoch 1] Batch 3500, Loss 0.49819937348365784\n",
            "[Training Epoch 1] Batch 3600, Loss 0.4707185924053192\n",
            "[Training Epoch 1] Batch 3700, Loss 0.5274159908294678\n",
            "[Training Epoch 1] Batch 3800, Loss 0.49948355555534363\n",
            "[Training Epoch 1] Batch 3900, Loss 0.5333442687988281\n",
            "[Training Epoch 1] Batch 4000, Loss 0.5062028169631958\n",
            "[Training Epoch 1] Batch 4100, Loss 0.5016922950744629\n",
            "[Training Epoch 1] Batch 4200, Loss 0.49543488025665283\n",
            "[Training Epoch 1] Batch 4300, Loss 0.5128827095031738\n",
            "[Training Epoch 1] Batch 4400, Loss 0.5064033269882202\n",
            "[Training Epoch 1] Batch 4500, Loss 0.47367048263549805\n",
            "[Training Epoch 1] Batch 4600, Loss 0.4911566376686096\n",
            "[Training Epoch 1] Batch 4700, Loss 0.4733109772205353\n",
            "[Training Epoch 1] Batch 4800, Loss 0.47908228635787964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 1] HR = 0.1018, NDCG = 0.0452\n",
            "Epoch 2 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 2] Batch 0, Loss 0.5183069109916687\n",
            "[Training Epoch 2] Batch 100, Loss 0.4943470060825348\n",
            "[Training Epoch 2] Batch 200, Loss 0.4967399537563324\n",
            "[Training Epoch 2] Batch 300, Loss 0.47916972637176514\n",
            "[Training Epoch 2] Batch 400, Loss 0.4874839186668396\n",
            "[Training Epoch 2] Batch 500, Loss 0.46967369318008423\n",
            "[Training Epoch 2] Batch 600, Loss 0.5061054825782776\n",
            "[Training Epoch 2] Batch 700, Loss 0.49123767018318176\n",
            "[Training Epoch 2] Batch 800, Loss 0.4934993088245392\n",
            "[Training Epoch 2] Batch 900, Loss 0.48308172821998596\n",
            "[Training Epoch 2] Batch 1000, Loss 0.49626249074935913\n",
            "[Training Epoch 2] Batch 1100, Loss 0.46662434935569763\n",
            "[Training Epoch 2] Batch 1200, Loss 0.5198988914489746\n",
            "[Training Epoch 2] Batch 1300, Loss 0.5275518894195557\n",
            "[Training Epoch 2] Batch 1400, Loss 0.4987758994102478\n",
            "[Training Epoch 2] Batch 1500, Loss 0.4773457944393158\n",
            "[Training Epoch 2] Batch 1600, Loss 0.48437565565109253\n",
            "[Training Epoch 2] Batch 1700, Loss 0.5113565921783447\n",
            "[Training Epoch 2] Batch 1800, Loss 0.4875277876853943\n",
            "[Training Epoch 2] Batch 1900, Loss 0.5177731513977051\n",
            "[Training Epoch 2] Batch 2000, Loss 0.527929425239563\n",
            "[Training Epoch 2] Batch 2100, Loss 0.4893949031829834\n",
            "[Training Epoch 2] Batch 2200, Loss 0.5150365829467773\n",
            "[Training Epoch 2] Batch 2300, Loss 0.512103796005249\n",
            "[Training Epoch 2] Batch 2400, Loss 0.504146158695221\n",
            "[Training Epoch 2] Batch 2500, Loss 0.5054517388343811\n",
            "[Training Epoch 2] Batch 2600, Loss 0.4778141379356384\n",
            "[Training Epoch 2] Batch 2700, Loss 0.5110712647438049\n",
            "[Training Epoch 2] Batch 2800, Loss 0.5503172874450684\n",
            "[Training Epoch 2] Batch 2900, Loss 0.4948132038116455\n",
            "[Training Epoch 2] Batch 3000, Loss 0.5339832305908203\n",
            "[Training Epoch 2] Batch 3100, Loss 0.5054560899734497\n",
            "[Training Epoch 2] Batch 3200, Loss 0.482774943113327\n",
            "[Training Epoch 2] Batch 3300, Loss 0.4874885678291321\n",
            "[Training Epoch 2] Batch 3400, Loss 0.5067917108535767\n",
            "[Training Epoch 2] Batch 3500, Loss 0.5186461210250854\n",
            "[Training Epoch 2] Batch 3600, Loss 0.5131926536560059\n",
            "[Training Epoch 2] Batch 3700, Loss 0.49949413537979126\n",
            "[Training Epoch 2] Batch 3800, Loss 0.4959166944026947\n",
            "[Training Epoch 2] Batch 3900, Loss 0.4720742702484131\n",
            "[Training Epoch 2] Batch 4000, Loss 0.4702047109603882\n",
            "[Training Epoch 2] Batch 4100, Loss 0.4847579300403595\n",
            "[Training Epoch 2] Batch 4200, Loss 0.5047685503959656\n",
            "[Training Epoch 2] Batch 4300, Loss 0.47997143864631653\n",
            "[Training Epoch 2] Batch 4400, Loss 0.4769866466522217\n",
            "[Training Epoch 2] Batch 4500, Loss 0.4835183024406433\n",
            "[Training Epoch 2] Batch 4600, Loss 0.5124613046646118\n",
            "[Training Epoch 2] Batch 4700, Loss 0.5393128991127014\n",
            "[Training Epoch 2] Batch 4800, Loss 0.4909811019897461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 2] HR = 0.1743, NDCG = 0.0880\n",
            "Epoch 3 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 3] Batch 0, Loss 0.4722650349140167\n",
            "[Training Epoch 3] Batch 100, Loss 0.460767924785614\n",
            "[Training Epoch 3] Batch 200, Loss 0.4538935720920563\n",
            "[Training Epoch 3] Batch 300, Loss 0.5012592077255249\n",
            "[Training Epoch 3] Batch 400, Loss 0.48231786489486694\n",
            "[Training Epoch 3] Batch 500, Loss 0.49031341075897217\n",
            "[Training Epoch 3] Batch 600, Loss 0.46080905199050903\n",
            "[Training Epoch 3] Batch 700, Loss 0.4734598994255066\n",
            "[Training Epoch 3] Batch 800, Loss 0.4512655735015869\n",
            "[Training Epoch 3] Batch 900, Loss 0.4607531726360321\n",
            "[Training Epoch 3] Batch 1000, Loss 0.4813666045665741\n",
            "[Training Epoch 3] Batch 1100, Loss 0.4877898395061493\n",
            "[Training Epoch 3] Batch 1200, Loss 0.46107885241508484\n",
            "[Training Epoch 3] Batch 1300, Loss 0.4831329882144928\n",
            "[Training Epoch 3] Batch 1400, Loss 0.4391944408416748\n",
            "[Training Epoch 3] Batch 1500, Loss 0.4502013325691223\n",
            "[Training Epoch 3] Batch 1600, Loss 0.441311776638031\n",
            "[Training Epoch 3] Batch 1700, Loss 0.4822120666503906\n",
            "[Training Epoch 3] Batch 1800, Loss 0.46069493889808655\n",
            "[Training Epoch 3] Batch 1900, Loss 0.4741021394729614\n",
            "[Training Epoch 3] Batch 2000, Loss 0.4129854738712311\n",
            "[Training Epoch 3] Batch 2100, Loss 0.450812965631485\n",
            "[Training Epoch 3] Batch 2200, Loss 0.4014854431152344\n",
            "[Training Epoch 3] Batch 2300, Loss 0.4410766065120697\n",
            "[Training Epoch 3] Batch 2400, Loss 0.4304954409599304\n",
            "[Training Epoch 3] Batch 2500, Loss 0.4380394220352173\n",
            "[Training Epoch 3] Batch 2600, Loss 0.4432085156440735\n",
            "[Training Epoch 3] Batch 2700, Loss 0.4659179151058197\n",
            "[Training Epoch 3] Batch 2800, Loss 0.41371431946754456\n",
            "[Training Epoch 3] Batch 2900, Loss 0.43586117029190063\n",
            "[Training Epoch 3] Batch 3000, Loss 0.44705459475517273\n",
            "[Training Epoch 3] Batch 3100, Loss 0.4058983325958252\n",
            "[Training Epoch 3] Batch 3200, Loss 0.4184146225452423\n",
            "[Training Epoch 3] Batch 3300, Loss 0.4086107015609741\n",
            "[Training Epoch 3] Batch 3400, Loss 0.38649237155914307\n",
            "[Training Epoch 3] Batch 3500, Loss 0.4244864583015442\n",
            "[Training Epoch 3] Batch 3600, Loss 0.4228004217147827\n",
            "[Training Epoch 3] Batch 3700, Loss 0.42128562927246094\n",
            "[Training Epoch 3] Batch 3800, Loss 0.3904225528240204\n",
            "[Training Epoch 3] Batch 3900, Loss 0.3890485465526581\n",
            "[Training Epoch 3] Batch 4000, Loss 0.4161278307437897\n",
            "[Training Epoch 3] Batch 4100, Loss 0.4135397672653198\n",
            "[Training Epoch 3] Batch 4200, Loss 0.3810668885707855\n",
            "[Training Epoch 3] Batch 4300, Loss 0.4086306393146515\n",
            "[Training Epoch 3] Batch 4400, Loss 0.3879939615726471\n",
            "[Training Epoch 3] Batch 4500, Loss 0.4037541151046753\n",
            "[Training Epoch 3] Batch 4600, Loss 0.3862755596637726\n",
            "[Training Epoch 3] Batch 4700, Loss 0.40322399139404297\n",
            "[Training Epoch 3] Batch 4800, Loss 0.4119006395339966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 3] HR = 0.3599, NDCG = 0.1912\n",
            "Epoch 4 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 4] Batch 0, Loss 0.39166998863220215\n",
            "[Training Epoch 4] Batch 100, Loss 0.3865374028682709\n",
            "[Training Epoch 4] Batch 200, Loss 0.3883734345436096\n",
            "[Training Epoch 4] Batch 300, Loss 0.4173715114593506\n",
            "[Training Epoch 4] Batch 400, Loss 0.40610748529434204\n",
            "[Training Epoch 4] Batch 500, Loss 0.37648719549179077\n",
            "[Training Epoch 4] Batch 600, Loss 0.37526628375053406\n",
            "[Training Epoch 4] Batch 700, Loss 0.3643900752067566\n",
            "[Training Epoch 4] Batch 800, Loss 0.3753286898136139\n",
            "[Training Epoch 4] Batch 900, Loss 0.3579120635986328\n",
            "[Training Epoch 4] Batch 1000, Loss 0.3687756359577179\n",
            "[Training Epoch 4] Batch 1100, Loss 0.38112229108810425\n",
            "[Training Epoch 4] Batch 1200, Loss 0.37774091958999634\n",
            "[Training Epoch 4] Batch 1300, Loss 0.4021266996860504\n",
            "[Training Epoch 4] Batch 1400, Loss 0.4133418798446655\n",
            "[Training Epoch 4] Batch 1500, Loss 0.37678462266921997\n",
            "[Training Epoch 4] Batch 1600, Loss 0.4057711064815521\n",
            "[Training Epoch 4] Batch 1700, Loss 0.3930039405822754\n",
            "[Training Epoch 4] Batch 1800, Loss 0.38926970958709717\n",
            "[Training Epoch 4] Batch 1900, Loss 0.3686257004737854\n",
            "[Training Epoch 4] Batch 2000, Loss 0.37692439556121826\n",
            "[Training Epoch 4] Batch 2100, Loss 0.3662933111190796\n",
            "[Training Epoch 4] Batch 2200, Loss 0.41953039169311523\n",
            "[Training Epoch 4] Batch 2300, Loss 0.37124842405319214\n",
            "[Training Epoch 4] Batch 2400, Loss 0.3730958104133606\n",
            "[Training Epoch 4] Batch 2500, Loss 0.39321663975715637\n",
            "[Training Epoch 4] Batch 2600, Loss 0.3827947676181793\n",
            "[Training Epoch 4] Batch 2700, Loss 0.3797875940799713\n",
            "[Training Epoch 4] Batch 2800, Loss 0.35361623764038086\n",
            "[Training Epoch 4] Batch 2900, Loss 0.3881109952926636\n",
            "[Training Epoch 4] Batch 3000, Loss 0.36945804953575134\n",
            "[Training Epoch 4] Batch 3100, Loss 0.36663079261779785\n",
            "[Training Epoch 4] Batch 3200, Loss 0.39630934596061707\n",
            "[Training Epoch 4] Batch 3300, Loss 0.3554978370666504\n",
            "[Training Epoch 4] Batch 3400, Loss 0.3653966784477234\n",
            "[Training Epoch 4] Batch 3500, Loss 0.3728793263435364\n",
            "[Training Epoch 4] Batch 3600, Loss 0.3652140498161316\n",
            "[Training Epoch 4] Batch 3700, Loss 0.350410521030426\n",
            "[Training Epoch 4] Batch 3800, Loss 0.36188021302223206\n",
            "[Training Epoch 4] Batch 3900, Loss 0.3785156309604645\n",
            "[Training Epoch 4] Batch 4000, Loss 0.3531181812286377\n",
            "[Training Epoch 4] Batch 4100, Loss 0.3586949110031128\n",
            "[Training Epoch 4] Batch 4200, Loss 0.35369348526000977\n",
            "[Training Epoch 4] Batch 4300, Loss 0.38449588418006897\n",
            "[Training Epoch 4] Batch 4400, Loss 0.36332377791404724\n",
            "[Training Epoch 4] Batch 4500, Loss 0.3901408314704895\n",
            "[Training Epoch 4] Batch 4600, Loss 0.3890498876571655\n",
            "[Training Epoch 4] Batch 4700, Loss 0.36048582196235657\n",
            "[Training Epoch 4] Batch 4800, Loss 0.37458890676498413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 4] HR = 0.4174, NDCG = 0.2293\n",
            "Epoch 5 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 5] Batch 0, Loss 0.35412365198135376\n",
            "[Training Epoch 5] Batch 100, Loss 0.3520087003707886\n",
            "[Training Epoch 5] Batch 200, Loss 0.3375002443790436\n",
            "[Training Epoch 5] Batch 300, Loss 0.40152955055236816\n",
            "[Training Epoch 5] Batch 400, Loss 0.3714994490146637\n",
            "[Training Epoch 5] Batch 500, Loss 0.34630513191223145\n",
            "[Training Epoch 5] Batch 600, Loss 0.35791438817977905\n",
            "[Training Epoch 5] Batch 700, Loss 0.33760279417037964\n",
            "[Training Epoch 5] Batch 800, Loss 0.33917728066444397\n",
            "[Training Epoch 5] Batch 900, Loss 0.3787689507007599\n",
            "[Training Epoch 5] Batch 1000, Loss 0.3520825207233429\n",
            "[Training Epoch 5] Batch 1100, Loss 0.34137505292892456\n",
            "[Training Epoch 5] Batch 1200, Loss 0.35788583755493164\n",
            "[Training Epoch 5] Batch 1300, Loss 0.3765084743499756\n",
            "[Training Epoch 5] Batch 1400, Loss 0.3649022579193115\n",
            "[Training Epoch 5] Batch 1500, Loss 0.37655478715896606\n",
            "[Training Epoch 5] Batch 1600, Loss 0.385924369096756\n",
            "[Training Epoch 5] Batch 1700, Loss 0.3500009775161743\n",
            "[Training Epoch 5] Batch 1800, Loss 0.37313348054885864\n",
            "[Training Epoch 5] Batch 1900, Loss 0.3685219883918762\n",
            "[Training Epoch 5] Batch 2000, Loss 0.36991721391677856\n",
            "[Training Epoch 5] Batch 2100, Loss 0.3628619909286499\n",
            "[Training Epoch 5] Batch 2200, Loss 0.34005531668663025\n",
            "[Training Epoch 5] Batch 2300, Loss 0.32190194725990295\n",
            "[Training Epoch 5] Batch 2400, Loss 0.3384985029697418\n",
            "[Training Epoch 5] Batch 2500, Loss 0.35674721002578735\n",
            "[Training Epoch 5] Batch 2600, Loss 0.3730979561805725\n",
            "[Training Epoch 5] Batch 2700, Loss 0.36849913001060486\n",
            "[Training Epoch 5] Batch 2800, Loss 0.35227668285369873\n",
            "[Training Epoch 5] Batch 2900, Loss 0.40590181946754456\n",
            "[Training Epoch 5] Batch 3000, Loss 0.4049115777015686\n",
            "[Training Epoch 5] Batch 3100, Loss 0.34150516986846924\n",
            "[Training Epoch 5] Batch 3200, Loss 0.35333573818206787\n",
            "[Training Epoch 5] Batch 3300, Loss 0.3775635063648224\n",
            "[Training Epoch 5] Batch 3400, Loss 0.3784836232662201\n",
            "[Training Epoch 5] Batch 3500, Loss 0.3572847843170166\n",
            "[Training Epoch 5] Batch 3600, Loss 0.36962050199508667\n",
            "[Training Epoch 5] Batch 3700, Loss 0.32596513628959656\n",
            "[Training Epoch 5] Batch 3800, Loss 0.353051096200943\n",
            "[Training Epoch 5] Batch 3900, Loss 0.34830090403556824\n",
            "[Training Epoch 5] Batch 4000, Loss 0.3535200357437134\n",
            "[Training Epoch 5] Batch 4100, Loss 0.36855000257492065\n",
            "[Training Epoch 5] Batch 4200, Loss 0.3692361116409302\n",
            "[Training Epoch 5] Batch 4300, Loss 0.3184894919395447\n",
            "[Training Epoch 5] Batch 4400, Loss 0.3371632993221283\n",
            "[Training Epoch 5] Batch 4500, Loss 0.35526925325393677\n",
            "[Training Epoch 5] Batch 4600, Loss 0.3486722707748413\n",
            "[Training Epoch 5] Batch 4700, Loss 0.3220207989215851\n",
            "[Training Epoch 5] Batch 4800, Loss 0.377959668636322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 5] HR = 0.4333, NDCG = 0.2407\n",
            "Epoch 6 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 6] Batch 0, Loss 0.36646217107772827\n",
            "[Training Epoch 6] Batch 100, Loss 0.3620430827140808\n",
            "[Training Epoch 6] Batch 200, Loss 0.34023532271385193\n",
            "[Training Epoch 6] Batch 300, Loss 0.3653809726238251\n",
            "[Training Epoch 6] Batch 400, Loss 0.3498869240283966\n",
            "[Training Epoch 6] Batch 500, Loss 0.3615694046020508\n",
            "[Training Epoch 6] Batch 600, Loss 0.36719271540641785\n",
            "[Training Epoch 6] Batch 700, Loss 0.364743709564209\n",
            "[Training Epoch 6] Batch 800, Loss 0.3807733654975891\n",
            "[Training Epoch 6] Batch 900, Loss 0.32320600748062134\n",
            "[Training Epoch 6] Batch 1000, Loss 0.36440378427505493\n",
            "[Training Epoch 6] Batch 1100, Loss 0.33752918243408203\n",
            "[Training Epoch 6] Batch 1200, Loss 0.38863539695739746\n",
            "[Training Epoch 6] Batch 1300, Loss 0.3732525706291199\n",
            "[Training Epoch 6] Batch 1400, Loss 0.34399136900901794\n",
            "[Training Epoch 6] Batch 1500, Loss 0.3546670079231262\n",
            "[Training Epoch 6] Batch 1600, Loss 0.35373711585998535\n",
            "[Training Epoch 6] Batch 1700, Loss 0.35513055324554443\n",
            "[Training Epoch 6] Batch 1800, Loss 0.3667319416999817\n",
            "[Training Epoch 6] Batch 1900, Loss 0.3552703857421875\n",
            "[Training Epoch 6] Batch 2000, Loss 0.3660036325454712\n",
            "[Training Epoch 6] Batch 2100, Loss 0.3974313735961914\n",
            "[Training Epoch 6] Batch 2200, Loss 0.36058008670806885\n",
            "[Training Epoch 6] Batch 2300, Loss 0.3440607190132141\n",
            "[Training Epoch 6] Batch 2400, Loss 0.31094789505004883\n",
            "[Training Epoch 6] Batch 2500, Loss 0.34321388602256775\n",
            "[Training Epoch 6] Batch 2600, Loss 0.3579736351966858\n",
            "[Training Epoch 6] Batch 2700, Loss 0.3726578950881958\n",
            "[Training Epoch 6] Batch 2800, Loss 0.3553835451602936\n",
            "[Training Epoch 6] Batch 2900, Loss 0.34095969796180725\n",
            "[Training Epoch 6] Batch 3000, Loss 0.337377667427063\n",
            "[Training Epoch 6] Batch 3100, Loss 0.34234195947647095\n",
            "[Training Epoch 6] Batch 3200, Loss 0.3547079265117645\n",
            "[Training Epoch 6] Batch 3300, Loss 0.3536075949668884\n",
            "[Training Epoch 6] Batch 3400, Loss 0.34624597430229187\n",
            "[Training Epoch 6] Batch 3500, Loss 0.35842806100845337\n",
            "[Training Epoch 6] Batch 3600, Loss 0.37364351749420166\n",
            "[Training Epoch 6] Batch 3700, Loss 0.371562659740448\n",
            "[Training Epoch 6] Batch 3800, Loss 0.3451778292655945\n",
            "[Training Epoch 6] Batch 3900, Loss 0.31678420305252075\n",
            "[Training Epoch 6] Batch 4000, Loss 0.3393528461456299\n",
            "[Training Epoch 6] Batch 4100, Loss 0.36378031969070435\n",
            "[Training Epoch 6] Batch 4200, Loss 0.3543497323989868\n",
            "[Training Epoch 6] Batch 4300, Loss 0.37227094173431396\n",
            "[Training Epoch 6] Batch 4400, Loss 0.3654623031616211\n",
            "[Training Epoch 6] Batch 4500, Loss 0.3770938813686371\n",
            "[Training Epoch 6] Batch 4600, Loss 0.3475677967071533\n",
            "[Training Epoch 6] Batch 4700, Loss 0.36165863275527954\n",
            "[Training Epoch 6] Batch 4800, Loss 0.3575340211391449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 6] HR = 0.4437, NDCG = 0.2453\n",
            "Epoch 7 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 7] Batch 0, Loss 0.33054858446121216\n",
            "[Training Epoch 7] Batch 100, Loss 0.33381012082099915\n",
            "[Training Epoch 7] Batch 200, Loss 0.3607165217399597\n",
            "[Training Epoch 7] Batch 300, Loss 0.33337917923927307\n",
            "[Training Epoch 7] Batch 400, Loss 0.3322591781616211\n",
            "[Training Epoch 7] Batch 500, Loss 0.34492549300193787\n",
            "[Training Epoch 7] Batch 600, Loss 0.357756644487381\n",
            "[Training Epoch 7] Batch 700, Loss 0.36998116970062256\n",
            "[Training Epoch 7] Batch 800, Loss 0.3789615035057068\n",
            "[Training Epoch 7] Batch 900, Loss 0.3669670820236206\n",
            "[Training Epoch 7] Batch 1000, Loss 0.3492127060890198\n",
            "[Training Epoch 7] Batch 1100, Loss 0.35776591300964355\n",
            "[Training Epoch 7] Batch 1200, Loss 0.335406094789505\n",
            "[Training Epoch 7] Batch 1300, Loss 0.36767351627349854\n",
            "[Training Epoch 7] Batch 1400, Loss 0.3362571895122528\n",
            "[Training Epoch 7] Batch 1500, Loss 0.35864725708961487\n",
            "[Training Epoch 7] Batch 1600, Loss 0.3427434265613556\n",
            "[Training Epoch 7] Batch 1700, Loss 0.3675653040409088\n",
            "[Training Epoch 7] Batch 1800, Loss 0.3460932970046997\n",
            "[Training Epoch 7] Batch 1900, Loss 0.3656720519065857\n",
            "[Training Epoch 7] Batch 2000, Loss 0.3713940680027008\n",
            "[Training Epoch 7] Batch 2100, Loss 0.33436423540115356\n",
            "[Training Epoch 7] Batch 2200, Loss 0.352791428565979\n",
            "[Training Epoch 7] Batch 2300, Loss 0.3669911324977875\n",
            "[Training Epoch 7] Batch 2400, Loss 0.31166309118270874\n",
            "[Training Epoch 7] Batch 2500, Loss 0.3402528166770935\n",
            "[Training Epoch 7] Batch 2600, Loss 0.33217060565948486\n",
            "[Training Epoch 7] Batch 2700, Loss 0.3596181273460388\n",
            "[Training Epoch 7] Batch 2800, Loss 0.3401232361793518\n",
            "[Training Epoch 7] Batch 2900, Loss 0.34386366605758667\n",
            "[Training Epoch 7] Batch 3000, Loss 0.35144540667533875\n",
            "[Training Epoch 7] Batch 3100, Loss 0.33988291025161743\n",
            "[Training Epoch 7] Batch 3200, Loss 0.3291073739528656\n",
            "[Training Epoch 7] Batch 3300, Loss 0.32982146739959717\n",
            "[Training Epoch 7] Batch 3400, Loss 0.3577512502670288\n",
            "[Training Epoch 7] Batch 3500, Loss 0.3566322326660156\n",
            "[Training Epoch 7] Batch 3600, Loss 0.35355374217033386\n",
            "[Training Epoch 7] Batch 3700, Loss 0.35956689715385437\n",
            "[Training Epoch 7] Batch 3800, Loss 0.34670865535736084\n",
            "[Training Epoch 7] Batch 3900, Loss 0.33724454045295715\n",
            "[Training Epoch 7] Batch 4000, Loss 0.3236417770385742\n",
            "[Training Epoch 7] Batch 4100, Loss 0.4036088287830353\n",
            "[Training Epoch 7] Batch 4200, Loss 0.3553921580314636\n",
            "[Training Epoch 7] Batch 4300, Loss 0.3567391037940979\n",
            "[Training Epoch 7] Batch 4400, Loss 0.31886792182922363\n",
            "[Training Epoch 7] Batch 4500, Loss 0.3411508798599243\n",
            "[Training Epoch 7] Batch 4600, Loss 0.3588894009590149\n",
            "[Training Epoch 7] Batch 4700, Loss 0.3512955904006958\n",
            "[Training Epoch 7] Batch 4800, Loss 0.3235194981098175\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 7] HR = 0.4513, NDCG = 0.2486\n",
            "Epoch 8 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 8] Batch 0, Loss 0.3248787522315979\n",
            "[Training Epoch 8] Batch 100, Loss 0.3459091782569885\n",
            "[Training Epoch 8] Batch 200, Loss 0.32412344217300415\n",
            "[Training Epoch 8] Batch 300, Loss 0.3559603989124298\n",
            "[Training Epoch 8] Batch 400, Loss 0.3401261568069458\n",
            "[Training Epoch 8] Batch 500, Loss 0.3364240527153015\n",
            "[Training Epoch 8] Batch 600, Loss 0.3487233817577362\n",
            "[Training Epoch 8] Batch 700, Loss 0.3317379057407379\n",
            "[Training Epoch 8] Batch 800, Loss 0.35013437271118164\n",
            "[Training Epoch 8] Batch 900, Loss 0.3402789831161499\n",
            "[Training Epoch 8] Batch 1000, Loss 0.34799516201019287\n",
            "[Training Epoch 8] Batch 1100, Loss 0.35011976957321167\n",
            "[Training Epoch 8] Batch 1200, Loss 0.33544883131980896\n",
            "[Training Epoch 8] Batch 1300, Loss 0.371391236782074\n",
            "[Training Epoch 8] Batch 1400, Loss 0.32087093591690063\n",
            "[Training Epoch 8] Batch 1500, Loss 0.3147215247154236\n",
            "[Training Epoch 8] Batch 1600, Loss 0.38077467679977417\n",
            "[Training Epoch 8] Batch 1700, Loss 0.3543506860733032\n",
            "[Training Epoch 8] Batch 1800, Loss 0.3110319972038269\n",
            "[Training Epoch 8] Batch 1900, Loss 0.32358914613723755\n",
            "[Training Epoch 8] Batch 2000, Loss 0.3237873315811157\n",
            "[Training Epoch 8] Batch 2100, Loss 0.3869294822216034\n",
            "[Training Epoch 8] Batch 2200, Loss 0.3449641466140747\n",
            "[Training Epoch 8] Batch 2300, Loss 0.3251456022262573\n",
            "[Training Epoch 8] Batch 2400, Loss 0.35539644956588745\n",
            "[Training Epoch 8] Batch 2500, Loss 0.3334343731403351\n",
            "[Training Epoch 8] Batch 2600, Loss 0.35858970880508423\n",
            "[Training Epoch 8] Batch 2700, Loss 0.3663792014122009\n",
            "[Training Epoch 8] Batch 2800, Loss 0.3286053538322449\n",
            "[Training Epoch 8] Batch 2900, Loss 0.33159294724464417\n",
            "[Training Epoch 8] Batch 3000, Loss 0.36429479718208313\n",
            "[Training Epoch 8] Batch 3100, Loss 0.33160126209259033\n",
            "[Training Epoch 8] Batch 3200, Loss 0.34788888692855835\n",
            "[Training Epoch 8] Batch 3300, Loss 0.33282721042633057\n",
            "[Training Epoch 8] Batch 3400, Loss 0.32936185598373413\n",
            "[Training Epoch 8] Batch 3500, Loss 0.32691046595573425\n",
            "[Training Epoch 8] Batch 3600, Loss 0.362521231174469\n",
            "[Training Epoch 8] Batch 3700, Loss 0.3243043124675751\n",
            "[Training Epoch 8] Batch 3800, Loss 0.3712051510810852\n",
            "[Training Epoch 8] Batch 3900, Loss 0.3345393240451813\n",
            "[Training Epoch 8] Batch 4000, Loss 0.32797327637672424\n",
            "[Training Epoch 8] Batch 4100, Loss 0.3296467065811157\n",
            "[Training Epoch 8] Batch 4200, Loss 0.3504209518432617\n",
            "[Training Epoch 8] Batch 4300, Loss 0.3520243167877197\n",
            "[Training Epoch 8] Batch 4400, Loss 0.3107765018939972\n",
            "[Training Epoch 8] Batch 4500, Loss 0.3362672030925751\n",
            "[Training Epoch 8] Batch 4600, Loss 0.3421427011489868\n",
            "[Training Epoch 8] Batch 4700, Loss 0.33113038539886475\n",
            "[Training Epoch 8] Batch 4800, Loss 0.3730078339576721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 8] HR = 0.4738, NDCG = 0.2604\n",
            "Epoch 9 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 9] Batch 0, Loss 0.3069000840187073\n",
            "[Training Epoch 9] Batch 100, Loss 0.3507038950920105\n",
            "[Training Epoch 9] Batch 200, Loss 0.3260765075683594\n",
            "[Training Epoch 9] Batch 300, Loss 0.33036160469055176\n",
            "[Training Epoch 9] Batch 400, Loss 0.3481259047985077\n",
            "[Training Epoch 9] Batch 500, Loss 0.34465160965919495\n",
            "[Training Epoch 9] Batch 600, Loss 0.3389371931552887\n",
            "[Training Epoch 9] Batch 700, Loss 0.3320132791996002\n",
            "[Training Epoch 9] Batch 800, Loss 0.3411506414413452\n",
            "[Training Epoch 9] Batch 900, Loss 0.33583155274391174\n",
            "[Training Epoch 9] Batch 1000, Loss 0.34984859824180603\n",
            "[Training Epoch 9] Batch 1100, Loss 0.313926637172699\n",
            "[Training Epoch 9] Batch 1200, Loss 0.3073936104774475\n",
            "[Training Epoch 9] Batch 1300, Loss 0.3701778054237366\n",
            "[Training Epoch 9] Batch 1400, Loss 0.3177640438079834\n",
            "[Training Epoch 9] Batch 1500, Loss 0.34786710143089294\n",
            "[Training Epoch 9] Batch 1600, Loss 0.3336239755153656\n",
            "[Training Epoch 9] Batch 1700, Loss 0.332483172416687\n",
            "[Training Epoch 9] Batch 1800, Loss 0.33510833978652954\n",
            "[Training Epoch 9] Batch 1900, Loss 0.3461556136608124\n",
            "[Training Epoch 9] Batch 2000, Loss 0.3010956645011902\n",
            "[Training Epoch 9] Batch 2100, Loss 0.36223044991493225\n",
            "[Training Epoch 9] Batch 2200, Loss 0.3019399642944336\n",
            "[Training Epoch 9] Batch 2300, Loss 0.33725810050964355\n",
            "[Training Epoch 9] Batch 2400, Loss 0.33188989758491516\n",
            "[Training Epoch 9] Batch 2500, Loss 0.342501699924469\n",
            "[Training Epoch 9] Batch 2600, Loss 0.3073434829711914\n",
            "[Training Epoch 9] Batch 2700, Loss 0.33580905199050903\n",
            "[Training Epoch 9] Batch 2800, Loss 0.3213469088077545\n",
            "[Training Epoch 9] Batch 2900, Loss 0.3130994737148285\n",
            "[Training Epoch 9] Batch 3000, Loss 0.3700026273727417\n",
            "[Training Epoch 9] Batch 3100, Loss 0.3275110721588135\n",
            "[Training Epoch 9] Batch 3200, Loss 0.3119213581085205\n",
            "[Training Epoch 9] Batch 3300, Loss 0.3392046093940735\n",
            "[Training Epoch 9] Batch 3400, Loss 0.3132801055908203\n",
            "[Training Epoch 9] Batch 3500, Loss 0.3321252465248108\n",
            "[Training Epoch 9] Batch 3600, Loss 0.31436002254486084\n",
            "[Training Epoch 9] Batch 3700, Loss 0.3106439411640167\n",
            "[Training Epoch 9] Batch 3800, Loss 0.30817943811416626\n",
            "[Training Epoch 9] Batch 3900, Loss 0.3341631293296814\n",
            "[Training Epoch 9] Batch 4000, Loss 0.32654279470443726\n",
            "[Training Epoch 9] Batch 4100, Loss 0.3186463713645935\n",
            "[Training Epoch 9] Batch 4200, Loss 0.34265202283859253\n",
            "[Training Epoch 9] Batch 4300, Loss 0.3461662530899048\n",
            "[Training Epoch 9] Batch 4400, Loss 0.3098642826080322\n",
            "[Training Epoch 9] Batch 4500, Loss 0.335817813873291\n",
            "[Training Epoch 9] Batch 4600, Loss 0.27276426553726196\n",
            "[Training Epoch 9] Batch 4700, Loss 0.30159562826156616\n",
            "[Training Epoch 9] Batch 4800, Loss 0.30767759680747986\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 9] HR = 0.4950, NDCG = 0.2742\n",
            "Epoch 10 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 10] Batch 0, Loss 0.3402019739151001\n",
            "[Training Epoch 10] Batch 100, Loss 0.2968432605266571\n",
            "[Training Epoch 10] Batch 200, Loss 0.30462944507598877\n",
            "[Training Epoch 10] Batch 300, Loss 0.34709346294403076\n",
            "[Training Epoch 10] Batch 400, Loss 0.32699933648109436\n",
            "[Training Epoch 10] Batch 500, Loss 0.3007580041885376\n",
            "[Training Epoch 10] Batch 600, Loss 0.2929394841194153\n",
            "[Training Epoch 10] Batch 700, Loss 0.30747103691101074\n",
            "[Training Epoch 10] Batch 800, Loss 0.31660324335098267\n",
            "[Training Epoch 10] Batch 900, Loss 0.33031144738197327\n",
            "[Training Epoch 10] Batch 1000, Loss 0.34658488631248474\n",
            "[Training Epoch 10] Batch 1100, Loss 0.29278117418289185\n",
            "[Training Epoch 10] Batch 1200, Loss 0.3388138711452484\n",
            "[Training Epoch 10] Batch 1300, Loss 0.3158605098724365\n",
            "[Training Epoch 10] Batch 1400, Loss 0.36141371726989746\n",
            "[Training Epoch 10] Batch 1500, Loss 0.32948005199432373\n",
            "[Training Epoch 10] Batch 1600, Loss 0.314253032207489\n",
            "[Training Epoch 10] Batch 1700, Loss 0.32304078340530396\n",
            "[Training Epoch 10] Batch 1800, Loss 0.31511467695236206\n",
            "[Training Epoch 10] Batch 1900, Loss 0.3132283687591553\n",
            "[Training Epoch 10] Batch 2000, Loss 0.3301616311073303\n",
            "[Training Epoch 10] Batch 2100, Loss 0.27724897861480713\n",
            "[Training Epoch 10] Batch 2200, Loss 0.32158344984054565\n",
            "[Training Epoch 10] Batch 2300, Loss 0.3125116229057312\n",
            "[Training Epoch 10] Batch 2400, Loss 0.30090272426605225\n",
            "[Training Epoch 10] Batch 2500, Loss 0.31984031200408936\n",
            "[Training Epoch 10] Batch 2600, Loss 0.3301292061805725\n",
            "[Training Epoch 10] Batch 2700, Loss 0.30190253257751465\n",
            "[Training Epoch 10] Batch 2800, Loss 0.32469743490219116\n",
            "[Training Epoch 10] Batch 2900, Loss 0.3498876690864563\n",
            "[Training Epoch 10] Batch 3000, Loss 0.32346826791763306\n",
            "[Training Epoch 10] Batch 3100, Loss 0.32050982117652893\n",
            "[Training Epoch 10] Batch 3200, Loss 0.31496188044548035\n",
            "[Training Epoch 10] Batch 3300, Loss 0.3080558180809021\n",
            "[Training Epoch 10] Batch 3400, Loss 0.34686458110809326\n",
            "[Training Epoch 10] Batch 3500, Loss 0.30832260847091675\n",
            "[Training Epoch 10] Batch 3600, Loss 0.29637405276298523\n",
            "[Training Epoch 10] Batch 3700, Loss 0.29872408509254456\n",
            "[Training Epoch 10] Batch 3800, Loss 0.32206863164901733\n",
            "[Training Epoch 10] Batch 3900, Loss 0.2922196686267853\n",
            "[Training Epoch 10] Batch 4000, Loss 0.28082308173179626\n",
            "[Training Epoch 10] Batch 4100, Loss 0.3198482394218445\n",
            "[Training Epoch 10] Batch 4200, Loss 0.32572507858276367\n",
            "[Training Epoch 10] Batch 4300, Loss 0.28811022639274597\n",
            "[Training Epoch 10] Batch 4400, Loss 0.3304106593132019\n",
            "[Training Epoch 10] Batch 4500, Loss 0.28812137246131897\n",
            "[Training Epoch 10] Batch 4600, Loss 0.3109842538833618\n",
            "[Training Epoch 10] Batch 4700, Loss 0.30529817938804626\n",
            "[Training Epoch 10] Batch 4800, Loss 0.2854377329349518\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 10] HR = 0.5161, NDCG = 0.2869\n",
            "Epoch 11 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 11] Batch 0, Loss 0.3223429322242737\n",
            "[Training Epoch 11] Batch 100, Loss 0.2992437481880188\n",
            "[Training Epoch 11] Batch 200, Loss 0.3077014684677124\n",
            "[Training Epoch 11] Batch 300, Loss 0.31257039308547974\n",
            "[Training Epoch 11] Batch 400, Loss 0.30179765820503235\n",
            "[Training Epoch 11] Batch 500, Loss 0.3037700355052948\n",
            "[Training Epoch 11] Batch 600, Loss 0.3518485426902771\n",
            "[Training Epoch 11] Batch 700, Loss 0.31587159633636475\n",
            "[Training Epoch 11] Batch 800, Loss 0.3049553632736206\n",
            "[Training Epoch 11] Batch 900, Loss 0.3009113073348999\n",
            "[Training Epoch 11] Batch 1000, Loss 0.29977673292160034\n",
            "[Training Epoch 11] Batch 1100, Loss 0.2972981929779053\n",
            "[Training Epoch 11] Batch 1200, Loss 0.26924800872802734\n",
            "[Training Epoch 11] Batch 1300, Loss 0.2950224280357361\n",
            "[Training Epoch 11] Batch 1400, Loss 0.3307254910469055\n",
            "[Training Epoch 11] Batch 1500, Loss 0.31024226546287537\n",
            "[Training Epoch 11] Batch 1600, Loss 0.2924491763114929\n",
            "[Training Epoch 11] Batch 1700, Loss 0.28400686383247375\n",
            "[Training Epoch 11] Batch 1800, Loss 0.31681784987449646\n",
            "[Training Epoch 11] Batch 1900, Loss 0.30823925137519836\n",
            "[Training Epoch 11] Batch 2000, Loss 0.31012439727783203\n",
            "[Training Epoch 11] Batch 2100, Loss 0.31307411193847656\n",
            "[Training Epoch 11] Batch 2200, Loss 0.33134233951568604\n",
            "[Training Epoch 11] Batch 2300, Loss 0.33596697449684143\n",
            "[Training Epoch 11] Batch 2400, Loss 0.2831963300704956\n",
            "[Training Epoch 11] Batch 2500, Loss 0.29748833179473877\n",
            "[Training Epoch 11] Batch 2600, Loss 0.32683491706848145\n",
            "[Training Epoch 11] Batch 2700, Loss 0.317992627620697\n",
            "[Training Epoch 11] Batch 2800, Loss 0.31124770641326904\n",
            "[Training Epoch 11] Batch 2900, Loss 0.2873426675796509\n",
            "[Training Epoch 11] Batch 3000, Loss 0.3199889063835144\n",
            "[Training Epoch 11] Batch 3100, Loss 0.27862638235092163\n",
            "[Training Epoch 11] Batch 3200, Loss 0.30440986156463623\n",
            "[Training Epoch 11] Batch 3300, Loss 0.31731390953063965\n",
            "[Training Epoch 11] Batch 3400, Loss 0.3241272568702698\n",
            "[Training Epoch 11] Batch 3500, Loss 0.3260120749473572\n",
            "[Training Epoch 11] Batch 3600, Loss 0.32869794964790344\n",
            "[Training Epoch 11] Batch 3700, Loss 0.30044010281562805\n",
            "[Training Epoch 11] Batch 3800, Loss 0.296417236328125\n",
            "[Training Epoch 11] Batch 3900, Loss 0.3158911466598511\n",
            "[Training Epoch 11] Batch 4000, Loss 0.3177778124809265\n",
            "[Training Epoch 11] Batch 4100, Loss 0.2863997220993042\n",
            "[Training Epoch 11] Batch 4200, Loss 0.2941769063472748\n",
            "[Training Epoch 11] Batch 4300, Loss 0.30718308687210083\n",
            "[Training Epoch 11] Batch 4400, Loss 0.29995617270469666\n",
            "[Training Epoch 11] Batch 4500, Loss 0.3145112693309784\n",
            "[Training Epoch 11] Batch 4600, Loss 0.30853909254074097\n",
            "[Training Epoch 11] Batch 4700, Loss 0.3050999641418457\n",
            "[Training Epoch 11] Batch 4800, Loss 0.29605695605278015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 11] HR = 0.5341, NDCG = 0.2969\n",
            "Epoch 12 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 12] Batch 0, Loss 0.33047330379486084\n",
            "[Training Epoch 12] Batch 100, Loss 0.30502331256866455\n",
            "[Training Epoch 12] Batch 200, Loss 0.3136619031429291\n",
            "[Training Epoch 12] Batch 300, Loss 0.31904730200767517\n",
            "[Training Epoch 12] Batch 400, Loss 0.2664441764354706\n",
            "[Training Epoch 12] Batch 500, Loss 0.2908293604850769\n",
            "[Training Epoch 12] Batch 600, Loss 0.2964279055595398\n",
            "[Training Epoch 12] Batch 700, Loss 0.3017594814300537\n",
            "[Training Epoch 12] Batch 800, Loss 0.29450517892837524\n",
            "[Training Epoch 12] Batch 900, Loss 0.333464115858078\n",
            "[Training Epoch 12] Batch 1000, Loss 0.29432815313339233\n",
            "[Training Epoch 12] Batch 1100, Loss 0.3057495951652527\n",
            "[Training Epoch 12] Batch 1200, Loss 0.31276899576187134\n",
            "[Training Epoch 12] Batch 1300, Loss 0.29733940958976746\n",
            "[Training Epoch 12] Batch 1400, Loss 0.32233503460884094\n",
            "[Training Epoch 12] Batch 1500, Loss 0.2808495759963989\n",
            "[Training Epoch 12] Batch 1600, Loss 0.2609163224697113\n",
            "[Training Epoch 12] Batch 1700, Loss 0.28926825523376465\n",
            "[Training Epoch 12] Batch 1800, Loss 0.2839536666870117\n",
            "[Training Epoch 12] Batch 1900, Loss 0.3098301589488983\n",
            "[Training Epoch 12] Batch 2000, Loss 0.3410823941230774\n",
            "[Training Epoch 12] Batch 2100, Loss 0.30388298630714417\n",
            "[Training Epoch 12] Batch 2200, Loss 0.2831040024757385\n",
            "[Training Epoch 12] Batch 2300, Loss 0.30245405435562134\n",
            "[Training Epoch 12] Batch 2400, Loss 0.30136996507644653\n",
            "[Training Epoch 12] Batch 2500, Loss 0.3307134509086609\n",
            "[Training Epoch 12] Batch 2600, Loss 0.3098405599594116\n",
            "[Training Epoch 12] Batch 2700, Loss 0.26472461223602295\n",
            "[Training Epoch 12] Batch 2800, Loss 0.2946580648422241\n",
            "[Training Epoch 12] Batch 2900, Loss 0.29910367727279663\n",
            "[Training Epoch 12] Batch 3000, Loss 0.3228691518306732\n",
            "[Training Epoch 12] Batch 3100, Loss 0.29011502861976624\n",
            "[Training Epoch 12] Batch 3200, Loss 0.30667024850845337\n",
            "[Training Epoch 12] Batch 3300, Loss 0.28490132093429565\n",
            "[Training Epoch 12] Batch 3400, Loss 0.31336379051208496\n",
            "[Training Epoch 12] Batch 3500, Loss 0.29019367694854736\n",
            "[Training Epoch 12] Batch 3600, Loss 0.2965705692768097\n",
            "[Training Epoch 12] Batch 3700, Loss 0.2903147041797638\n",
            "[Training Epoch 12] Batch 3800, Loss 0.2926759719848633\n",
            "[Training Epoch 12] Batch 3900, Loss 0.2734087109565735\n",
            "[Training Epoch 12] Batch 4000, Loss 0.28587979078292847\n",
            "[Training Epoch 12] Batch 4100, Loss 0.29639798402786255\n",
            "[Training Epoch 12] Batch 4200, Loss 0.29619547724723816\n",
            "[Training Epoch 12] Batch 4300, Loss 0.2680150866508484\n",
            "[Training Epoch 12] Batch 4400, Loss 0.28306418657302856\n",
            "[Training Epoch 12] Batch 4500, Loss 0.27383750677108765\n",
            "[Training Epoch 12] Batch 4600, Loss 0.3088226318359375\n",
            "[Training Epoch 12] Batch 4700, Loss 0.30208125710487366\n",
            "[Training Epoch 12] Batch 4800, Loss 0.30954283475875854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 12] HR = 0.5470, NDCG = 0.3063\n",
            "Epoch 13 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 13] Batch 0, Loss 0.29430681467056274\n",
            "[Training Epoch 13] Batch 100, Loss 0.2610669732093811\n",
            "[Training Epoch 13] Batch 200, Loss 0.3079391121864319\n",
            "[Training Epoch 13] Batch 300, Loss 0.3074312210083008\n",
            "[Training Epoch 13] Batch 400, Loss 0.31753844022750854\n",
            "[Training Epoch 13] Batch 500, Loss 0.26991474628448486\n",
            "[Training Epoch 13] Batch 600, Loss 0.281150221824646\n",
            "[Training Epoch 13] Batch 700, Loss 0.29701268672943115\n",
            "[Training Epoch 13] Batch 800, Loss 0.2737917900085449\n",
            "[Training Epoch 13] Batch 900, Loss 0.3032762408256531\n",
            "[Training Epoch 13] Batch 1000, Loss 0.3132916986942291\n",
            "[Training Epoch 13] Batch 1100, Loss 0.31015679240226746\n",
            "[Training Epoch 13] Batch 1200, Loss 0.32499590516090393\n",
            "[Training Epoch 13] Batch 1300, Loss 0.31324589252471924\n",
            "[Training Epoch 13] Batch 1400, Loss 0.31352367997169495\n",
            "[Training Epoch 13] Batch 1500, Loss 0.2673770487308502\n",
            "[Training Epoch 13] Batch 1600, Loss 0.27559804916381836\n",
            "[Training Epoch 13] Batch 1700, Loss 0.30413132905960083\n",
            "[Training Epoch 13] Batch 1800, Loss 0.2872011661529541\n",
            "[Training Epoch 13] Batch 1900, Loss 0.31014785170555115\n",
            "[Training Epoch 13] Batch 2000, Loss 0.3039955794811249\n",
            "[Training Epoch 13] Batch 2100, Loss 0.3125836253166199\n",
            "[Training Epoch 13] Batch 2200, Loss 0.3227694034576416\n",
            "[Training Epoch 13] Batch 2300, Loss 0.3048935830593109\n",
            "[Training Epoch 13] Batch 2400, Loss 0.30239635705947876\n",
            "[Training Epoch 13] Batch 2500, Loss 0.2718226909637451\n",
            "[Training Epoch 13] Batch 2600, Loss 0.3205191195011139\n",
            "[Training Epoch 13] Batch 2700, Loss 0.29987019300460815\n",
            "[Training Epoch 13] Batch 2800, Loss 0.27810442447662354\n",
            "[Training Epoch 13] Batch 2900, Loss 0.26742058992385864\n",
            "[Training Epoch 13] Batch 3000, Loss 0.3092423677444458\n",
            "[Training Epoch 13] Batch 3100, Loss 0.2827095687389374\n",
            "[Training Epoch 13] Batch 3200, Loss 0.30757129192352295\n",
            "[Training Epoch 13] Batch 3300, Loss 0.27478715777397156\n",
            "[Training Epoch 13] Batch 3400, Loss 0.3018549382686615\n",
            "[Training Epoch 13] Batch 3500, Loss 0.2950150966644287\n",
            "[Training Epoch 13] Batch 3600, Loss 0.2982586622238159\n",
            "[Training Epoch 13] Batch 3700, Loss 0.2914886176586151\n",
            "[Training Epoch 13] Batch 3800, Loss 0.3225274682044983\n",
            "[Training Epoch 13] Batch 3900, Loss 0.29319292306900024\n",
            "[Training Epoch 13] Batch 4000, Loss 0.28723081946372986\n",
            "[Training Epoch 13] Batch 4100, Loss 0.2799411416053772\n",
            "[Training Epoch 13] Batch 4200, Loss 0.3043361306190491\n",
            "[Training Epoch 13] Batch 4300, Loss 0.29787731170654297\n",
            "[Training Epoch 13] Batch 4400, Loss 0.2876766622066498\n",
            "[Training Epoch 13] Batch 4500, Loss 0.31832414865493774\n",
            "[Training Epoch 13] Batch 4600, Loss 0.27141493558883667\n",
            "[Training Epoch 13] Batch 4700, Loss 0.2978161871433258\n",
            "[Training Epoch 13] Batch 4800, Loss 0.28742608428001404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 13] HR = 0.5613, NDCG = 0.3138\n",
            "Epoch 14 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 14] Batch 0, Loss 0.2790549695491791\n",
            "[Training Epoch 14] Batch 100, Loss 0.2735772728919983\n",
            "[Training Epoch 14] Batch 200, Loss 0.2942783832550049\n",
            "[Training Epoch 14] Batch 300, Loss 0.2608504891395569\n",
            "[Training Epoch 14] Batch 400, Loss 0.3136659264564514\n",
            "[Training Epoch 14] Batch 500, Loss 0.27101799845695496\n",
            "[Training Epoch 14] Batch 600, Loss 0.2730488181114197\n",
            "[Training Epoch 14] Batch 700, Loss 0.3020428419113159\n",
            "[Training Epoch 14] Batch 800, Loss 0.30244937539100647\n",
            "[Training Epoch 14] Batch 900, Loss 0.29530078172683716\n",
            "[Training Epoch 14] Batch 1000, Loss 0.31681084632873535\n",
            "[Training Epoch 14] Batch 1100, Loss 0.33711299300193787\n",
            "[Training Epoch 14] Batch 1200, Loss 0.2962644696235657\n",
            "[Training Epoch 14] Batch 1300, Loss 0.28984373807907104\n",
            "[Training Epoch 14] Batch 1400, Loss 0.32683390378952026\n",
            "[Training Epoch 14] Batch 1500, Loss 0.2709977626800537\n",
            "[Training Epoch 14] Batch 1600, Loss 0.29293957352638245\n",
            "[Training Epoch 14] Batch 1700, Loss 0.28357475996017456\n",
            "[Training Epoch 14] Batch 1800, Loss 0.25160735845565796\n",
            "[Training Epoch 14] Batch 1900, Loss 0.3049356937408447\n",
            "[Training Epoch 14] Batch 2000, Loss 0.2838873863220215\n",
            "[Training Epoch 14] Batch 2100, Loss 0.29625535011291504\n",
            "[Training Epoch 14] Batch 2200, Loss 0.2793486714363098\n",
            "[Training Epoch 14] Batch 2300, Loss 0.28069639205932617\n",
            "[Training Epoch 14] Batch 2400, Loss 0.30247390270233154\n",
            "[Training Epoch 14] Batch 2500, Loss 0.2663387060165405\n",
            "[Training Epoch 14] Batch 2600, Loss 0.30696427822113037\n",
            "[Training Epoch 14] Batch 2700, Loss 0.32501542568206787\n",
            "[Training Epoch 14] Batch 2800, Loss 0.2914324402809143\n",
            "[Training Epoch 14] Batch 2900, Loss 0.31007155776023865\n",
            "[Training Epoch 14] Batch 3000, Loss 0.2858183681964874\n",
            "[Training Epoch 14] Batch 3100, Loss 0.31034791469573975\n",
            "[Training Epoch 14] Batch 3200, Loss 0.29512742161750793\n",
            "[Training Epoch 14] Batch 3300, Loss 0.3023958206176758\n",
            "[Training Epoch 14] Batch 3400, Loss 0.2905482053756714\n",
            "[Training Epoch 14] Batch 3500, Loss 0.2912997007369995\n",
            "[Training Epoch 14] Batch 3600, Loss 0.2880082130432129\n",
            "[Training Epoch 14] Batch 3700, Loss 0.3070715367794037\n",
            "[Training Epoch 14] Batch 3800, Loss 0.2957645654678345\n",
            "[Training Epoch 14] Batch 3900, Loss 0.26060450077056885\n",
            "[Training Epoch 14] Batch 4000, Loss 0.2931588590145111\n",
            "[Training Epoch 14] Batch 4100, Loss 0.31185752153396606\n",
            "[Training Epoch 14] Batch 4200, Loss 0.2820427417755127\n",
            "[Training Epoch 14] Batch 4300, Loss 0.283932626247406\n",
            "[Training Epoch 14] Batch 4400, Loss 0.30170169472694397\n",
            "[Training Epoch 14] Batch 4500, Loss 0.2652159333229065\n",
            "[Training Epoch 14] Batch 4600, Loss 0.2927968502044678\n",
            "[Training Epoch 14] Batch 4700, Loss 0.2655060589313507\n",
            "[Training Epoch 14] Batch 4800, Loss 0.29687613248825073\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 14] HR = 0.5752, NDCG = 0.3221\n",
            "Epoch 15 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 15] Batch 0, Loss 0.29912662506103516\n",
            "[Training Epoch 15] Batch 100, Loss 0.2970578074455261\n",
            "[Training Epoch 15] Batch 200, Loss 0.31920087337493896\n",
            "[Training Epoch 15] Batch 300, Loss 0.29146623611450195\n",
            "[Training Epoch 15] Batch 400, Loss 0.25858378410339355\n",
            "[Training Epoch 15] Batch 500, Loss 0.3192991614341736\n",
            "[Training Epoch 15] Batch 600, Loss 0.2941286265850067\n",
            "[Training Epoch 15] Batch 700, Loss 0.2923869490623474\n",
            "[Training Epoch 15] Batch 800, Loss 0.26842036843299866\n",
            "[Training Epoch 15] Batch 900, Loss 0.3202420473098755\n",
            "[Training Epoch 15] Batch 1000, Loss 0.28446054458618164\n",
            "[Training Epoch 15] Batch 1100, Loss 0.29705333709716797\n",
            "[Training Epoch 15] Batch 1200, Loss 0.29559025168418884\n",
            "[Training Epoch 15] Batch 1300, Loss 0.3094523847103119\n",
            "[Training Epoch 15] Batch 1400, Loss 0.30822962522506714\n",
            "[Training Epoch 15] Batch 1500, Loss 0.27801769971847534\n",
            "[Training Epoch 15] Batch 1600, Loss 0.3013210594654083\n",
            "[Training Epoch 15] Batch 1700, Loss 0.29628920555114746\n",
            "[Training Epoch 15] Batch 1800, Loss 0.28872382640838623\n",
            "[Training Epoch 15] Batch 1900, Loss 0.2780005633831024\n",
            "[Training Epoch 15] Batch 2000, Loss 0.2864166498184204\n",
            "[Training Epoch 15] Batch 2100, Loss 0.3036825656890869\n",
            "[Training Epoch 15] Batch 2200, Loss 0.2809559404850006\n",
            "[Training Epoch 15] Batch 2300, Loss 0.2826876640319824\n",
            "[Training Epoch 15] Batch 2400, Loss 0.3174608647823334\n",
            "[Training Epoch 15] Batch 2500, Loss 0.3139673173427582\n",
            "[Training Epoch 15] Batch 2600, Loss 0.310428261756897\n",
            "[Training Epoch 15] Batch 2700, Loss 0.29740679264068604\n",
            "[Training Epoch 15] Batch 2800, Loss 0.28327104449272156\n",
            "[Training Epoch 15] Batch 2900, Loss 0.28915417194366455\n",
            "[Training Epoch 15] Batch 3000, Loss 0.2780598998069763\n",
            "[Training Epoch 15] Batch 3100, Loss 0.307660847902298\n",
            "[Training Epoch 15] Batch 3200, Loss 0.2774839699268341\n",
            "[Training Epoch 15] Batch 3300, Loss 0.3242000937461853\n",
            "[Training Epoch 15] Batch 3400, Loss 0.27485984563827515\n",
            "[Training Epoch 15] Batch 3500, Loss 0.33223769068717957\n",
            "[Training Epoch 15] Batch 3600, Loss 0.30952194333076477\n",
            "[Training Epoch 15] Batch 3700, Loss 0.2713910937309265\n",
            "[Training Epoch 15] Batch 3800, Loss 0.2705684006214142\n",
            "[Training Epoch 15] Batch 3900, Loss 0.30685076117515564\n",
            "[Training Epoch 15] Batch 4000, Loss 0.3083474636077881\n",
            "[Training Epoch 15] Batch 4100, Loss 0.281639039516449\n",
            "[Training Epoch 15] Batch 4200, Loss 0.28390008211135864\n",
            "[Training Epoch 15] Batch 4300, Loss 0.3007045388221741\n",
            "[Training Epoch 15] Batch 4400, Loss 0.2990805208683014\n",
            "[Training Epoch 15] Batch 4500, Loss 0.3157750964164734\n",
            "[Training Epoch 15] Batch 4600, Loss 0.2923029661178589\n",
            "[Training Epoch 15] Batch 4700, Loss 0.25863397121429443\n",
            "[Training Epoch 15] Batch 4800, Loss 0.3130503296852112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 15] HR = 0.5856, NDCG = 0.3281\n",
            "Epoch 16 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 16] Batch 0, Loss 0.30118507146835327\n",
            "[Training Epoch 16] Batch 100, Loss 0.2869979739189148\n",
            "[Training Epoch 16] Batch 200, Loss 0.28732985258102417\n",
            "[Training Epoch 16] Batch 300, Loss 0.27677711844444275\n",
            "[Training Epoch 16] Batch 400, Loss 0.3038474917411804\n",
            "[Training Epoch 16] Batch 500, Loss 0.2504599094390869\n",
            "[Training Epoch 16] Batch 600, Loss 0.2965576648712158\n",
            "[Training Epoch 16] Batch 700, Loss 0.2728160619735718\n",
            "[Training Epoch 16] Batch 800, Loss 0.3066418170928955\n",
            "[Training Epoch 16] Batch 900, Loss 0.2768723964691162\n",
            "[Training Epoch 16] Batch 1000, Loss 0.2830308973789215\n",
            "[Training Epoch 16] Batch 1100, Loss 0.316245973110199\n",
            "[Training Epoch 16] Batch 1200, Loss 0.28213104605674744\n",
            "[Training Epoch 16] Batch 1300, Loss 0.2946664094924927\n",
            "[Training Epoch 16] Batch 1400, Loss 0.28546997904777527\n",
            "[Training Epoch 16] Batch 1500, Loss 0.28686827421188354\n",
            "[Training Epoch 16] Batch 1600, Loss 0.2784948945045471\n",
            "[Training Epoch 16] Batch 1700, Loss 0.2767314910888672\n",
            "[Training Epoch 16] Batch 1800, Loss 0.3117496967315674\n",
            "[Training Epoch 16] Batch 1900, Loss 0.3043746054172516\n",
            "[Training Epoch 16] Batch 2000, Loss 0.2972085773944855\n",
            "[Training Epoch 16] Batch 2100, Loss 0.2842884063720703\n",
            "[Training Epoch 16] Batch 2200, Loss 0.29684409499168396\n",
            "[Training Epoch 16] Batch 2300, Loss 0.2787613570690155\n",
            "[Training Epoch 16] Batch 2400, Loss 0.2887464165687561\n",
            "[Training Epoch 16] Batch 2500, Loss 0.3043672740459442\n",
            "[Training Epoch 16] Batch 2600, Loss 0.2901439964771271\n",
            "[Training Epoch 16] Batch 2700, Loss 0.25879496335983276\n",
            "[Training Epoch 16] Batch 2800, Loss 0.29319024085998535\n",
            "[Training Epoch 16] Batch 2900, Loss 0.29350829124450684\n",
            "[Training Epoch 16] Batch 3000, Loss 0.30890822410583496\n",
            "[Training Epoch 16] Batch 3100, Loss 0.294994056224823\n",
            "[Training Epoch 16] Batch 3200, Loss 0.29932019114494324\n",
            "[Training Epoch 16] Batch 3300, Loss 0.2750898599624634\n",
            "[Training Epoch 16] Batch 3400, Loss 0.3006906509399414\n",
            "[Training Epoch 16] Batch 3500, Loss 0.28183454275131226\n",
            "[Training Epoch 16] Batch 3600, Loss 0.290981650352478\n",
            "[Training Epoch 16] Batch 3700, Loss 0.2942458391189575\n",
            "[Training Epoch 16] Batch 3800, Loss 0.25943177938461304\n",
            "[Training Epoch 16] Batch 3900, Loss 0.2737627625465393\n",
            "[Training Epoch 16] Batch 4000, Loss 0.30116474628448486\n",
            "[Training Epoch 16] Batch 4100, Loss 0.28701117634773254\n",
            "[Training Epoch 16] Batch 4200, Loss 0.27113237977027893\n",
            "[Training Epoch 16] Batch 4300, Loss 0.2565504312515259\n",
            "[Training Epoch 16] Batch 4400, Loss 0.3087386190891266\n",
            "[Training Epoch 16] Batch 4500, Loss 0.2836555540561676\n",
            "[Training Epoch 16] Batch 4600, Loss 0.2719882130622864\n",
            "[Training Epoch 16] Batch 4700, Loss 0.2721728980541229\n",
            "[Training Epoch 16] Batch 4800, Loss 0.2675570845603943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 16] HR = 0.5929, NDCG = 0.3336\n",
            "Epoch 17 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 17] Batch 0, Loss 0.2670666575431824\n",
            "[Training Epoch 17] Batch 100, Loss 0.25492334365844727\n",
            "[Training Epoch 17] Batch 200, Loss 0.29698991775512695\n",
            "[Training Epoch 17] Batch 300, Loss 0.2882941663265228\n",
            "[Training Epoch 17] Batch 400, Loss 0.2965424656867981\n",
            "[Training Epoch 17] Batch 500, Loss 0.2828764319419861\n",
            "[Training Epoch 17] Batch 600, Loss 0.2783476412296295\n",
            "[Training Epoch 17] Batch 700, Loss 0.29904481768608093\n",
            "[Training Epoch 17] Batch 800, Loss 0.2618504464626312\n",
            "[Training Epoch 17] Batch 900, Loss 0.2650587558746338\n",
            "[Training Epoch 17] Batch 1000, Loss 0.2829379141330719\n",
            "[Training Epoch 17] Batch 1100, Loss 0.2784079313278198\n",
            "[Training Epoch 17] Batch 1200, Loss 0.2634216547012329\n",
            "[Training Epoch 17] Batch 1300, Loss 0.2652420401573181\n",
            "[Training Epoch 17] Batch 1400, Loss 0.31255850195884705\n",
            "[Training Epoch 17] Batch 1500, Loss 0.2898881435394287\n",
            "[Training Epoch 17] Batch 1600, Loss 0.27010488510131836\n",
            "[Training Epoch 17] Batch 1700, Loss 0.2764275074005127\n",
            "[Training Epoch 17] Batch 1800, Loss 0.2737559974193573\n",
            "[Training Epoch 17] Batch 1900, Loss 0.3077409863471985\n",
            "[Training Epoch 17] Batch 2000, Loss 0.2875535786151886\n",
            "[Training Epoch 17] Batch 2100, Loss 0.2789708375930786\n",
            "[Training Epoch 17] Batch 2200, Loss 0.29051005840301514\n",
            "[Training Epoch 17] Batch 2300, Loss 0.26947230100631714\n",
            "[Training Epoch 17] Batch 2400, Loss 0.2931026816368103\n",
            "[Training Epoch 17] Batch 2500, Loss 0.2768915891647339\n",
            "[Training Epoch 17] Batch 2600, Loss 0.26518672704696655\n",
            "[Training Epoch 17] Batch 2700, Loss 0.3208920657634735\n",
            "[Training Epoch 17] Batch 2800, Loss 0.2962789535522461\n",
            "[Training Epoch 17] Batch 2900, Loss 0.2939656376838684\n",
            "[Training Epoch 17] Batch 3000, Loss 0.27807241678237915\n",
            "[Training Epoch 17] Batch 3100, Loss 0.27448680996894836\n",
            "[Training Epoch 17] Batch 3200, Loss 0.28759562969207764\n",
            "[Training Epoch 17] Batch 3300, Loss 0.30050623416900635\n",
            "[Training Epoch 17] Batch 3400, Loss 0.3071478009223938\n",
            "[Training Epoch 17] Batch 3500, Loss 0.32432064414024353\n",
            "[Training Epoch 17] Batch 3600, Loss 0.26691877841949463\n",
            "[Training Epoch 17] Batch 3700, Loss 0.2764981985092163\n",
            "[Training Epoch 17] Batch 3800, Loss 0.2929409146308899\n",
            "[Training Epoch 17] Batch 3900, Loss 0.29155370593070984\n",
            "[Training Epoch 17] Batch 4000, Loss 0.30683210492134094\n",
            "[Training Epoch 17] Batch 4100, Loss 0.27709075808525085\n",
            "[Training Epoch 17] Batch 4200, Loss 0.2780001759529114\n",
            "[Training Epoch 17] Batch 4300, Loss 0.3196253478527069\n",
            "[Training Epoch 17] Batch 4400, Loss 0.2736901044845581\n",
            "[Training Epoch 17] Batch 4500, Loss 0.2949846386909485\n",
            "[Training Epoch 17] Batch 4600, Loss 0.2804409861564636\n",
            "[Training Epoch 17] Batch 4700, Loss 0.2776992619037628\n",
            "[Training Epoch 17] Batch 4800, Loss 0.27812260389328003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 17] HR = 0.6048, NDCG = 0.3402\n",
            "Epoch 18 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 18] Batch 0, Loss 0.298471599817276\n",
            "[Training Epoch 18] Batch 100, Loss 0.2866045832633972\n",
            "[Training Epoch 18] Batch 200, Loss 0.27871155738830566\n",
            "[Training Epoch 18] Batch 300, Loss 0.2547752559185028\n",
            "[Training Epoch 18] Batch 400, Loss 0.2663278877735138\n",
            "[Training Epoch 18] Batch 500, Loss 0.2901487350463867\n",
            "[Training Epoch 18] Batch 600, Loss 0.246256023645401\n",
            "[Training Epoch 18] Batch 700, Loss 0.26795074343681335\n",
            "[Training Epoch 18] Batch 800, Loss 0.26262107491493225\n",
            "[Training Epoch 18] Batch 900, Loss 0.25913912057876587\n",
            "[Training Epoch 18] Batch 1000, Loss 0.2940301299095154\n",
            "[Training Epoch 18] Batch 1100, Loss 0.2829684615135193\n",
            "[Training Epoch 18] Batch 1200, Loss 0.277106374502182\n",
            "[Training Epoch 18] Batch 1300, Loss 0.25049787759780884\n",
            "[Training Epoch 18] Batch 1400, Loss 0.2381313592195511\n",
            "[Training Epoch 18] Batch 1500, Loss 0.3055686950683594\n",
            "[Training Epoch 18] Batch 1600, Loss 0.30282357335090637\n",
            "[Training Epoch 18] Batch 1700, Loss 0.2855711579322815\n",
            "[Training Epoch 18] Batch 1800, Loss 0.306793749332428\n",
            "[Training Epoch 18] Batch 1900, Loss 0.2592517137527466\n",
            "[Training Epoch 18] Batch 2000, Loss 0.2682967185974121\n",
            "[Training Epoch 18] Batch 2100, Loss 0.258291095495224\n",
            "[Training Epoch 18] Batch 2200, Loss 0.27247703075408936\n",
            "[Training Epoch 18] Batch 2300, Loss 0.2566641569137573\n",
            "[Training Epoch 18] Batch 2400, Loss 0.2664349377155304\n",
            "[Training Epoch 18] Batch 2500, Loss 0.2557864785194397\n",
            "[Training Epoch 18] Batch 2600, Loss 0.27653974294662476\n",
            "[Training Epoch 18] Batch 2700, Loss 0.29696518182754517\n",
            "[Training Epoch 18] Batch 2800, Loss 0.28939974308013916\n",
            "[Training Epoch 18] Batch 2900, Loss 0.30943286418914795\n",
            "[Training Epoch 18] Batch 3000, Loss 0.2660278081893921\n",
            "[Training Epoch 18] Batch 3100, Loss 0.25994426012039185\n",
            "[Training Epoch 18] Batch 3200, Loss 0.23469209671020508\n",
            "[Training Epoch 18] Batch 3300, Loss 0.29319727420806885\n",
            "[Training Epoch 18] Batch 3400, Loss 0.32958412170410156\n",
            "[Training Epoch 18] Batch 3500, Loss 0.25912174582481384\n",
            "[Training Epoch 18] Batch 3600, Loss 0.27502936124801636\n",
            "[Training Epoch 18] Batch 3700, Loss 0.2861239016056061\n",
            "[Training Epoch 18] Batch 3800, Loss 0.2814558744430542\n",
            "[Training Epoch 18] Batch 3900, Loss 0.2760549783706665\n",
            "[Training Epoch 18] Batch 4000, Loss 0.2588978707790375\n",
            "[Training Epoch 18] Batch 4100, Loss 0.2952924966812134\n",
            "[Training Epoch 18] Batch 4200, Loss 0.2739725112915039\n",
            "[Training Epoch 18] Batch 4300, Loss 0.2830534875392914\n",
            "[Training Epoch 18] Batch 4400, Loss 0.28219473361968994\n",
            "[Training Epoch 18] Batch 4500, Loss 0.2683399021625519\n",
            "[Training Epoch 18] Batch 4600, Loss 0.2666580379009247\n",
            "[Training Epoch 18] Batch 4700, Loss 0.2860580086708069\n",
            "[Training Epoch 18] Batch 4800, Loss 0.2635863423347473\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 18] HR = 0.6131, NDCG = 0.3445\n",
            "Epoch 19 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 19] Batch 0, Loss 0.2852809429168701\n",
            "[Training Epoch 19] Batch 100, Loss 0.25849270820617676\n",
            "[Training Epoch 19] Batch 200, Loss 0.26946112513542175\n",
            "[Training Epoch 19] Batch 300, Loss 0.26310449838638306\n",
            "[Training Epoch 19] Batch 400, Loss 0.27608931064605713\n",
            "[Training Epoch 19] Batch 500, Loss 0.24310454726219177\n",
            "[Training Epoch 19] Batch 600, Loss 0.2742164433002472\n",
            "[Training Epoch 19] Batch 700, Loss 0.26784372329711914\n",
            "[Training Epoch 19] Batch 800, Loss 0.2843415141105652\n",
            "[Training Epoch 19] Batch 900, Loss 0.30737364292144775\n",
            "[Training Epoch 19] Batch 1000, Loss 0.3089943826198578\n",
            "[Training Epoch 19] Batch 1100, Loss 0.2901090383529663\n",
            "[Training Epoch 19] Batch 1200, Loss 0.2864444851875305\n",
            "[Training Epoch 19] Batch 1300, Loss 0.2837236821651459\n",
            "[Training Epoch 19] Batch 1400, Loss 0.2863209843635559\n",
            "[Training Epoch 19] Batch 1500, Loss 0.27196958661079407\n",
            "[Training Epoch 19] Batch 1600, Loss 0.287273108959198\n",
            "[Training Epoch 19] Batch 1700, Loss 0.2610532343387604\n",
            "[Training Epoch 19] Batch 1800, Loss 0.2724362015724182\n",
            "[Training Epoch 19] Batch 1900, Loss 0.2799125611782074\n",
            "[Training Epoch 19] Batch 2000, Loss 0.3071211278438568\n",
            "[Training Epoch 19] Batch 2100, Loss 0.2779776453971863\n",
            "[Training Epoch 19] Batch 2200, Loss 0.28292542695999146\n",
            "[Training Epoch 19] Batch 2300, Loss 0.2809436321258545\n",
            "[Training Epoch 19] Batch 2400, Loss 0.2912474572658539\n",
            "[Training Epoch 19] Batch 2500, Loss 0.26452702283859253\n",
            "[Training Epoch 19] Batch 2600, Loss 0.26554569602012634\n",
            "[Training Epoch 19] Batch 2700, Loss 0.24107789993286133\n",
            "[Training Epoch 19] Batch 2800, Loss 0.27453285455703735\n",
            "[Training Epoch 19] Batch 2900, Loss 0.2713416516780853\n",
            "[Training Epoch 19] Batch 3000, Loss 0.26017045974731445\n",
            "[Training Epoch 19] Batch 3100, Loss 0.28259536623954773\n",
            "[Training Epoch 19] Batch 3200, Loss 0.2906187176704407\n",
            "[Training Epoch 19] Batch 3300, Loss 0.2729901075363159\n",
            "[Training Epoch 19] Batch 3400, Loss 0.2873963713645935\n",
            "[Training Epoch 19] Batch 3500, Loss 0.267688125371933\n",
            "[Training Epoch 19] Batch 3600, Loss 0.29239577054977417\n",
            "[Training Epoch 19] Batch 3700, Loss 0.2717527449131012\n",
            "[Training Epoch 19] Batch 3800, Loss 0.2759506106376648\n",
            "[Training Epoch 19] Batch 3900, Loss 0.2796124815940857\n",
            "[Training Epoch 19] Batch 4000, Loss 0.30336862802505493\n",
            "[Training Epoch 19] Batch 4100, Loss 0.26016679406166077\n",
            "[Training Epoch 19] Batch 4200, Loss 0.26856258511543274\n",
            "[Training Epoch 19] Batch 4300, Loss 0.28827300667762756\n",
            "[Training Epoch 19] Batch 4400, Loss 0.28682059049606323\n",
            "[Training Epoch 19] Batch 4500, Loss 0.264813631772995\n",
            "[Training Epoch 19] Batch 4600, Loss 0.2794804871082306\n",
            "[Training Epoch 19] Batch 4700, Loss 0.278043270111084\n",
            "[Training Epoch 19] Batch 4800, Loss 0.2652274966239929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 19] HR = 0.6179, NDCG = 0.3490\n",
            "Epoch 20 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 20] Batch 0, Loss 0.2639835774898529\n",
            "[Training Epoch 20] Batch 100, Loss 0.2641054391860962\n",
            "[Training Epoch 20] Batch 200, Loss 0.2587309777736664\n",
            "[Training Epoch 20] Batch 300, Loss 0.2759864032268524\n",
            "[Training Epoch 20] Batch 400, Loss 0.27180758118629456\n",
            "[Training Epoch 20] Batch 500, Loss 0.2585163116455078\n",
            "[Training Epoch 20] Batch 600, Loss 0.28420254588127136\n",
            "[Training Epoch 20] Batch 700, Loss 0.2725047469139099\n",
            "[Training Epoch 20] Batch 800, Loss 0.2562834620475769\n",
            "[Training Epoch 20] Batch 900, Loss 0.2696106433868408\n",
            "[Training Epoch 20] Batch 1000, Loss 0.27350324392318726\n",
            "[Training Epoch 20] Batch 1100, Loss 0.25471001863479614\n",
            "[Training Epoch 20] Batch 1200, Loss 0.27637961506843567\n",
            "[Training Epoch 20] Batch 1300, Loss 0.25973016023635864\n",
            "[Training Epoch 20] Batch 1400, Loss 0.2950446307659149\n",
            "[Training Epoch 20] Batch 1500, Loss 0.2663758099079132\n",
            "[Training Epoch 20] Batch 1600, Loss 0.2655956745147705\n",
            "[Training Epoch 20] Batch 1700, Loss 0.2655540704727173\n",
            "[Training Epoch 20] Batch 1800, Loss 0.2892311215400696\n",
            "[Training Epoch 20] Batch 1900, Loss 0.28267332911491394\n",
            "[Training Epoch 20] Batch 2000, Loss 0.29178065061569214\n",
            "[Training Epoch 20] Batch 2100, Loss 0.31508684158325195\n",
            "[Training Epoch 20] Batch 2200, Loss 0.28549259901046753\n",
            "[Training Epoch 20] Batch 2300, Loss 0.26441556215286255\n",
            "[Training Epoch 20] Batch 2400, Loss 0.26114967465400696\n",
            "[Training Epoch 20] Batch 2500, Loss 0.28838661313056946\n",
            "[Training Epoch 20] Batch 2600, Loss 0.26715177297592163\n",
            "[Training Epoch 20] Batch 2700, Loss 0.26094430685043335\n",
            "[Training Epoch 20] Batch 2800, Loss 0.2846875786781311\n",
            "[Training Epoch 20] Batch 2900, Loss 0.2881591320037842\n",
            "[Training Epoch 20] Batch 3000, Loss 0.28424549102783203\n",
            "[Training Epoch 20] Batch 3100, Loss 0.27814120054244995\n",
            "[Training Epoch 20] Batch 3200, Loss 0.26402056217193604\n",
            "[Training Epoch 20] Batch 3300, Loss 0.2875044047832489\n",
            "[Training Epoch 20] Batch 3400, Loss 0.2521868944168091\n",
            "[Training Epoch 20] Batch 3500, Loss 0.2521596848964691\n",
            "[Training Epoch 20] Batch 3600, Loss 0.2868064343929291\n",
            "[Training Epoch 20] Batch 3700, Loss 0.26600074768066406\n",
            "[Training Epoch 20] Batch 3800, Loss 0.2556394934654236\n",
            "[Training Epoch 20] Batch 3900, Loss 0.2827048599720001\n",
            "[Training Epoch 20] Batch 4000, Loss 0.2903427481651306\n",
            "[Training Epoch 20] Batch 4100, Loss 0.27421805262565613\n",
            "[Training Epoch 20] Batch 4200, Loss 0.3001195192337036\n",
            "[Training Epoch 20] Batch 4300, Loss 0.3006478548049927\n",
            "[Training Epoch 20] Batch 4400, Loss 0.2898108959197998\n",
            "[Training Epoch 20] Batch 4500, Loss 0.26054367423057556\n",
            "[Training Epoch 20] Batch 4600, Loss 0.27110347151756287\n",
            "[Training Epoch 20] Batch 4700, Loss 0.2557675242424011\n",
            "[Training Epoch 20] Batch 4800, Loss 0.2829618752002716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 20] HR = 0.6195, NDCG = 0.3517\n",
            "Epoch 21 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 21] Batch 0, Loss 0.26170268654823303\n",
            "[Training Epoch 21] Batch 100, Loss 0.2654023766517639\n",
            "[Training Epoch 21] Batch 200, Loss 0.2853182256221771\n",
            "[Training Epoch 21] Batch 300, Loss 0.2795942425727844\n",
            "[Training Epoch 21] Batch 400, Loss 0.25648224353790283\n",
            "[Training Epoch 21] Batch 500, Loss 0.29024896025657654\n",
            "[Training Epoch 21] Batch 600, Loss 0.2565263509750366\n",
            "[Training Epoch 21] Batch 700, Loss 0.2907853126525879\n",
            "[Training Epoch 21] Batch 800, Loss 0.29587969183921814\n",
            "[Training Epoch 21] Batch 900, Loss 0.30493152141571045\n",
            "[Training Epoch 21] Batch 1000, Loss 0.2735270857810974\n",
            "[Training Epoch 21] Batch 1100, Loss 0.26175132393836975\n",
            "[Training Epoch 21] Batch 1200, Loss 0.26926782727241516\n",
            "[Training Epoch 21] Batch 1300, Loss 0.298057496547699\n",
            "[Training Epoch 21] Batch 1400, Loss 0.27425089478492737\n",
            "[Training Epoch 21] Batch 1500, Loss 0.2680951952934265\n",
            "[Training Epoch 21] Batch 1600, Loss 0.2462160736322403\n",
            "[Training Epoch 21] Batch 1700, Loss 0.2638102173805237\n",
            "[Training Epoch 21] Batch 1800, Loss 0.2663815915584564\n",
            "[Training Epoch 21] Batch 1900, Loss 0.2740371823310852\n",
            "[Training Epoch 21] Batch 2000, Loss 0.2615445852279663\n",
            "[Training Epoch 21] Batch 2100, Loss 0.2737550437450409\n",
            "[Training Epoch 21] Batch 2200, Loss 0.2748948037624359\n",
            "[Training Epoch 21] Batch 2300, Loss 0.29601216316223145\n",
            "[Training Epoch 21] Batch 2400, Loss 0.25348421931266785\n",
            "[Training Epoch 21] Batch 2500, Loss 0.2712674140930176\n",
            "[Training Epoch 21] Batch 2600, Loss 0.2551906108856201\n",
            "[Training Epoch 21] Batch 2700, Loss 0.25766825675964355\n",
            "[Training Epoch 21] Batch 2800, Loss 0.2475929856300354\n",
            "[Training Epoch 21] Batch 2900, Loss 0.2950616776943207\n",
            "[Training Epoch 21] Batch 3000, Loss 0.2830756902694702\n",
            "[Training Epoch 21] Batch 3100, Loss 0.264004647731781\n",
            "[Training Epoch 21] Batch 3200, Loss 0.23822563886642456\n",
            "[Training Epoch 21] Batch 3300, Loss 0.27978426218032837\n",
            "[Training Epoch 21] Batch 3400, Loss 0.2448013871908188\n",
            "[Training Epoch 21] Batch 3500, Loss 0.3011387884616852\n",
            "[Training Epoch 21] Batch 3600, Loss 0.2610892653465271\n",
            "[Training Epoch 21] Batch 3700, Loss 0.29115796089172363\n",
            "[Training Epoch 21] Batch 3800, Loss 0.2532162666320801\n",
            "[Training Epoch 21] Batch 3900, Loss 0.24689501523971558\n",
            "[Training Epoch 21] Batch 4000, Loss 0.2824716866016388\n",
            "[Training Epoch 21] Batch 4100, Loss 0.29007384181022644\n",
            "[Training Epoch 21] Batch 4200, Loss 0.2946161925792694\n",
            "[Training Epoch 21] Batch 4300, Loss 0.2550470232963562\n",
            "[Training Epoch 21] Batch 4400, Loss 0.2588828206062317\n",
            "[Training Epoch 21] Batch 4500, Loss 0.29705747961997986\n",
            "[Training Epoch 21] Batch 4600, Loss 0.25317269563674927\n",
            "[Training Epoch 21] Batch 4700, Loss 0.2834444046020508\n",
            "[Training Epoch 21] Batch 4800, Loss 0.25701001286506653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 21] HR = 0.6252, NDCG = 0.3532\n",
            "Epoch 22 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 22] Batch 0, Loss 0.2684886157512665\n",
            "[Training Epoch 22] Batch 100, Loss 0.2933884859085083\n",
            "[Training Epoch 22] Batch 200, Loss 0.2761927843093872\n",
            "[Training Epoch 22] Batch 300, Loss 0.26828309893608093\n",
            "[Training Epoch 22] Batch 400, Loss 0.28832870721817017\n",
            "[Training Epoch 22] Batch 500, Loss 0.28363552689552307\n",
            "[Training Epoch 22] Batch 600, Loss 0.28536221385002136\n",
            "[Training Epoch 22] Batch 700, Loss 0.2834967374801636\n",
            "[Training Epoch 22] Batch 800, Loss 0.2584679126739502\n",
            "[Training Epoch 22] Batch 900, Loss 0.28378546237945557\n",
            "[Training Epoch 22] Batch 1000, Loss 0.24624350666999817\n",
            "[Training Epoch 22] Batch 1100, Loss 0.2835627496242523\n",
            "[Training Epoch 22] Batch 1200, Loss 0.26558637619018555\n",
            "[Training Epoch 22] Batch 1300, Loss 0.27450013160705566\n",
            "[Training Epoch 22] Batch 1400, Loss 0.2549739181995392\n",
            "[Training Epoch 22] Batch 1500, Loss 0.2419104427099228\n",
            "[Training Epoch 22] Batch 1600, Loss 0.2445138841867447\n",
            "[Training Epoch 22] Batch 1700, Loss 0.26435014605522156\n",
            "[Training Epoch 22] Batch 1800, Loss 0.2664971649646759\n",
            "[Training Epoch 22] Batch 1900, Loss 0.245366632938385\n",
            "[Training Epoch 22] Batch 2000, Loss 0.262565553188324\n",
            "[Training Epoch 22] Batch 2100, Loss 0.27369529008865356\n",
            "[Training Epoch 22] Batch 2200, Loss 0.2534918785095215\n",
            "[Training Epoch 22] Batch 2300, Loss 0.2653862237930298\n",
            "[Training Epoch 22] Batch 2400, Loss 0.2724536657333374\n",
            "[Training Epoch 22] Batch 2500, Loss 0.26269328594207764\n",
            "[Training Epoch 22] Batch 2600, Loss 0.2708417773246765\n",
            "[Training Epoch 22] Batch 2700, Loss 0.2561647593975067\n",
            "[Training Epoch 22] Batch 2800, Loss 0.2700759172439575\n",
            "[Training Epoch 22] Batch 2900, Loss 0.2732478976249695\n",
            "[Training Epoch 22] Batch 3000, Loss 0.2736121118068695\n",
            "[Training Epoch 22] Batch 3100, Loss 0.2666950225830078\n",
            "[Training Epoch 22] Batch 3200, Loss 0.2872997522354126\n",
            "[Training Epoch 22] Batch 3300, Loss 0.24026568233966827\n",
            "[Training Epoch 22] Batch 3400, Loss 0.26014482975006104\n",
            "[Training Epoch 22] Batch 3500, Loss 0.26042020320892334\n",
            "[Training Epoch 22] Batch 3600, Loss 0.26866820454597473\n",
            "[Training Epoch 22] Batch 3700, Loss 0.2932863235473633\n",
            "[Training Epoch 22] Batch 3800, Loss 0.2902279198169708\n",
            "[Training Epoch 22] Batch 3900, Loss 0.26955264806747437\n",
            "[Training Epoch 22] Batch 4000, Loss 0.2887158989906311\n",
            "[Training Epoch 22] Batch 4100, Loss 0.2782245874404907\n",
            "[Training Epoch 22] Batch 4200, Loss 0.2547851800918579\n",
            "[Training Epoch 22] Batch 4300, Loss 0.2780221998691559\n",
            "[Training Epoch 22] Batch 4400, Loss 0.27543169260025024\n",
            "[Training Epoch 22] Batch 4500, Loss 0.2708011269569397\n",
            "[Training Epoch 22] Batch 4600, Loss 0.25837916135787964\n",
            "[Training Epoch 22] Batch 4700, Loss 0.2555113434791565\n",
            "[Training Epoch 22] Batch 4800, Loss 0.28508269786834717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 22] HR = 0.6283, NDCG = 0.3562\n",
            "Epoch 23 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 23] Batch 0, Loss 0.29061591625213623\n",
            "[Training Epoch 23] Batch 100, Loss 0.26806989312171936\n",
            "[Training Epoch 23] Batch 200, Loss 0.2826497554779053\n",
            "[Training Epoch 23] Batch 300, Loss 0.2739294469356537\n",
            "[Training Epoch 23] Batch 400, Loss 0.28093421459198\n",
            "[Training Epoch 23] Batch 500, Loss 0.29685986042022705\n",
            "[Training Epoch 23] Batch 600, Loss 0.24743223190307617\n",
            "[Training Epoch 23] Batch 700, Loss 0.28889229893684387\n",
            "[Training Epoch 23] Batch 800, Loss 0.27936235070228577\n",
            "[Training Epoch 23] Batch 900, Loss 0.27789920568466187\n",
            "[Training Epoch 23] Batch 1000, Loss 0.2607944905757904\n",
            "[Training Epoch 23] Batch 1100, Loss 0.2851189374923706\n",
            "[Training Epoch 23] Batch 1200, Loss 0.2633839249610901\n",
            "[Training Epoch 23] Batch 1300, Loss 0.2517260015010834\n",
            "[Training Epoch 23] Batch 1400, Loss 0.27869048714637756\n",
            "[Training Epoch 23] Batch 1500, Loss 0.2736475467681885\n",
            "[Training Epoch 23] Batch 1600, Loss 0.28776705265045166\n",
            "[Training Epoch 23] Batch 1700, Loss 0.2551066279411316\n",
            "[Training Epoch 23] Batch 1800, Loss 0.25954920053482056\n",
            "[Training Epoch 23] Batch 1900, Loss 0.3114211857318878\n",
            "[Training Epoch 23] Batch 2000, Loss 0.28268909454345703\n",
            "[Training Epoch 23] Batch 2100, Loss 0.26666468381881714\n",
            "[Training Epoch 23] Batch 2200, Loss 0.2752673029899597\n",
            "[Training Epoch 23] Batch 2300, Loss 0.25540828704833984\n",
            "[Training Epoch 23] Batch 2400, Loss 0.28013336658477783\n",
            "[Training Epoch 23] Batch 2500, Loss 0.27400046586990356\n",
            "[Training Epoch 23] Batch 2600, Loss 0.27265411615371704\n",
            "[Training Epoch 23] Batch 2700, Loss 0.27466171979904175\n",
            "[Training Epoch 23] Batch 2800, Loss 0.2773258090019226\n",
            "[Training Epoch 23] Batch 2900, Loss 0.2899537980556488\n",
            "[Training Epoch 23] Batch 3000, Loss 0.2550038695335388\n",
            "[Training Epoch 23] Batch 3100, Loss 0.24833428859710693\n",
            "[Training Epoch 23] Batch 3200, Loss 0.2574159502983093\n",
            "[Training Epoch 23] Batch 3300, Loss 0.2681524455547333\n",
            "[Training Epoch 23] Batch 3400, Loss 0.2887915372848511\n",
            "[Training Epoch 23] Batch 3500, Loss 0.30407172441482544\n",
            "[Training Epoch 23] Batch 3600, Loss 0.24771325290203094\n",
            "[Training Epoch 23] Batch 3700, Loss 0.27056679129600525\n",
            "[Training Epoch 23] Batch 3800, Loss 0.2678186595439911\n",
            "[Training Epoch 23] Batch 3900, Loss 0.283136248588562\n",
            "[Training Epoch 23] Batch 4000, Loss 0.24362120032310486\n",
            "[Training Epoch 23] Batch 4100, Loss 0.29693445563316345\n",
            "[Training Epoch 23] Batch 4200, Loss 0.27636098861694336\n",
            "[Training Epoch 23] Batch 4300, Loss 0.2890778183937073\n",
            "[Training Epoch 23] Batch 4400, Loss 0.3197234869003296\n",
            "[Training Epoch 23] Batch 4500, Loss 0.2688092589378357\n",
            "[Training Epoch 23] Batch 4600, Loss 0.2748486399650574\n",
            "[Training Epoch 23] Batch 4700, Loss 0.2894858121871948\n",
            "[Training Epoch 23] Batch 4800, Loss 0.2642633318901062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 23] HR = 0.6344, NDCG = 0.3592\n",
            "Epoch 24 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 24] Batch 0, Loss 0.2934737801551819\n",
            "[Training Epoch 24] Batch 100, Loss 0.2631048262119293\n",
            "[Training Epoch 24] Batch 200, Loss 0.2828977704048157\n",
            "[Training Epoch 24] Batch 300, Loss 0.2811394929885864\n",
            "[Training Epoch 24] Batch 400, Loss 0.27383553981781006\n",
            "[Training Epoch 24] Batch 500, Loss 0.2739969491958618\n",
            "[Training Epoch 24] Batch 600, Loss 0.2895638942718506\n",
            "[Training Epoch 24] Batch 700, Loss 0.267784982919693\n",
            "[Training Epoch 24] Batch 800, Loss 0.29370731115341187\n",
            "[Training Epoch 24] Batch 900, Loss 0.2622488737106323\n",
            "[Training Epoch 24] Batch 1000, Loss 0.2679325044155121\n",
            "[Training Epoch 24] Batch 1100, Loss 0.251308798789978\n",
            "[Training Epoch 24] Batch 1200, Loss 0.2725886106491089\n",
            "[Training Epoch 24] Batch 1300, Loss 0.2917940616607666\n",
            "[Training Epoch 24] Batch 1400, Loss 0.26971012353897095\n",
            "[Training Epoch 24] Batch 1500, Loss 0.2578977346420288\n",
            "[Training Epoch 24] Batch 1600, Loss 0.24130168557167053\n",
            "[Training Epoch 24] Batch 1700, Loss 0.29570668935775757\n",
            "[Training Epoch 24] Batch 1800, Loss 0.2761068344116211\n",
            "[Training Epoch 24] Batch 1900, Loss 0.26758599281311035\n",
            "[Training Epoch 24] Batch 2000, Loss 0.25975021719932556\n",
            "[Training Epoch 24] Batch 2100, Loss 0.2578316330909729\n",
            "[Training Epoch 24] Batch 2200, Loss 0.25449779629707336\n",
            "[Training Epoch 24] Batch 2300, Loss 0.29331183433532715\n",
            "[Training Epoch 24] Batch 2400, Loss 0.26319536566734314\n",
            "[Training Epoch 24] Batch 2500, Loss 0.3169301152229309\n",
            "[Training Epoch 24] Batch 2600, Loss 0.25379759073257446\n",
            "[Training Epoch 24] Batch 2700, Loss 0.25011754035949707\n",
            "[Training Epoch 24] Batch 2800, Loss 0.26687097549438477\n",
            "[Training Epoch 24] Batch 2900, Loss 0.2601950168609619\n",
            "[Training Epoch 24] Batch 3000, Loss 0.28100645542144775\n",
            "[Training Epoch 24] Batch 3100, Loss 0.24548721313476562\n",
            "[Training Epoch 24] Batch 3200, Loss 0.2754848003387451\n",
            "[Training Epoch 24] Batch 3300, Loss 0.269797682762146\n",
            "[Training Epoch 24] Batch 3400, Loss 0.252482533454895\n",
            "[Training Epoch 24] Batch 3500, Loss 0.26237308979034424\n",
            "[Training Epoch 24] Batch 3600, Loss 0.27248579263687134\n",
            "[Training Epoch 24] Batch 3700, Loss 0.27182137966156006\n",
            "[Training Epoch 24] Batch 3800, Loss 0.2772271931171417\n",
            "[Training Epoch 24] Batch 3900, Loss 0.27468907833099365\n",
            "[Training Epoch 24] Batch 4000, Loss 0.2641434073448181\n",
            "[Training Epoch 24] Batch 4100, Loss 0.29626166820526123\n",
            "[Training Epoch 24] Batch 4200, Loss 0.29115334153175354\n",
            "[Training Epoch 24] Batch 4300, Loss 0.2809869050979614\n",
            "[Training Epoch 24] Batch 4400, Loss 0.2563934028148651\n",
            "[Training Epoch 24] Batch 4500, Loss 0.3021218776702881\n",
            "[Training Epoch 24] Batch 4600, Loss 0.26864874362945557\n",
            "[Training Epoch 24] Batch 4700, Loss 0.24878567457199097\n",
            "[Training Epoch 24] Batch 4800, Loss 0.25328701734542847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 24] HR = 0.6315, NDCG = 0.3595\n",
            "Epoch 25 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 25] Batch 0, Loss 0.28140127658843994\n",
            "[Training Epoch 25] Batch 100, Loss 0.2757742404937744\n",
            "[Training Epoch 25] Batch 200, Loss 0.27504971623420715\n",
            "[Training Epoch 25] Batch 300, Loss 0.2923796772956848\n",
            "[Training Epoch 25] Batch 400, Loss 0.2505027651786804\n",
            "[Training Epoch 25] Batch 500, Loss 0.3053825795650482\n",
            "[Training Epoch 25] Batch 600, Loss 0.2647988796234131\n",
            "[Training Epoch 25] Batch 700, Loss 0.2712385058403015\n",
            "[Training Epoch 25] Batch 800, Loss 0.26839515566825867\n",
            "[Training Epoch 25] Batch 900, Loss 0.279252290725708\n",
            "[Training Epoch 25] Batch 1000, Loss 0.243769109249115\n",
            "[Training Epoch 25] Batch 1100, Loss 0.2662855386734009\n",
            "[Training Epoch 25] Batch 1200, Loss 0.269875705242157\n",
            "[Training Epoch 25] Batch 1300, Loss 0.277652382850647\n",
            "[Training Epoch 25] Batch 1400, Loss 0.310778945684433\n",
            "[Training Epoch 25] Batch 1500, Loss 0.2664932608604431\n",
            "[Training Epoch 25] Batch 1600, Loss 0.2580341100692749\n",
            "[Training Epoch 25] Batch 1700, Loss 0.27698415517807007\n",
            "[Training Epoch 25] Batch 1800, Loss 0.2960716485977173\n",
            "[Training Epoch 25] Batch 1900, Loss 0.25013765692710876\n",
            "[Training Epoch 25] Batch 2000, Loss 0.26517683267593384\n",
            "[Training Epoch 25] Batch 2100, Loss 0.27389001846313477\n",
            "[Training Epoch 25] Batch 2200, Loss 0.26866862177848816\n",
            "[Training Epoch 25] Batch 2300, Loss 0.2606041729450226\n",
            "[Training Epoch 25] Batch 2400, Loss 0.28989094495773315\n",
            "[Training Epoch 25] Batch 2500, Loss 0.2672997713088989\n",
            "[Training Epoch 25] Batch 2600, Loss 0.27860593795776367\n",
            "[Training Epoch 25] Batch 2700, Loss 0.29135164618492126\n",
            "[Training Epoch 25] Batch 2800, Loss 0.2853114604949951\n",
            "[Training Epoch 25] Batch 2900, Loss 0.25483283400535583\n",
            "[Training Epoch 25] Batch 3000, Loss 0.2650599479675293\n",
            "[Training Epoch 25] Batch 3100, Loss 0.25091636180877686\n",
            "[Training Epoch 25] Batch 3200, Loss 0.2783966660499573\n",
            "[Training Epoch 25] Batch 3300, Loss 0.27607813477516174\n",
            "[Training Epoch 25] Batch 3400, Loss 0.26948046684265137\n",
            "[Training Epoch 25] Batch 3500, Loss 0.28297632932662964\n",
            "[Training Epoch 25] Batch 3600, Loss 0.28604674339294434\n",
            "[Training Epoch 25] Batch 3700, Loss 0.2689673602581024\n",
            "[Training Epoch 25] Batch 3800, Loss 0.2731435000896454\n",
            "[Training Epoch 25] Batch 3900, Loss 0.2914468050003052\n",
            "[Training Epoch 25] Batch 4000, Loss 0.2610049247741699\n",
            "[Training Epoch 25] Batch 4100, Loss 0.25970152020454407\n",
            "[Training Epoch 25] Batch 4200, Loss 0.23610565066337585\n",
            "[Training Epoch 25] Batch 4300, Loss 0.25084730982780457\n",
            "[Training Epoch 25] Batch 4400, Loss 0.29225921630859375\n",
            "[Training Epoch 25] Batch 4500, Loss 0.287941575050354\n",
            "[Training Epoch 25] Batch 4600, Loss 0.27721893787384033\n",
            "[Training Epoch 25] Batch 4700, Loss 0.2979872226715088\n",
            "[Training Epoch 25] Batch 4800, Loss 0.2726966142654419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 25] HR = 0.6325, NDCG = 0.3619\n",
            "Epoch 26 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 26] Batch 0, Loss 0.28194618225097656\n",
            "[Training Epoch 26] Batch 100, Loss 0.2637576162815094\n",
            "[Training Epoch 26] Batch 200, Loss 0.31029045581817627\n",
            "[Training Epoch 26] Batch 300, Loss 0.28792351484298706\n",
            "[Training Epoch 26] Batch 400, Loss 0.25905659794807434\n",
            "[Training Epoch 26] Batch 500, Loss 0.24513834714889526\n",
            "[Training Epoch 26] Batch 600, Loss 0.25618118047714233\n",
            "[Training Epoch 26] Batch 700, Loss 0.2604649066925049\n",
            "[Training Epoch 26] Batch 800, Loss 0.26607251167297363\n",
            "[Training Epoch 26] Batch 900, Loss 0.2617512047290802\n",
            "[Training Epoch 26] Batch 1000, Loss 0.30551692843437195\n",
            "[Training Epoch 26] Batch 1100, Loss 0.2846854329109192\n",
            "[Training Epoch 26] Batch 1200, Loss 0.31313759088516235\n",
            "[Training Epoch 26] Batch 1300, Loss 0.24237139523029327\n",
            "[Training Epoch 26] Batch 1400, Loss 0.2886274755001068\n",
            "[Training Epoch 26] Batch 1500, Loss 0.2705894410610199\n",
            "[Training Epoch 26] Batch 1600, Loss 0.28272831439971924\n",
            "[Training Epoch 26] Batch 1700, Loss 0.2549322843551636\n",
            "[Training Epoch 26] Batch 1800, Loss 0.29272085428237915\n",
            "[Training Epoch 26] Batch 1900, Loss 0.24877944588661194\n",
            "[Training Epoch 26] Batch 2000, Loss 0.2575794458389282\n",
            "[Training Epoch 26] Batch 2100, Loss 0.2696557939052582\n",
            "[Training Epoch 26] Batch 2200, Loss 0.2875943183898926\n",
            "[Training Epoch 26] Batch 2300, Loss 0.2756412625312805\n",
            "[Training Epoch 26] Batch 2400, Loss 0.26269206404685974\n",
            "[Training Epoch 26] Batch 2500, Loss 0.26381126046180725\n",
            "[Training Epoch 26] Batch 2600, Loss 0.2627873420715332\n",
            "[Training Epoch 26] Batch 2700, Loss 0.25880542397499084\n",
            "[Training Epoch 26] Batch 2800, Loss 0.28790295124053955\n",
            "[Training Epoch 26] Batch 2900, Loss 0.27195432782173157\n",
            "[Training Epoch 26] Batch 3000, Loss 0.2536191940307617\n",
            "[Training Epoch 26] Batch 3100, Loss 0.2781311869621277\n",
            "[Training Epoch 26] Batch 3200, Loss 0.2829704284667969\n",
            "[Training Epoch 26] Batch 3300, Loss 0.27576667070388794\n",
            "[Training Epoch 26] Batch 3400, Loss 0.27172601222991943\n",
            "[Training Epoch 26] Batch 3500, Loss 0.27824676036834717\n",
            "[Training Epoch 26] Batch 3600, Loss 0.2612180709838867\n",
            "[Training Epoch 26] Batch 3700, Loss 0.30778858065605164\n",
            "[Training Epoch 26] Batch 3800, Loss 0.2706601619720459\n",
            "[Training Epoch 26] Batch 3900, Loss 0.25409460067749023\n",
            "[Training Epoch 26] Batch 4000, Loss 0.289333313703537\n",
            "[Training Epoch 26] Batch 4100, Loss 0.26364222168922424\n",
            "[Training Epoch 26] Batch 4200, Loss 0.2953663468360901\n",
            "[Training Epoch 26] Batch 4300, Loss 0.28183478116989136\n",
            "[Training Epoch 26] Batch 4400, Loss 0.27897006273269653\n",
            "[Training Epoch 26] Batch 4500, Loss 0.2638114094734192\n",
            "[Training Epoch 26] Batch 4600, Loss 0.29722359776496887\n",
            "[Training Epoch 26] Batch 4700, Loss 0.25705206394195557\n",
            "[Training Epoch 26] Batch 4800, Loss 0.27845263481140137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 26] HR = 0.6354, NDCG = 0.3630\n",
            "Epoch 27 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 27] Batch 0, Loss 0.25027626752853394\n",
            "[Training Epoch 27] Batch 100, Loss 0.2737520933151245\n",
            "[Training Epoch 27] Batch 200, Loss 0.2500872313976288\n",
            "[Training Epoch 27] Batch 300, Loss 0.28038275241851807\n",
            "[Training Epoch 27] Batch 400, Loss 0.26945775747299194\n",
            "[Training Epoch 27] Batch 500, Loss 0.2538587749004364\n",
            "[Training Epoch 27] Batch 600, Loss 0.2629889249801636\n",
            "[Training Epoch 27] Batch 700, Loss 0.2690526247024536\n",
            "[Training Epoch 27] Batch 800, Loss 0.2484433501958847\n",
            "[Training Epoch 27] Batch 900, Loss 0.2583308219909668\n",
            "[Training Epoch 27] Batch 1000, Loss 0.2604590654373169\n",
            "[Training Epoch 27] Batch 1100, Loss 0.27909961342811584\n",
            "[Training Epoch 27] Batch 1200, Loss 0.2868717312812805\n",
            "[Training Epoch 27] Batch 1300, Loss 0.2577970027923584\n",
            "[Training Epoch 27] Batch 1400, Loss 0.25523126125335693\n",
            "[Training Epoch 27] Batch 1500, Loss 0.2752952575683594\n",
            "[Training Epoch 27] Batch 1600, Loss 0.2628447711467743\n",
            "[Training Epoch 27] Batch 1700, Loss 0.23183825612068176\n",
            "[Training Epoch 27] Batch 1800, Loss 0.283586710691452\n",
            "[Training Epoch 27] Batch 1900, Loss 0.2666928172111511\n",
            "[Training Epoch 27] Batch 2000, Loss 0.2776942849159241\n",
            "[Training Epoch 27] Batch 2100, Loss 0.2831163704395294\n",
            "[Training Epoch 27] Batch 2200, Loss 0.2547115981578827\n",
            "[Training Epoch 27] Batch 2300, Loss 0.2721586227416992\n",
            "[Training Epoch 27] Batch 2400, Loss 0.26383423805236816\n",
            "[Training Epoch 27] Batch 2500, Loss 0.28835558891296387\n",
            "[Training Epoch 27] Batch 2600, Loss 0.2736348807811737\n",
            "[Training Epoch 27] Batch 2700, Loss 0.2552463412284851\n",
            "[Training Epoch 27] Batch 2800, Loss 0.2670048475265503\n",
            "[Training Epoch 27] Batch 2900, Loss 0.2551100552082062\n",
            "[Training Epoch 27] Batch 3000, Loss 0.2586669921875\n",
            "[Training Epoch 27] Batch 3100, Loss 0.2520179748535156\n",
            "[Training Epoch 27] Batch 3200, Loss 0.259307324886322\n",
            "[Training Epoch 27] Batch 3300, Loss 0.2683677077293396\n",
            "[Training Epoch 27] Batch 3400, Loss 0.2843892574310303\n",
            "[Training Epoch 27] Batch 3500, Loss 0.2510497570037842\n",
            "[Training Epoch 27] Batch 3600, Loss 0.2684294283390045\n",
            "[Training Epoch 27] Batch 3700, Loss 0.27819961309432983\n",
            "[Training Epoch 27] Batch 3800, Loss 0.25763052701950073\n",
            "[Training Epoch 27] Batch 3900, Loss 0.2799243628978729\n",
            "[Training Epoch 27] Batch 4000, Loss 0.2724810838699341\n",
            "[Training Epoch 27] Batch 4100, Loss 0.26084044575691223\n",
            "[Training Epoch 27] Batch 4200, Loss 0.2632478177547455\n",
            "[Training Epoch 27] Batch 4300, Loss 0.2513895034790039\n",
            "[Training Epoch 27] Batch 4400, Loss 0.2682366371154785\n",
            "[Training Epoch 27] Batch 4500, Loss 0.26612532138824463\n",
            "[Training Epoch 27] Batch 4600, Loss 0.2905174493789673\n",
            "[Training Epoch 27] Batch 4700, Loss 0.2748028635978699\n",
            "[Training Epoch 27] Batch 4800, Loss 0.2657319903373718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 27] HR = 0.6361, NDCG = 0.3649\n",
            "Epoch 28 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 28] Batch 0, Loss 0.24764710664749146\n",
            "[Training Epoch 28] Batch 100, Loss 0.2633304297924042\n",
            "[Training Epoch 28] Batch 200, Loss 0.2939316928386688\n",
            "[Training Epoch 28] Batch 300, Loss 0.28824663162231445\n",
            "[Training Epoch 28] Batch 400, Loss 0.2448640763759613\n",
            "[Training Epoch 28] Batch 500, Loss 0.2802827060222626\n",
            "[Training Epoch 28] Batch 600, Loss 0.2632512152194977\n",
            "[Training Epoch 28] Batch 700, Loss 0.2555830478668213\n",
            "[Training Epoch 28] Batch 800, Loss 0.27228671312332153\n",
            "[Training Epoch 28] Batch 900, Loss 0.2567674517631531\n",
            "[Training Epoch 28] Batch 1000, Loss 0.2593894600868225\n",
            "[Training Epoch 28] Batch 1100, Loss 0.27461808919906616\n",
            "[Training Epoch 28] Batch 1200, Loss 0.23003530502319336\n",
            "[Training Epoch 28] Batch 1300, Loss 0.2582384943962097\n",
            "[Training Epoch 28] Batch 1400, Loss 0.2843121290206909\n",
            "[Training Epoch 28] Batch 1500, Loss 0.26732388138771057\n",
            "[Training Epoch 28] Batch 1600, Loss 0.2645947337150574\n",
            "[Training Epoch 28] Batch 1700, Loss 0.28910303115844727\n",
            "[Training Epoch 28] Batch 1800, Loss 0.23839329183101654\n",
            "[Training Epoch 28] Batch 1900, Loss 0.27231284976005554\n",
            "[Training Epoch 28] Batch 2000, Loss 0.2705841064453125\n",
            "[Training Epoch 28] Batch 2100, Loss 0.2731544077396393\n",
            "[Training Epoch 28] Batch 2200, Loss 0.2995212972164154\n",
            "[Training Epoch 28] Batch 2300, Loss 0.29680919647216797\n",
            "[Training Epoch 28] Batch 2400, Loss 0.26917174458503723\n",
            "[Training Epoch 28] Batch 2500, Loss 0.27038753032684326\n",
            "[Training Epoch 28] Batch 2600, Loss 0.26372748613357544\n",
            "[Training Epoch 28] Batch 2700, Loss 0.2588745057582855\n",
            "[Training Epoch 28] Batch 2800, Loss 0.25956153869628906\n",
            "[Training Epoch 28] Batch 2900, Loss 0.2693502902984619\n",
            "[Training Epoch 28] Batch 3000, Loss 0.27188941836357117\n",
            "[Training Epoch 28] Batch 3100, Loss 0.2768489718437195\n",
            "[Training Epoch 28] Batch 3200, Loss 0.26564493775367737\n",
            "[Training Epoch 28] Batch 3300, Loss 0.2583405375480652\n",
            "[Training Epoch 28] Batch 3400, Loss 0.2791460454463959\n",
            "[Training Epoch 28] Batch 3500, Loss 0.2546791434288025\n",
            "[Training Epoch 28] Batch 3600, Loss 0.2537344992160797\n",
            "[Training Epoch 28] Batch 3700, Loss 0.2788882851600647\n",
            "[Training Epoch 28] Batch 3800, Loss 0.2657201588153839\n",
            "[Training Epoch 28] Batch 3900, Loss 0.2730664312839508\n",
            "[Training Epoch 28] Batch 4000, Loss 0.24929149448871613\n",
            "[Training Epoch 28] Batch 4100, Loss 0.2862268090248108\n",
            "[Training Epoch 28] Batch 4200, Loss 0.24893872439861298\n",
            "[Training Epoch 28] Batch 4300, Loss 0.2885276675224304\n",
            "[Training Epoch 28] Batch 4400, Loss 0.2894631028175354\n",
            "[Training Epoch 28] Batch 4500, Loss 0.2826170325279236\n",
            "[Training Epoch 28] Batch 4600, Loss 0.232172429561615\n",
            "[Training Epoch 28] Batch 4700, Loss 0.2720683217048645\n",
            "[Training Epoch 28] Batch 4800, Loss 0.2756666839122772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 28] HR = 0.6364, NDCG = 0.3641\n",
            "Epoch 29 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 29] Batch 0, Loss 0.28164830803871155\n",
            "[Training Epoch 29] Batch 100, Loss 0.24642986059188843\n",
            "[Training Epoch 29] Batch 200, Loss 0.27234336733818054\n",
            "[Training Epoch 29] Batch 300, Loss 0.2826511859893799\n",
            "[Training Epoch 29] Batch 400, Loss 0.2483372986316681\n",
            "[Training Epoch 29] Batch 500, Loss 0.26526060700416565\n",
            "[Training Epoch 29] Batch 600, Loss 0.2894219160079956\n",
            "[Training Epoch 29] Batch 700, Loss 0.27623915672302246\n",
            "[Training Epoch 29] Batch 800, Loss 0.26232898235321045\n",
            "[Training Epoch 29] Batch 900, Loss 0.26891249418258667\n",
            "[Training Epoch 29] Batch 1000, Loss 0.25510817766189575\n",
            "[Training Epoch 29] Batch 1100, Loss 0.2595653235912323\n",
            "[Training Epoch 29] Batch 1200, Loss 0.24186450242996216\n",
            "[Training Epoch 29] Batch 1300, Loss 0.3017203211784363\n",
            "[Training Epoch 29] Batch 1400, Loss 0.2713932991027832\n",
            "[Training Epoch 29] Batch 1500, Loss 0.2898368239402771\n",
            "[Training Epoch 29] Batch 1600, Loss 0.26311641931533813\n",
            "[Training Epoch 29] Batch 1700, Loss 0.2582378387451172\n",
            "[Training Epoch 29] Batch 1800, Loss 0.2795003056526184\n",
            "[Training Epoch 29] Batch 1900, Loss 0.2512836456298828\n",
            "[Training Epoch 29] Batch 2000, Loss 0.28857898712158203\n",
            "[Training Epoch 29] Batch 2100, Loss 0.24898938834667206\n",
            "[Training Epoch 29] Batch 2200, Loss 0.26190900802612305\n",
            "[Training Epoch 29] Batch 2300, Loss 0.25456809997558594\n",
            "[Training Epoch 29] Batch 2400, Loss 0.2882709205150604\n",
            "[Training Epoch 29] Batch 2500, Loss 0.2613258957862854\n",
            "[Training Epoch 29] Batch 2600, Loss 0.27527952194213867\n",
            "[Training Epoch 29] Batch 2700, Loss 0.27786657214164734\n",
            "[Training Epoch 29] Batch 2800, Loss 0.24250397086143494\n",
            "[Training Epoch 29] Batch 2900, Loss 0.27220213413238525\n",
            "[Training Epoch 29] Batch 3000, Loss 0.26349443197250366\n",
            "[Training Epoch 29] Batch 3100, Loss 0.27434998750686646\n",
            "[Training Epoch 29] Batch 3200, Loss 0.25872403383255005\n",
            "[Training Epoch 29] Batch 3300, Loss 0.2663121223449707\n",
            "[Training Epoch 29] Batch 3400, Loss 0.3123805522918701\n",
            "[Training Epoch 29] Batch 3500, Loss 0.2867916524410248\n",
            "[Training Epoch 29] Batch 3600, Loss 0.2768639028072357\n",
            "[Training Epoch 29] Batch 3700, Loss 0.28573116660118103\n",
            "[Training Epoch 29] Batch 3800, Loss 0.29644349217414856\n",
            "[Training Epoch 29] Batch 3900, Loss 0.27796870470046997\n",
            "[Training Epoch 29] Batch 4000, Loss 0.2608600854873657\n",
            "[Training Epoch 29] Batch 4100, Loss 0.2528071999549866\n",
            "[Training Epoch 29] Batch 4200, Loss 0.2598363757133484\n",
            "[Training Epoch 29] Batch 4300, Loss 0.2830813527107239\n",
            "[Training Epoch 29] Batch 4400, Loss 0.27948397397994995\n",
            "[Training Epoch 29] Batch 4500, Loss 0.25799262523651123\n",
            "[Training Epoch 29] Batch 4600, Loss 0.28817877173423767\n",
            "[Training Epoch 29] Batch 4700, Loss 0.2449798285961151\n",
            "[Training Epoch 29] Batch 4800, Loss 0.2672160267829895\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 29] HR = 0.6364, NDCG = 0.3654\n",
            "Epoch 30 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 30] Batch 0, Loss 0.2777230739593506\n",
            "[Training Epoch 30] Batch 100, Loss 0.2726726233959198\n",
            "[Training Epoch 30] Batch 200, Loss 0.25421983003616333\n",
            "[Training Epoch 30] Batch 300, Loss 0.2744990885257721\n",
            "[Training Epoch 30] Batch 400, Loss 0.2687687277793884\n",
            "[Training Epoch 30] Batch 500, Loss 0.2621857523918152\n",
            "[Training Epoch 30] Batch 600, Loss 0.2682759165763855\n",
            "[Training Epoch 30] Batch 700, Loss 0.25424110889434814\n",
            "[Training Epoch 30] Batch 800, Loss 0.25917258858680725\n",
            "[Training Epoch 30] Batch 900, Loss 0.2650696635246277\n",
            "[Training Epoch 30] Batch 1000, Loss 0.25844132900238037\n",
            "[Training Epoch 30] Batch 1100, Loss 0.25425079464912415\n",
            "[Training Epoch 30] Batch 1200, Loss 0.27007031440734863\n",
            "[Training Epoch 30] Batch 1300, Loss 0.23445962369441986\n",
            "[Training Epoch 30] Batch 1400, Loss 0.27468588948249817\n",
            "[Training Epoch 30] Batch 1500, Loss 0.2598532438278198\n",
            "[Training Epoch 30] Batch 1600, Loss 0.29526710510253906\n",
            "[Training Epoch 30] Batch 1700, Loss 0.29492896795272827\n",
            "[Training Epoch 30] Batch 1800, Loss 0.2645236849784851\n",
            "[Training Epoch 30] Batch 1900, Loss 0.26828300952911377\n",
            "[Training Epoch 30] Batch 2000, Loss 0.25516587495803833\n",
            "[Training Epoch 30] Batch 2100, Loss 0.3086920976638794\n",
            "[Training Epoch 30] Batch 2200, Loss 0.2390601485967636\n",
            "[Training Epoch 30] Batch 2300, Loss 0.27096468210220337\n",
            "[Training Epoch 30] Batch 2400, Loss 0.25107473134994507\n",
            "[Training Epoch 30] Batch 2500, Loss 0.26273566484451294\n",
            "[Training Epoch 30] Batch 2600, Loss 0.2783607244491577\n",
            "[Training Epoch 30] Batch 2700, Loss 0.27790704369544983\n",
            "[Training Epoch 30] Batch 2800, Loss 0.25219622254371643\n",
            "[Training Epoch 30] Batch 2900, Loss 0.24608203768730164\n",
            "[Training Epoch 30] Batch 3000, Loss 0.25770848989486694\n",
            "[Training Epoch 30] Batch 3100, Loss 0.27404311299324036\n",
            "[Training Epoch 30] Batch 3200, Loss 0.27825435996055603\n",
            "[Training Epoch 30] Batch 3300, Loss 0.2721770107746124\n",
            "[Training Epoch 30] Batch 3400, Loss 0.27707868814468384\n",
            "[Training Epoch 30] Batch 3500, Loss 0.2649235129356384\n",
            "[Training Epoch 30] Batch 3600, Loss 0.2658833861351013\n",
            "[Training Epoch 30] Batch 3700, Loss 0.28086018562316895\n",
            "[Training Epoch 30] Batch 3800, Loss 0.24427223205566406\n",
            "[Training Epoch 30] Batch 3900, Loss 0.2727944254875183\n",
            "[Training Epoch 30] Batch 4000, Loss 0.26032546162605286\n",
            "[Training Epoch 30] Batch 4100, Loss 0.25863632559776306\n",
            "[Training Epoch 30] Batch 4200, Loss 0.26567327976226807\n",
            "[Training Epoch 30] Batch 4300, Loss 0.27520668506622314\n",
            "[Training Epoch 30] Batch 4400, Loss 0.26659923791885376\n",
            "[Training Epoch 30] Batch 4500, Loss 0.25947368144989014\n",
            "[Training Epoch 30] Batch 4600, Loss 0.273726224899292\n",
            "[Training Epoch 30] Batch 4700, Loss 0.25817859172821045\n",
            "[Training Epoch 30] Batch 4800, Loss 0.24202381074428558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 30] HR = 0.6382, NDCG = 0.3661\n",
            "Epoch 31 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 31] Batch 0, Loss 0.2692142724990845\n",
            "[Training Epoch 31] Batch 100, Loss 0.3158191740512848\n",
            "[Training Epoch 31] Batch 200, Loss 0.27298036217689514\n",
            "[Training Epoch 31] Batch 300, Loss 0.2971068024635315\n",
            "[Training Epoch 31] Batch 400, Loss 0.25988268852233887\n",
            "[Training Epoch 31] Batch 500, Loss 0.24951234459877014\n",
            "[Training Epoch 31] Batch 600, Loss 0.2645379900932312\n",
            "[Training Epoch 31] Batch 700, Loss 0.2915623188018799\n",
            "[Training Epoch 31] Batch 800, Loss 0.2777774930000305\n",
            "[Training Epoch 31] Batch 900, Loss 0.24929207563400269\n",
            "[Training Epoch 31] Batch 1000, Loss 0.27451393008232117\n",
            "[Training Epoch 31] Batch 1100, Loss 0.27479416131973267\n",
            "[Training Epoch 31] Batch 1200, Loss 0.2652912437915802\n",
            "[Training Epoch 31] Batch 1300, Loss 0.2501007318496704\n",
            "[Training Epoch 31] Batch 1400, Loss 0.26980388164520264\n",
            "[Training Epoch 31] Batch 1500, Loss 0.2542937994003296\n",
            "[Training Epoch 31] Batch 1600, Loss 0.25526145100593567\n",
            "[Training Epoch 31] Batch 1700, Loss 0.2599637806415558\n",
            "[Training Epoch 31] Batch 1800, Loss 0.29349297285079956\n",
            "[Training Epoch 31] Batch 1900, Loss 0.2782803773880005\n",
            "[Training Epoch 31] Batch 2000, Loss 0.24296346306800842\n",
            "[Training Epoch 31] Batch 2100, Loss 0.2655457854270935\n",
            "[Training Epoch 31] Batch 2200, Loss 0.2585026025772095\n",
            "[Training Epoch 31] Batch 2300, Loss 0.25879913568496704\n",
            "[Training Epoch 31] Batch 2400, Loss 0.27049678564071655\n",
            "[Training Epoch 31] Batch 2500, Loss 0.28289997577667236\n",
            "[Training Epoch 31] Batch 2600, Loss 0.26903071999549866\n",
            "[Training Epoch 31] Batch 2700, Loss 0.263649046421051\n",
            "[Training Epoch 31] Batch 2800, Loss 0.28671887516975403\n",
            "[Training Epoch 31] Batch 2900, Loss 0.2518655061721802\n",
            "[Training Epoch 31] Batch 3000, Loss 0.2699553668498993\n",
            "[Training Epoch 31] Batch 3100, Loss 0.28320184350013733\n",
            "[Training Epoch 31] Batch 3200, Loss 0.2658771574497223\n",
            "[Training Epoch 31] Batch 3300, Loss 0.25848716497421265\n",
            "[Training Epoch 31] Batch 3400, Loss 0.22379864752292633\n",
            "[Training Epoch 31] Batch 3500, Loss 0.28198832273483276\n",
            "[Training Epoch 31] Batch 3600, Loss 0.2622394561767578\n",
            "[Training Epoch 31] Batch 3700, Loss 0.2719498574733734\n",
            "[Training Epoch 31] Batch 3800, Loss 0.3208425045013428\n",
            "[Training Epoch 31] Batch 3900, Loss 0.30964750051498413\n",
            "[Training Epoch 31] Batch 4000, Loss 0.2797233462333679\n",
            "[Training Epoch 31] Batch 4100, Loss 0.27173978090286255\n",
            "[Training Epoch 31] Batch 4200, Loss 0.2690804600715637\n",
            "[Training Epoch 31] Batch 4300, Loss 0.2885472774505615\n",
            "[Training Epoch 31] Batch 4400, Loss 0.27718883752822876\n",
            "[Training Epoch 31] Batch 4500, Loss 0.29583340883255005\n",
            "[Training Epoch 31] Batch 4600, Loss 0.27386635541915894\n",
            "[Training Epoch 31] Batch 4700, Loss 0.2934151887893677\n",
            "[Training Epoch 31] Batch 4800, Loss 0.28817564249038696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 31] HR = 0.6382, NDCG = 0.3663\n",
            "Epoch 32 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 32] Batch 0, Loss 0.24065044522285461\n",
            "[Training Epoch 32] Batch 100, Loss 0.24146078526973724\n",
            "[Training Epoch 32] Batch 200, Loss 0.2683272361755371\n",
            "[Training Epoch 32] Batch 300, Loss 0.2675786018371582\n",
            "[Training Epoch 32] Batch 400, Loss 0.25823870301246643\n",
            "[Training Epoch 32] Batch 500, Loss 0.26062124967575073\n",
            "[Training Epoch 32] Batch 600, Loss 0.2894217371940613\n",
            "[Training Epoch 32] Batch 700, Loss 0.26349467039108276\n",
            "[Training Epoch 32] Batch 800, Loss 0.2568683624267578\n",
            "[Training Epoch 32] Batch 900, Loss 0.2471841424703598\n",
            "[Training Epoch 32] Batch 1000, Loss 0.27831071615219116\n",
            "[Training Epoch 32] Batch 1100, Loss 0.27956798672676086\n",
            "[Training Epoch 32] Batch 1200, Loss 0.2737618684768677\n",
            "[Training Epoch 32] Batch 1300, Loss 0.2704443037509918\n",
            "[Training Epoch 32] Batch 1400, Loss 0.27007216215133667\n",
            "[Training Epoch 32] Batch 1500, Loss 0.2573135793209076\n",
            "[Training Epoch 32] Batch 1600, Loss 0.2573612332344055\n",
            "[Training Epoch 32] Batch 1700, Loss 0.26698100566864014\n",
            "[Training Epoch 32] Batch 1800, Loss 0.25160855054855347\n",
            "[Training Epoch 32] Batch 1900, Loss 0.2599257528781891\n",
            "[Training Epoch 32] Batch 2000, Loss 0.26510101556777954\n",
            "[Training Epoch 32] Batch 2100, Loss 0.2894365191459656\n",
            "[Training Epoch 32] Batch 2200, Loss 0.2623567581176758\n",
            "[Training Epoch 32] Batch 2300, Loss 0.26764172315597534\n",
            "[Training Epoch 32] Batch 2400, Loss 0.279316782951355\n",
            "[Training Epoch 32] Batch 2500, Loss 0.27895650267601013\n",
            "[Training Epoch 32] Batch 2600, Loss 0.2665257453918457\n",
            "[Training Epoch 32] Batch 2700, Loss 0.2526167631149292\n",
            "[Training Epoch 32] Batch 2800, Loss 0.28230369091033936\n",
            "[Training Epoch 32] Batch 2900, Loss 0.26009729504585266\n",
            "[Training Epoch 32] Batch 3000, Loss 0.25237688422203064\n",
            "[Training Epoch 32] Batch 3100, Loss 0.27088919281959534\n",
            "[Training Epoch 32] Batch 3200, Loss 0.27235186100006104\n",
            "[Training Epoch 32] Batch 3300, Loss 0.28412044048309326\n",
            "[Training Epoch 32] Batch 3400, Loss 0.26868802309036255\n",
            "[Training Epoch 32] Batch 3500, Loss 0.275898277759552\n",
            "[Training Epoch 32] Batch 3600, Loss 0.2697452902793884\n",
            "[Training Epoch 32] Batch 3700, Loss 0.2808408737182617\n",
            "[Training Epoch 32] Batch 3800, Loss 0.2671796977519989\n",
            "[Training Epoch 32] Batch 3900, Loss 0.2713758945465088\n",
            "[Training Epoch 32] Batch 4000, Loss 0.23529449105262756\n",
            "[Training Epoch 32] Batch 4100, Loss 0.24326282739639282\n",
            "[Training Epoch 32] Batch 4200, Loss 0.27725082635879517\n",
            "[Training Epoch 32] Batch 4300, Loss 0.3073420524597168\n",
            "[Training Epoch 32] Batch 4400, Loss 0.2897062301635742\n",
            "[Training Epoch 32] Batch 4500, Loss 0.2514229416847229\n",
            "[Training Epoch 32] Batch 4600, Loss 0.2553730607032776\n",
            "[Training Epoch 32] Batch 4700, Loss 0.2664094567298889\n",
            "[Training Epoch 32] Batch 4800, Loss 0.2773783802986145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 32] HR = 0.6389, NDCG = 0.3672\n",
            "Epoch 33 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 33] Batch 0, Loss 0.2671922445297241\n",
            "[Training Epoch 33] Batch 100, Loss 0.29059332609176636\n",
            "[Training Epoch 33] Batch 200, Loss 0.2841745913028717\n",
            "[Training Epoch 33] Batch 300, Loss 0.2518061101436615\n",
            "[Training Epoch 33] Batch 400, Loss 0.2864580452442169\n",
            "[Training Epoch 33] Batch 500, Loss 0.26530247926712036\n",
            "[Training Epoch 33] Batch 600, Loss 0.2825128734111786\n",
            "[Training Epoch 33] Batch 700, Loss 0.2872052788734436\n",
            "[Training Epoch 33] Batch 800, Loss 0.29713159799575806\n",
            "[Training Epoch 33] Batch 900, Loss 0.2755082845687866\n",
            "[Training Epoch 33] Batch 1000, Loss 0.2248712182044983\n",
            "[Training Epoch 33] Batch 1100, Loss 0.2589876651763916\n",
            "[Training Epoch 33] Batch 1200, Loss 0.2689415216445923\n",
            "[Training Epoch 33] Batch 1300, Loss 0.29108718037605286\n",
            "[Training Epoch 33] Batch 1400, Loss 0.3178764283657074\n",
            "[Training Epoch 33] Batch 1500, Loss 0.23632855713367462\n",
            "[Training Epoch 33] Batch 1600, Loss 0.2735920250415802\n",
            "[Training Epoch 33] Batch 1700, Loss 0.24050980806350708\n",
            "[Training Epoch 33] Batch 1800, Loss 0.2649005353450775\n",
            "[Training Epoch 33] Batch 1900, Loss 0.27210289239883423\n",
            "[Training Epoch 33] Batch 2000, Loss 0.24968533217906952\n",
            "[Training Epoch 33] Batch 2100, Loss 0.2809002101421356\n",
            "[Training Epoch 33] Batch 2200, Loss 0.2938332259654999\n",
            "[Training Epoch 33] Batch 2300, Loss 0.2886839509010315\n",
            "[Training Epoch 33] Batch 2400, Loss 0.2585650682449341\n",
            "[Training Epoch 33] Batch 2500, Loss 0.26927947998046875\n",
            "[Training Epoch 33] Batch 2600, Loss 0.2677665948867798\n",
            "[Training Epoch 33] Batch 2700, Loss 0.26117193698883057\n",
            "[Training Epoch 33] Batch 2800, Loss 0.29304665327072144\n",
            "[Training Epoch 33] Batch 2900, Loss 0.29960697889328003\n",
            "[Training Epoch 33] Batch 3000, Loss 0.22882521152496338\n",
            "[Training Epoch 33] Batch 3100, Loss 0.24862322211265564\n",
            "[Training Epoch 33] Batch 3200, Loss 0.30089935660362244\n",
            "[Training Epoch 33] Batch 3300, Loss 0.27890723943710327\n",
            "[Training Epoch 33] Batch 3400, Loss 0.2279491275548935\n",
            "[Training Epoch 33] Batch 3500, Loss 0.2879619002342224\n",
            "[Training Epoch 33] Batch 3600, Loss 0.26116788387298584\n",
            "[Training Epoch 33] Batch 3700, Loss 0.2755056321620941\n",
            "[Training Epoch 33] Batch 3800, Loss 0.2598201632499695\n",
            "[Training Epoch 33] Batch 3900, Loss 0.27407801151275635\n",
            "[Training Epoch 33] Batch 4000, Loss 0.24130891263484955\n",
            "[Training Epoch 33] Batch 4100, Loss 0.27641159296035767\n",
            "[Training Epoch 33] Batch 4200, Loss 0.2506546676158905\n",
            "[Training Epoch 33] Batch 4300, Loss 0.24004298448562622\n",
            "[Training Epoch 33] Batch 4400, Loss 0.28283247351646423\n",
            "[Training Epoch 33] Batch 4500, Loss 0.26412782073020935\n",
            "[Training Epoch 33] Batch 4600, Loss 0.2278120219707489\n",
            "[Training Epoch 33] Batch 4700, Loss 0.2730081081390381\n",
            "[Training Epoch 33] Batch 4800, Loss 0.2849571704864502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 33] HR = 0.6376, NDCG = 0.3674\n",
            "Epoch 34 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 34] Batch 0, Loss 0.2493741810321808\n",
            "[Training Epoch 34] Batch 100, Loss 0.2519376277923584\n",
            "[Training Epoch 34] Batch 200, Loss 0.2678997814655304\n",
            "[Training Epoch 34] Batch 300, Loss 0.2561812698841095\n",
            "[Training Epoch 34] Batch 400, Loss 0.251895546913147\n",
            "[Training Epoch 34] Batch 500, Loss 0.2522541284561157\n",
            "[Training Epoch 34] Batch 600, Loss 0.27519717812538147\n",
            "[Training Epoch 34] Batch 700, Loss 0.30036622285842896\n",
            "[Training Epoch 34] Batch 800, Loss 0.24592912197113037\n",
            "[Training Epoch 34] Batch 900, Loss 0.2755087614059448\n",
            "[Training Epoch 34] Batch 1000, Loss 0.2800494432449341\n",
            "[Training Epoch 34] Batch 1100, Loss 0.2703839838504791\n",
            "[Training Epoch 34] Batch 1200, Loss 0.23819103837013245\n",
            "[Training Epoch 34] Batch 1300, Loss 0.2479993999004364\n",
            "[Training Epoch 34] Batch 1400, Loss 0.2754227817058563\n",
            "[Training Epoch 34] Batch 1500, Loss 0.2659945785999298\n",
            "[Training Epoch 34] Batch 1600, Loss 0.2562788128852844\n",
            "[Training Epoch 34] Batch 1700, Loss 0.2801356911659241\n",
            "[Training Epoch 34] Batch 1800, Loss 0.26251158118247986\n",
            "[Training Epoch 34] Batch 1900, Loss 0.2729339003562927\n",
            "[Training Epoch 34] Batch 2000, Loss 0.25799041986465454\n",
            "[Training Epoch 34] Batch 2100, Loss 0.2592741847038269\n",
            "[Training Epoch 34] Batch 2200, Loss 0.2700210213661194\n",
            "[Training Epoch 34] Batch 2300, Loss 0.24867284297943115\n",
            "[Training Epoch 34] Batch 2400, Loss 0.28352582454681396\n",
            "[Training Epoch 34] Batch 2500, Loss 0.28022006154060364\n",
            "[Training Epoch 34] Batch 2600, Loss 0.2608396112918854\n",
            "[Training Epoch 34] Batch 2700, Loss 0.30457156896591187\n",
            "[Training Epoch 34] Batch 2800, Loss 0.2922695279121399\n",
            "[Training Epoch 34] Batch 2900, Loss 0.2411024272441864\n",
            "[Training Epoch 34] Batch 3000, Loss 0.27837133407592773\n",
            "[Training Epoch 34] Batch 3100, Loss 0.25348395109176636\n",
            "[Training Epoch 34] Batch 3200, Loss 0.2864530682563782\n",
            "[Training Epoch 34] Batch 3300, Loss 0.24627277255058289\n",
            "[Training Epoch 34] Batch 3400, Loss 0.2890535295009613\n",
            "[Training Epoch 34] Batch 3500, Loss 0.22283247113227844\n",
            "[Training Epoch 34] Batch 3600, Loss 0.2612554430961609\n",
            "[Training Epoch 34] Batch 3700, Loss 0.26395079493522644\n",
            "[Training Epoch 34] Batch 3800, Loss 0.24467423558235168\n",
            "[Training Epoch 34] Batch 3900, Loss 0.26426514983177185\n",
            "[Training Epoch 34] Batch 4000, Loss 0.2649807631969452\n",
            "[Training Epoch 34] Batch 4100, Loss 0.2409735769033432\n",
            "[Training Epoch 34] Batch 4200, Loss 0.26229649782180786\n",
            "[Training Epoch 34] Batch 4300, Loss 0.27480703592300415\n",
            "[Training Epoch 34] Batch 4400, Loss 0.2692055106163025\n",
            "[Training Epoch 34] Batch 4500, Loss 0.27339595556259155\n",
            "[Training Epoch 34] Batch 4600, Loss 0.23411613702774048\n",
            "[Training Epoch 34] Batch 4700, Loss 0.24616190791130066\n",
            "[Training Epoch 34] Batch 4800, Loss 0.2619209289550781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 34] HR = 0.6387, NDCG = 0.3681\n",
            "Epoch 35 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 35] Batch 0, Loss 0.24008387327194214\n",
            "[Training Epoch 35] Batch 100, Loss 0.267192542552948\n",
            "[Training Epoch 35] Batch 200, Loss 0.25825634598731995\n",
            "[Training Epoch 35] Batch 300, Loss 0.2537742555141449\n",
            "[Training Epoch 35] Batch 400, Loss 0.2721085250377655\n",
            "[Training Epoch 35] Batch 500, Loss 0.26907116174697876\n",
            "[Training Epoch 35] Batch 600, Loss 0.2647770643234253\n",
            "[Training Epoch 35] Batch 700, Loss 0.23817771673202515\n",
            "[Training Epoch 35] Batch 800, Loss 0.2753608226776123\n",
            "[Training Epoch 35] Batch 900, Loss 0.2513881325721741\n",
            "[Training Epoch 35] Batch 1000, Loss 0.27822768688201904\n",
            "[Training Epoch 35] Batch 1100, Loss 0.2814175486564636\n",
            "[Training Epoch 35] Batch 1200, Loss 0.2649558484554291\n",
            "[Training Epoch 35] Batch 1300, Loss 0.2650972306728363\n",
            "[Training Epoch 35] Batch 1400, Loss 0.23090797662734985\n",
            "[Training Epoch 35] Batch 1500, Loss 0.2553131878376007\n",
            "[Training Epoch 35] Batch 1600, Loss 0.2775176167488098\n",
            "[Training Epoch 35] Batch 1700, Loss 0.28165414929389954\n",
            "[Training Epoch 35] Batch 1800, Loss 0.2476331740617752\n",
            "[Training Epoch 35] Batch 1900, Loss 0.27557480335235596\n",
            "[Training Epoch 35] Batch 2000, Loss 0.2696138620376587\n",
            "[Training Epoch 35] Batch 2100, Loss 0.2575766444206238\n",
            "[Training Epoch 35] Batch 2200, Loss 0.25368472933769226\n",
            "[Training Epoch 35] Batch 2300, Loss 0.25868839025497437\n",
            "[Training Epoch 35] Batch 2400, Loss 0.2541331648826599\n",
            "[Training Epoch 35] Batch 2500, Loss 0.26497331261634827\n",
            "[Training Epoch 35] Batch 2600, Loss 0.264626145362854\n",
            "[Training Epoch 35] Batch 2700, Loss 0.2842158079147339\n",
            "[Training Epoch 35] Batch 2800, Loss 0.27749234437942505\n",
            "[Training Epoch 35] Batch 2900, Loss 0.30333343148231506\n",
            "[Training Epoch 35] Batch 3000, Loss 0.2464625984430313\n",
            "[Training Epoch 35] Batch 3100, Loss 0.26615434885025024\n",
            "[Training Epoch 35] Batch 3200, Loss 0.27008557319641113\n",
            "[Training Epoch 35] Batch 3300, Loss 0.2646617889404297\n",
            "[Training Epoch 35] Batch 3400, Loss 0.24048206210136414\n",
            "[Training Epoch 35] Batch 3500, Loss 0.27998483180999756\n",
            "[Training Epoch 35] Batch 3600, Loss 0.25327783823013306\n",
            "[Training Epoch 35] Batch 3700, Loss 0.2650740444660187\n",
            "[Training Epoch 35] Batch 3800, Loss 0.2657514810562134\n",
            "[Training Epoch 35] Batch 3900, Loss 0.27637743949890137\n",
            "[Training Epoch 35] Batch 4000, Loss 0.26500001549720764\n",
            "[Training Epoch 35] Batch 4100, Loss 0.2649908661842346\n",
            "[Training Epoch 35] Batch 4200, Loss 0.26949459314346313\n",
            "[Training Epoch 35] Batch 4300, Loss 0.2541790008544922\n",
            "[Training Epoch 35] Batch 4400, Loss 0.2406139373779297\n",
            "[Training Epoch 35] Batch 4500, Loss 0.3224616050720215\n",
            "[Training Epoch 35] Batch 4600, Loss 0.2787744998931885\n",
            "[Training Epoch 35] Batch 4700, Loss 0.27694272994995117\n",
            "[Training Epoch 35] Batch 4800, Loss 0.2757130563259125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 35] HR = 0.6427, NDCG = 0.3701\n",
            "Epoch 36 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 36] Batch 0, Loss 0.2636578381061554\n",
            "[Training Epoch 36] Batch 100, Loss 0.2857208847999573\n",
            "[Training Epoch 36] Batch 200, Loss 0.25270164012908936\n",
            "[Training Epoch 36] Batch 300, Loss 0.2681771516799927\n",
            "[Training Epoch 36] Batch 400, Loss 0.27001047134399414\n",
            "[Training Epoch 36] Batch 500, Loss 0.28036004304885864\n",
            "[Training Epoch 36] Batch 600, Loss 0.2688385248184204\n",
            "[Training Epoch 36] Batch 700, Loss 0.2367062270641327\n",
            "[Training Epoch 36] Batch 800, Loss 0.2525629699230194\n",
            "[Training Epoch 36] Batch 900, Loss 0.2794482111930847\n",
            "[Training Epoch 36] Batch 1000, Loss 0.26631322503089905\n",
            "[Training Epoch 36] Batch 1100, Loss 0.2590811252593994\n",
            "[Training Epoch 36] Batch 1200, Loss 0.2537316679954529\n",
            "[Training Epoch 36] Batch 1300, Loss 0.28278103470802307\n",
            "[Training Epoch 36] Batch 1400, Loss 0.28871774673461914\n",
            "[Training Epoch 36] Batch 1500, Loss 0.2656419277191162\n",
            "[Training Epoch 36] Batch 1600, Loss 0.2518560290336609\n",
            "[Training Epoch 36] Batch 1700, Loss 0.28645646572113037\n",
            "[Training Epoch 36] Batch 1800, Loss 0.2873268127441406\n",
            "[Training Epoch 36] Batch 1900, Loss 0.2869715094566345\n",
            "[Training Epoch 36] Batch 2000, Loss 0.28011554479599\n",
            "[Training Epoch 36] Batch 2100, Loss 0.2683749794960022\n",
            "[Training Epoch 36] Batch 2200, Loss 0.2705458402633667\n",
            "[Training Epoch 36] Batch 2300, Loss 0.2823542356491089\n",
            "[Training Epoch 36] Batch 2400, Loss 0.25765174627304077\n",
            "[Training Epoch 36] Batch 2500, Loss 0.24808526039123535\n",
            "[Training Epoch 36] Batch 2600, Loss 0.2910313606262207\n",
            "[Training Epoch 36] Batch 2700, Loss 0.24250981211662292\n",
            "[Training Epoch 36] Batch 2800, Loss 0.25291064381599426\n",
            "[Training Epoch 36] Batch 2900, Loss 0.3087221086025238\n",
            "[Training Epoch 36] Batch 3000, Loss 0.263007253408432\n",
            "[Training Epoch 36] Batch 3100, Loss 0.2763599753379822\n",
            "[Training Epoch 36] Batch 3200, Loss 0.27254772186279297\n",
            "[Training Epoch 36] Batch 3300, Loss 0.256341814994812\n",
            "[Training Epoch 36] Batch 3400, Loss 0.24916699528694153\n",
            "[Training Epoch 36] Batch 3500, Loss 0.2263697236776352\n",
            "[Training Epoch 36] Batch 3600, Loss 0.26743191480636597\n",
            "[Training Epoch 36] Batch 3700, Loss 0.2599378824234009\n",
            "[Training Epoch 36] Batch 3800, Loss 0.23589715361595154\n",
            "[Training Epoch 36] Batch 3900, Loss 0.26350027322769165\n",
            "[Training Epoch 36] Batch 4000, Loss 0.27212899923324585\n",
            "[Training Epoch 36] Batch 4100, Loss 0.26797592639923096\n",
            "[Training Epoch 36] Batch 4200, Loss 0.3006221354007721\n",
            "[Training Epoch 36] Batch 4300, Loss 0.27103516459465027\n",
            "[Training Epoch 36] Batch 4400, Loss 0.26581934094429016\n",
            "[Training Epoch 36] Batch 4500, Loss 0.24787211418151855\n",
            "[Training Epoch 36] Batch 4600, Loss 0.2651668190956116\n",
            "[Training Epoch 36] Batch 4700, Loss 0.27581876516342163\n",
            "[Training Epoch 36] Batch 4800, Loss 0.28403007984161377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 36] HR = 0.6411, NDCG = 0.3712\n",
            "Epoch 37 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 37] Batch 0, Loss 0.2513887286186218\n",
            "[Training Epoch 37] Batch 100, Loss 0.25750023126602173\n",
            "[Training Epoch 37] Batch 200, Loss 0.268398642539978\n",
            "[Training Epoch 37] Batch 300, Loss 0.2673094570636749\n",
            "[Training Epoch 37] Batch 400, Loss 0.2660587728023529\n",
            "[Training Epoch 37] Batch 500, Loss 0.23613527417182922\n",
            "[Training Epoch 37] Batch 600, Loss 0.26639455556869507\n",
            "[Training Epoch 37] Batch 700, Loss 0.2861012816429138\n",
            "[Training Epoch 37] Batch 800, Loss 0.2672044634819031\n",
            "[Training Epoch 37] Batch 900, Loss 0.27567917108535767\n",
            "[Training Epoch 37] Batch 1000, Loss 0.24041292071342468\n",
            "[Training Epoch 37] Batch 1100, Loss 0.3067567050457001\n",
            "[Training Epoch 37] Batch 1200, Loss 0.25990286469459534\n",
            "[Training Epoch 37] Batch 1300, Loss 0.25983285903930664\n",
            "[Training Epoch 37] Batch 1400, Loss 0.2879055440425873\n",
            "[Training Epoch 37] Batch 1500, Loss 0.2721100449562073\n",
            "[Training Epoch 37] Batch 1600, Loss 0.27896666526794434\n",
            "[Training Epoch 37] Batch 1700, Loss 0.2593396306037903\n",
            "[Training Epoch 37] Batch 1800, Loss 0.27160656452178955\n",
            "[Training Epoch 37] Batch 1900, Loss 0.26128625869750977\n",
            "[Training Epoch 37] Batch 2000, Loss 0.2752072215080261\n",
            "[Training Epoch 37] Batch 2100, Loss 0.2753460109233856\n",
            "[Training Epoch 37] Batch 2200, Loss 0.2523133158683777\n",
            "[Training Epoch 37] Batch 2300, Loss 0.25152966380119324\n",
            "[Training Epoch 37] Batch 2400, Loss 0.25280579924583435\n",
            "[Training Epoch 37] Batch 2500, Loss 0.27180564403533936\n",
            "[Training Epoch 37] Batch 2600, Loss 0.26551321148872375\n",
            "[Training Epoch 37] Batch 2700, Loss 0.2505852282047272\n",
            "[Training Epoch 37] Batch 2800, Loss 0.27525246143341064\n",
            "[Training Epoch 37] Batch 2900, Loss 0.25667938590049744\n",
            "[Training Epoch 37] Batch 3000, Loss 0.2918030619621277\n",
            "[Training Epoch 37] Batch 3100, Loss 0.269788533449173\n",
            "[Training Epoch 37] Batch 3200, Loss 0.24382567405700684\n",
            "[Training Epoch 37] Batch 3300, Loss 0.24546308815479279\n",
            "[Training Epoch 37] Batch 3400, Loss 0.26791703701019287\n",
            "[Training Epoch 37] Batch 3500, Loss 0.2710237503051758\n",
            "[Training Epoch 37] Batch 3600, Loss 0.2916463613510132\n",
            "[Training Epoch 37] Batch 3700, Loss 0.2583872675895691\n",
            "[Training Epoch 37] Batch 3800, Loss 0.2664928138256073\n",
            "[Training Epoch 37] Batch 3900, Loss 0.2705078721046448\n",
            "[Training Epoch 37] Batch 4000, Loss 0.2534993886947632\n",
            "[Training Epoch 37] Batch 4100, Loss 0.2598625719547272\n",
            "[Training Epoch 37] Batch 4200, Loss 0.25894129276275635\n",
            "[Training Epoch 37] Batch 4300, Loss 0.30182382464408875\n",
            "[Training Epoch 37] Batch 4400, Loss 0.25253888964653015\n",
            "[Training Epoch 37] Batch 4500, Loss 0.28993135690689087\n",
            "[Training Epoch 37] Batch 4600, Loss 0.2975042760372162\n",
            "[Training Epoch 37] Batch 4700, Loss 0.2592010498046875\n",
            "[Training Epoch 37] Batch 4800, Loss 0.24738404154777527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 37] HR = 0.6379, NDCG = 0.3687\n",
            "Epoch 38 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 38] Batch 0, Loss 0.257956326007843\n",
            "[Training Epoch 38] Batch 100, Loss 0.2424124777317047\n",
            "[Training Epoch 38] Batch 200, Loss 0.24722245335578918\n",
            "[Training Epoch 38] Batch 300, Loss 0.26773330569267273\n",
            "[Training Epoch 38] Batch 400, Loss 0.2641640901565552\n",
            "[Training Epoch 38] Batch 500, Loss 0.30595046281814575\n",
            "[Training Epoch 38] Batch 600, Loss 0.2676914632320404\n",
            "[Training Epoch 38] Batch 700, Loss 0.311165988445282\n",
            "[Training Epoch 38] Batch 800, Loss 0.30802828073501587\n",
            "[Training Epoch 38] Batch 900, Loss 0.24825695157051086\n",
            "[Training Epoch 38] Batch 1000, Loss 0.24898257851600647\n",
            "[Training Epoch 38] Batch 1100, Loss 0.2718507647514343\n",
            "[Training Epoch 38] Batch 1200, Loss 0.2638828158378601\n",
            "[Training Epoch 38] Batch 1300, Loss 0.2643149793148041\n",
            "[Training Epoch 38] Batch 1400, Loss 0.2624879479408264\n",
            "[Training Epoch 38] Batch 1500, Loss 0.2394259124994278\n",
            "[Training Epoch 38] Batch 1600, Loss 0.2624746263027191\n",
            "[Training Epoch 38] Batch 1700, Loss 0.29066798090934753\n",
            "[Training Epoch 38] Batch 1800, Loss 0.2535204589366913\n",
            "[Training Epoch 38] Batch 1900, Loss 0.28811708092689514\n",
            "[Training Epoch 38] Batch 2000, Loss 0.29552361369132996\n",
            "[Training Epoch 38] Batch 2100, Loss 0.2762104272842407\n",
            "[Training Epoch 38] Batch 2200, Loss 0.2531895935535431\n",
            "[Training Epoch 38] Batch 2300, Loss 0.25453388690948486\n",
            "[Training Epoch 38] Batch 2400, Loss 0.26105374097824097\n",
            "[Training Epoch 38] Batch 2500, Loss 0.27194592356681824\n",
            "[Training Epoch 38] Batch 2600, Loss 0.30476900935173035\n",
            "[Training Epoch 38] Batch 2700, Loss 0.2554554343223572\n",
            "[Training Epoch 38] Batch 2800, Loss 0.2746238708496094\n",
            "[Training Epoch 38] Batch 2900, Loss 0.2720099091529846\n",
            "[Training Epoch 38] Batch 3000, Loss 0.2701655626296997\n",
            "[Training Epoch 38] Batch 3100, Loss 0.2409963458776474\n",
            "[Training Epoch 38] Batch 3200, Loss 0.2302553951740265\n",
            "[Training Epoch 38] Batch 3300, Loss 0.26518481969833374\n",
            "[Training Epoch 38] Batch 3400, Loss 0.2754293978214264\n",
            "[Training Epoch 38] Batch 3500, Loss 0.254721999168396\n",
            "[Training Epoch 38] Batch 3600, Loss 0.2626801133155823\n",
            "[Training Epoch 38] Batch 3700, Loss 0.28434836864471436\n",
            "[Training Epoch 38] Batch 3800, Loss 0.27721697092056274\n",
            "[Training Epoch 38] Batch 3900, Loss 0.2969588041305542\n",
            "[Training Epoch 38] Batch 4000, Loss 0.2502291202545166\n",
            "[Training Epoch 38] Batch 4100, Loss 0.26470455527305603\n",
            "[Training Epoch 38] Batch 4200, Loss 0.24795958399772644\n",
            "[Training Epoch 38] Batch 4300, Loss 0.25507840514183044\n",
            "[Training Epoch 38] Batch 4400, Loss 0.2587846517562866\n",
            "[Training Epoch 38] Batch 4500, Loss 0.2947571873664856\n",
            "[Training Epoch 38] Batch 4600, Loss 0.24286200106143951\n",
            "[Training Epoch 38] Batch 4700, Loss 0.2515726685523987\n",
            "[Training Epoch 38] Batch 4800, Loss 0.26785415410995483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 38] HR = 0.6416, NDCG = 0.3690\n",
            "Epoch 39 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 39] Batch 0, Loss 0.2752746641635895\n",
            "[Training Epoch 39] Batch 100, Loss 0.27314114570617676\n",
            "[Training Epoch 39] Batch 200, Loss 0.24006596207618713\n",
            "[Training Epoch 39] Batch 300, Loss 0.2705589830875397\n",
            "[Training Epoch 39] Batch 400, Loss 0.24836331605911255\n",
            "[Training Epoch 39] Batch 500, Loss 0.26524263620376587\n",
            "[Training Epoch 39] Batch 600, Loss 0.31674647331237793\n",
            "[Training Epoch 39] Batch 700, Loss 0.2890031933784485\n",
            "[Training Epoch 39] Batch 800, Loss 0.2768487334251404\n",
            "[Training Epoch 39] Batch 900, Loss 0.23840108513832092\n",
            "[Training Epoch 39] Batch 1000, Loss 0.25731781125068665\n",
            "[Training Epoch 39] Batch 1100, Loss 0.2778514325618744\n",
            "[Training Epoch 39] Batch 1200, Loss 0.26076552271842957\n",
            "[Training Epoch 39] Batch 1300, Loss 0.2505339980125427\n",
            "[Training Epoch 39] Batch 1400, Loss 0.2653847932815552\n",
            "[Training Epoch 39] Batch 1500, Loss 0.3123874068260193\n",
            "[Training Epoch 39] Batch 1600, Loss 0.2674121558666229\n",
            "[Training Epoch 39] Batch 1700, Loss 0.24977688491344452\n",
            "[Training Epoch 39] Batch 1800, Loss 0.24782992899417877\n",
            "[Training Epoch 39] Batch 1900, Loss 0.25769686698913574\n",
            "[Training Epoch 39] Batch 2000, Loss 0.23809316754341125\n",
            "[Training Epoch 39] Batch 2100, Loss 0.28814560174942017\n",
            "[Training Epoch 39] Batch 2200, Loss 0.2528787851333618\n",
            "[Training Epoch 39] Batch 2300, Loss 0.26663705706596375\n",
            "[Training Epoch 39] Batch 2400, Loss 0.30239996314048767\n",
            "[Training Epoch 39] Batch 2500, Loss 0.27275216579437256\n",
            "[Training Epoch 39] Batch 2600, Loss 0.24273979663848877\n",
            "[Training Epoch 39] Batch 2700, Loss 0.27256131172180176\n",
            "[Training Epoch 39] Batch 2800, Loss 0.22819335758686066\n",
            "[Training Epoch 39] Batch 2900, Loss 0.27259016036987305\n",
            "[Training Epoch 39] Batch 3000, Loss 0.24310371279716492\n",
            "[Training Epoch 39] Batch 3100, Loss 0.3018682897090912\n",
            "[Training Epoch 39] Batch 3200, Loss 0.2911352515220642\n",
            "[Training Epoch 39] Batch 3300, Loss 0.2534579038619995\n",
            "[Training Epoch 39] Batch 3400, Loss 0.2647205889225006\n",
            "[Training Epoch 39] Batch 3500, Loss 0.27949607372283936\n",
            "[Training Epoch 39] Batch 3600, Loss 0.2672026753425598\n",
            "[Training Epoch 39] Batch 3700, Loss 0.31307029724121094\n",
            "[Training Epoch 39] Batch 3800, Loss 0.2772378623485565\n",
            "[Training Epoch 39] Batch 3900, Loss 0.27850064635276794\n",
            "[Training Epoch 39] Batch 4000, Loss 0.22481828927993774\n",
            "[Training Epoch 39] Batch 4100, Loss 0.29345762729644775\n",
            "[Training Epoch 39] Batch 4200, Loss 0.26267313957214355\n",
            "[Training Epoch 39] Batch 4300, Loss 0.2444106489419937\n",
            "[Training Epoch 39] Batch 4400, Loss 0.2597752809524536\n",
            "[Training Epoch 39] Batch 4500, Loss 0.30366945266723633\n",
            "[Training Epoch 39] Batch 4600, Loss 0.27302151918411255\n",
            "[Training Epoch 39] Batch 4700, Loss 0.26397189497947693\n",
            "[Training Epoch 39] Batch 4800, Loss 0.28141671419143677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 39] HR = 0.6391, NDCG = 0.3695\n",
            "Epoch 40 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 40] Batch 0, Loss 0.28107038140296936\n",
            "[Training Epoch 40] Batch 100, Loss 0.2737663984298706\n",
            "[Training Epoch 40] Batch 200, Loss 0.2773968279361725\n",
            "[Training Epoch 40] Batch 300, Loss 0.2706359624862671\n",
            "[Training Epoch 40] Batch 400, Loss 0.2547653317451477\n",
            "[Training Epoch 40] Batch 500, Loss 0.2664162516593933\n",
            "[Training Epoch 40] Batch 600, Loss 0.2599899172782898\n",
            "[Training Epoch 40] Batch 700, Loss 0.28811705112457275\n",
            "[Training Epoch 40] Batch 800, Loss 0.2645617723464966\n",
            "[Training Epoch 40] Batch 900, Loss 0.2973783016204834\n",
            "[Training Epoch 40] Batch 1000, Loss 0.25527533888816833\n",
            "[Training Epoch 40] Batch 1100, Loss 0.2622030973434448\n",
            "[Training Epoch 40] Batch 1200, Loss 0.2714610993862152\n",
            "[Training Epoch 40] Batch 1300, Loss 0.2440197467803955\n",
            "[Training Epoch 40] Batch 1400, Loss 0.27011922001838684\n",
            "[Training Epoch 40] Batch 1500, Loss 0.28546199202537537\n",
            "[Training Epoch 40] Batch 1600, Loss 0.2638806700706482\n",
            "[Training Epoch 40] Batch 1700, Loss 0.2171328365802765\n",
            "[Training Epoch 40] Batch 1800, Loss 0.2681008577346802\n",
            "[Training Epoch 40] Batch 1900, Loss 0.2643176019191742\n",
            "[Training Epoch 40] Batch 2000, Loss 0.29354774951934814\n",
            "[Training Epoch 40] Batch 2100, Loss 0.3003009855747223\n",
            "[Training Epoch 40] Batch 2200, Loss 0.2829709053039551\n",
            "[Training Epoch 40] Batch 2300, Loss 0.26409944891929626\n",
            "[Training Epoch 40] Batch 2400, Loss 0.26025599241256714\n",
            "[Training Epoch 40] Batch 2500, Loss 0.303672194480896\n",
            "[Training Epoch 40] Batch 2600, Loss 0.26379886269569397\n",
            "[Training Epoch 40] Batch 2700, Loss 0.259467214345932\n",
            "[Training Epoch 40] Batch 2800, Loss 0.25982674956321716\n",
            "[Training Epoch 40] Batch 2900, Loss 0.2506105303764343\n",
            "[Training Epoch 40] Batch 3000, Loss 0.30845916271209717\n",
            "[Training Epoch 40] Batch 3100, Loss 0.2765890061855316\n",
            "[Training Epoch 40] Batch 3200, Loss 0.2545146048069\n",
            "[Training Epoch 40] Batch 3300, Loss 0.2897450923919678\n",
            "[Training Epoch 40] Batch 3400, Loss 0.26788610219955444\n",
            "[Training Epoch 40] Batch 3500, Loss 0.2690364122390747\n",
            "[Training Epoch 40] Batch 3600, Loss 0.249261274933815\n",
            "[Training Epoch 40] Batch 3700, Loss 0.24853074550628662\n",
            "[Training Epoch 40] Batch 3800, Loss 0.29541441798210144\n",
            "[Training Epoch 40] Batch 3900, Loss 0.26159411668777466\n",
            "[Training Epoch 40] Batch 4000, Loss 0.28045928478240967\n",
            "[Training Epoch 40] Batch 4100, Loss 0.2658282220363617\n",
            "[Training Epoch 40] Batch 4200, Loss 0.2593649625778198\n",
            "[Training Epoch 40] Batch 4300, Loss 0.26084306836128235\n",
            "[Training Epoch 40] Batch 4400, Loss 0.26564836502075195\n",
            "[Training Epoch 40] Batch 4500, Loss 0.2758970260620117\n",
            "[Training Epoch 40] Batch 4600, Loss 0.269805371761322\n",
            "[Training Epoch 40] Batch 4700, Loss 0.290027916431427\n",
            "[Training Epoch 40] Batch 4800, Loss 0.271202027797699\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 40] HR = 0.6412, NDCG = 0.3717\n",
            "Epoch 41 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 41] Batch 0, Loss 0.25860390067100525\n",
            "[Training Epoch 41] Batch 100, Loss 0.2700865566730499\n",
            "[Training Epoch 41] Batch 200, Loss 0.26958709955215454\n",
            "[Training Epoch 41] Batch 300, Loss 0.2731335163116455\n",
            "[Training Epoch 41] Batch 400, Loss 0.23376506567001343\n",
            "[Training Epoch 41] Batch 500, Loss 0.2531285881996155\n",
            "[Training Epoch 41] Batch 600, Loss 0.2577342391014099\n",
            "[Training Epoch 41] Batch 700, Loss 0.27174440026283264\n",
            "[Training Epoch 41] Batch 800, Loss 0.24524126946926117\n",
            "[Training Epoch 41] Batch 900, Loss 0.2864583730697632\n",
            "[Training Epoch 41] Batch 1000, Loss 0.2550628185272217\n",
            "[Training Epoch 41] Batch 1100, Loss 0.2864978611469269\n",
            "[Training Epoch 41] Batch 1200, Loss 0.225615993142128\n",
            "[Training Epoch 41] Batch 1300, Loss 0.2686771750450134\n",
            "[Training Epoch 41] Batch 1400, Loss 0.25385063886642456\n",
            "[Training Epoch 41] Batch 1500, Loss 0.2621697783470154\n",
            "[Training Epoch 41] Batch 1600, Loss 0.28669703006744385\n",
            "[Training Epoch 41] Batch 1700, Loss 0.2559320032596588\n",
            "[Training Epoch 41] Batch 1800, Loss 0.2455293983221054\n",
            "[Training Epoch 41] Batch 1900, Loss 0.29695260524749756\n",
            "[Training Epoch 41] Batch 2000, Loss 0.25186270475387573\n",
            "[Training Epoch 41] Batch 2100, Loss 0.2381119728088379\n",
            "[Training Epoch 41] Batch 2200, Loss 0.24669069051742554\n",
            "[Training Epoch 41] Batch 2300, Loss 0.2369992733001709\n",
            "[Training Epoch 41] Batch 2400, Loss 0.27101898193359375\n",
            "[Training Epoch 41] Batch 2500, Loss 0.28218281269073486\n",
            "[Training Epoch 41] Batch 2600, Loss 0.2755342721939087\n",
            "[Training Epoch 41] Batch 2700, Loss 0.2791876196861267\n",
            "[Training Epoch 41] Batch 2800, Loss 0.26268428564071655\n",
            "[Training Epoch 41] Batch 2900, Loss 0.29137900471687317\n",
            "[Training Epoch 41] Batch 3000, Loss 0.26372116804122925\n",
            "[Training Epoch 41] Batch 3100, Loss 0.25281059741973877\n",
            "[Training Epoch 41] Batch 3200, Loss 0.24810515344142914\n",
            "[Training Epoch 41] Batch 3300, Loss 0.25694483518600464\n",
            "[Training Epoch 41] Batch 3400, Loss 0.23852276802062988\n",
            "[Training Epoch 41] Batch 3500, Loss 0.2567310929298401\n",
            "[Training Epoch 41] Batch 3600, Loss 0.29991498589515686\n",
            "[Training Epoch 41] Batch 3700, Loss 0.249815434217453\n",
            "[Training Epoch 41] Batch 3800, Loss 0.2861796021461487\n",
            "[Training Epoch 41] Batch 3900, Loss 0.27331459522247314\n",
            "[Training Epoch 41] Batch 4000, Loss 0.248575359582901\n",
            "[Training Epoch 41] Batch 4100, Loss 0.26073750853538513\n",
            "[Training Epoch 41] Batch 4200, Loss 0.25208860635757446\n",
            "[Training Epoch 41] Batch 4300, Loss 0.26079967617988586\n",
            "[Training Epoch 41] Batch 4400, Loss 0.2637917399406433\n",
            "[Training Epoch 41] Batch 4500, Loss 0.26481354236602783\n",
            "[Training Epoch 41] Batch 4600, Loss 0.24079039692878723\n",
            "[Training Epoch 41] Batch 4700, Loss 0.2710023522377014\n",
            "[Training Epoch 41] Batch 4800, Loss 0.25069165229797363\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 41] HR = 0.6422, NDCG = 0.3715\n",
            "Epoch 42 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 42] Batch 0, Loss 0.2802462577819824\n",
            "[Training Epoch 42] Batch 100, Loss 0.26098379492759705\n",
            "[Training Epoch 42] Batch 200, Loss 0.2685222625732422\n",
            "[Training Epoch 42] Batch 300, Loss 0.26116979122161865\n",
            "[Training Epoch 42] Batch 400, Loss 0.27072811126708984\n",
            "[Training Epoch 42] Batch 500, Loss 0.31899023056030273\n",
            "[Training Epoch 42] Batch 600, Loss 0.2504822611808777\n",
            "[Training Epoch 42] Batch 700, Loss 0.27092647552490234\n",
            "[Training Epoch 42] Batch 800, Loss 0.26648980379104614\n",
            "[Training Epoch 42] Batch 900, Loss 0.2560011148452759\n",
            "[Training Epoch 42] Batch 1000, Loss 0.26950323581695557\n",
            "[Training Epoch 42] Batch 1100, Loss 0.2764171361923218\n",
            "[Training Epoch 42] Batch 1200, Loss 0.2548542618751526\n",
            "[Training Epoch 42] Batch 1300, Loss 0.26474612951278687\n",
            "[Training Epoch 42] Batch 1400, Loss 0.2611452043056488\n",
            "[Training Epoch 42] Batch 1500, Loss 0.2711132764816284\n",
            "[Training Epoch 42] Batch 1600, Loss 0.26440516114234924\n",
            "[Training Epoch 42] Batch 1700, Loss 0.26447173953056335\n",
            "[Training Epoch 42] Batch 1800, Loss 0.27299052476882935\n",
            "[Training Epoch 42] Batch 1900, Loss 0.25692373514175415\n",
            "[Training Epoch 42] Batch 2000, Loss 0.2702024579048157\n",
            "[Training Epoch 42] Batch 2100, Loss 0.2778604030609131\n",
            "[Training Epoch 42] Batch 2200, Loss 0.2601299583911896\n",
            "[Training Epoch 42] Batch 2300, Loss 0.27338454127311707\n",
            "[Training Epoch 42] Batch 2400, Loss 0.28663212060928345\n",
            "[Training Epoch 42] Batch 2500, Loss 0.25754183530807495\n",
            "[Training Epoch 42] Batch 2600, Loss 0.25861483812332153\n",
            "[Training Epoch 42] Batch 2700, Loss 0.2883191406726837\n",
            "[Training Epoch 42] Batch 2800, Loss 0.26919811964035034\n",
            "[Training Epoch 42] Batch 2900, Loss 0.2680787444114685\n",
            "[Training Epoch 42] Batch 3000, Loss 0.27454009652137756\n",
            "[Training Epoch 42] Batch 3100, Loss 0.28690293431282043\n",
            "[Training Epoch 42] Batch 3200, Loss 0.28979170322418213\n",
            "[Training Epoch 42] Batch 3300, Loss 0.2392851710319519\n",
            "[Training Epoch 42] Batch 3400, Loss 0.24777722358703613\n",
            "[Training Epoch 42] Batch 3500, Loss 0.2529600262641907\n",
            "[Training Epoch 42] Batch 3600, Loss 0.25925445556640625\n",
            "[Training Epoch 42] Batch 3700, Loss 0.28381866216659546\n",
            "[Training Epoch 42] Batch 3800, Loss 0.25195807218551636\n",
            "[Training Epoch 42] Batch 3900, Loss 0.2779039442539215\n",
            "[Training Epoch 42] Batch 4000, Loss 0.26676732301712036\n",
            "[Training Epoch 42] Batch 4100, Loss 0.2545279562473297\n",
            "[Training Epoch 42] Batch 4200, Loss 0.24431240558624268\n",
            "[Training Epoch 42] Batch 4300, Loss 0.2480812668800354\n",
            "[Training Epoch 42] Batch 4400, Loss 0.24697056412696838\n",
            "[Training Epoch 42] Batch 4500, Loss 0.22430455684661865\n",
            "[Training Epoch 42] Batch 4600, Loss 0.2569442391395569\n",
            "[Training Epoch 42] Batch 4700, Loss 0.27512744069099426\n",
            "[Training Epoch 42] Batch 4800, Loss 0.24269697070121765\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 42] HR = 0.6404, NDCG = 0.3719\n",
            "Epoch 43 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 43] Batch 0, Loss 0.27829962968826294\n",
            "[Training Epoch 43] Batch 100, Loss 0.27092814445495605\n",
            "[Training Epoch 43] Batch 200, Loss 0.27548032999038696\n",
            "[Training Epoch 43] Batch 300, Loss 0.2647387981414795\n",
            "[Training Epoch 43] Batch 400, Loss 0.24624645709991455\n",
            "[Training Epoch 43] Batch 500, Loss 0.2423575222492218\n",
            "[Training Epoch 43] Batch 600, Loss 0.27723610401153564\n",
            "[Training Epoch 43] Batch 700, Loss 0.28579291701316833\n",
            "[Training Epoch 43] Batch 800, Loss 0.2660592794418335\n",
            "[Training Epoch 43] Batch 900, Loss 0.26686400175094604\n",
            "[Training Epoch 43] Batch 1000, Loss 0.2685016393661499\n",
            "[Training Epoch 43] Batch 1100, Loss 0.2497125267982483\n",
            "[Training Epoch 43] Batch 1200, Loss 0.26192179322242737\n",
            "[Training Epoch 43] Batch 1300, Loss 0.26206934452056885\n",
            "[Training Epoch 43] Batch 1400, Loss 0.26520612835884094\n",
            "[Training Epoch 43] Batch 1500, Loss 0.2411278486251831\n",
            "[Training Epoch 43] Batch 1600, Loss 0.2814328074455261\n",
            "[Training Epoch 43] Batch 1700, Loss 0.27285078167915344\n",
            "[Training Epoch 43] Batch 1800, Loss 0.28191137313842773\n",
            "[Training Epoch 43] Batch 1900, Loss 0.2529607117176056\n",
            "[Training Epoch 43] Batch 2000, Loss 0.2582390308380127\n",
            "[Training Epoch 43] Batch 2100, Loss 0.2710786461830139\n",
            "[Training Epoch 43] Batch 2200, Loss 0.273503839969635\n",
            "[Training Epoch 43] Batch 2300, Loss 0.26208996772766113\n",
            "[Training Epoch 43] Batch 2400, Loss 0.27678704261779785\n",
            "[Training Epoch 43] Batch 2500, Loss 0.25779545307159424\n",
            "[Training Epoch 43] Batch 2600, Loss 0.27439919114112854\n",
            "[Training Epoch 43] Batch 2700, Loss 0.2661571502685547\n",
            "[Training Epoch 43] Batch 2800, Loss 0.30789291858673096\n",
            "[Training Epoch 43] Batch 2900, Loss 0.29313069581985474\n",
            "[Training Epoch 43] Batch 3000, Loss 0.2838105261325836\n",
            "[Training Epoch 43] Batch 3100, Loss 0.2848416268825531\n",
            "[Training Epoch 43] Batch 3200, Loss 0.26372748613357544\n",
            "[Training Epoch 43] Batch 3300, Loss 0.23941001296043396\n",
            "[Training Epoch 43] Batch 3400, Loss 0.287600576877594\n",
            "[Training Epoch 43] Batch 3500, Loss 0.2698078453540802\n",
            "[Training Epoch 43] Batch 3600, Loss 0.2649335265159607\n",
            "[Training Epoch 43] Batch 3700, Loss 0.257811039686203\n",
            "[Training Epoch 43] Batch 3800, Loss 0.2697937488555908\n",
            "[Training Epoch 43] Batch 3900, Loss 0.2588624358177185\n",
            "[Training Epoch 43] Batch 4000, Loss 0.26285648345947266\n",
            "[Training Epoch 43] Batch 4100, Loss 0.2942829132080078\n",
            "[Training Epoch 43] Batch 4200, Loss 0.24058638513088226\n",
            "[Training Epoch 43] Batch 4300, Loss 0.29671627283096313\n",
            "[Training Epoch 43] Batch 4400, Loss 0.28703445196151733\n",
            "[Training Epoch 43] Batch 4500, Loss 0.2862992286682129\n",
            "[Training Epoch 43] Batch 4600, Loss 0.2586732506752014\n",
            "[Training Epoch 43] Batch 4700, Loss 0.29852229356765747\n",
            "[Training Epoch 43] Batch 4800, Loss 0.256303995847702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 43] HR = 0.6396, NDCG = 0.3702\n",
            "Epoch 44 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 44] Batch 0, Loss 0.2941306233406067\n",
            "[Training Epoch 44] Batch 100, Loss 0.2621952295303345\n",
            "[Training Epoch 44] Batch 200, Loss 0.27709800004959106\n",
            "[Training Epoch 44] Batch 300, Loss 0.2582400441169739\n",
            "[Training Epoch 44] Batch 400, Loss 0.29220569133758545\n",
            "[Training Epoch 44] Batch 500, Loss 0.24613162875175476\n",
            "[Training Epoch 44] Batch 600, Loss 0.2842332124710083\n",
            "[Training Epoch 44] Batch 700, Loss 0.24779537320137024\n",
            "[Training Epoch 44] Batch 800, Loss 0.28713446855545044\n",
            "[Training Epoch 44] Batch 900, Loss 0.25951260328292847\n",
            "[Training Epoch 44] Batch 1000, Loss 0.2862352132797241\n",
            "[Training Epoch 44] Batch 1100, Loss 0.27730676531791687\n",
            "[Training Epoch 44] Batch 1200, Loss 0.2775811553001404\n",
            "[Training Epoch 44] Batch 1300, Loss 0.3010077476501465\n",
            "[Training Epoch 44] Batch 1400, Loss 0.278029203414917\n",
            "[Training Epoch 44] Batch 1500, Loss 0.2871708571910858\n",
            "[Training Epoch 44] Batch 1600, Loss 0.2422000765800476\n",
            "[Training Epoch 44] Batch 1700, Loss 0.266175776720047\n",
            "[Training Epoch 44] Batch 1800, Loss 0.24453867971897125\n",
            "[Training Epoch 44] Batch 1900, Loss 0.26476991176605225\n",
            "[Training Epoch 44] Batch 2000, Loss 0.25146380066871643\n",
            "[Training Epoch 44] Batch 2100, Loss 0.24054670333862305\n",
            "[Training Epoch 44] Batch 2200, Loss 0.27570903301239014\n",
            "[Training Epoch 44] Batch 2300, Loss 0.2843242883682251\n",
            "[Training Epoch 44] Batch 2400, Loss 0.2809722423553467\n",
            "[Training Epoch 44] Batch 2500, Loss 0.24164152145385742\n",
            "[Training Epoch 44] Batch 2600, Loss 0.28961077332496643\n",
            "[Training Epoch 44] Batch 2700, Loss 0.28031522035598755\n",
            "[Training Epoch 44] Batch 2800, Loss 0.25390106439590454\n",
            "[Training Epoch 44] Batch 2900, Loss 0.25667330622673035\n",
            "[Training Epoch 44] Batch 3000, Loss 0.2524130344390869\n",
            "[Training Epoch 44] Batch 3100, Loss 0.26707494258880615\n",
            "[Training Epoch 44] Batch 3200, Loss 0.26889246702194214\n",
            "[Training Epoch 44] Batch 3300, Loss 0.2950252294540405\n",
            "[Training Epoch 44] Batch 3400, Loss 0.26247483491897583\n",
            "[Training Epoch 44] Batch 3500, Loss 0.2718849778175354\n",
            "[Training Epoch 44] Batch 3600, Loss 0.285624623298645\n",
            "[Training Epoch 44] Batch 3700, Loss 0.28088265657424927\n",
            "[Training Epoch 44] Batch 3800, Loss 0.23728032410144806\n",
            "[Training Epoch 44] Batch 3900, Loss 0.2628161907196045\n",
            "[Training Epoch 44] Batch 4000, Loss 0.2899653911590576\n",
            "[Training Epoch 44] Batch 4100, Loss 0.27278363704681396\n",
            "[Training Epoch 44] Batch 4200, Loss 0.2730790376663208\n",
            "[Training Epoch 44] Batch 4300, Loss 0.2518311142921448\n",
            "[Training Epoch 44] Batch 4400, Loss 0.2555275559425354\n",
            "[Training Epoch 44] Batch 4500, Loss 0.2620214819908142\n",
            "[Training Epoch 44] Batch 4600, Loss 0.24524249136447906\n",
            "[Training Epoch 44] Batch 4700, Loss 0.24424245953559875\n",
            "[Training Epoch 44] Batch 4800, Loss 0.259970486164093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 44] HR = 0.6386, NDCG = 0.3700\n",
            "Epoch 45 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 45] Batch 0, Loss 0.23716041445732117\n",
            "[Training Epoch 45] Batch 100, Loss 0.27250775694847107\n",
            "[Training Epoch 45] Batch 200, Loss 0.25829699635505676\n",
            "[Training Epoch 45] Batch 300, Loss 0.29660922288894653\n",
            "[Training Epoch 45] Batch 400, Loss 0.25948429107666016\n",
            "[Training Epoch 45] Batch 500, Loss 0.26316583156585693\n",
            "[Training Epoch 45] Batch 600, Loss 0.28921204805374146\n",
            "[Training Epoch 45] Batch 700, Loss 0.248152494430542\n",
            "[Training Epoch 45] Batch 800, Loss 0.25202545523643494\n",
            "[Training Epoch 45] Batch 900, Loss 0.2865276336669922\n",
            "[Training Epoch 45] Batch 1000, Loss 0.25398871302604675\n",
            "[Training Epoch 45] Batch 1100, Loss 0.27681463956832886\n",
            "[Training Epoch 45] Batch 1200, Loss 0.25496429204940796\n",
            "[Training Epoch 45] Batch 1300, Loss 0.22455906867980957\n",
            "[Training Epoch 45] Batch 1400, Loss 0.2542155385017395\n",
            "[Training Epoch 45] Batch 1500, Loss 0.3010815382003784\n",
            "[Training Epoch 45] Batch 1600, Loss 0.2504245638847351\n",
            "[Training Epoch 45] Batch 1700, Loss 0.26722365617752075\n",
            "[Training Epoch 45] Batch 1800, Loss 0.2611352801322937\n",
            "[Training Epoch 45] Batch 1900, Loss 0.271756112575531\n",
            "[Training Epoch 45] Batch 2000, Loss 0.2917845845222473\n",
            "[Training Epoch 45] Batch 2100, Loss 0.27608802914619446\n",
            "[Training Epoch 45] Batch 2200, Loss 0.265927791595459\n",
            "[Training Epoch 45] Batch 2300, Loss 0.24322225153446198\n",
            "[Training Epoch 45] Batch 2400, Loss 0.2724605202674866\n",
            "[Training Epoch 45] Batch 2500, Loss 0.26201361417770386\n",
            "[Training Epoch 45] Batch 2600, Loss 0.26132023334503174\n",
            "[Training Epoch 45] Batch 2700, Loss 0.27517178654670715\n",
            "[Training Epoch 45] Batch 2800, Loss 0.2573307752609253\n",
            "[Training Epoch 45] Batch 2900, Loss 0.2629459500312805\n",
            "[Training Epoch 45] Batch 3000, Loss 0.2801738977432251\n",
            "[Training Epoch 45] Batch 3100, Loss 0.26915693283081055\n",
            "[Training Epoch 45] Batch 3200, Loss 0.31246381998062134\n",
            "[Training Epoch 45] Batch 3300, Loss 0.2654954195022583\n",
            "[Training Epoch 45] Batch 3400, Loss 0.27853214740753174\n",
            "[Training Epoch 45] Batch 3500, Loss 0.2556704878807068\n",
            "[Training Epoch 45] Batch 3600, Loss 0.23405437171459198\n",
            "[Training Epoch 45] Batch 3700, Loss 0.2749088704586029\n",
            "[Training Epoch 45] Batch 3800, Loss 0.23197440803050995\n",
            "[Training Epoch 45] Batch 3900, Loss 0.2681567072868347\n",
            "[Training Epoch 45] Batch 4000, Loss 0.28850916028022766\n",
            "[Training Epoch 45] Batch 4100, Loss 0.2932404577732086\n",
            "[Training Epoch 45] Batch 4200, Loss 0.27230194211006165\n",
            "[Training Epoch 45] Batch 4300, Loss 0.252249538898468\n",
            "[Training Epoch 45] Batch 4400, Loss 0.2639981508255005\n",
            "[Training Epoch 45] Batch 4500, Loss 0.2535581588745117\n",
            "[Training Epoch 45] Batch 4600, Loss 0.2778093218803406\n",
            "[Training Epoch 45] Batch 4700, Loss 0.23555003106594086\n",
            "[Training Epoch 45] Batch 4800, Loss 0.2774457037448883\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 45] HR = 0.6381, NDCG = 0.3693\n",
            "Epoch 46 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 46] Batch 0, Loss 0.2691430449485779\n",
            "[Training Epoch 46] Batch 100, Loss 0.24649250507354736\n",
            "[Training Epoch 46] Batch 200, Loss 0.2732270359992981\n",
            "[Training Epoch 46] Batch 300, Loss 0.2728317379951477\n",
            "[Training Epoch 46] Batch 400, Loss 0.270729124546051\n",
            "[Training Epoch 46] Batch 500, Loss 0.2702145278453827\n",
            "[Training Epoch 46] Batch 600, Loss 0.2787780463695526\n",
            "[Training Epoch 46] Batch 700, Loss 0.23174914717674255\n",
            "[Training Epoch 46] Batch 800, Loss 0.26633328199386597\n",
            "[Training Epoch 46] Batch 900, Loss 0.23811423778533936\n",
            "[Training Epoch 46] Batch 1000, Loss 0.2511705458164215\n",
            "[Training Epoch 46] Batch 1100, Loss 0.24697743356227875\n",
            "[Training Epoch 46] Batch 1200, Loss 0.2993467450141907\n",
            "[Training Epoch 46] Batch 1300, Loss 0.25115519762039185\n",
            "[Training Epoch 46] Batch 1400, Loss 0.2649064064025879\n",
            "[Training Epoch 46] Batch 1500, Loss 0.22293223440647125\n",
            "[Training Epoch 46] Batch 1600, Loss 0.2651599943637848\n",
            "[Training Epoch 46] Batch 1700, Loss 0.27280157804489136\n",
            "[Training Epoch 46] Batch 1800, Loss 0.27861490845680237\n",
            "[Training Epoch 46] Batch 1900, Loss 0.2839266061782837\n",
            "[Training Epoch 46] Batch 2000, Loss 0.25852492451667786\n",
            "[Training Epoch 46] Batch 2100, Loss 0.27255451679229736\n",
            "[Training Epoch 46] Batch 2200, Loss 0.2359108328819275\n",
            "[Training Epoch 46] Batch 2300, Loss 0.26358264684677124\n",
            "[Training Epoch 46] Batch 2400, Loss 0.27358517050743103\n",
            "[Training Epoch 46] Batch 2500, Loss 0.23228923976421356\n",
            "[Training Epoch 46] Batch 2600, Loss 0.26292914152145386\n",
            "[Training Epoch 46] Batch 2700, Loss 0.2928774654865265\n",
            "[Training Epoch 46] Batch 2800, Loss 0.24951905012130737\n",
            "[Training Epoch 46] Batch 2900, Loss 0.26801756024360657\n",
            "[Training Epoch 46] Batch 3000, Loss 0.2523016333580017\n",
            "[Training Epoch 46] Batch 3100, Loss 0.25813814997673035\n",
            "[Training Epoch 46] Batch 3200, Loss 0.2551056444644928\n",
            "[Training Epoch 46] Batch 3300, Loss 0.2519257664680481\n",
            "[Training Epoch 46] Batch 3400, Loss 0.25727567076683044\n",
            "[Training Epoch 46] Batch 3500, Loss 0.2585732638835907\n",
            "[Training Epoch 46] Batch 3600, Loss 0.26230502128601074\n",
            "[Training Epoch 46] Batch 3700, Loss 0.2503725588321686\n",
            "[Training Epoch 46] Batch 3800, Loss 0.27536556124687195\n",
            "[Training Epoch 46] Batch 3900, Loss 0.24315190315246582\n",
            "[Training Epoch 46] Batch 4000, Loss 0.2870798408985138\n",
            "[Training Epoch 46] Batch 4100, Loss 0.25998654961586\n",
            "[Training Epoch 46] Batch 4200, Loss 0.27529120445251465\n",
            "[Training Epoch 46] Batch 4300, Loss 0.2632756531238556\n",
            "[Training Epoch 46] Batch 4400, Loss 0.2744258642196655\n",
            "[Training Epoch 46] Batch 4500, Loss 0.2739269733428955\n",
            "[Training Epoch 46] Batch 4600, Loss 0.2728692293167114\n",
            "[Training Epoch 46] Batch 4700, Loss 0.25210893154144287\n",
            "[Training Epoch 46] Batch 4800, Loss 0.26044243574142456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 46] HR = 0.6382, NDCG = 0.3681\n",
            "Epoch 47 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 47] Batch 0, Loss 0.28267019987106323\n",
            "[Training Epoch 47] Batch 100, Loss 0.2599904537200928\n",
            "[Training Epoch 47] Batch 200, Loss 0.2574797570705414\n",
            "[Training Epoch 47] Batch 300, Loss 0.2812972962856293\n",
            "[Training Epoch 47] Batch 400, Loss 0.28268447518348694\n",
            "[Training Epoch 47] Batch 500, Loss 0.2678411304950714\n",
            "[Training Epoch 47] Batch 600, Loss 0.27913782000541687\n",
            "[Training Epoch 47] Batch 700, Loss 0.2988812327384949\n",
            "[Training Epoch 47] Batch 800, Loss 0.2633439302444458\n",
            "[Training Epoch 47] Batch 900, Loss 0.28075873851776123\n",
            "[Training Epoch 47] Batch 1000, Loss 0.2758728563785553\n",
            "[Training Epoch 47] Batch 1100, Loss 0.2671646773815155\n",
            "[Training Epoch 47] Batch 1200, Loss 0.29993289709091187\n",
            "[Training Epoch 47] Batch 1300, Loss 0.2603870630264282\n",
            "[Training Epoch 47] Batch 1400, Loss 0.26698654890060425\n",
            "[Training Epoch 47] Batch 1500, Loss 0.2778741121292114\n",
            "[Training Epoch 47] Batch 1600, Loss 0.28157374262809753\n",
            "[Training Epoch 47] Batch 1700, Loss 0.24312767386436462\n",
            "[Training Epoch 47] Batch 1800, Loss 0.2741418778896332\n",
            "[Training Epoch 47] Batch 1900, Loss 0.2673291862010956\n",
            "[Training Epoch 47] Batch 2000, Loss 0.2897493243217468\n",
            "[Training Epoch 47] Batch 2100, Loss 0.254546195268631\n",
            "[Training Epoch 47] Batch 2200, Loss 0.24885256588459015\n",
            "[Training Epoch 47] Batch 2300, Loss 0.2556533217430115\n",
            "[Training Epoch 47] Batch 2400, Loss 0.28416067361831665\n",
            "[Training Epoch 47] Batch 2500, Loss 0.2782982289791107\n",
            "[Training Epoch 47] Batch 2600, Loss 0.28503257036209106\n",
            "[Training Epoch 47] Batch 2700, Loss 0.26549631357192993\n",
            "[Training Epoch 47] Batch 2800, Loss 0.27760326862335205\n",
            "[Training Epoch 47] Batch 2900, Loss 0.2820088863372803\n",
            "[Training Epoch 47] Batch 3000, Loss 0.2683645784854889\n",
            "[Training Epoch 47] Batch 3100, Loss 0.2505999803543091\n",
            "[Training Epoch 47] Batch 3200, Loss 0.27564406394958496\n",
            "[Training Epoch 47] Batch 3300, Loss 0.253299742937088\n",
            "[Training Epoch 47] Batch 3400, Loss 0.28608226776123047\n",
            "[Training Epoch 47] Batch 3500, Loss 0.2882115840911865\n",
            "[Training Epoch 47] Batch 3600, Loss 0.25446590781211853\n",
            "[Training Epoch 47] Batch 3700, Loss 0.2698393166065216\n",
            "[Training Epoch 47] Batch 3800, Loss 0.24617363512516022\n",
            "[Training Epoch 47] Batch 3900, Loss 0.23388046026229858\n",
            "[Training Epoch 47] Batch 4000, Loss 0.25599244236946106\n",
            "[Training Epoch 47] Batch 4100, Loss 0.2689640522003174\n",
            "[Training Epoch 47] Batch 4200, Loss 0.28463250398635864\n",
            "[Training Epoch 47] Batch 4300, Loss 0.281721293926239\n",
            "[Training Epoch 47] Batch 4400, Loss 0.26105114817619324\n",
            "[Training Epoch 47] Batch 4500, Loss 0.24098241329193115\n",
            "[Training Epoch 47] Batch 4600, Loss 0.2523471713066101\n",
            "[Training Epoch 47] Batch 4700, Loss 0.2649219334125519\n",
            "[Training Epoch 47] Batch 4800, Loss 0.2396305501461029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 47] HR = 0.6424, NDCG = 0.3707\n",
            "Epoch 48 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 48] Batch 0, Loss 0.28201010823249817\n",
            "[Training Epoch 48] Batch 100, Loss 0.27097180485725403\n",
            "[Training Epoch 48] Batch 200, Loss 0.22351427376270294\n",
            "[Training Epoch 48] Batch 300, Loss 0.27172237634658813\n",
            "[Training Epoch 48] Batch 400, Loss 0.24830973148345947\n",
            "[Training Epoch 48] Batch 500, Loss 0.2548782229423523\n",
            "[Training Epoch 48] Batch 600, Loss 0.2448868751525879\n",
            "[Training Epoch 48] Batch 700, Loss 0.2756783962249756\n",
            "[Training Epoch 48] Batch 800, Loss 0.2612994909286499\n",
            "[Training Epoch 48] Batch 900, Loss 0.29865899682044983\n",
            "[Training Epoch 48] Batch 1000, Loss 0.2631993293762207\n",
            "[Training Epoch 48] Batch 1100, Loss 0.24450741708278656\n",
            "[Training Epoch 48] Batch 1200, Loss 0.2820882797241211\n",
            "[Training Epoch 48] Batch 1300, Loss 0.2865036427974701\n",
            "[Training Epoch 48] Batch 1400, Loss 0.2560880780220032\n",
            "[Training Epoch 48] Batch 1500, Loss 0.2651863396167755\n",
            "[Training Epoch 48] Batch 1600, Loss 0.25539731979370117\n",
            "[Training Epoch 48] Batch 1700, Loss 0.2922452688217163\n",
            "[Training Epoch 48] Batch 1800, Loss 0.2812386155128479\n",
            "[Training Epoch 48] Batch 1900, Loss 0.3062170743942261\n",
            "[Training Epoch 48] Batch 2000, Loss 0.27286824584007263\n",
            "[Training Epoch 48] Batch 2100, Loss 0.2618764340877533\n",
            "[Training Epoch 48] Batch 2200, Loss 0.2813494801521301\n",
            "[Training Epoch 48] Batch 2300, Loss 0.25018876791000366\n",
            "[Training Epoch 48] Batch 2400, Loss 0.25908076763153076\n",
            "[Training Epoch 48] Batch 2500, Loss 0.26476985216140747\n",
            "[Training Epoch 48] Batch 2600, Loss 0.272280216217041\n",
            "[Training Epoch 48] Batch 2700, Loss 0.22350478172302246\n",
            "[Training Epoch 48] Batch 2800, Loss 0.26176726818084717\n",
            "[Training Epoch 48] Batch 2900, Loss 0.29609525203704834\n",
            "[Training Epoch 48] Batch 3000, Loss 0.2515794038772583\n",
            "[Training Epoch 48] Batch 3100, Loss 0.2555277943611145\n",
            "[Training Epoch 48] Batch 3200, Loss 0.2439209669828415\n",
            "[Training Epoch 48] Batch 3300, Loss 0.24848636984825134\n",
            "[Training Epoch 48] Batch 3400, Loss 0.2726741135120392\n",
            "[Training Epoch 48] Batch 3500, Loss 0.26889917254447937\n",
            "[Training Epoch 48] Batch 3600, Loss 0.2983643412590027\n",
            "[Training Epoch 48] Batch 3700, Loss 0.2708379626274109\n",
            "[Training Epoch 48] Batch 3800, Loss 0.27970433235168457\n",
            "[Training Epoch 48] Batch 3900, Loss 0.23101702332496643\n",
            "[Training Epoch 48] Batch 4000, Loss 0.23936545848846436\n",
            "[Training Epoch 48] Batch 4100, Loss 0.2633836269378662\n",
            "[Training Epoch 48] Batch 4200, Loss 0.26201924681663513\n",
            "[Training Epoch 48] Batch 4300, Loss 0.27470308542251587\n",
            "[Training Epoch 48] Batch 4400, Loss 0.2789562940597534\n",
            "[Training Epoch 48] Batch 4500, Loss 0.27426958084106445\n",
            "[Training Epoch 48] Batch 4600, Loss 0.24821214377880096\n",
            "[Training Epoch 48] Batch 4700, Loss 0.25665634870529175\n",
            "[Training Epoch 48] Batch 4800, Loss 0.2697994112968445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 48] HR = 0.6414, NDCG = 0.3703\n",
            "Epoch 49 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 49] Batch 0, Loss 0.2507019639015198\n",
            "[Training Epoch 49] Batch 100, Loss 0.262159526348114\n",
            "[Training Epoch 49] Batch 200, Loss 0.2661300301551819\n",
            "[Training Epoch 49] Batch 300, Loss 0.2664315104484558\n",
            "[Training Epoch 49] Batch 400, Loss 0.2735365927219391\n",
            "[Training Epoch 49] Batch 500, Loss 0.24473875761032104\n",
            "[Training Epoch 49] Batch 600, Loss 0.26049840450286865\n",
            "[Training Epoch 49] Batch 700, Loss 0.27238065004348755\n",
            "[Training Epoch 49] Batch 800, Loss 0.2805250883102417\n",
            "[Training Epoch 49] Batch 900, Loss 0.26832759380340576\n",
            "[Training Epoch 49] Batch 1000, Loss 0.2732204794883728\n",
            "[Training Epoch 49] Batch 1100, Loss 0.24928665161132812\n",
            "[Training Epoch 49] Batch 1200, Loss 0.2434392273426056\n",
            "[Training Epoch 49] Batch 1300, Loss 0.2653866112232208\n",
            "[Training Epoch 49] Batch 1400, Loss 0.26915132999420166\n",
            "[Training Epoch 49] Batch 1500, Loss 0.25356435775756836\n",
            "[Training Epoch 49] Batch 1600, Loss 0.24152112007141113\n",
            "[Training Epoch 49] Batch 1700, Loss 0.25285962224006653\n",
            "[Training Epoch 49] Batch 1800, Loss 0.26562532782554626\n",
            "[Training Epoch 49] Batch 1900, Loss 0.28657546639442444\n",
            "[Training Epoch 49] Batch 2000, Loss 0.2899708151817322\n",
            "[Training Epoch 49] Batch 2100, Loss 0.24764272570610046\n",
            "[Training Epoch 49] Batch 2200, Loss 0.246352419257164\n",
            "[Training Epoch 49] Batch 2300, Loss 0.26086366176605225\n",
            "[Training Epoch 49] Batch 2400, Loss 0.23021578788757324\n",
            "[Training Epoch 49] Batch 2500, Loss 0.24105507135391235\n",
            "[Training Epoch 49] Batch 2600, Loss 0.24066555500030518\n",
            "[Training Epoch 49] Batch 2700, Loss 0.2668057084083557\n",
            "[Training Epoch 49] Batch 2800, Loss 0.2597484588623047\n",
            "[Training Epoch 49] Batch 2900, Loss 0.27487000823020935\n",
            "[Training Epoch 49] Batch 3000, Loss 0.2633954882621765\n",
            "[Training Epoch 49] Batch 3100, Loss 0.26295003294944763\n",
            "[Training Epoch 49] Batch 3200, Loss 0.28423169255256653\n",
            "[Training Epoch 49] Batch 3300, Loss 0.2416919320821762\n",
            "[Training Epoch 49] Batch 3400, Loss 0.2658896744251251\n",
            "[Training Epoch 49] Batch 3500, Loss 0.2607669234275818\n",
            "[Training Epoch 49] Batch 3600, Loss 0.2730923891067505\n",
            "[Training Epoch 49] Batch 3700, Loss 0.25157517194747925\n",
            "[Training Epoch 49] Batch 3800, Loss 0.30095648765563965\n",
            "[Training Epoch 49] Batch 3900, Loss 0.28017204999923706\n",
            "[Training Epoch 49] Batch 4000, Loss 0.26043033599853516\n",
            "[Training Epoch 49] Batch 4100, Loss 0.27622008323669434\n",
            "[Training Epoch 49] Batch 4200, Loss 0.2945178151130676\n",
            "[Training Epoch 49] Batch 4300, Loss 0.2678055465221405\n",
            "[Training Epoch 49] Batch 4400, Loss 0.25121891498565674\n",
            "[Training Epoch 49] Batch 4500, Loss 0.2740105390548706\n",
            "[Training Epoch 49] Batch 4600, Loss 0.28096437454223633\n",
            "[Training Epoch 49] Batch 4700, Loss 0.26545000076293945\n",
            "[Training Epoch 49] Batch 4800, Loss 0.23031556606292725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 49] HR = 0.6409, NDCG = 0.3705\n",
            "Epoch 50 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 50] Batch 0, Loss 0.25068554282188416\n",
            "[Training Epoch 50] Batch 100, Loss 0.2678484320640564\n",
            "[Training Epoch 50] Batch 200, Loss 0.26106885075569153\n",
            "[Training Epoch 50] Batch 300, Loss 0.26173996925354004\n",
            "[Training Epoch 50] Batch 400, Loss 0.2567541003227234\n",
            "[Training Epoch 50] Batch 500, Loss 0.2623896300792694\n",
            "[Training Epoch 50] Batch 600, Loss 0.29790282249450684\n",
            "[Training Epoch 50] Batch 700, Loss 0.2745230793952942\n",
            "[Training Epoch 50] Batch 800, Loss 0.24984332919120789\n",
            "[Training Epoch 50] Batch 900, Loss 0.27362003922462463\n",
            "[Training Epoch 50] Batch 1000, Loss 0.25888848304748535\n",
            "[Training Epoch 50] Batch 1100, Loss 0.2414804995059967\n",
            "[Training Epoch 50] Batch 1200, Loss 0.25897079706192017\n",
            "[Training Epoch 50] Batch 1300, Loss 0.27318087220191956\n",
            "[Training Epoch 50] Batch 1400, Loss 0.2758423089981079\n",
            "[Training Epoch 50] Batch 1500, Loss 0.2604411840438843\n",
            "[Training Epoch 50] Batch 1600, Loss 0.2941824197769165\n",
            "[Training Epoch 50] Batch 1700, Loss 0.28103160858154297\n",
            "[Training Epoch 50] Batch 1800, Loss 0.24812284111976624\n",
            "[Training Epoch 50] Batch 1900, Loss 0.25855815410614014\n",
            "[Training Epoch 50] Batch 2000, Loss 0.27686724066734314\n",
            "[Training Epoch 50] Batch 2100, Loss 0.23986981809139252\n",
            "[Training Epoch 50] Batch 2200, Loss 0.2575909495353699\n",
            "[Training Epoch 50] Batch 2300, Loss 0.28335487842559814\n",
            "[Training Epoch 50] Batch 2400, Loss 0.24567560851573944\n",
            "[Training Epoch 50] Batch 2500, Loss 0.271353542804718\n",
            "[Training Epoch 50] Batch 2600, Loss 0.29573267698287964\n",
            "[Training Epoch 50] Batch 2700, Loss 0.2601226270198822\n",
            "[Training Epoch 50] Batch 2800, Loss 0.23028290271759033\n",
            "[Training Epoch 50] Batch 2900, Loss 0.2576425075531006\n",
            "[Training Epoch 50] Batch 3000, Loss 0.26870447397232056\n",
            "[Training Epoch 50] Batch 3100, Loss 0.25552189350128174\n",
            "[Training Epoch 50] Batch 3200, Loss 0.28411513566970825\n",
            "[Training Epoch 50] Batch 3300, Loss 0.2650667428970337\n",
            "[Training Epoch 50] Batch 3400, Loss 0.27114543318748474\n",
            "[Training Epoch 50] Batch 3500, Loss 0.242936372756958\n",
            "[Training Epoch 50] Batch 3600, Loss 0.29350295662879944\n",
            "[Training Epoch 50] Batch 3700, Loss 0.24793510138988495\n",
            "[Training Epoch 50] Batch 3800, Loss 0.2654755413532257\n",
            "[Training Epoch 50] Batch 3900, Loss 0.2933182716369629\n",
            "[Training Epoch 50] Batch 4000, Loss 0.23679515719413757\n",
            "[Training Epoch 50] Batch 4100, Loss 0.27424147725105286\n",
            "[Training Epoch 50] Batch 4200, Loss 0.26366126537323\n",
            "[Training Epoch 50] Batch 4300, Loss 0.2638503313064575\n",
            "[Training Epoch 50] Batch 4400, Loss 0.2728961706161499\n",
            "[Training Epoch 50] Batch 4500, Loss 0.2843284010887146\n",
            "[Training Epoch 50] Batch 4600, Loss 0.2658831477165222\n",
            "[Training Epoch 50] Batch 4700, Loss 0.28212758898735046\n",
            "[Training Epoch 50] Batch 4800, Loss 0.2412949651479721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 50] HR = 0.6411, NDCG = 0.3719\n",
            "Epoch 51 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 51] Batch 0, Loss 0.2649502754211426\n",
            "[Training Epoch 51] Batch 100, Loss 0.25445419549942017\n",
            "[Training Epoch 51] Batch 200, Loss 0.27588221430778503\n",
            "[Training Epoch 51] Batch 300, Loss 0.28025221824645996\n",
            "[Training Epoch 51] Batch 400, Loss 0.26703882217407227\n",
            "[Training Epoch 51] Batch 500, Loss 0.2733609676361084\n",
            "[Training Epoch 51] Batch 600, Loss 0.25267496705055237\n",
            "[Training Epoch 51] Batch 700, Loss 0.279481440782547\n",
            "[Training Epoch 51] Batch 800, Loss 0.24779629707336426\n",
            "[Training Epoch 51] Batch 900, Loss 0.2746223509311676\n",
            "[Training Epoch 51] Batch 1000, Loss 0.2902645468711853\n",
            "[Training Epoch 51] Batch 1100, Loss 0.2603994607925415\n",
            "[Training Epoch 51] Batch 1200, Loss 0.26160773634910583\n",
            "[Training Epoch 51] Batch 1300, Loss 0.2634779214859009\n",
            "[Training Epoch 51] Batch 1400, Loss 0.23985207080841064\n",
            "[Training Epoch 51] Batch 1500, Loss 0.2588593363761902\n",
            "[Training Epoch 51] Batch 1600, Loss 0.27170079946517944\n",
            "[Training Epoch 51] Batch 1700, Loss 0.247684046626091\n",
            "[Training Epoch 51] Batch 1800, Loss 0.27586835622787476\n",
            "[Training Epoch 51] Batch 1900, Loss 0.26073890924453735\n",
            "[Training Epoch 51] Batch 2000, Loss 0.26357728242874146\n",
            "[Training Epoch 51] Batch 2100, Loss 0.2759981155395508\n",
            "[Training Epoch 51] Batch 2200, Loss 0.25257596373558044\n",
            "[Training Epoch 51] Batch 2300, Loss 0.24941599369049072\n",
            "[Training Epoch 51] Batch 2400, Loss 0.2725059390068054\n",
            "[Training Epoch 51] Batch 2500, Loss 0.26636165380477905\n",
            "[Training Epoch 51] Batch 2600, Loss 0.2619060277938843\n",
            "[Training Epoch 51] Batch 2700, Loss 0.25601309537887573\n",
            "[Training Epoch 51] Batch 2800, Loss 0.2591348886489868\n",
            "[Training Epoch 51] Batch 2900, Loss 0.24417532980442047\n",
            "[Training Epoch 51] Batch 3000, Loss 0.23429912328720093\n",
            "[Training Epoch 51] Batch 3100, Loss 0.24312564730644226\n",
            "[Training Epoch 51] Batch 3200, Loss 0.2732033431529999\n",
            "[Training Epoch 51] Batch 3300, Loss 0.2506088614463806\n",
            "[Training Epoch 51] Batch 3400, Loss 0.252798855304718\n",
            "[Training Epoch 51] Batch 3500, Loss 0.24603405594825745\n",
            "[Training Epoch 51] Batch 3600, Loss 0.2913045585155487\n",
            "[Training Epoch 51] Batch 3700, Loss 0.27267205715179443\n",
            "[Training Epoch 51] Batch 3800, Loss 0.27062976360321045\n",
            "[Training Epoch 51] Batch 3900, Loss 0.2711177468299866\n",
            "[Training Epoch 51] Batch 4000, Loss 0.28021717071533203\n",
            "[Training Epoch 51] Batch 4100, Loss 0.27581244707107544\n",
            "[Training Epoch 51] Batch 4200, Loss 0.28541576862335205\n",
            "[Training Epoch 51] Batch 4300, Loss 0.2626044750213623\n",
            "[Training Epoch 51] Batch 4400, Loss 0.27922654151916504\n",
            "[Training Epoch 51] Batch 4500, Loss 0.26610004901885986\n",
            "[Training Epoch 51] Batch 4600, Loss 0.26736611127853394\n",
            "[Training Epoch 51] Batch 4700, Loss 0.26819998025894165\n",
            "[Training Epoch 51] Batch 4800, Loss 0.2536822557449341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 51] HR = 0.6414, NDCG = 0.3698\n",
            "Epoch 52 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 52] Batch 0, Loss 0.2785322070121765\n",
            "[Training Epoch 52] Batch 100, Loss 0.27469199895858765\n",
            "[Training Epoch 52] Batch 200, Loss 0.28634440898895264\n",
            "[Training Epoch 52] Batch 300, Loss 0.2674960792064667\n",
            "[Training Epoch 52] Batch 400, Loss 0.26175373792648315\n",
            "[Training Epoch 52] Batch 500, Loss 0.259814977645874\n",
            "[Training Epoch 52] Batch 600, Loss 0.25221508741378784\n",
            "[Training Epoch 52] Batch 700, Loss 0.26954185962677\n",
            "[Training Epoch 52] Batch 800, Loss 0.27711230516433716\n",
            "[Training Epoch 52] Batch 900, Loss 0.2480168640613556\n",
            "[Training Epoch 52] Batch 1000, Loss 0.24483513832092285\n",
            "[Training Epoch 52] Batch 1100, Loss 0.2618640661239624\n",
            "[Training Epoch 52] Batch 1200, Loss 0.27602875232696533\n",
            "[Training Epoch 52] Batch 1300, Loss 0.24066504836082458\n",
            "[Training Epoch 52] Batch 1400, Loss 0.24694088101387024\n",
            "[Training Epoch 52] Batch 1500, Loss 0.2632156014442444\n",
            "[Training Epoch 52] Batch 1600, Loss 0.2656988501548767\n",
            "[Training Epoch 52] Batch 1700, Loss 0.24764321744441986\n",
            "[Training Epoch 52] Batch 1800, Loss 0.28480425477027893\n",
            "[Training Epoch 52] Batch 1900, Loss 0.2593228220939636\n",
            "[Training Epoch 52] Batch 2000, Loss 0.27120843529701233\n",
            "[Training Epoch 52] Batch 2100, Loss 0.25170737504959106\n",
            "[Training Epoch 52] Batch 2200, Loss 0.28024372458457947\n",
            "[Training Epoch 52] Batch 2300, Loss 0.2763635814189911\n",
            "[Training Epoch 52] Batch 2400, Loss 0.2750605046749115\n",
            "[Training Epoch 52] Batch 2500, Loss 0.24493290483951569\n",
            "[Training Epoch 52] Batch 2600, Loss 0.24744389951229095\n",
            "[Training Epoch 52] Batch 2700, Loss 0.287271112203598\n",
            "[Training Epoch 52] Batch 2800, Loss 0.27334514260292053\n",
            "[Training Epoch 52] Batch 2900, Loss 0.265428364276886\n",
            "[Training Epoch 52] Batch 3000, Loss 0.26071253418922424\n",
            "[Training Epoch 52] Batch 3100, Loss 0.27793005108833313\n",
            "[Training Epoch 52] Batch 3200, Loss 0.2589905858039856\n",
            "[Training Epoch 52] Batch 3300, Loss 0.26756805181503296\n",
            "[Training Epoch 52] Batch 3400, Loss 0.25447583198547363\n",
            "[Training Epoch 52] Batch 3500, Loss 0.2768896520137787\n",
            "[Training Epoch 52] Batch 3600, Loss 0.26499828696250916\n",
            "[Training Epoch 52] Batch 3700, Loss 0.2632485032081604\n",
            "[Training Epoch 52] Batch 3800, Loss 0.25223782658576965\n",
            "[Training Epoch 52] Batch 3900, Loss 0.26410189270973206\n",
            "[Training Epoch 52] Batch 4000, Loss 0.2637253999710083\n",
            "[Training Epoch 52] Batch 4100, Loss 0.27166977524757385\n",
            "[Training Epoch 52] Batch 4200, Loss 0.2401554137468338\n",
            "[Training Epoch 52] Batch 4300, Loss 0.31213778257369995\n",
            "[Training Epoch 52] Batch 4400, Loss 0.2626950442790985\n",
            "[Training Epoch 52] Batch 4500, Loss 0.2549871802330017\n",
            "[Training Epoch 52] Batch 4600, Loss 0.2410513460636139\n",
            "[Training Epoch 52] Batch 4700, Loss 0.23245860636234283\n",
            "[Training Epoch 52] Batch 4800, Loss 0.26854217052459717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 52] HR = 0.6399, NDCG = 0.3707\n",
            "Epoch 53 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 53] Batch 0, Loss 0.2600781321525574\n",
            "[Training Epoch 53] Batch 100, Loss 0.277998149394989\n",
            "[Training Epoch 53] Batch 200, Loss 0.2764711081981659\n",
            "[Training Epoch 53] Batch 300, Loss 0.23880130052566528\n",
            "[Training Epoch 53] Batch 400, Loss 0.2669835090637207\n",
            "[Training Epoch 53] Batch 500, Loss 0.2944658398628235\n",
            "[Training Epoch 53] Batch 600, Loss 0.29451850056648254\n",
            "[Training Epoch 53] Batch 700, Loss 0.2719886898994446\n",
            "[Training Epoch 53] Batch 800, Loss 0.26794782280921936\n",
            "[Training Epoch 53] Batch 900, Loss 0.26717138290405273\n",
            "[Training Epoch 53] Batch 1000, Loss 0.24558019638061523\n",
            "[Training Epoch 53] Batch 1100, Loss 0.2786375880241394\n",
            "[Training Epoch 53] Batch 1200, Loss 0.27243995666503906\n",
            "[Training Epoch 53] Batch 1300, Loss 0.2856418490409851\n",
            "[Training Epoch 53] Batch 1400, Loss 0.27546119689941406\n",
            "[Training Epoch 53] Batch 1500, Loss 0.2377423346042633\n",
            "[Training Epoch 53] Batch 1600, Loss 0.2532809376716614\n",
            "[Training Epoch 53] Batch 1700, Loss 0.2622990906238556\n",
            "[Training Epoch 53] Batch 1800, Loss 0.24312710762023926\n",
            "[Training Epoch 53] Batch 1900, Loss 0.2849274277687073\n",
            "[Training Epoch 53] Batch 2000, Loss 0.2447439283132553\n",
            "[Training Epoch 53] Batch 2100, Loss 0.24562153220176697\n",
            "[Training Epoch 53] Batch 2200, Loss 0.2763010263442993\n",
            "[Training Epoch 53] Batch 2300, Loss 0.25719499588012695\n",
            "[Training Epoch 53] Batch 2400, Loss 0.2833978533744812\n",
            "[Training Epoch 53] Batch 2500, Loss 0.2631324529647827\n",
            "[Training Epoch 53] Batch 2600, Loss 0.3056018352508545\n",
            "[Training Epoch 53] Batch 2700, Loss 0.27838653326034546\n",
            "[Training Epoch 53] Batch 2800, Loss 0.26562929153442383\n",
            "[Training Epoch 53] Batch 2900, Loss 0.2623384892940521\n",
            "[Training Epoch 53] Batch 3000, Loss 0.2748042941093445\n",
            "[Training Epoch 53] Batch 3100, Loss 0.25939273834228516\n",
            "[Training Epoch 53] Batch 3200, Loss 0.2489403784275055\n",
            "[Training Epoch 53] Batch 3300, Loss 0.2586805820465088\n",
            "[Training Epoch 53] Batch 3400, Loss 0.25229543447494507\n",
            "[Training Epoch 53] Batch 3500, Loss 0.2402278035879135\n",
            "[Training Epoch 53] Batch 3600, Loss 0.26638612151145935\n",
            "[Training Epoch 53] Batch 3700, Loss 0.266696035861969\n",
            "[Training Epoch 53] Batch 3800, Loss 0.2561449408531189\n",
            "[Training Epoch 53] Batch 3900, Loss 0.2330533117055893\n",
            "[Training Epoch 53] Batch 4000, Loss 0.2573927640914917\n",
            "[Training Epoch 53] Batch 4100, Loss 0.28635066747665405\n",
            "[Training Epoch 53] Batch 4200, Loss 0.2775561213493347\n",
            "[Training Epoch 53] Batch 4300, Loss 0.2561763525009155\n",
            "[Training Epoch 53] Batch 4400, Loss 0.25048476457595825\n",
            "[Training Epoch 53] Batch 4500, Loss 0.26223936676979065\n",
            "[Training Epoch 53] Batch 4600, Loss 0.2576081156730652\n",
            "[Training Epoch 53] Batch 4700, Loss 0.262931227684021\n",
            "[Training Epoch 53] Batch 4800, Loss 0.29728907346725464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 53] HR = 0.6404, NDCG = 0.3695\n",
            "Epoch 54 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 54] Batch 0, Loss 0.25174081325531006\n",
            "[Training Epoch 54] Batch 100, Loss 0.27908584475517273\n",
            "[Training Epoch 54] Batch 200, Loss 0.27562659978866577\n",
            "[Training Epoch 54] Batch 300, Loss 0.28821438550949097\n",
            "[Training Epoch 54] Batch 400, Loss 0.2730117738246918\n",
            "[Training Epoch 54] Batch 500, Loss 0.2726614475250244\n",
            "[Training Epoch 54] Batch 600, Loss 0.27139753103256226\n",
            "[Training Epoch 54] Batch 700, Loss 0.24497327208518982\n",
            "[Training Epoch 54] Batch 800, Loss 0.2688365578651428\n",
            "[Training Epoch 54] Batch 900, Loss 0.2509011924266815\n",
            "[Training Epoch 54] Batch 1000, Loss 0.27848348021507263\n",
            "[Training Epoch 54] Batch 1100, Loss 0.25778424739837646\n",
            "[Training Epoch 54] Batch 1200, Loss 0.2538641691207886\n",
            "[Training Epoch 54] Batch 1300, Loss 0.27038195729255676\n",
            "[Training Epoch 54] Batch 1400, Loss 0.25483065843582153\n",
            "[Training Epoch 54] Batch 1500, Loss 0.2508213520050049\n",
            "[Training Epoch 54] Batch 1600, Loss 0.23547068238258362\n",
            "[Training Epoch 54] Batch 1700, Loss 0.26165270805358887\n",
            "[Training Epoch 54] Batch 1800, Loss 0.2763861119747162\n",
            "[Training Epoch 54] Batch 1900, Loss 0.2649669647216797\n",
            "[Training Epoch 54] Batch 2000, Loss 0.2696552276611328\n",
            "[Training Epoch 54] Batch 2100, Loss 0.27107489109039307\n",
            "[Training Epoch 54] Batch 2200, Loss 0.26251840591430664\n",
            "[Training Epoch 54] Batch 2300, Loss 0.2596195340156555\n",
            "[Training Epoch 54] Batch 2400, Loss 0.2810382544994354\n",
            "[Training Epoch 54] Batch 2500, Loss 0.2563217878341675\n",
            "[Training Epoch 54] Batch 2600, Loss 0.2470068335533142\n",
            "[Training Epoch 54] Batch 2700, Loss 0.2528288960456848\n",
            "[Training Epoch 54] Batch 2800, Loss 0.2833724617958069\n",
            "[Training Epoch 54] Batch 2900, Loss 0.2834806442260742\n",
            "[Training Epoch 54] Batch 3000, Loss 0.2686852812767029\n",
            "[Training Epoch 54] Batch 3100, Loss 0.24514320492744446\n",
            "[Training Epoch 54] Batch 3200, Loss 0.24058358371257782\n",
            "[Training Epoch 54] Batch 3300, Loss 0.24125441908836365\n",
            "[Training Epoch 54] Batch 3400, Loss 0.2770404815673828\n",
            "[Training Epoch 54] Batch 3500, Loss 0.2504597008228302\n",
            "[Training Epoch 54] Batch 3600, Loss 0.26036548614501953\n",
            "[Training Epoch 54] Batch 3700, Loss 0.28381165862083435\n",
            "[Training Epoch 54] Batch 3800, Loss 0.2787715792655945\n",
            "[Training Epoch 54] Batch 3900, Loss 0.2720811069011688\n",
            "[Training Epoch 54] Batch 4000, Loss 0.28010380268096924\n",
            "[Training Epoch 54] Batch 4100, Loss 0.2633683979511261\n",
            "[Training Epoch 54] Batch 4200, Loss 0.25274303555488586\n",
            "[Training Epoch 54] Batch 4300, Loss 0.2753077447414398\n",
            "[Training Epoch 54] Batch 4400, Loss 0.27409183979034424\n",
            "[Training Epoch 54] Batch 4500, Loss 0.2581970691680908\n",
            "[Training Epoch 54] Batch 4600, Loss 0.2511000633239746\n",
            "[Training Epoch 54] Batch 4700, Loss 0.23541224002838135\n",
            "[Training Epoch 54] Batch 4800, Loss 0.2801264822483063\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 54] HR = 0.6421, NDCG = 0.3697\n",
            "Epoch 55 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 55] Batch 0, Loss 0.24641798436641693\n",
            "[Training Epoch 55] Batch 100, Loss 0.2662658095359802\n",
            "[Training Epoch 55] Batch 200, Loss 0.23930683732032776\n",
            "[Training Epoch 55] Batch 300, Loss 0.24706733226776123\n",
            "[Training Epoch 55] Batch 400, Loss 0.25972092151641846\n",
            "[Training Epoch 55] Batch 500, Loss 0.26956498622894287\n",
            "[Training Epoch 55] Batch 600, Loss 0.27472978830337524\n",
            "[Training Epoch 55] Batch 700, Loss 0.2784458100795746\n",
            "[Training Epoch 55] Batch 800, Loss 0.234932541847229\n",
            "[Training Epoch 55] Batch 900, Loss 0.2774369716644287\n",
            "[Training Epoch 55] Batch 1000, Loss 0.28914445638656616\n",
            "[Training Epoch 55] Batch 1100, Loss 0.2607579827308655\n",
            "[Training Epoch 55] Batch 1200, Loss 0.26825469732284546\n",
            "[Training Epoch 55] Batch 1300, Loss 0.26401787996292114\n",
            "[Training Epoch 55] Batch 1400, Loss 0.2785101532936096\n",
            "[Training Epoch 55] Batch 1500, Loss 0.2671869993209839\n",
            "[Training Epoch 55] Batch 1600, Loss 0.2690201997756958\n",
            "[Training Epoch 55] Batch 1700, Loss 0.29840201139450073\n",
            "[Training Epoch 55] Batch 1800, Loss 0.2746375799179077\n",
            "[Training Epoch 55] Batch 1900, Loss 0.25963205099105835\n",
            "[Training Epoch 55] Batch 2000, Loss 0.2737703323364258\n",
            "[Training Epoch 55] Batch 2100, Loss 0.2665429711341858\n",
            "[Training Epoch 55] Batch 2200, Loss 0.2533314526081085\n",
            "[Training Epoch 55] Batch 2300, Loss 0.27558842301368713\n",
            "[Training Epoch 55] Batch 2400, Loss 0.26591262221336365\n",
            "[Training Epoch 55] Batch 2500, Loss 0.2536075711250305\n",
            "[Training Epoch 55] Batch 2600, Loss 0.2593052089214325\n",
            "[Training Epoch 55] Batch 2700, Loss 0.2697979211807251\n",
            "[Training Epoch 55] Batch 2800, Loss 0.2550412118434906\n",
            "[Training Epoch 55] Batch 2900, Loss 0.2742266058921814\n",
            "[Training Epoch 55] Batch 3000, Loss 0.24532920122146606\n",
            "[Training Epoch 55] Batch 3100, Loss 0.24936480820178986\n",
            "[Training Epoch 55] Batch 3200, Loss 0.2812801003456116\n",
            "[Training Epoch 55] Batch 3300, Loss 0.259219765663147\n",
            "[Training Epoch 55] Batch 3400, Loss 0.22695958614349365\n",
            "[Training Epoch 55] Batch 3500, Loss 0.2700265347957611\n",
            "[Training Epoch 55] Batch 3600, Loss 0.2785249352455139\n",
            "[Training Epoch 55] Batch 3700, Loss 0.2726338505744934\n",
            "[Training Epoch 55] Batch 3800, Loss 0.27423781156539917\n",
            "[Training Epoch 55] Batch 3900, Loss 0.2519909739494324\n",
            "[Training Epoch 55] Batch 4000, Loss 0.2590281367301941\n",
            "[Training Epoch 55] Batch 4100, Loss 0.27943915128707886\n",
            "[Training Epoch 55] Batch 4200, Loss 0.27051791548728943\n",
            "[Training Epoch 55] Batch 4300, Loss 0.25956207513809204\n",
            "[Training Epoch 55] Batch 4400, Loss 0.2678988575935364\n",
            "[Training Epoch 55] Batch 4500, Loss 0.24176760017871857\n",
            "[Training Epoch 55] Batch 4600, Loss 0.23019111156463623\n",
            "[Training Epoch 55] Batch 4700, Loss 0.2628997564315796\n",
            "[Training Epoch 55] Batch 4800, Loss 0.24736560881137848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 55] HR = 0.6406, NDCG = 0.3694\n",
            "Epoch 56 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 56] Batch 0, Loss 0.24635565280914307\n",
            "[Training Epoch 56] Batch 100, Loss 0.2844497859477997\n",
            "[Training Epoch 56] Batch 200, Loss 0.24482882022857666\n",
            "[Training Epoch 56] Batch 300, Loss 0.25032174587249756\n",
            "[Training Epoch 56] Batch 400, Loss 0.2863312363624573\n",
            "[Training Epoch 56] Batch 500, Loss 0.2865820527076721\n",
            "[Training Epoch 56] Batch 600, Loss 0.24096906185150146\n",
            "[Training Epoch 56] Batch 700, Loss 0.26703962683677673\n",
            "[Training Epoch 56] Batch 800, Loss 0.2663416862487793\n",
            "[Training Epoch 56] Batch 900, Loss 0.2611863315105438\n",
            "[Training Epoch 56] Batch 1000, Loss 0.27527379989624023\n",
            "[Training Epoch 56] Batch 1100, Loss 0.24194341897964478\n",
            "[Training Epoch 56] Batch 1200, Loss 0.26079338788986206\n",
            "[Training Epoch 56] Batch 1300, Loss 0.26179784536361694\n",
            "[Training Epoch 56] Batch 1400, Loss 0.2703022360801697\n",
            "[Training Epoch 56] Batch 1500, Loss 0.2760758697986603\n",
            "[Training Epoch 56] Batch 1600, Loss 0.2807433307170868\n",
            "[Training Epoch 56] Batch 1700, Loss 0.26690131425857544\n",
            "[Training Epoch 56] Batch 1800, Loss 0.24956637620925903\n",
            "[Training Epoch 56] Batch 1900, Loss 0.2605324387550354\n",
            "[Training Epoch 56] Batch 2000, Loss 0.2760869264602661\n",
            "[Training Epoch 56] Batch 2100, Loss 0.27890264987945557\n",
            "[Training Epoch 56] Batch 2200, Loss 0.24883046746253967\n",
            "[Training Epoch 56] Batch 2300, Loss 0.23187898099422455\n",
            "[Training Epoch 56] Batch 2400, Loss 0.2829475700855255\n",
            "[Training Epoch 56] Batch 2500, Loss 0.28057461977005005\n",
            "[Training Epoch 56] Batch 2600, Loss 0.241866797208786\n",
            "[Training Epoch 56] Batch 2700, Loss 0.266480952501297\n",
            "[Training Epoch 56] Batch 2800, Loss 0.2424742877483368\n",
            "[Training Epoch 56] Batch 2900, Loss 0.2654648423194885\n",
            "[Training Epoch 56] Batch 3000, Loss 0.23732276260852814\n",
            "[Training Epoch 56] Batch 3100, Loss 0.24393080174922943\n",
            "[Training Epoch 56] Batch 3200, Loss 0.28512024879455566\n",
            "[Training Epoch 56] Batch 3300, Loss 0.26140081882476807\n",
            "[Training Epoch 56] Batch 3400, Loss 0.2634996771812439\n",
            "[Training Epoch 56] Batch 3500, Loss 0.2641979455947876\n",
            "[Training Epoch 56] Batch 3600, Loss 0.27580752968788147\n",
            "[Training Epoch 56] Batch 3700, Loss 0.2596496045589447\n",
            "[Training Epoch 56] Batch 3800, Loss 0.2584691345691681\n",
            "[Training Epoch 56] Batch 3900, Loss 0.2348603904247284\n",
            "[Training Epoch 56] Batch 4000, Loss 0.24501405656337738\n",
            "[Training Epoch 56] Batch 4100, Loss 0.2716127038002014\n",
            "[Training Epoch 56] Batch 4200, Loss 0.27222394943237305\n",
            "[Training Epoch 56] Batch 4300, Loss 0.28371763229370117\n",
            "[Training Epoch 56] Batch 4400, Loss 0.2384319305419922\n",
            "[Training Epoch 56] Batch 4500, Loss 0.2680473327636719\n",
            "[Training Epoch 56] Batch 4600, Loss 0.262249231338501\n",
            "[Training Epoch 56] Batch 4700, Loss 0.2575986981391907\n",
            "[Training Epoch 56] Batch 4800, Loss 0.22167527675628662\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 56] HR = 0.6377, NDCG = 0.3694\n",
            "Epoch 57 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 57] Batch 0, Loss 0.25985264778137207\n",
            "[Training Epoch 57] Batch 100, Loss 0.27331143617630005\n",
            "[Training Epoch 57] Batch 200, Loss 0.27632635831832886\n",
            "[Training Epoch 57] Batch 300, Loss 0.29703474044799805\n",
            "[Training Epoch 57] Batch 400, Loss 0.25215286016464233\n",
            "[Training Epoch 57] Batch 500, Loss 0.2640390396118164\n",
            "[Training Epoch 57] Batch 600, Loss 0.26209691166877747\n",
            "[Training Epoch 57] Batch 700, Loss 0.2594529986381531\n",
            "[Training Epoch 57] Batch 800, Loss 0.2513912618160248\n",
            "[Training Epoch 57] Batch 900, Loss 0.2621527910232544\n",
            "[Training Epoch 57] Batch 1000, Loss 0.2530257999897003\n",
            "[Training Epoch 57] Batch 1100, Loss 0.2393730878829956\n",
            "[Training Epoch 57] Batch 1200, Loss 0.25987687706947327\n",
            "[Training Epoch 57] Batch 1300, Loss 0.27230149507522583\n",
            "[Training Epoch 57] Batch 1400, Loss 0.25034719705581665\n",
            "[Training Epoch 57] Batch 1500, Loss 0.26215410232543945\n",
            "[Training Epoch 57] Batch 1600, Loss 0.27415525913238525\n",
            "[Training Epoch 57] Batch 1700, Loss 0.2771216630935669\n",
            "[Training Epoch 57] Batch 1800, Loss 0.27596601843833923\n",
            "[Training Epoch 57] Batch 1900, Loss 0.264890193939209\n",
            "[Training Epoch 57] Batch 2000, Loss 0.25392478704452515\n",
            "[Training Epoch 57] Batch 2100, Loss 0.2533206343650818\n",
            "[Training Epoch 57] Batch 2200, Loss 0.274333119392395\n",
            "[Training Epoch 57] Batch 2300, Loss 0.25968360900878906\n",
            "[Training Epoch 57] Batch 2400, Loss 0.27654409408569336\n",
            "[Training Epoch 57] Batch 2500, Loss 0.26419878005981445\n",
            "[Training Epoch 57] Batch 2600, Loss 0.2606837749481201\n",
            "[Training Epoch 57] Batch 2700, Loss 0.3042103350162506\n",
            "[Training Epoch 57] Batch 2800, Loss 0.2565532624721527\n",
            "[Training Epoch 57] Batch 2900, Loss 0.25544503331184387\n",
            "[Training Epoch 57] Batch 3000, Loss 0.2696428894996643\n",
            "[Training Epoch 57] Batch 3100, Loss 0.2756481170654297\n",
            "[Training Epoch 57] Batch 3200, Loss 0.256225049495697\n",
            "[Training Epoch 57] Batch 3300, Loss 0.2549329400062561\n",
            "[Training Epoch 57] Batch 3400, Loss 0.2761628329753876\n",
            "[Training Epoch 57] Batch 3500, Loss 0.2541095018386841\n",
            "[Training Epoch 57] Batch 3600, Loss 0.264299213886261\n",
            "[Training Epoch 57] Batch 3700, Loss 0.2586975395679474\n",
            "[Training Epoch 57] Batch 3800, Loss 0.25220417976379395\n",
            "[Training Epoch 57] Batch 3900, Loss 0.2874601483345032\n",
            "[Training Epoch 57] Batch 4000, Loss 0.27520349621772766\n",
            "[Training Epoch 57] Batch 4100, Loss 0.28482580184936523\n",
            "[Training Epoch 57] Batch 4200, Loss 0.23958396911621094\n",
            "[Training Epoch 57] Batch 4300, Loss 0.2408679872751236\n",
            "[Training Epoch 57] Batch 4400, Loss 0.2878507077693939\n",
            "[Training Epoch 57] Batch 4500, Loss 0.2739205062389374\n",
            "[Training Epoch 57] Batch 4600, Loss 0.27590739727020264\n",
            "[Training Epoch 57] Batch 4700, Loss 0.26500746607780457\n",
            "[Training Epoch 57] Batch 4800, Loss 0.2630050480365753\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 57] HR = 0.6399, NDCG = 0.3701\n",
            "Epoch 58 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 58] Batch 0, Loss 0.2486899197101593\n",
            "[Training Epoch 58] Batch 100, Loss 0.25908970832824707\n",
            "[Training Epoch 58] Batch 200, Loss 0.24344077706336975\n",
            "[Training Epoch 58] Batch 300, Loss 0.2295273244380951\n",
            "[Training Epoch 58] Batch 400, Loss 0.27630406618118286\n",
            "[Training Epoch 58] Batch 500, Loss 0.28445491194725037\n",
            "[Training Epoch 58] Batch 600, Loss 0.25298988819122314\n",
            "[Training Epoch 58] Batch 700, Loss 0.2640427350997925\n",
            "[Training Epoch 58] Batch 800, Loss 0.27053600549697876\n",
            "[Training Epoch 58] Batch 900, Loss 0.24563971161842346\n",
            "[Training Epoch 58] Batch 1000, Loss 0.2520745098590851\n",
            "[Training Epoch 58] Batch 1100, Loss 0.23026889562606812\n",
            "[Training Epoch 58] Batch 1200, Loss 0.26122403144836426\n",
            "[Training Epoch 58] Batch 1300, Loss 0.2428404539823532\n",
            "[Training Epoch 58] Batch 1400, Loss 0.2643376588821411\n",
            "[Training Epoch 58] Batch 1500, Loss 0.24667395651340485\n",
            "[Training Epoch 58] Batch 1600, Loss 0.2702949047088623\n",
            "[Training Epoch 58] Batch 1700, Loss 0.276488721370697\n",
            "[Training Epoch 58] Batch 1800, Loss 0.2628781199455261\n",
            "[Training Epoch 58] Batch 1900, Loss 0.2684633135795593\n",
            "[Training Epoch 58] Batch 2000, Loss 0.2561999559402466\n",
            "[Training Epoch 58] Batch 2100, Loss 0.25897812843322754\n",
            "[Training Epoch 58] Batch 2200, Loss 0.2648341655731201\n",
            "[Training Epoch 58] Batch 2300, Loss 0.26291340589523315\n",
            "[Training Epoch 58] Batch 2400, Loss 0.2753913402557373\n",
            "[Training Epoch 58] Batch 2500, Loss 0.2513912320137024\n",
            "[Training Epoch 58] Batch 2600, Loss 0.26996880769729614\n",
            "[Training Epoch 58] Batch 2700, Loss 0.24593374133110046\n",
            "[Training Epoch 58] Batch 2800, Loss 0.24813243746757507\n",
            "[Training Epoch 58] Batch 2900, Loss 0.24564272165298462\n",
            "[Training Epoch 58] Batch 3000, Loss 0.2784965932369232\n",
            "[Training Epoch 58] Batch 3100, Loss 0.27160435914993286\n",
            "[Training Epoch 58] Batch 3200, Loss 0.2741272449493408\n",
            "[Training Epoch 58] Batch 3300, Loss 0.2796408236026764\n",
            "[Training Epoch 58] Batch 3400, Loss 0.25270164012908936\n",
            "[Training Epoch 58] Batch 3500, Loss 0.27275341749191284\n",
            "[Training Epoch 58] Batch 3600, Loss 0.2799556851387024\n",
            "[Training Epoch 58] Batch 3700, Loss 0.2615266740322113\n",
            "[Training Epoch 58] Batch 3800, Loss 0.2600887417793274\n",
            "[Training Epoch 58] Batch 3900, Loss 0.23361065983772278\n",
            "[Training Epoch 58] Batch 4000, Loss 0.24552136659622192\n",
            "[Training Epoch 58] Batch 4100, Loss 0.24388009309768677\n",
            "[Training Epoch 58] Batch 4200, Loss 0.24337996542453766\n",
            "[Training Epoch 58] Batch 4300, Loss 0.27109456062316895\n",
            "[Training Epoch 58] Batch 4400, Loss 0.2708425223827362\n",
            "[Training Epoch 58] Batch 4500, Loss 0.25468748807907104\n",
            "[Training Epoch 58] Batch 4600, Loss 0.2509037256240845\n",
            "[Training Epoch 58] Batch 4700, Loss 0.2776331305503845\n",
            "[Training Epoch 58] Batch 4800, Loss 0.26908326148986816\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 58] HR = 0.6399, NDCG = 0.3687\n",
            "Epoch 59 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 59] Batch 0, Loss 0.24588674306869507\n",
            "[Training Epoch 59] Batch 100, Loss 0.2944100499153137\n",
            "[Training Epoch 59] Batch 200, Loss 0.2692185044288635\n",
            "[Training Epoch 59] Batch 300, Loss 0.2773808240890503\n",
            "[Training Epoch 59] Batch 400, Loss 0.25828319787979126\n",
            "[Training Epoch 59] Batch 500, Loss 0.252871572971344\n",
            "[Training Epoch 59] Batch 600, Loss 0.3107242286205292\n",
            "[Training Epoch 59] Batch 700, Loss 0.2577288746833801\n",
            "[Training Epoch 59] Batch 800, Loss 0.2810024619102478\n",
            "[Training Epoch 59] Batch 900, Loss 0.267892062664032\n",
            "[Training Epoch 59] Batch 1000, Loss 0.26482754945755005\n",
            "[Training Epoch 59] Batch 1100, Loss 0.2518535852432251\n",
            "[Training Epoch 59] Batch 1200, Loss 0.27926403284072876\n",
            "[Training Epoch 59] Batch 1300, Loss 0.2551702558994293\n",
            "[Training Epoch 59] Batch 1400, Loss 0.27445244789123535\n",
            "[Training Epoch 59] Batch 1500, Loss 0.27753710746765137\n",
            "[Training Epoch 59] Batch 1600, Loss 0.2738674283027649\n",
            "[Training Epoch 59] Batch 1700, Loss 0.26316940784454346\n",
            "[Training Epoch 59] Batch 1800, Loss 0.26129984855651855\n",
            "[Training Epoch 59] Batch 1900, Loss 0.27826908230781555\n",
            "[Training Epoch 59] Batch 2000, Loss 0.26133087277412415\n",
            "[Training Epoch 59] Batch 2100, Loss 0.26360172033309937\n",
            "[Training Epoch 59] Batch 2200, Loss 0.2709193229675293\n",
            "[Training Epoch 59] Batch 2300, Loss 0.25697237253189087\n",
            "[Training Epoch 59] Batch 2400, Loss 0.25343698263168335\n",
            "[Training Epoch 59] Batch 2500, Loss 0.2617807388305664\n",
            "[Training Epoch 59] Batch 2600, Loss 0.24109159409999847\n",
            "[Training Epoch 59] Batch 2700, Loss 0.23119261860847473\n",
            "[Training Epoch 59] Batch 2800, Loss 0.2588956952095032\n",
            "[Training Epoch 59] Batch 2900, Loss 0.278739869594574\n",
            "[Training Epoch 59] Batch 3000, Loss 0.2581430673599243\n",
            "[Training Epoch 59] Batch 3100, Loss 0.2605571150779724\n",
            "[Training Epoch 59] Batch 3200, Loss 0.23478057980537415\n",
            "[Training Epoch 59] Batch 3300, Loss 0.2842266857624054\n",
            "[Training Epoch 59] Batch 3400, Loss 0.26780885457992554\n",
            "[Training Epoch 59] Batch 3500, Loss 0.2523973882198334\n",
            "[Training Epoch 59] Batch 3600, Loss 0.26729580760002136\n",
            "[Training Epoch 59] Batch 3700, Loss 0.2627594769001007\n",
            "[Training Epoch 59] Batch 3800, Loss 0.24445761740207672\n",
            "[Training Epoch 59] Batch 3900, Loss 0.23349922895431519\n",
            "[Training Epoch 59] Batch 4000, Loss 0.24924784898757935\n",
            "[Training Epoch 59] Batch 4100, Loss 0.30141136050224304\n",
            "[Training Epoch 59] Batch 4200, Loss 0.2724919319152832\n",
            "[Training Epoch 59] Batch 4300, Loss 0.29795175790786743\n",
            "[Training Epoch 59] Batch 4400, Loss 0.25310301780700684\n",
            "[Training Epoch 59] Batch 4500, Loss 0.28978466987609863\n",
            "[Training Epoch 59] Batch 4600, Loss 0.2662737965583801\n",
            "[Training Epoch 59] Batch 4700, Loss 0.2359907478094101\n",
            "[Training Epoch 59] Batch 4800, Loss 0.26829272508621216\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 59] HR = 0.6402, NDCG = 0.3687\n",
            "Epoch 60 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 60] Batch 0, Loss 0.2588968575000763\n",
            "[Training Epoch 60] Batch 100, Loss 0.23960354924201965\n",
            "[Training Epoch 60] Batch 200, Loss 0.24083420634269714\n",
            "[Training Epoch 60] Batch 300, Loss 0.2672143578529358\n",
            "[Training Epoch 60] Batch 400, Loss 0.2698432505130768\n",
            "[Training Epoch 60] Batch 500, Loss 0.25309911370277405\n",
            "[Training Epoch 60] Batch 600, Loss 0.22979608178138733\n",
            "[Training Epoch 60] Batch 700, Loss 0.2762446999549866\n",
            "[Training Epoch 60] Batch 800, Loss 0.26763755083084106\n",
            "[Training Epoch 60] Batch 900, Loss 0.26638519763946533\n",
            "[Training Epoch 60] Batch 1000, Loss 0.2528785169124603\n",
            "[Training Epoch 60] Batch 1100, Loss 0.25559133291244507\n",
            "[Training Epoch 60] Batch 1200, Loss 0.31846481561660767\n",
            "[Training Epoch 60] Batch 1300, Loss 0.26877328753471375\n",
            "[Training Epoch 60] Batch 1400, Loss 0.25730881094932556\n",
            "[Training Epoch 60] Batch 1500, Loss 0.260485976934433\n",
            "[Training Epoch 60] Batch 1600, Loss 0.2660257816314697\n",
            "[Training Epoch 60] Batch 1700, Loss 0.24022385478019714\n",
            "[Training Epoch 60] Batch 1800, Loss 0.261648952960968\n",
            "[Training Epoch 60] Batch 1900, Loss 0.2567848861217499\n",
            "[Training Epoch 60] Batch 2000, Loss 0.26237502694129944\n",
            "[Training Epoch 60] Batch 2100, Loss 0.2707752585411072\n",
            "[Training Epoch 60] Batch 2200, Loss 0.2843160629272461\n",
            "[Training Epoch 60] Batch 2300, Loss 0.27837544679641724\n",
            "[Training Epoch 60] Batch 2400, Loss 0.25785961747169495\n",
            "[Training Epoch 60] Batch 2500, Loss 0.2466765195131302\n",
            "[Training Epoch 60] Batch 2600, Loss 0.2670048177242279\n",
            "[Training Epoch 60] Batch 2700, Loss 0.24562452733516693\n",
            "[Training Epoch 60] Batch 2800, Loss 0.27431637048721313\n",
            "[Training Epoch 60] Batch 2900, Loss 0.2898065745830536\n",
            "[Training Epoch 60] Batch 3000, Loss 0.26549020409584045\n",
            "[Training Epoch 60] Batch 3100, Loss 0.24532350897789001\n",
            "[Training Epoch 60] Batch 3200, Loss 0.26179614663124084\n",
            "[Training Epoch 60] Batch 3300, Loss 0.2681931257247925\n",
            "[Training Epoch 60] Batch 3400, Loss 0.2676655352115631\n",
            "[Training Epoch 60] Batch 3500, Loss 0.2611352205276489\n",
            "[Training Epoch 60] Batch 3600, Loss 0.27592724561691284\n",
            "[Training Epoch 60] Batch 3700, Loss 0.2722415626049042\n",
            "[Training Epoch 60] Batch 3800, Loss 0.23317351937294006\n",
            "[Training Epoch 60] Batch 3900, Loss 0.284302294254303\n",
            "[Training Epoch 60] Batch 4000, Loss 0.2897748351097107\n",
            "[Training Epoch 60] Batch 4100, Loss 0.2575518488883972\n",
            "[Training Epoch 60] Batch 4200, Loss 0.2554014027118683\n",
            "[Training Epoch 60] Batch 4300, Loss 0.23949074745178223\n",
            "[Training Epoch 60] Batch 4400, Loss 0.27257680892944336\n",
            "[Training Epoch 60] Batch 4500, Loss 0.25164133310317993\n",
            "[Training Epoch 60] Batch 4600, Loss 0.2793724238872528\n",
            "[Training Epoch 60] Batch 4700, Loss 0.24101445078849792\n",
            "[Training Epoch 60] Batch 4800, Loss 0.28502702713012695\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 60] HR = 0.6396, NDCG = 0.3705\n",
            "Epoch 61 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 61] Batch 0, Loss 0.23284558951854706\n",
            "[Training Epoch 61] Batch 100, Loss 0.2755756080150604\n",
            "[Training Epoch 61] Batch 200, Loss 0.282045841217041\n",
            "[Training Epoch 61] Batch 300, Loss 0.2482071816921234\n",
            "[Training Epoch 61] Batch 400, Loss 0.2696189880371094\n",
            "[Training Epoch 61] Batch 500, Loss 0.2610533833503723\n",
            "[Training Epoch 61] Batch 600, Loss 0.23446854948997498\n",
            "[Training Epoch 61] Batch 700, Loss 0.2609837055206299\n",
            "[Training Epoch 61] Batch 800, Loss 0.276384562253952\n",
            "[Training Epoch 61] Batch 900, Loss 0.25121501088142395\n",
            "[Training Epoch 61] Batch 1000, Loss 0.25303950905799866\n",
            "[Training Epoch 61] Batch 1100, Loss 0.24391743540763855\n",
            "[Training Epoch 61] Batch 1200, Loss 0.24262621998786926\n",
            "[Training Epoch 61] Batch 1300, Loss 0.2528553009033203\n",
            "[Training Epoch 61] Batch 1400, Loss 0.2885552644729614\n",
            "[Training Epoch 61] Batch 1500, Loss 0.2280586212873459\n",
            "[Training Epoch 61] Batch 1600, Loss 0.26528018712997437\n",
            "[Training Epoch 61] Batch 1700, Loss 0.2878004014492035\n",
            "[Training Epoch 61] Batch 1800, Loss 0.2872205674648285\n",
            "[Training Epoch 61] Batch 1900, Loss 0.26950061321258545\n",
            "[Training Epoch 61] Batch 2000, Loss 0.26871776580810547\n",
            "[Training Epoch 61] Batch 2100, Loss 0.2755473256111145\n",
            "[Training Epoch 61] Batch 2200, Loss 0.2752978205680847\n",
            "[Training Epoch 61] Batch 2300, Loss 0.2693746089935303\n",
            "[Training Epoch 61] Batch 2400, Loss 0.29868191480636597\n",
            "[Training Epoch 61] Batch 2500, Loss 0.2644934356212616\n",
            "[Training Epoch 61] Batch 2600, Loss 0.2633858323097229\n",
            "[Training Epoch 61] Batch 2700, Loss 0.2639397382736206\n",
            "[Training Epoch 61] Batch 2800, Loss 0.26808130741119385\n",
            "[Training Epoch 61] Batch 2900, Loss 0.24987538158893585\n",
            "[Training Epoch 61] Batch 3000, Loss 0.2717967927455902\n",
            "[Training Epoch 61] Batch 3100, Loss 0.28242284059524536\n",
            "[Training Epoch 61] Batch 3200, Loss 0.24388974905014038\n",
            "[Training Epoch 61] Batch 3300, Loss 0.26142945885658264\n",
            "[Training Epoch 61] Batch 3400, Loss 0.25630322098731995\n",
            "[Training Epoch 61] Batch 3500, Loss 0.25635015964508057\n",
            "[Training Epoch 61] Batch 3600, Loss 0.25505003333091736\n",
            "[Training Epoch 61] Batch 3700, Loss 0.26537254452705383\n",
            "[Training Epoch 61] Batch 3800, Loss 0.27837511897087097\n",
            "[Training Epoch 61] Batch 3900, Loss 0.27440184354782104\n",
            "[Training Epoch 61] Batch 4000, Loss 0.248663067817688\n",
            "[Training Epoch 61] Batch 4100, Loss 0.2580525279045105\n",
            "[Training Epoch 61] Batch 4200, Loss 0.25248897075653076\n",
            "[Training Epoch 61] Batch 4300, Loss 0.2794783115386963\n",
            "[Training Epoch 61] Batch 4400, Loss 0.26851773262023926\n",
            "[Training Epoch 61] Batch 4500, Loss 0.27550703287124634\n",
            "[Training Epoch 61] Batch 4600, Loss 0.2541900873184204\n",
            "[Training Epoch 61] Batch 4700, Loss 0.2700832784175873\n",
            "[Training Epoch 61] Batch 4800, Loss 0.2782277464866638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 61] HR = 0.6411, NDCG = 0.3707\n",
            "Epoch 62 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 62] Batch 0, Loss 0.2626200020313263\n",
            "[Training Epoch 62] Batch 100, Loss 0.27791929244995117\n",
            "[Training Epoch 62] Batch 200, Loss 0.27621281147003174\n",
            "[Training Epoch 62] Batch 300, Loss 0.267719030380249\n",
            "[Training Epoch 62] Batch 400, Loss 0.25594207644462585\n",
            "[Training Epoch 62] Batch 500, Loss 0.24956142902374268\n",
            "[Training Epoch 62] Batch 600, Loss 0.24059170484542847\n",
            "[Training Epoch 62] Batch 700, Loss 0.260297954082489\n",
            "[Training Epoch 62] Batch 800, Loss 0.26711130142211914\n",
            "[Training Epoch 62] Batch 900, Loss 0.23508289456367493\n",
            "[Training Epoch 62] Batch 1000, Loss 0.2289711833000183\n",
            "[Training Epoch 62] Batch 1100, Loss 0.2586616277694702\n",
            "[Training Epoch 62] Batch 1200, Loss 0.2508735656738281\n",
            "[Training Epoch 62] Batch 1300, Loss 0.2730066776275635\n",
            "[Training Epoch 62] Batch 1400, Loss 0.24222658574581146\n",
            "[Training Epoch 62] Batch 1500, Loss 0.2434743493795395\n",
            "[Training Epoch 62] Batch 1600, Loss 0.2537752091884613\n",
            "[Training Epoch 62] Batch 1700, Loss 0.2790248394012451\n",
            "[Training Epoch 62] Batch 1800, Loss 0.26709842681884766\n",
            "[Training Epoch 62] Batch 1900, Loss 0.264832466840744\n",
            "[Training Epoch 62] Batch 2000, Loss 0.25874489545822144\n",
            "[Training Epoch 62] Batch 2100, Loss 0.2768545150756836\n",
            "[Training Epoch 62] Batch 2200, Loss 0.23925559222698212\n",
            "[Training Epoch 62] Batch 2300, Loss 0.25716572999954224\n",
            "[Training Epoch 62] Batch 2400, Loss 0.24084308743476868\n",
            "[Training Epoch 62] Batch 2500, Loss 0.2642937898635864\n",
            "[Training Epoch 62] Batch 2600, Loss 0.2969261407852173\n",
            "[Training Epoch 62] Batch 2700, Loss 0.2563215494155884\n",
            "[Training Epoch 62] Batch 2800, Loss 0.24810191988945007\n",
            "[Training Epoch 62] Batch 2900, Loss 0.23760825395584106\n",
            "[Training Epoch 62] Batch 3000, Loss 0.25450053811073303\n",
            "[Training Epoch 62] Batch 3100, Loss 0.22554533183574677\n",
            "[Training Epoch 62] Batch 3200, Loss 0.2496986836194992\n",
            "[Training Epoch 62] Batch 3300, Loss 0.24715577065944672\n",
            "[Training Epoch 62] Batch 3400, Loss 0.280337393283844\n",
            "[Training Epoch 62] Batch 3500, Loss 0.27610886096954346\n",
            "[Training Epoch 62] Batch 3600, Loss 0.24487198889255524\n",
            "[Training Epoch 62] Batch 3700, Loss 0.25753775238990784\n",
            "[Training Epoch 62] Batch 3800, Loss 0.2575231194496155\n",
            "[Training Epoch 62] Batch 3900, Loss 0.26242613792419434\n",
            "[Training Epoch 62] Batch 4000, Loss 0.28810858726501465\n",
            "[Training Epoch 62] Batch 4100, Loss 0.23831672966480255\n",
            "[Training Epoch 62] Batch 4200, Loss 0.2641691565513611\n",
            "[Training Epoch 62] Batch 4300, Loss 0.2425341010093689\n",
            "[Training Epoch 62] Batch 4400, Loss 0.26779571175575256\n",
            "[Training Epoch 62] Batch 4500, Loss 0.26149627566337585\n",
            "[Training Epoch 62] Batch 4600, Loss 0.2436947226524353\n",
            "[Training Epoch 62] Batch 4700, Loss 0.2658647894859314\n",
            "[Training Epoch 62] Batch 4800, Loss 0.24179494380950928\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 62] HR = 0.6394, NDCG = 0.3707\n",
            "Epoch 63 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 63] Batch 0, Loss 0.28884685039520264\n",
            "[Training Epoch 63] Batch 100, Loss 0.2741556167602539\n",
            "[Training Epoch 63] Batch 200, Loss 0.23158672451972961\n",
            "[Training Epoch 63] Batch 300, Loss 0.2865865230560303\n",
            "[Training Epoch 63] Batch 400, Loss 0.26407551765441895\n",
            "[Training Epoch 63] Batch 500, Loss 0.27797162532806396\n",
            "[Training Epoch 63] Batch 600, Loss 0.25052815675735474\n",
            "[Training Epoch 63] Batch 700, Loss 0.250882625579834\n",
            "[Training Epoch 63] Batch 800, Loss 0.2739565968513489\n",
            "[Training Epoch 63] Batch 900, Loss 0.24182143807411194\n",
            "[Training Epoch 63] Batch 1000, Loss 0.26777997612953186\n",
            "[Training Epoch 63] Batch 1100, Loss 0.2805224061012268\n",
            "[Training Epoch 63] Batch 1200, Loss 0.26759251952171326\n",
            "[Training Epoch 63] Batch 1300, Loss 0.2837866544723511\n",
            "[Training Epoch 63] Batch 1400, Loss 0.2757863402366638\n",
            "[Training Epoch 63] Batch 1500, Loss 0.30753329396247864\n",
            "[Training Epoch 63] Batch 1600, Loss 0.27553069591522217\n",
            "[Training Epoch 63] Batch 1700, Loss 0.2612583041191101\n",
            "[Training Epoch 63] Batch 1800, Loss 0.24821995198726654\n",
            "[Training Epoch 63] Batch 1900, Loss 0.2539510130882263\n",
            "[Training Epoch 63] Batch 2000, Loss 0.2488917112350464\n",
            "[Training Epoch 63] Batch 2100, Loss 0.2884063422679901\n",
            "[Training Epoch 63] Batch 2200, Loss 0.2814141511917114\n",
            "[Training Epoch 63] Batch 2300, Loss 0.29505273699760437\n",
            "[Training Epoch 63] Batch 2400, Loss 0.26276639103889465\n",
            "[Training Epoch 63] Batch 2500, Loss 0.2614957392215729\n",
            "[Training Epoch 63] Batch 2600, Loss 0.2809836268424988\n",
            "[Training Epoch 63] Batch 2700, Loss 0.2562241852283478\n",
            "[Training Epoch 63] Batch 2800, Loss 0.2650468945503235\n",
            "[Training Epoch 63] Batch 2900, Loss 0.27326473593711853\n",
            "[Training Epoch 63] Batch 3000, Loss 0.2720387279987335\n",
            "[Training Epoch 63] Batch 3100, Loss 0.27065396308898926\n",
            "[Training Epoch 63] Batch 3200, Loss 0.26848548650741577\n",
            "[Training Epoch 63] Batch 3300, Loss 0.28459858894348145\n",
            "[Training Epoch 63] Batch 3400, Loss 0.26318591833114624\n",
            "[Training Epoch 63] Batch 3500, Loss 0.3101477026939392\n",
            "[Training Epoch 63] Batch 3600, Loss 0.24840673804283142\n",
            "[Training Epoch 63] Batch 3700, Loss 0.27642399072647095\n",
            "[Training Epoch 63] Batch 3800, Loss 0.23788270354270935\n",
            "[Training Epoch 63] Batch 3900, Loss 0.2660887837409973\n",
            "[Training Epoch 63] Batch 4000, Loss 0.2397572100162506\n",
            "[Training Epoch 63] Batch 4100, Loss 0.2644336223602295\n",
            "[Training Epoch 63] Batch 4200, Loss 0.26909610629081726\n",
            "[Training Epoch 63] Batch 4300, Loss 0.2600087523460388\n",
            "[Training Epoch 63] Batch 4400, Loss 0.29390084743499756\n",
            "[Training Epoch 63] Batch 4500, Loss 0.2509620785713196\n",
            "[Training Epoch 63] Batch 4600, Loss 0.2667763829231262\n",
            "[Training Epoch 63] Batch 4700, Loss 0.26468726992607117\n",
            "[Training Epoch 63] Batch 4800, Loss 0.2563340663909912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 63] HR = 0.6373, NDCG = 0.3706\n",
            "Epoch 64 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 64] Batch 0, Loss 0.2652198076248169\n",
            "[Training Epoch 64] Batch 100, Loss 0.320015549659729\n",
            "[Training Epoch 64] Batch 200, Loss 0.27031445503234863\n",
            "[Training Epoch 64] Batch 300, Loss 0.2523382306098938\n",
            "[Training Epoch 64] Batch 400, Loss 0.2720463275909424\n",
            "[Training Epoch 64] Batch 500, Loss 0.27514779567718506\n",
            "[Training Epoch 64] Batch 600, Loss 0.25888776779174805\n",
            "[Training Epoch 64] Batch 700, Loss 0.2528313994407654\n",
            "[Training Epoch 64] Batch 800, Loss 0.2568342685699463\n",
            "[Training Epoch 64] Batch 900, Loss 0.2590181827545166\n",
            "[Training Epoch 64] Batch 1000, Loss 0.28399407863616943\n",
            "[Training Epoch 64] Batch 1100, Loss 0.26843416690826416\n",
            "[Training Epoch 64] Batch 1200, Loss 0.30335670709609985\n",
            "[Training Epoch 64] Batch 1300, Loss 0.2570731043815613\n",
            "[Training Epoch 64] Batch 1400, Loss 0.2995411455631256\n",
            "[Training Epoch 64] Batch 1500, Loss 0.2220977246761322\n",
            "[Training Epoch 64] Batch 1600, Loss 0.2637818157672882\n",
            "[Training Epoch 64] Batch 1700, Loss 0.2701794505119324\n",
            "[Training Epoch 64] Batch 1800, Loss 0.2448853850364685\n",
            "[Training Epoch 64] Batch 1900, Loss 0.2975012958049774\n",
            "[Training Epoch 64] Batch 2000, Loss 0.2592654824256897\n",
            "[Training Epoch 64] Batch 2100, Loss 0.2742612957954407\n",
            "[Training Epoch 64] Batch 2200, Loss 0.21765568852424622\n",
            "[Training Epoch 64] Batch 2300, Loss 0.2599303126335144\n",
            "[Training Epoch 64] Batch 2400, Loss 0.26577866077423096\n",
            "[Training Epoch 64] Batch 2500, Loss 0.2471892535686493\n",
            "[Training Epoch 64] Batch 2600, Loss 0.26247331500053406\n",
            "[Training Epoch 64] Batch 2700, Loss 0.2688167095184326\n",
            "[Training Epoch 64] Batch 2800, Loss 0.2965947389602661\n",
            "[Training Epoch 64] Batch 2900, Loss 0.2812248170375824\n",
            "[Training Epoch 64] Batch 3000, Loss 0.25238147377967834\n",
            "[Training Epoch 64] Batch 3100, Loss 0.2777104377746582\n",
            "[Training Epoch 64] Batch 3200, Loss 0.24799323081970215\n",
            "[Training Epoch 64] Batch 3300, Loss 0.28284379839897156\n",
            "[Training Epoch 64] Batch 3400, Loss 0.26815325021743774\n",
            "[Training Epoch 64] Batch 3500, Loss 0.24891424179077148\n",
            "[Training Epoch 64] Batch 3600, Loss 0.25977832078933716\n",
            "[Training Epoch 64] Batch 3700, Loss 0.27474063634872437\n",
            "[Training Epoch 64] Batch 3800, Loss 0.25754791498184204\n",
            "[Training Epoch 64] Batch 3900, Loss 0.2761456072330475\n",
            "[Training Epoch 64] Batch 4000, Loss 0.255595326423645\n",
            "[Training Epoch 64] Batch 4100, Loss 0.25903624296188354\n",
            "[Training Epoch 64] Batch 4200, Loss 0.25630736351013184\n",
            "[Training Epoch 64] Batch 4300, Loss 0.24752289056777954\n",
            "[Training Epoch 64] Batch 4400, Loss 0.2863900363445282\n",
            "[Training Epoch 64] Batch 4500, Loss 0.2616332769393921\n",
            "[Training Epoch 64] Batch 4600, Loss 0.25118952989578247\n",
            "[Training Epoch 64] Batch 4700, Loss 0.2612347900867462\n",
            "[Training Epoch 64] Batch 4800, Loss 0.250567227602005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 64] HR = 0.6377, NDCG = 0.3696\n",
            "Epoch 65 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 65] Batch 0, Loss 0.2862246632575989\n",
            "[Training Epoch 65] Batch 100, Loss 0.28712475299835205\n",
            "[Training Epoch 65] Batch 200, Loss 0.26255038380622864\n",
            "[Training Epoch 65] Batch 300, Loss 0.2849586308002472\n",
            "[Training Epoch 65] Batch 400, Loss 0.2638642489910126\n",
            "[Training Epoch 65] Batch 500, Loss 0.2581765949726105\n",
            "[Training Epoch 65] Batch 600, Loss 0.2576025724411011\n",
            "[Training Epoch 65] Batch 700, Loss 0.24943408370018005\n",
            "[Training Epoch 65] Batch 800, Loss 0.26270902156829834\n",
            "[Training Epoch 65] Batch 900, Loss 0.26854264736175537\n",
            "[Training Epoch 65] Batch 1000, Loss 0.27754151821136475\n",
            "[Training Epoch 65] Batch 1100, Loss 0.2494707703590393\n",
            "[Training Epoch 65] Batch 1200, Loss 0.25603264570236206\n",
            "[Training Epoch 65] Batch 1300, Loss 0.24794775247573853\n",
            "[Training Epoch 65] Batch 1400, Loss 0.26916083693504333\n",
            "[Training Epoch 65] Batch 1500, Loss 0.23099154233932495\n",
            "[Training Epoch 65] Batch 1600, Loss 0.24001431465148926\n",
            "[Training Epoch 65] Batch 1700, Loss 0.2685868740081787\n",
            "[Training Epoch 65] Batch 1800, Loss 0.23983314633369446\n",
            "[Training Epoch 65] Batch 1900, Loss 0.2585838735103607\n",
            "[Training Epoch 65] Batch 2000, Loss 0.280570387840271\n",
            "[Training Epoch 65] Batch 2100, Loss 0.26160621643066406\n",
            "[Training Epoch 65] Batch 2200, Loss 0.2627076506614685\n",
            "[Training Epoch 65] Batch 2300, Loss 0.2548794448375702\n",
            "[Training Epoch 65] Batch 2400, Loss 0.2972763478755951\n",
            "[Training Epoch 65] Batch 2500, Loss 0.2570294439792633\n",
            "[Training Epoch 65] Batch 2600, Loss 0.25030216574668884\n",
            "[Training Epoch 65] Batch 2700, Loss 0.2823612093925476\n",
            "[Training Epoch 65] Batch 2800, Loss 0.27450013160705566\n",
            "[Training Epoch 65] Batch 2900, Loss 0.2847420871257782\n",
            "[Training Epoch 65] Batch 3000, Loss 0.23616552352905273\n",
            "[Training Epoch 65] Batch 3100, Loss 0.2819872498512268\n",
            "[Training Epoch 65] Batch 3200, Loss 0.2637619376182556\n",
            "[Training Epoch 65] Batch 3300, Loss 0.27894046902656555\n",
            "[Training Epoch 65] Batch 3400, Loss 0.2725757956504822\n",
            "[Training Epoch 65] Batch 3500, Loss 0.24688845872879028\n",
            "[Training Epoch 65] Batch 3600, Loss 0.2673232853412628\n",
            "[Training Epoch 65] Batch 3700, Loss 0.2716231048107147\n",
            "[Training Epoch 65] Batch 3800, Loss 0.2585243582725525\n",
            "[Training Epoch 65] Batch 3900, Loss 0.2645891010761261\n",
            "[Training Epoch 65] Batch 4000, Loss 0.24403022229671478\n",
            "[Training Epoch 65] Batch 4100, Loss 0.27147984504699707\n",
            "[Training Epoch 65] Batch 4200, Loss 0.2633373737335205\n",
            "[Training Epoch 65] Batch 4300, Loss 0.27884483337402344\n",
            "[Training Epoch 65] Batch 4400, Loss 0.26696282625198364\n",
            "[Training Epoch 65] Batch 4500, Loss 0.26077693700790405\n",
            "[Training Epoch 65] Batch 4600, Loss 0.26281142234802246\n",
            "[Training Epoch 65] Batch 4700, Loss 0.2819632887840271\n",
            "[Training Epoch 65] Batch 4800, Loss 0.2535112500190735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 65] HR = 0.6384, NDCG = 0.3701\n",
            "Epoch 66 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 66] Batch 0, Loss 0.2681351900100708\n",
            "[Training Epoch 66] Batch 100, Loss 0.24666766822338104\n",
            "[Training Epoch 66] Batch 200, Loss 0.2898055613040924\n",
            "[Training Epoch 66] Batch 300, Loss 0.2793711721897125\n",
            "[Training Epoch 66] Batch 400, Loss 0.25566017627716064\n",
            "[Training Epoch 66] Batch 500, Loss 0.26837658882141113\n",
            "[Training Epoch 66] Batch 600, Loss 0.25736016035079956\n",
            "[Training Epoch 66] Batch 700, Loss 0.26736685633659363\n",
            "[Training Epoch 66] Batch 800, Loss 0.2908215820789337\n",
            "[Training Epoch 66] Batch 900, Loss 0.24656599760055542\n",
            "[Training Epoch 66] Batch 1000, Loss 0.24383175373077393\n",
            "[Training Epoch 66] Batch 1100, Loss 0.2781241536140442\n",
            "[Training Epoch 66] Batch 1200, Loss 0.2630992531776428\n",
            "[Training Epoch 66] Batch 1300, Loss 0.27311182022094727\n",
            "[Training Epoch 66] Batch 1400, Loss 0.2683834135532379\n",
            "[Training Epoch 66] Batch 1500, Loss 0.2758662700653076\n",
            "[Training Epoch 66] Batch 1600, Loss 0.2322726845741272\n",
            "[Training Epoch 66] Batch 1700, Loss 0.27678847312927246\n",
            "[Training Epoch 66] Batch 1800, Loss 0.24886709451675415\n",
            "[Training Epoch 66] Batch 1900, Loss 0.2580796182155609\n",
            "[Training Epoch 66] Batch 2000, Loss 0.2929394245147705\n",
            "[Training Epoch 66] Batch 2100, Loss 0.2911672592163086\n",
            "[Training Epoch 66] Batch 2200, Loss 0.24596577882766724\n",
            "[Training Epoch 66] Batch 2300, Loss 0.25516200065612793\n",
            "[Training Epoch 66] Batch 2400, Loss 0.289162278175354\n",
            "[Training Epoch 66] Batch 2500, Loss 0.27050328254699707\n",
            "[Training Epoch 66] Batch 2600, Loss 0.29341256618499756\n",
            "[Training Epoch 66] Batch 2700, Loss 0.2774345278739929\n",
            "[Training Epoch 66] Batch 2800, Loss 0.2621336877346039\n",
            "[Training Epoch 66] Batch 2900, Loss 0.2507275938987732\n",
            "[Training Epoch 66] Batch 3000, Loss 0.26248955726623535\n",
            "[Training Epoch 66] Batch 3100, Loss 0.2488366663455963\n",
            "[Training Epoch 66] Batch 3200, Loss 0.24759303033351898\n",
            "[Training Epoch 66] Batch 3300, Loss 0.24564874172210693\n",
            "[Training Epoch 66] Batch 3400, Loss 0.27424442768096924\n",
            "[Training Epoch 66] Batch 3500, Loss 0.2652183175086975\n",
            "[Training Epoch 66] Batch 3600, Loss 0.2726958394050598\n",
            "[Training Epoch 66] Batch 3700, Loss 0.2562728524208069\n",
            "[Training Epoch 66] Batch 3800, Loss 0.25247883796691895\n",
            "[Training Epoch 66] Batch 3900, Loss 0.27567189931869507\n",
            "[Training Epoch 66] Batch 4000, Loss 0.28006303310394287\n",
            "[Training Epoch 66] Batch 4100, Loss 0.2794947624206543\n",
            "[Training Epoch 66] Batch 4200, Loss 0.25143560767173767\n",
            "[Training Epoch 66] Batch 4300, Loss 0.26754000782966614\n",
            "[Training Epoch 66] Batch 4400, Loss 0.2718203663825989\n",
            "[Training Epoch 66] Batch 4500, Loss 0.25144654512405396\n",
            "[Training Epoch 66] Batch 4600, Loss 0.2540096640586853\n",
            "[Training Epoch 66] Batch 4700, Loss 0.2695635259151459\n",
            "[Training Epoch 66] Batch 4800, Loss 0.25476977229118347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 66] HR = 0.6381, NDCG = 0.3694\n",
            "Epoch 67 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 67] Batch 0, Loss 0.2643050253391266\n",
            "[Training Epoch 67] Batch 100, Loss 0.2769450545310974\n",
            "[Training Epoch 67] Batch 200, Loss 0.28142374753952026\n",
            "[Training Epoch 67] Batch 300, Loss 0.26063981652259827\n",
            "[Training Epoch 67] Batch 400, Loss 0.2432745099067688\n",
            "[Training Epoch 67] Batch 500, Loss 0.27216681838035583\n",
            "[Training Epoch 67] Batch 600, Loss 0.2932196259498596\n",
            "[Training Epoch 67] Batch 700, Loss 0.25502538681030273\n",
            "[Training Epoch 67] Batch 800, Loss 0.27913281321525574\n",
            "[Training Epoch 67] Batch 900, Loss 0.2789214253425598\n",
            "[Training Epoch 67] Batch 1000, Loss 0.25489217042922974\n",
            "[Training Epoch 67] Batch 1100, Loss 0.24456076323986053\n",
            "[Training Epoch 67] Batch 1200, Loss 0.2594112455844879\n",
            "[Training Epoch 67] Batch 1300, Loss 0.24673256278038025\n",
            "[Training Epoch 67] Batch 1400, Loss 0.2566746473312378\n",
            "[Training Epoch 67] Batch 1500, Loss 0.2544865012168884\n",
            "[Training Epoch 67] Batch 1600, Loss 0.2488439977169037\n",
            "[Training Epoch 67] Batch 1700, Loss 0.25761640071868896\n",
            "[Training Epoch 67] Batch 1800, Loss 0.2530827522277832\n",
            "[Training Epoch 67] Batch 1900, Loss 0.27470508217811584\n",
            "[Training Epoch 67] Batch 2000, Loss 0.23084187507629395\n",
            "[Training Epoch 67] Batch 2100, Loss 0.2589571475982666\n",
            "[Training Epoch 67] Batch 2200, Loss 0.2872580289840698\n",
            "[Training Epoch 67] Batch 2300, Loss 0.27121490240097046\n",
            "[Training Epoch 67] Batch 2400, Loss 0.23131690919399261\n",
            "[Training Epoch 67] Batch 2500, Loss 0.234593003988266\n",
            "[Training Epoch 67] Batch 2600, Loss 0.28026658296585083\n",
            "[Training Epoch 67] Batch 2700, Loss 0.24580736458301544\n",
            "[Training Epoch 67] Batch 2800, Loss 0.2631519138813019\n",
            "[Training Epoch 67] Batch 2900, Loss 0.2623938322067261\n",
            "[Training Epoch 67] Batch 3000, Loss 0.2531301975250244\n",
            "[Training Epoch 67] Batch 3100, Loss 0.2490481585264206\n",
            "[Training Epoch 67] Batch 3200, Loss 0.2781120240688324\n",
            "[Training Epoch 67] Batch 3300, Loss 0.2648676037788391\n",
            "[Training Epoch 67] Batch 3400, Loss 0.2712402939796448\n",
            "[Training Epoch 67] Batch 3500, Loss 0.26847654581069946\n",
            "[Training Epoch 67] Batch 3600, Loss 0.26717817783355713\n",
            "[Training Epoch 67] Batch 3700, Loss 0.2447693794965744\n",
            "[Training Epoch 67] Batch 3800, Loss 0.2701098322868347\n",
            "[Training Epoch 67] Batch 3900, Loss 0.2620164155960083\n",
            "[Training Epoch 67] Batch 4000, Loss 0.26251307129859924\n",
            "[Training Epoch 67] Batch 4100, Loss 0.2915401756763458\n",
            "[Training Epoch 67] Batch 4200, Loss 0.2503582537174225\n",
            "[Training Epoch 67] Batch 4300, Loss 0.28283968567848206\n",
            "[Training Epoch 67] Batch 4400, Loss 0.284809947013855\n",
            "[Training Epoch 67] Batch 4500, Loss 0.27132362127304077\n",
            "[Training Epoch 67] Batch 4600, Loss 0.24539007246494293\n",
            "[Training Epoch 67] Batch 4700, Loss 0.27291733026504517\n",
            "[Training Epoch 67] Batch 4800, Loss 0.2645881175994873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 67] HR = 0.6394, NDCG = 0.3695\n",
            "Epoch 68 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 68] Batch 0, Loss 0.264666885137558\n",
            "[Training Epoch 68] Batch 100, Loss 0.2599245011806488\n",
            "[Training Epoch 68] Batch 200, Loss 0.25299543142318726\n",
            "[Training Epoch 68] Batch 300, Loss 0.2616444528102875\n",
            "[Training Epoch 68] Batch 400, Loss 0.28107723593711853\n",
            "[Training Epoch 68] Batch 500, Loss 0.2748098373413086\n",
            "[Training Epoch 68] Batch 600, Loss 0.2697892487049103\n",
            "[Training Epoch 68] Batch 700, Loss 0.25382938981056213\n",
            "[Training Epoch 68] Batch 800, Loss 0.27407729625701904\n",
            "[Training Epoch 68] Batch 900, Loss 0.25855880975723267\n",
            "[Training Epoch 68] Batch 1000, Loss 0.279049813747406\n",
            "[Training Epoch 68] Batch 1100, Loss 0.2796766757965088\n",
            "[Training Epoch 68] Batch 1200, Loss 0.26330137252807617\n",
            "[Training Epoch 68] Batch 1300, Loss 0.25543278455734253\n",
            "[Training Epoch 68] Batch 1400, Loss 0.27475637197494507\n",
            "[Training Epoch 68] Batch 1500, Loss 0.25174883008003235\n",
            "[Training Epoch 68] Batch 1600, Loss 0.27419406175613403\n",
            "[Training Epoch 68] Batch 1700, Loss 0.2881835103034973\n",
            "[Training Epoch 68] Batch 1800, Loss 0.25538963079452515\n",
            "[Training Epoch 68] Batch 1900, Loss 0.2499832808971405\n",
            "[Training Epoch 68] Batch 2000, Loss 0.24371378123760223\n",
            "[Training Epoch 68] Batch 2100, Loss 0.2696499526500702\n",
            "[Training Epoch 68] Batch 2200, Loss 0.23612917959690094\n",
            "[Training Epoch 68] Batch 2300, Loss 0.2342274934053421\n",
            "[Training Epoch 68] Batch 2400, Loss 0.25487011671066284\n",
            "[Training Epoch 68] Batch 2500, Loss 0.2973434627056122\n",
            "[Training Epoch 68] Batch 2600, Loss 0.2614824175834656\n",
            "[Training Epoch 68] Batch 2700, Loss 0.2408960461616516\n",
            "[Training Epoch 68] Batch 2800, Loss 0.2746891677379608\n",
            "[Training Epoch 68] Batch 2900, Loss 0.25399500131607056\n",
            "[Training Epoch 68] Batch 3000, Loss 0.25587719678878784\n",
            "[Training Epoch 68] Batch 3100, Loss 0.264072448015213\n",
            "[Training Epoch 68] Batch 3200, Loss 0.2601267099380493\n",
            "[Training Epoch 68] Batch 3300, Loss 0.2648230195045471\n",
            "[Training Epoch 68] Batch 3400, Loss 0.25705525279045105\n",
            "[Training Epoch 68] Batch 3500, Loss 0.2606133222579956\n",
            "[Training Epoch 68] Batch 3600, Loss 0.25086262822151184\n",
            "[Training Epoch 68] Batch 3700, Loss 0.24526265263557434\n",
            "[Training Epoch 68] Batch 3800, Loss 0.25699225068092346\n",
            "[Training Epoch 68] Batch 3900, Loss 0.2590407729148865\n",
            "[Training Epoch 68] Batch 4000, Loss 0.243272602558136\n",
            "[Training Epoch 68] Batch 4100, Loss 0.25006020069122314\n",
            "[Training Epoch 68] Batch 4200, Loss 0.2531917095184326\n",
            "[Training Epoch 68] Batch 4300, Loss 0.2591937780380249\n",
            "[Training Epoch 68] Batch 4400, Loss 0.2748945355415344\n",
            "[Training Epoch 68] Batch 4500, Loss 0.26841670274734497\n",
            "[Training Epoch 68] Batch 4600, Loss 0.2977074980735779\n",
            "[Training Epoch 68] Batch 4700, Loss 0.22336480021476746\n",
            "[Training Epoch 68] Batch 4800, Loss 0.2594946324825287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 68] HR = 0.6387, NDCG = 0.3700\n",
            "Epoch 69 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 69] Batch 0, Loss 0.26549723744392395\n",
            "[Training Epoch 69] Batch 100, Loss 0.2415524125099182\n",
            "[Training Epoch 69] Batch 200, Loss 0.26098087430000305\n",
            "[Training Epoch 69] Batch 300, Loss 0.2770198583602905\n",
            "[Training Epoch 69] Batch 400, Loss 0.26175782084465027\n",
            "[Training Epoch 69] Batch 500, Loss 0.2691882252693176\n",
            "[Training Epoch 69] Batch 600, Loss 0.2711179256439209\n",
            "[Training Epoch 69] Batch 700, Loss 0.2750132083892822\n",
            "[Training Epoch 69] Batch 800, Loss 0.2680991590023041\n",
            "[Training Epoch 69] Batch 900, Loss 0.26330018043518066\n",
            "[Training Epoch 69] Batch 1000, Loss 0.25262561440467834\n",
            "[Training Epoch 69] Batch 1100, Loss 0.24276015162467957\n",
            "[Training Epoch 69] Batch 1200, Loss 0.2548229992389679\n",
            "[Training Epoch 69] Batch 1300, Loss 0.2542975842952728\n",
            "[Training Epoch 69] Batch 1400, Loss 0.26179051399230957\n",
            "[Training Epoch 69] Batch 1500, Loss 0.2614966332912445\n",
            "[Training Epoch 69] Batch 1600, Loss 0.2562517523765564\n",
            "[Training Epoch 69] Batch 1700, Loss 0.25500404834747314\n",
            "[Training Epoch 69] Batch 1800, Loss 0.2733897566795349\n",
            "[Training Epoch 69] Batch 1900, Loss 0.26413941383361816\n",
            "[Training Epoch 69] Batch 2000, Loss 0.2542876601219177\n",
            "[Training Epoch 69] Batch 2100, Loss 0.2766648530960083\n",
            "[Training Epoch 69] Batch 2200, Loss 0.27340176701545715\n",
            "[Training Epoch 69] Batch 2300, Loss 0.25485026836395264\n",
            "[Training Epoch 69] Batch 2400, Loss 0.25096890330314636\n",
            "[Training Epoch 69] Batch 2500, Loss 0.2636646032333374\n",
            "[Training Epoch 69] Batch 2600, Loss 0.2940545678138733\n",
            "[Training Epoch 69] Batch 2700, Loss 0.2903379201889038\n",
            "[Training Epoch 69] Batch 2800, Loss 0.2730688452720642\n",
            "[Training Epoch 69] Batch 2900, Loss 0.2754961848258972\n",
            "[Training Epoch 69] Batch 3000, Loss 0.26803603768348694\n",
            "[Training Epoch 69] Batch 3100, Loss 0.28074610233306885\n",
            "[Training Epoch 69] Batch 3200, Loss 0.28497350215911865\n",
            "[Training Epoch 69] Batch 3300, Loss 0.2799968719482422\n",
            "[Training Epoch 69] Batch 3400, Loss 0.27610915899276733\n",
            "[Training Epoch 69] Batch 3500, Loss 0.24211686849594116\n",
            "[Training Epoch 69] Batch 3600, Loss 0.2533995509147644\n",
            "[Training Epoch 69] Batch 3700, Loss 0.2438787817955017\n",
            "[Training Epoch 69] Batch 3800, Loss 0.2501762807369232\n",
            "[Training Epoch 69] Batch 3900, Loss 0.260824590921402\n",
            "[Training Epoch 69] Batch 4000, Loss 0.2728450894355774\n",
            "[Training Epoch 69] Batch 4100, Loss 0.2931196093559265\n",
            "[Training Epoch 69] Batch 4200, Loss 0.3064703941345215\n",
            "[Training Epoch 69] Batch 4300, Loss 0.2465038150548935\n",
            "[Training Epoch 69] Batch 4400, Loss 0.27120912075042725\n",
            "[Training Epoch 69] Batch 4500, Loss 0.27353423833847046\n",
            "[Training Epoch 69] Batch 4600, Loss 0.25057104229927063\n",
            "[Training Epoch 69] Batch 4700, Loss 0.25515812635421753\n",
            "[Training Epoch 69] Batch 4800, Loss 0.2711980938911438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 69] HR = 0.6377, NDCG = 0.3681\n",
            "Epoch 70 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 70] Batch 0, Loss 0.28181129693984985\n",
            "[Training Epoch 70] Batch 100, Loss 0.2630593478679657\n",
            "[Training Epoch 70] Batch 200, Loss 0.24049508571624756\n",
            "[Training Epoch 70] Batch 300, Loss 0.2529446482658386\n",
            "[Training Epoch 70] Batch 400, Loss 0.2632993757724762\n",
            "[Training Epoch 70] Batch 500, Loss 0.2911943793296814\n",
            "[Training Epoch 70] Batch 600, Loss 0.27862972021102905\n",
            "[Training Epoch 70] Batch 700, Loss 0.281402051448822\n",
            "[Training Epoch 70] Batch 800, Loss 0.2494732141494751\n",
            "[Training Epoch 70] Batch 900, Loss 0.24828337132930756\n",
            "[Training Epoch 70] Batch 1000, Loss 0.2890217900276184\n",
            "[Training Epoch 70] Batch 1100, Loss 0.24435275793075562\n",
            "[Training Epoch 70] Batch 1200, Loss 0.26668334007263184\n",
            "[Training Epoch 70] Batch 1300, Loss 0.24845658242702484\n",
            "[Training Epoch 70] Batch 1400, Loss 0.29182589054107666\n",
            "[Training Epoch 70] Batch 1500, Loss 0.2622610926628113\n",
            "[Training Epoch 70] Batch 1600, Loss 0.24863412976264954\n",
            "[Training Epoch 70] Batch 1700, Loss 0.25240474939346313\n",
            "[Training Epoch 70] Batch 1800, Loss 0.25850385427474976\n",
            "[Training Epoch 70] Batch 1900, Loss 0.27214521169662476\n",
            "[Training Epoch 70] Batch 2000, Loss 0.27397191524505615\n",
            "[Training Epoch 70] Batch 2100, Loss 0.26253578066825867\n",
            "[Training Epoch 70] Batch 2200, Loss 0.25426000356674194\n",
            "[Training Epoch 70] Batch 2300, Loss 0.25273314118385315\n",
            "[Training Epoch 70] Batch 2400, Loss 0.24251997470855713\n",
            "[Training Epoch 70] Batch 2500, Loss 0.2809898555278778\n",
            "[Training Epoch 70] Batch 2600, Loss 0.27038323879241943\n",
            "[Training Epoch 70] Batch 2700, Loss 0.29424214363098145\n",
            "[Training Epoch 70] Batch 2800, Loss 0.2484741359949112\n",
            "[Training Epoch 70] Batch 2900, Loss 0.26047879457473755\n",
            "[Training Epoch 70] Batch 3000, Loss 0.27786704897880554\n",
            "[Training Epoch 70] Batch 3100, Loss 0.2597081959247589\n",
            "[Training Epoch 70] Batch 3200, Loss 0.2492823600769043\n",
            "[Training Epoch 70] Batch 3300, Loss 0.2484314739704132\n",
            "[Training Epoch 70] Batch 3400, Loss 0.2521645724773407\n",
            "[Training Epoch 70] Batch 3500, Loss 0.2852274477481842\n",
            "[Training Epoch 70] Batch 3600, Loss 0.24035656452178955\n",
            "[Training Epoch 70] Batch 3700, Loss 0.27626609802246094\n",
            "[Training Epoch 70] Batch 3800, Loss 0.2607092261314392\n",
            "[Training Epoch 70] Batch 3900, Loss 0.24694466590881348\n",
            "[Training Epoch 70] Batch 4000, Loss 0.2701064944267273\n",
            "[Training Epoch 70] Batch 4100, Loss 0.28818410634994507\n",
            "[Training Epoch 70] Batch 4200, Loss 0.23291000723838806\n",
            "[Training Epoch 70] Batch 4300, Loss 0.2524239718914032\n",
            "[Training Epoch 70] Batch 4400, Loss 0.2791447043418884\n",
            "[Training Epoch 70] Batch 4500, Loss 0.2686515152454376\n",
            "[Training Epoch 70] Batch 4600, Loss 0.28587186336517334\n",
            "[Training Epoch 70] Batch 4700, Loss 0.27980872988700867\n",
            "[Training Epoch 70] Batch 4800, Loss 0.2763100862503052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 70] HR = 0.6363, NDCG = 0.3692\n",
            "Epoch 71 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 71] Batch 0, Loss 0.25484907627105713\n",
            "[Training Epoch 71] Batch 100, Loss 0.2826702296733856\n",
            "[Training Epoch 71] Batch 200, Loss 0.27204984426498413\n",
            "[Training Epoch 71] Batch 300, Loss 0.2811281383037567\n",
            "[Training Epoch 71] Batch 400, Loss 0.2918727993965149\n",
            "[Training Epoch 71] Batch 500, Loss 0.25911709666252136\n",
            "[Training Epoch 71] Batch 600, Loss 0.26266953349113464\n",
            "[Training Epoch 71] Batch 700, Loss 0.24636675417423248\n",
            "[Training Epoch 71] Batch 800, Loss 0.22585642337799072\n",
            "[Training Epoch 71] Batch 900, Loss 0.24880236387252808\n",
            "[Training Epoch 71] Batch 1000, Loss 0.26344209909439087\n",
            "[Training Epoch 71] Batch 1100, Loss 0.27232304215431213\n",
            "[Training Epoch 71] Batch 1200, Loss 0.2697666883468628\n",
            "[Training Epoch 71] Batch 1300, Loss 0.28233134746551514\n",
            "[Training Epoch 71] Batch 1400, Loss 0.2538299262523651\n",
            "[Training Epoch 71] Batch 1500, Loss 0.23563852906227112\n",
            "[Training Epoch 71] Batch 1600, Loss 0.2678647041320801\n",
            "[Training Epoch 71] Batch 1700, Loss 0.2679237127304077\n",
            "[Training Epoch 71] Batch 1800, Loss 0.2770466208457947\n",
            "[Training Epoch 71] Batch 1900, Loss 0.24733653664588928\n",
            "[Training Epoch 71] Batch 2000, Loss 0.26615601778030396\n",
            "[Training Epoch 71] Batch 2100, Loss 0.2683103084564209\n",
            "[Training Epoch 71] Batch 2200, Loss 0.2599674463272095\n",
            "[Training Epoch 71] Batch 2300, Loss 0.2829410135746002\n",
            "[Training Epoch 71] Batch 2400, Loss 0.263680100440979\n",
            "[Training Epoch 71] Batch 2500, Loss 0.2731919586658478\n",
            "[Training Epoch 71] Batch 2600, Loss 0.24744603037834167\n",
            "[Training Epoch 71] Batch 2700, Loss 0.2680652141571045\n",
            "[Training Epoch 71] Batch 2800, Loss 0.2746129035949707\n",
            "[Training Epoch 71] Batch 2900, Loss 0.25247472524642944\n",
            "[Training Epoch 71] Batch 3000, Loss 0.22416448593139648\n",
            "[Training Epoch 71] Batch 3100, Loss 0.28236573934555054\n",
            "[Training Epoch 71] Batch 3200, Loss 0.2440003603696823\n",
            "[Training Epoch 71] Batch 3300, Loss 0.2623797059059143\n",
            "[Training Epoch 71] Batch 3400, Loss 0.26642751693725586\n",
            "[Training Epoch 71] Batch 3500, Loss 0.25569313764572144\n",
            "[Training Epoch 71] Batch 3600, Loss 0.2938784956932068\n",
            "[Training Epoch 71] Batch 3700, Loss 0.26170623302459717\n",
            "[Training Epoch 71] Batch 3800, Loss 0.28816911578178406\n",
            "[Training Epoch 71] Batch 3900, Loss 0.29380711913108826\n",
            "[Training Epoch 71] Batch 4000, Loss 0.2541790008544922\n",
            "[Training Epoch 71] Batch 4100, Loss 0.2553342282772064\n",
            "[Training Epoch 71] Batch 4200, Loss 0.27439919114112854\n",
            "[Training Epoch 71] Batch 4300, Loss 0.25725507736206055\n",
            "[Training Epoch 71] Batch 4400, Loss 0.2554103136062622\n",
            "[Training Epoch 71] Batch 4500, Loss 0.27744874358177185\n",
            "[Training Epoch 71] Batch 4600, Loss 0.2565702199935913\n",
            "[Training Epoch 71] Batch 4700, Loss 0.2600778341293335\n",
            "[Training Epoch 71] Batch 4800, Loss 0.2748892903327942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 71] HR = 0.6371, NDCG = 0.3699\n",
            "Epoch 72 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 72] Batch 0, Loss 0.24351376295089722\n",
            "[Training Epoch 72] Batch 100, Loss 0.26491737365722656\n",
            "[Training Epoch 72] Batch 200, Loss 0.26656749844551086\n",
            "[Training Epoch 72] Batch 300, Loss 0.2603395879268646\n",
            "[Training Epoch 72] Batch 400, Loss 0.2563996911048889\n",
            "[Training Epoch 72] Batch 500, Loss 0.2728562355041504\n",
            "[Training Epoch 72] Batch 600, Loss 0.2535055875778198\n",
            "[Training Epoch 72] Batch 700, Loss 0.2638827860355377\n",
            "[Training Epoch 72] Batch 800, Loss 0.2784741222858429\n",
            "[Training Epoch 72] Batch 900, Loss 0.24177837371826172\n",
            "[Training Epoch 72] Batch 1000, Loss 0.2767150402069092\n",
            "[Training Epoch 72] Batch 1100, Loss 0.25910449028015137\n",
            "[Training Epoch 72] Batch 1200, Loss 0.2630015015602112\n",
            "[Training Epoch 72] Batch 1300, Loss 0.25615739822387695\n",
            "[Training Epoch 72] Batch 1400, Loss 0.23822617530822754\n",
            "[Training Epoch 72] Batch 1500, Loss 0.24184668064117432\n",
            "[Training Epoch 72] Batch 1600, Loss 0.2872024476528168\n",
            "[Training Epoch 72] Batch 1700, Loss 0.26433175802230835\n",
            "[Training Epoch 72] Batch 1800, Loss 0.24438051879405975\n",
            "[Training Epoch 72] Batch 1900, Loss 0.28826338052749634\n",
            "[Training Epoch 72] Batch 2000, Loss 0.2559810280799866\n",
            "[Training Epoch 72] Batch 2100, Loss 0.23573650419712067\n",
            "[Training Epoch 72] Batch 2200, Loss 0.26594656705856323\n",
            "[Training Epoch 72] Batch 2300, Loss 0.22348913550376892\n",
            "[Training Epoch 72] Batch 2400, Loss 0.27504396438598633\n",
            "[Training Epoch 72] Batch 2500, Loss 0.2729266285896301\n",
            "[Training Epoch 72] Batch 2600, Loss 0.2642703652381897\n",
            "[Training Epoch 72] Batch 2700, Loss 0.2418840527534485\n",
            "[Training Epoch 72] Batch 2800, Loss 0.258331835269928\n",
            "[Training Epoch 72] Batch 2900, Loss 0.2767641842365265\n",
            "[Training Epoch 72] Batch 3000, Loss 0.2672489285469055\n",
            "[Training Epoch 72] Batch 3100, Loss 0.24932155013084412\n",
            "[Training Epoch 72] Batch 3200, Loss 0.2488492876291275\n",
            "[Training Epoch 72] Batch 3300, Loss 0.2767728567123413\n",
            "[Training Epoch 72] Batch 3400, Loss 0.25485146045684814\n",
            "[Training Epoch 72] Batch 3500, Loss 0.26628196239471436\n",
            "[Training Epoch 72] Batch 3600, Loss 0.275511234998703\n",
            "[Training Epoch 72] Batch 3700, Loss 0.278842031955719\n",
            "[Training Epoch 72] Batch 3800, Loss 0.2556867003440857\n",
            "[Training Epoch 72] Batch 3900, Loss 0.2710532546043396\n",
            "[Training Epoch 72] Batch 4000, Loss 0.25998014211654663\n",
            "[Training Epoch 72] Batch 4100, Loss 0.28266629576683044\n",
            "[Training Epoch 72] Batch 4200, Loss 0.27561697363853455\n",
            "[Training Epoch 72] Batch 4300, Loss 0.2765839397907257\n",
            "[Training Epoch 72] Batch 4400, Loss 0.2641860842704773\n",
            "[Training Epoch 72] Batch 4500, Loss 0.26688092947006226\n",
            "[Training Epoch 72] Batch 4600, Loss 0.2762659788131714\n",
            "[Training Epoch 72] Batch 4700, Loss 0.25260108709335327\n",
            "[Training Epoch 72] Batch 4800, Loss 0.25954604148864746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 72] HR = 0.6387, NDCG = 0.3691\n",
            "Epoch 73 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 73] Batch 0, Loss 0.2531532943248749\n",
            "[Training Epoch 73] Batch 100, Loss 0.26358866691589355\n",
            "[Training Epoch 73] Batch 200, Loss 0.2499905377626419\n",
            "[Training Epoch 73] Batch 300, Loss 0.26708221435546875\n",
            "[Training Epoch 73] Batch 400, Loss 0.27859795093536377\n",
            "[Training Epoch 73] Batch 500, Loss 0.23770827054977417\n",
            "[Training Epoch 73] Batch 600, Loss 0.2527666389942169\n",
            "[Training Epoch 73] Batch 700, Loss 0.27626869082450867\n",
            "[Training Epoch 73] Batch 800, Loss 0.2714618444442749\n",
            "[Training Epoch 73] Batch 900, Loss 0.26390987634658813\n",
            "[Training Epoch 73] Batch 1000, Loss 0.2956858277320862\n",
            "[Training Epoch 73] Batch 1100, Loss 0.23113051056861877\n",
            "[Training Epoch 73] Batch 1200, Loss 0.27287864685058594\n",
            "[Training Epoch 73] Batch 1300, Loss 0.25674301385879517\n",
            "[Training Epoch 73] Batch 1400, Loss 0.31228047609329224\n",
            "[Training Epoch 73] Batch 1500, Loss 0.24372659623622894\n",
            "[Training Epoch 73] Batch 1600, Loss 0.2549198269844055\n",
            "[Training Epoch 73] Batch 1700, Loss 0.260351300239563\n",
            "[Training Epoch 73] Batch 1800, Loss 0.2854262888431549\n",
            "[Training Epoch 73] Batch 1900, Loss 0.25642672181129456\n",
            "[Training Epoch 73] Batch 2000, Loss 0.24227744340896606\n",
            "[Training Epoch 73] Batch 2100, Loss 0.2518276572227478\n",
            "[Training Epoch 73] Batch 2200, Loss 0.267059326171875\n",
            "[Training Epoch 73] Batch 2300, Loss 0.24243439733982086\n",
            "[Training Epoch 73] Batch 2400, Loss 0.2687399685382843\n",
            "[Training Epoch 73] Batch 2500, Loss 0.2703818082809448\n",
            "[Training Epoch 73] Batch 2600, Loss 0.24997401237487793\n",
            "[Training Epoch 73] Batch 2700, Loss 0.2708284258842468\n",
            "[Training Epoch 73] Batch 2800, Loss 0.2520567774772644\n",
            "[Training Epoch 73] Batch 2900, Loss 0.26003503799438477\n",
            "[Training Epoch 73] Batch 3000, Loss 0.27840471267700195\n",
            "[Training Epoch 73] Batch 3100, Loss 0.2803146243095398\n",
            "[Training Epoch 73] Batch 3200, Loss 0.25465118885040283\n",
            "[Training Epoch 73] Batch 3300, Loss 0.27942079305648804\n",
            "[Training Epoch 73] Batch 3400, Loss 0.26116061210632324\n",
            "[Training Epoch 73] Batch 3500, Loss 0.2610398530960083\n",
            "[Training Epoch 73] Batch 3600, Loss 0.2815054953098297\n",
            "[Training Epoch 73] Batch 3700, Loss 0.2590142488479614\n",
            "[Training Epoch 73] Batch 3800, Loss 0.2628251910209656\n",
            "[Training Epoch 73] Batch 3900, Loss 0.25793981552124023\n",
            "[Training Epoch 73] Batch 4000, Loss 0.2871553301811218\n",
            "[Training Epoch 73] Batch 4100, Loss 0.22883331775665283\n",
            "[Training Epoch 73] Batch 4200, Loss 0.2670983672142029\n",
            "[Training Epoch 73] Batch 4300, Loss 0.25007694959640503\n",
            "[Training Epoch 73] Batch 4400, Loss 0.2884645462036133\n",
            "[Training Epoch 73] Batch 4500, Loss 0.25656425952911377\n",
            "[Training Epoch 73] Batch 4600, Loss 0.27175700664520264\n",
            "[Training Epoch 73] Batch 4700, Loss 0.2506495714187622\n",
            "[Training Epoch 73] Batch 4800, Loss 0.2803075909614563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 73] HR = 0.6356, NDCG = 0.3682\n",
            "Epoch 74 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 74] Batch 0, Loss 0.27208298444747925\n",
            "[Training Epoch 74] Batch 100, Loss 0.2683822512626648\n",
            "[Training Epoch 74] Batch 200, Loss 0.2805674076080322\n",
            "[Training Epoch 74] Batch 300, Loss 0.2619422972202301\n",
            "[Training Epoch 74] Batch 400, Loss 0.2930852174758911\n",
            "[Training Epoch 74] Batch 500, Loss 0.2498517483472824\n",
            "[Training Epoch 74] Batch 600, Loss 0.2610127925872803\n",
            "[Training Epoch 74] Batch 700, Loss 0.25172704458236694\n",
            "[Training Epoch 74] Batch 800, Loss 0.25655436515808105\n",
            "[Training Epoch 74] Batch 900, Loss 0.2559663653373718\n",
            "[Training Epoch 74] Batch 1000, Loss 0.25376367568969727\n",
            "[Training Epoch 74] Batch 1100, Loss 0.2332141399383545\n",
            "[Training Epoch 74] Batch 1200, Loss 0.2549506425857544\n",
            "[Training Epoch 74] Batch 1300, Loss 0.26526153087615967\n",
            "[Training Epoch 74] Batch 1400, Loss 0.27761679887771606\n",
            "[Training Epoch 74] Batch 1500, Loss 0.2588684558868408\n",
            "[Training Epoch 74] Batch 1600, Loss 0.2561749219894409\n",
            "[Training Epoch 74] Batch 1700, Loss 0.27545028924942017\n",
            "[Training Epoch 74] Batch 1800, Loss 0.26827335357666016\n",
            "[Training Epoch 74] Batch 1900, Loss 0.2401793897151947\n",
            "[Training Epoch 74] Batch 2000, Loss 0.30555033683776855\n",
            "[Training Epoch 74] Batch 2100, Loss 0.2795857787132263\n",
            "[Training Epoch 74] Batch 2200, Loss 0.26745444536209106\n",
            "[Training Epoch 74] Batch 2300, Loss 0.2493744194507599\n",
            "[Training Epoch 74] Batch 2400, Loss 0.24755001068115234\n",
            "[Training Epoch 74] Batch 2500, Loss 0.24845458567142487\n",
            "[Training Epoch 74] Batch 2600, Loss 0.2936186194419861\n",
            "[Training Epoch 74] Batch 2700, Loss 0.24579490721225739\n",
            "[Training Epoch 74] Batch 2800, Loss 0.27690818905830383\n",
            "[Training Epoch 74] Batch 2900, Loss 0.28246229887008667\n",
            "[Training Epoch 74] Batch 3000, Loss 0.2681918740272522\n",
            "[Training Epoch 74] Batch 3100, Loss 0.2735121250152588\n",
            "[Training Epoch 74] Batch 3200, Loss 0.24390721321105957\n",
            "[Training Epoch 74] Batch 3300, Loss 0.27895912528038025\n",
            "[Training Epoch 74] Batch 3400, Loss 0.28478896617889404\n",
            "[Training Epoch 74] Batch 3500, Loss 0.27552270889282227\n",
            "[Training Epoch 74] Batch 3600, Loss 0.2842620015144348\n",
            "[Training Epoch 74] Batch 3700, Loss 0.28025245666503906\n",
            "[Training Epoch 74] Batch 3800, Loss 0.2636955976486206\n",
            "[Training Epoch 74] Batch 3900, Loss 0.2845933437347412\n",
            "[Training Epoch 74] Batch 4000, Loss 0.2831634283065796\n",
            "[Training Epoch 74] Batch 4100, Loss 0.2660392224788666\n",
            "[Training Epoch 74] Batch 4200, Loss 0.25262242555618286\n",
            "[Training Epoch 74] Batch 4300, Loss 0.25824904441833496\n",
            "[Training Epoch 74] Batch 4400, Loss 0.24748548865318298\n",
            "[Training Epoch 74] Batch 4500, Loss 0.2670190632343292\n",
            "[Training Epoch 74] Batch 4600, Loss 0.25119587779045105\n",
            "[Training Epoch 74] Batch 4700, Loss 0.257580041885376\n",
            "[Training Epoch 74] Batch 4800, Loss 0.2537841796875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 74] HR = 0.6358, NDCG = 0.3690\n",
            "Epoch 75 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 75] Batch 0, Loss 0.2681633234024048\n",
            "[Training Epoch 75] Batch 100, Loss 0.27864396572113037\n",
            "[Training Epoch 75] Batch 200, Loss 0.2800588011741638\n",
            "[Training Epoch 75] Batch 300, Loss 0.28481802344322205\n",
            "[Training Epoch 75] Batch 400, Loss 0.23926988244056702\n",
            "[Training Epoch 75] Batch 500, Loss 0.24501731991767883\n",
            "[Training Epoch 75] Batch 600, Loss 0.2673023045063019\n",
            "[Training Epoch 75] Batch 700, Loss 0.25611239671707153\n",
            "[Training Epoch 75] Batch 800, Loss 0.2873815894126892\n",
            "[Training Epoch 75] Batch 900, Loss 0.26038625836372375\n",
            "[Training Epoch 75] Batch 1000, Loss 0.26882171630859375\n",
            "[Training Epoch 75] Batch 1100, Loss 0.2721738815307617\n",
            "[Training Epoch 75] Batch 1200, Loss 0.27225011587142944\n",
            "[Training Epoch 75] Batch 1300, Loss 0.2830294966697693\n",
            "[Training Epoch 75] Batch 1400, Loss 0.25265634059906006\n",
            "[Training Epoch 75] Batch 1500, Loss 0.2714792788028717\n",
            "[Training Epoch 75] Batch 1600, Loss 0.2667767405509949\n",
            "[Training Epoch 75] Batch 1700, Loss 0.23776650428771973\n",
            "[Training Epoch 75] Batch 1800, Loss 0.275560200214386\n",
            "[Training Epoch 75] Batch 1900, Loss 0.24954527616500854\n",
            "[Training Epoch 75] Batch 2000, Loss 0.2645842134952545\n",
            "[Training Epoch 75] Batch 2100, Loss 0.24997740983963013\n",
            "[Training Epoch 75] Batch 2200, Loss 0.2567836046218872\n",
            "[Training Epoch 75] Batch 2300, Loss 0.2243017703294754\n",
            "[Training Epoch 75] Batch 2400, Loss 0.25911945104599\n",
            "[Training Epoch 75] Batch 2500, Loss 0.25248754024505615\n",
            "[Training Epoch 75] Batch 2600, Loss 0.25393468141555786\n",
            "[Training Epoch 75] Batch 2700, Loss 0.23839323222637177\n",
            "[Training Epoch 75] Batch 2800, Loss 0.26639509201049805\n",
            "[Training Epoch 75] Batch 2900, Loss 0.27082008123397827\n",
            "[Training Epoch 75] Batch 3000, Loss 0.2736034393310547\n",
            "[Training Epoch 75] Batch 3100, Loss 0.29091736674308777\n",
            "[Training Epoch 75] Batch 3200, Loss 0.279299259185791\n",
            "[Training Epoch 75] Batch 3300, Loss 0.24366778135299683\n",
            "[Training Epoch 75] Batch 3400, Loss 0.26852262020111084\n",
            "[Training Epoch 75] Batch 3500, Loss 0.2504994571208954\n",
            "[Training Epoch 75] Batch 3600, Loss 0.29228675365448\n",
            "[Training Epoch 75] Batch 3700, Loss 0.2813389301300049\n",
            "[Training Epoch 75] Batch 3800, Loss 0.2648216485977173\n",
            "[Training Epoch 75] Batch 3900, Loss 0.2926402986049652\n",
            "[Training Epoch 75] Batch 4000, Loss 0.2698162794113159\n",
            "[Training Epoch 75] Batch 4100, Loss 0.24881431460380554\n",
            "[Training Epoch 75] Batch 4200, Loss 0.26123902201652527\n",
            "[Training Epoch 75] Batch 4300, Loss 0.2783908247947693\n",
            "[Training Epoch 75] Batch 4400, Loss 0.25339198112487793\n",
            "[Training Epoch 75] Batch 4500, Loss 0.252653032541275\n",
            "[Training Epoch 75] Batch 4600, Loss 0.2546326220035553\n",
            "[Training Epoch 75] Batch 4700, Loss 0.254219651222229\n",
            "[Training Epoch 75] Batch 4800, Loss 0.26477113366127014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 75] HR = 0.6363, NDCG = 0.3683\n",
            "Epoch 76 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 76] Batch 0, Loss 0.2681666612625122\n",
            "[Training Epoch 76] Batch 100, Loss 0.2615303695201874\n",
            "[Training Epoch 76] Batch 200, Loss 0.2621954679489136\n",
            "[Training Epoch 76] Batch 300, Loss 0.24215367436408997\n",
            "[Training Epoch 76] Batch 400, Loss 0.2742699086666107\n",
            "[Training Epoch 76] Batch 500, Loss 0.25469470024108887\n",
            "[Training Epoch 76] Batch 600, Loss 0.24981878697872162\n",
            "[Training Epoch 76] Batch 700, Loss 0.23522844910621643\n",
            "[Training Epoch 76] Batch 800, Loss 0.256588876247406\n",
            "[Training Epoch 76] Batch 900, Loss 0.26107823848724365\n",
            "[Training Epoch 76] Batch 1000, Loss 0.24870170652866364\n",
            "[Training Epoch 76] Batch 1100, Loss 0.2765164375305176\n",
            "[Training Epoch 76] Batch 1200, Loss 0.2638126015663147\n",
            "[Training Epoch 76] Batch 1300, Loss 0.2611055076122284\n",
            "[Training Epoch 76] Batch 1400, Loss 0.2519066333770752\n",
            "[Training Epoch 76] Batch 1500, Loss 0.2912534773349762\n",
            "[Training Epoch 76] Batch 1600, Loss 0.23652905225753784\n",
            "[Training Epoch 76] Batch 1700, Loss 0.29040226340293884\n",
            "[Training Epoch 76] Batch 1800, Loss 0.24705560505390167\n",
            "[Training Epoch 76] Batch 1900, Loss 0.2603960335254669\n",
            "[Training Epoch 76] Batch 2000, Loss 0.2725570797920227\n",
            "[Training Epoch 76] Batch 2100, Loss 0.27919501066207886\n",
            "[Training Epoch 76] Batch 2200, Loss 0.2651017904281616\n",
            "[Training Epoch 76] Batch 2300, Loss 0.2864028215408325\n",
            "[Training Epoch 76] Batch 2400, Loss 0.27630072832107544\n",
            "[Training Epoch 76] Batch 2500, Loss 0.2702808976173401\n",
            "[Training Epoch 76] Batch 2600, Loss 0.2519632577896118\n",
            "[Training Epoch 76] Batch 2700, Loss 0.2540140748023987\n",
            "[Training Epoch 76] Batch 2800, Loss 0.2705203890800476\n",
            "[Training Epoch 76] Batch 2900, Loss 0.2719804644584656\n",
            "[Training Epoch 76] Batch 3000, Loss 0.2715826630592346\n",
            "[Training Epoch 76] Batch 3100, Loss 0.2843050956726074\n",
            "[Training Epoch 76] Batch 3200, Loss 0.238648921251297\n",
            "[Training Epoch 76] Batch 3300, Loss 0.24830380082130432\n",
            "[Training Epoch 76] Batch 3400, Loss 0.2840114235877991\n",
            "[Training Epoch 76] Batch 3500, Loss 0.24795565009117126\n",
            "[Training Epoch 76] Batch 3600, Loss 0.28699660301208496\n",
            "[Training Epoch 76] Batch 3700, Loss 0.22856399416923523\n",
            "[Training Epoch 76] Batch 3800, Loss 0.26802271604537964\n",
            "[Training Epoch 76] Batch 3900, Loss 0.24436402320861816\n",
            "[Training Epoch 76] Batch 4000, Loss 0.27918902039527893\n",
            "[Training Epoch 76] Batch 4100, Loss 0.2504279911518097\n",
            "[Training Epoch 76] Batch 4200, Loss 0.2526138722896576\n",
            "[Training Epoch 76] Batch 4300, Loss 0.2941495180130005\n",
            "[Training Epoch 76] Batch 4400, Loss 0.25864294171333313\n",
            "[Training Epoch 76] Batch 4500, Loss 0.3092091679573059\n",
            "[Training Epoch 76] Batch 4600, Loss 0.3128936290740967\n",
            "[Training Epoch 76] Batch 4700, Loss 0.2653413712978363\n",
            "[Training Epoch 76] Batch 4800, Loss 0.27339327335357666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 76] HR = 0.6371, NDCG = 0.3691\n",
            "Epoch 77 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 77] Batch 0, Loss 0.2742006778717041\n",
            "[Training Epoch 77] Batch 100, Loss 0.2755126655101776\n",
            "[Training Epoch 77] Batch 200, Loss 0.24996420741081238\n",
            "[Training Epoch 77] Batch 300, Loss 0.26371243596076965\n",
            "[Training Epoch 77] Batch 400, Loss 0.2739090323448181\n",
            "[Training Epoch 77] Batch 500, Loss 0.2694963216781616\n",
            "[Training Epoch 77] Batch 600, Loss 0.2889290452003479\n",
            "[Training Epoch 77] Batch 700, Loss 0.30259841680526733\n",
            "[Training Epoch 77] Batch 800, Loss 0.2573220729827881\n",
            "[Training Epoch 77] Batch 900, Loss 0.2879624366760254\n",
            "[Training Epoch 77] Batch 1000, Loss 0.26521915197372437\n",
            "[Training Epoch 77] Batch 1100, Loss 0.2172604352235794\n",
            "[Training Epoch 77] Batch 1200, Loss 0.2656914293766022\n",
            "[Training Epoch 77] Batch 1300, Loss 0.2693641781806946\n",
            "[Training Epoch 77] Batch 1400, Loss 0.24207201600074768\n",
            "[Training Epoch 77] Batch 1500, Loss 0.2374173104763031\n",
            "[Training Epoch 77] Batch 1600, Loss 0.25649869441986084\n",
            "[Training Epoch 77] Batch 1700, Loss 0.23876945674419403\n",
            "[Training Epoch 77] Batch 1800, Loss 0.24234804511070251\n",
            "[Training Epoch 77] Batch 1900, Loss 0.26521962881088257\n",
            "[Training Epoch 77] Batch 2000, Loss 0.26270467042922974\n",
            "[Training Epoch 77] Batch 2100, Loss 0.2590630352497101\n",
            "[Training Epoch 77] Batch 2200, Loss 0.27091801166534424\n",
            "[Training Epoch 77] Batch 2300, Loss 0.26406729221343994\n",
            "[Training Epoch 77] Batch 2400, Loss 0.2756749391555786\n",
            "[Training Epoch 77] Batch 2500, Loss 0.2866854965686798\n",
            "[Training Epoch 77] Batch 2600, Loss 0.27834439277648926\n",
            "[Training Epoch 77] Batch 2700, Loss 0.2845304608345032\n",
            "[Training Epoch 77] Batch 2800, Loss 0.2516413927078247\n",
            "[Training Epoch 77] Batch 2900, Loss 0.27732932567596436\n",
            "[Training Epoch 77] Batch 3000, Loss 0.27407681941986084\n",
            "[Training Epoch 77] Batch 3100, Loss 0.24404975771903992\n",
            "[Training Epoch 77] Batch 3200, Loss 0.2729831039905548\n",
            "[Training Epoch 77] Batch 3300, Loss 0.3017827272415161\n",
            "[Training Epoch 77] Batch 3400, Loss 0.26482436060905457\n",
            "[Training Epoch 77] Batch 3500, Loss 0.27274301648139954\n",
            "[Training Epoch 77] Batch 3600, Loss 0.2714032530784607\n",
            "[Training Epoch 77] Batch 3700, Loss 0.2624589800834656\n",
            "[Training Epoch 77] Batch 3800, Loss 0.2830859124660492\n",
            "[Training Epoch 77] Batch 3900, Loss 0.23881232738494873\n",
            "[Training Epoch 77] Batch 4000, Loss 0.23840318620204926\n",
            "[Training Epoch 77] Batch 4100, Loss 0.273409366607666\n",
            "[Training Epoch 77] Batch 4200, Loss 0.2584085464477539\n",
            "[Training Epoch 77] Batch 4300, Loss 0.2500097453594208\n",
            "[Training Epoch 77] Batch 4400, Loss 0.2895006537437439\n",
            "[Training Epoch 77] Batch 4500, Loss 0.27298158407211304\n",
            "[Training Epoch 77] Batch 4600, Loss 0.2760618329048157\n",
            "[Training Epoch 77] Batch 4700, Loss 0.2651374638080597\n",
            "[Training Epoch 77] Batch 4800, Loss 0.2418331503868103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 77] HR = 0.6386, NDCG = 0.3683\n",
            "Epoch 78 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 78] Batch 0, Loss 0.29476436972618103\n",
            "[Training Epoch 78] Batch 100, Loss 0.24145302176475525\n",
            "[Training Epoch 78] Batch 200, Loss 0.25242286920547485\n",
            "[Training Epoch 78] Batch 300, Loss 0.27380767464637756\n",
            "[Training Epoch 78] Batch 400, Loss 0.31120407581329346\n",
            "[Training Epoch 78] Batch 500, Loss 0.2565537691116333\n",
            "[Training Epoch 78] Batch 600, Loss 0.2802262306213379\n",
            "[Training Epoch 78] Batch 700, Loss 0.25763243436813354\n",
            "[Training Epoch 78] Batch 800, Loss 0.2542070746421814\n",
            "[Training Epoch 78] Batch 900, Loss 0.2549465596675873\n",
            "[Training Epoch 78] Batch 1000, Loss 0.2642894983291626\n",
            "[Training Epoch 78] Batch 1100, Loss 0.24945864081382751\n",
            "[Training Epoch 78] Batch 1200, Loss 0.2687133550643921\n",
            "[Training Epoch 78] Batch 1300, Loss 0.270612508058548\n",
            "[Training Epoch 78] Batch 1400, Loss 0.2648126482963562\n",
            "[Training Epoch 78] Batch 1500, Loss 0.2544046938419342\n",
            "[Training Epoch 78] Batch 1600, Loss 0.2354467809200287\n",
            "[Training Epoch 78] Batch 1700, Loss 0.2811322808265686\n",
            "[Training Epoch 78] Batch 1800, Loss 0.2843339741230011\n",
            "[Training Epoch 78] Batch 1900, Loss 0.2590283751487732\n",
            "[Training Epoch 78] Batch 2000, Loss 0.24026009440422058\n",
            "[Training Epoch 78] Batch 2100, Loss 0.2800716757774353\n",
            "[Training Epoch 78] Batch 2200, Loss 0.2501463294029236\n",
            "[Training Epoch 78] Batch 2300, Loss 0.2787349820137024\n",
            "[Training Epoch 78] Batch 2400, Loss 0.2823339104652405\n",
            "[Training Epoch 78] Batch 2500, Loss 0.25289076566696167\n",
            "[Training Epoch 78] Batch 2600, Loss 0.28330564498901367\n",
            "[Training Epoch 78] Batch 2700, Loss 0.283464252948761\n",
            "[Training Epoch 78] Batch 2800, Loss 0.2647593021392822\n",
            "[Training Epoch 78] Batch 2900, Loss 0.26714015007019043\n",
            "[Training Epoch 78] Batch 3000, Loss 0.21775707602500916\n",
            "[Training Epoch 78] Batch 3100, Loss 0.2834203541278839\n",
            "[Training Epoch 78] Batch 3200, Loss 0.24960654973983765\n",
            "[Training Epoch 78] Batch 3300, Loss 0.2671074867248535\n",
            "[Training Epoch 78] Batch 3400, Loss 0.25943511724472046\n",
            "[Training Epoch 78] Batch 3500, Loss 0.30483922362327576\n",
            "[Training Epoch 78] Batch 3600, Loss 0.27599456906318665\n",
            "[Training Epoch 78] Batch 3700, Loss 0.2822610139846802\n",
            "[Training Epoch 78] Batch 3800, Loss 0.2541903257369995\n",
            "[Training Epoch 78] Batch 3900, Loss 0.2837377190589905\n",
            "[Training Epoch 78] Batch 4000, Loss 0.26535966992378235\n",
            "[Training Epoch 78] Batch 4100, Loss 0.2607298195362091\n",
            "[Training Epoch 78] Batch 4200, Loss 0.23715710639953613\n",
            "[Training Epoch 78] Batch 4300, Loss 0.23806005716323853\n",
            "[Training Epoch 78] Batch 4400, Loss 0.2852346897125244\n",
            "[Training Epoch 78] Batch 4500, Loss 0.25204455852508545\n",
            "[Training Epoch 78] Batch 4600, Loss 0.28472065925598145\n",
            "[Training Epoch 78] Batch 4700, Loss 0.25037524104118347\n",
            "[Training Epoch 78] Batch 4800, Loss 0.24767765402793884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 78] HR = 0.6391, NDCG = 0.3709\n",
            "Epoch 79 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 79] Batch 0, Loss 0.243118017911911\n",
            "[Training Epoch 79] Batch 100, Loss 0.264204740524292\n",
            "[Training Epoch 79] Batch 200, Loss 0.2585713267326355\n",
            "[Training Epoch 79] Batch 300, Loss 0.26498839259147644\n",
            "[Training Epoch 79] Batch 400, Loss 0.27572935819625854\n",
            "[Training Epoch 79] Batch 500, Loss 0.2639439105987549\n",
            "[Training Epoch 79] Batch 600, Loss 0.28569164872169495\n",
            "[Training Epoch 79] Batch 700, Loss 0.2537347376346588\n",
            "[Training Epoch 79] Batch 800, Loss 0.26174280047416687\n",
            "[Training Epoch 79] Batch 900, Loss 0.24191291630268097\n",
            "[Training Epoch 79] Batch 1000, Loss 0.23534059524536133\n",
            "[Training Epoch 79] Batch 1100, Loss 0.25277769565582275\n",
            "[Training Epoch 79] Batch 1200, Loss 0.2499372661113739\n",
            "[Training Epoch 79] Batch 1300, Loss 0.25914493203163147\n",
            "[Training Epoch 79] Batch 1400, Loss 0.2908892035484314\n",
            "[Training Epoch 79] Batch 1500, Loss 0.2527247965335846\n",
            "[Training Epoch 79] Batch 1600, Loss 0.28806382417678833\n",
            "[Training Epoch 79] Batch 1700, Loss 0.27181077003479004\n",
            "[Training Epoch 79] Batch 1800, Loss 0.2436063140630722\n",
            "[Training Epoch 79] Batch 1900, Loss 0.2547999322414398\n",
            "[Training Epoch 79] Batch 2000, Loss 0.25966203212738037\n",
            "[Training Epoch 79] Batch 2100, Loss 0.24939888715744019\n",
            "[Training Epoch 79] Batch 2200, Loss 0.26893946528434753\n",
            "[Training Epoch 79] Batch 2300, Loss 0.2692166268825531\n",
            "[Training Epoch 79] Batch 2400, Loss 0.2540796399116516\n",
            "[Training Epoch 79] Batch 2500, Loss 0.2654147446155548\n",
            "[Training Epoch 79] Batch 2600, Loss 0.26464712619781494\n",
            "[Training Epoch 79] Batch 2700, Loss 0.25770533084869385\n",
            "[Training Epoch 79] Batch 2800, Loss 0.269031286239624\n",
            "[Training Epoch 79] Batch 2900, Loss 0.25325050950050354\n",
            "[Training Epoch 79] Batch 3000, Loss 0.29793524742126465\n",
            "[Training Epoch 79] Batch 3100, Loss 0.2572649121284485\n",
            "[Training Epoch 79] Batch 3200, Loss 0.2694432735443115\n",
            "[Training Epoch 79] Batch 3300, Loss 0.26003944873809814\n",
            "[Training Epoch 79] Batch 3400, Loss 0.2480672299861908\n",
            "[Training Epoch 79] Batch 3500, Loss 0.2505587339401245\n",
            "[Training Epoch 79] Batch 3600, Loss 0.2730293869972229\n",
            "[Training Epoch 79] Batch 3700, Loss 0.23906832933425903\n",
            "[Training Epoch 79] Batch 3800, Loss 0.2827155590057373\n",
            "[Training Epoch 79] Batch 3900, Loss 0.29265326261520386\n",
            "[Training Epoch 79] Batch 4000, Loss 0.2664252817630768\n",
            "[Training Epoch 79] Batch 4100, Loss 0.24197176098823547\n",
            "[Training Epoch 79] Batch 4200, Loss 0.2610377371311188\n",
            "[Training Epoch 79] Batch 4300, Loss 0.275562047958374\n",
            "[Training Epoch 79] Batch 4400, Loss 0.28491461277008057\n",
            "[Training Epoch 79] Batch 4500, Loss 0.2664162516593933\n",
            "[Training Epoch 79] Batch 4600, Loss 0.245465487241745\n",
            "[Training Epoch 79] Batch 4700, Loss 0.29835784435272217\n",
            "[Training Epoch 79] Batch 4800, Loss 0.25760912895202637\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 79] HR = 0.6387, NDCG = 0.3701\n",
            "Epoch 80 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 80] Batch 0, Loss 0.2730594873428345\n",
            "[Training Epoch 80] Batch 100, Loss 0.24984928965568542\n",
            "[Training Epoch 80] Batch 200, Loss 0.286750465631485\n",
            "[Training Epoch 80] Batch 300, Loss 0.30380141735076904\n",
            "[Training Epoch 80] Batch 400, Loss 0.2698154151439667\n",
            "[Training Epoch 80] Batch 500, Loss 0.2657204866409302\n",
            "[Training Epoch 80] Batch 600, Loss 0.2878098785877228\n",
            "[Training Epoch 80] Batch 700, Loss 0.2474493384361267\n",
            "[Training Epoch 80] Batch 800, Loss 0.2634773254394531\n",
            "[Training Epoch 80] Batch 900, Loss 0.27381715178489685\n",
            "[Training Epoch 80] Batch 1000, Loss 0.2781665325164795\n",
            "[Training Epoch 80] Batch 1100, Loss 0.26969268918037415\n",
            "[Training Epoch 80] Batch 1200, Loss 0.23888292908668518\n",
            "[Training Epoch 80] Batch 1300, Loss 0.25709110498428345\n",
            "[Training Epoch 80] Batch 1400, Loss 0.26276201009750366\n",
            "[Training Epoch 80] Batch 1500, Loss 0.26258793473243713\n",
            "[Training Epoch 80] Batch 1600, Loss 0.2511228024959564\n",
            "[Training Epoch 80] Batch 1700, Loss 0.2296108603477478\n",
            "[Training Epoch 80] Batch 1800, Loss 0.2692180275917053\n",
            "[Training Epoch 80] Batch 1900, Loss 0.25199294090270996\n",
            "[Training Epoch 80] Batch 2000, Loss 0.24746407568454742\n",
            "[Training Epoch 80] Batch 2100, Loss 0.2723385691642761\n",
            "[Training Epoch 80] Batch 2200, Loss 0.25978904962539673\n",
            "[Training Epoch 80] Batch 2300, Loss 0.24698548018932343\n",
            "[Training Epoch 80] Batch 2400, Loss 0.2560131549835205\n",
            "[Training Epoch 80] Batch 2500, Loss 0.28022462129592896\n",
            "[Training Epoch 80] Batch 2600, Loss 0.2740446925163269\n",
            "[Training Epoch 80] Batch 2700, Loss 0.2310742437839508\n",
            "[Training Epoch 80] Batch 2800, Loss 0.2682892382144928\n",
            "[Training Epoch 80] Batch 2900, Loss 0.2433634102344513\n",
            "[Training Epoch 80] Batch 3000, Loss 0.2557911276817322\n",
            "[Training Epoch 80] Batch 3100, Loss 0.2632128894329071\n",
            "[Training Epoch 80] Batch 3200, Loss 0.28958654403686523\n",
            "[Training Epoch 80] Batch 3300, Loss 0.2555372714996338\n",
            "[Training Epoch 80] Batch 3400, Loss 0.27764007449150085\n",
            "[Training Epoch 80] Batch 3500, Loss 0.2535528242588043\n",
            "[Training Epoch 80] Batch 3600, Loss 0.27324527502059937\n",
            "[Training Epoch 80] Batch 3700, Loss 0.2867169976234436\n",
            "[Training Epoch 80] Batch 3800, Loss 0.26465436816215515\n",
            "[Training Epoch 80] Batch 3900, Loss 0.2692628502845764\n",
            "[Training Epoch 80] Batch 4000, Loss 0.2535015940666199\n",
            "[Training Epoch 80] Batch 4100, Loss 0.2620090842247009\n",
            "[Training Epoch 80] Batch 4200, Loss 0.2603505849838257\n",
            "[Training Epoch 80] Batch 4300, Loss 0.2694692313671112\n",
            "[Training Epoch 80] Batch 4400, Loss 0.30702000856399536\n",
            "[Training Epoch 80] Batch 4500, Loss 0.29440581798553467\n",
            "[Training Epoch 80] Batch 4600, Loss 0.2605023980140686\n",
            "[Training Epoch 80] Batch 4700, Loss 0.2837991714477539\n",
            "[Training Epoch 80] Batch 4800, Loss 0.24104849994182587\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 80] HR = 0.6371, NDCG = 0.3691\n",
            "Epoch 81 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 81] Batch 0, Loss 0.2590091824531555\n",
            "[Training Epoch 81] Batch 100, Loss 0.25716692209243774\n",
            "[Training Epoch 81] Batch 200, Loss 0.25378894805908203\n",
            "[Training Epoch 81] Batch 300, Loss 0.2640705108642578\n",
            "[Training Epoch 81] Batch 400, Loss 0.2660857141017914\n",
            "[Training Epoch 81] Batch 500, Loss 0.22596776485443115\n",
            "[Training Epoch 81] Batch 600, Loss 0.25621819496154785\n",
            "[Training Epoch 81] Batch 700, Loss 0.2646092176437378\n",
            "[Training Epoch 81] Batch 800, Loss 0.2797063887119293\n",
            "[Training Epoch 81] Batch 900, Loss 0.2596663236618042\n",
            "[Training Epoch 81] Batch 1000, Loss 0.272189199924469\n",
            "[Training Epoch 81] Batch 1100, Loss 0.2773359417915344\n",
            "[Training Epoch 81] Batch 1200, Loss 0.2561897933483124\n",
            "[Training Epoch 81] Batch 1300, Loss 0.288030743598938\n",
            "[Training Epoch 81] Batch 1400, Loss 0.266814649105072\n",
            "[Training Epoch 81] Batch 1500, Loss 0.2636394500732422\n",
            "[Training Epoch 81] Batch 1600, Loss 0.2622425854206085\n",
            "[Training Epoch 81] Batch 1700, Loss 0.2613464295864105\n",
            "[Training Epoch 81] Batch 1800, Loss 0.26656365394592285\n",
            "[Training Epoch 81] Batch 1900, Loss 0.2741106152534485\n",
            "[Training Epoch 81] Batch 2000, Loss 0.2712223529815674\n",
            "[Training Epoch 81] Batch 2100, Loss 0.2802099287509918\n",
            "[Training Epoch 81] Batch 2200, Loss 0.28367358446121216\n",
            "[Training Epoch 81] Batch 2300, Loss 0.24415342509746552\n",
            "[Training Epoch 81] Batch 2400, Loss 0.2855456471443176\n",
            "[Training Epoch 81] Batch 2500, Loss 0.2586778402328491\n",
            "[Training Epoch 81] Batch 2600, Loss 0.26376765966415405\n",
            "[Training Epoch 81] Batch 2700, Loss 0.2700842618942261\n",
            "[Training Epoch 81] Batch 2800, Loss 0.26999595761299133\n",
            "[Training Epoch 81] Batch 2900, Loss 0.26564711332321167\n",
            "[Training Epoch 81] Batch 3000, Loss 0.26141226291656494\n",
            "[Training Epoch 81] Batch 3100, Loss 0.29648780822753906\n",
            "[Training Epoch 81] Batch 3200, Loss 0.28364166617393494\n",
            "[Training Epoch 81] Batch 3300, Loss 0.26817765831947327\n",
            "[Training Epoch 81] Batch 3400, Loss 0.25433552265167236\n",
            "[Training Epoch 81] Batch 3500, Loss 0.2615985870361328\n",
            "[Training Epoch 81] Batch 3600, Loss 0.25538545846939087\n",
            "[Training Epoch 81] Batch 3700, Loss 0.26469722390174866\n",
            "[Training Epoch 81] Batch 3800, Loss 0.24798724055290222\n",
            "[Training Epoch 81] Batch 3900, Loss 0.2798769772052765\n",
            "[Training Epoch 81] Batch 4000, Loss 0.2912130057811737\n",
            "[Training Epoch 81] Batch 4100, Loss 0.2557951807975769\n",
            "[Training Epoch 81] Batch 4200, Loss 0.26681503653526306\n",
            "[Training Epoch 81] Batch 4300, Loss 0.2863004207611084\n",
            "[Training Epoch 81] Batch 4400, Loss 0.2736763656139374\n",
            "[Training Epoch 81] Batch 4500, Loss 0.278635174036026\n",
            "[Training Epoch 81] Batch 4600, Loss 0.2531101107597351\n",
            "[Training Epoch 81] Batch 4700, Loss 0.273213267326355\n",
            "[Training Epoch 81] Batch 4800, Loss 0.27678683400154114\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 81] HR = 0.6377, NDCG = 0.3677\n",
            "Epoch 82 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 82] Batch 0, Loss 0.26961296796798706\n",
            "[Training Epoch 82] Batch 100, Loss 0.2534428834915161\n",
            "[Training Epoch 82] Batch 200, Loss 0.2537514567375183\n",
            "[Training Epoch 82] Batch 300, Loss 0.25286024808883667\n",
            "[Training Epoch 82] Batch 400, Loss 0.2621656358242035\n",
            "[Training Epoch 82] Batch 500, Loss 0.24307237565517426\n",
            "[Training Epoch 82] Batch 600, Loss 0.26782718300819397\n",
            "[Training Epoch 82] Batch 700, Loss 0.2783493101596832\n",
            "[Training Epoch 82] Batch 800, Loss 0.2398444563150406\n",
            "[Training Epoch 82] Batch 900, Loss 0.26406940817832947\n",
            "[Training Epoch 82] Batch 1000, Loss 0.2733433246612549\n",
            "[Training Epoch 82] Batch 1100, Loss 0.2342708259820938\n",
            "[Training Epoch 82] Batch 1200, Loss 0.28517991304397583\n",
            "[Training Epoch 82] Batch 1300, Loss 0.2901378571987152\n",
            "[Training Epoch 82] Batch 1400, Loss 0.24845430254936218\n",
            "[Training Epoch 82] Batch 1500, Loss 0.2678968608379364\n",
            "[Training Epoch 82] Batch 1600, Loss 0.24831682443618774\n",
            "[Training Epoch 82] Batch 1700, Loss 0.2694377303123474\n",
            "[Training Epoch 82] Batch 1800, Loss 0.22720018029212952\n",
            "[Training Epoch 82] Batch 1900, Loss 0.2669665813446045\n",
            "[Training Epoch 82] Batch 2000, Loss 0.29316961765289307\n",
            "[Training Epoch 82] Batch 2100, Loss 0.28268134593963623\n",
            "[Training Epoch 82] Batch 2200, Loss 0.25852280855178833\n",
            "[Training Epoch 82] Batch 2300, Loss 0.266859769821167\n",
            "[Training Epoch 82] Batch 2400, Loss 0.2583844065666199\n",
            "[Training Epoch 82] Batch 2500, Loss 0.2730691432952881\n",
            "[Training Epoch 82] Batch 2600, Loss 0.26122015714645386\n",
            "[Training Epoch 82] Batch 2700, Loss 0.2686864137649536\n",
            "[Training Epoch 82] Batch 2800, Loss 0.27215826511383057\n",
            "[Training Epoch 82] Batch 2900, Loss 0.2691628038883209\n",
            "[Training Epoch 82] Batch 3000, Loss 0.2554827928543091\n",
            "[Training Epoch 82] Batch 3100, Loss 0.2797688841819763\n",
            "[Training Epoch 82] Batch 3200, Loss 0.27324414253234863\n",
            "[Training Epoch 82] Batch 3300, Loss 0.26633715629577637\n",
            "[Training Epoch 82] Batch 3400, Loss 0.2485380917787552\n",
            "[Training Epoch 82] Batch 3500, Loss 0.2638826370239258\n",
            "[Training Epoch 82] Batch 3600, Loss 0.26172521710395813\n",
            "[Training Epoch 82] Batch 3700, Loss 0.26165971159935\n",
            "[Training Epoch 82] Batch 3800, Loss 0.26506954431533813\n",
            "[Training Epoch 82] Batch 3900, Loss 0.25923576951026917\n",
            "[Training Epoch 82] Batch 4000, Loss 0.2724993824958801\n",
            "[Training Epoch 82] Batch 4100, Loss 0.25523823499679565\n",
            "[Training Epoch 82] Batch 4200, Loss 0.2666569650173187\n",
            "[Training Epoch 82] Batch 4300, Loss 0.27107420563697815\n",
            "[Training Epoch 82] Batch 4400, Loss 0.29196763038635254\n",
            "[Training Epoch 82] Batch 4500, Loss 0.2762499451637268\n",
            "[Training Epoch 82] Batch 4600, Loss 0.2708550691604614\n",
            "[Training Epoch 82] Batch 4700, Loss 0.28294163942337036\n",
            "[Training Epoch 82] Batch 4800, Loss 0.25756001472473145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 82] HR = 0.6392, NDCG = 0.3684\n",
            "Epoch 83 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 83] Batch 0, Loss 0.25442343950271606\n",
            "[Training Epoch 83] Batch 100, Loss 0.26205316185951233\n",
            "[Training Epoch 83] Batch 200, Loss 0.2714022994041443\n",
            "[Training Epoch 83] Batch 300, Loss 0.28178882598876953\n",
            "[Training Epoch 83] Batch 400, Loss 0.2626460790634155\n",
            "[Training Epoch 83] Batch 500, Loss 0.26422587037086487\n",
            "[Training Epoch 83] Batch 600, Loss 0.2518008351325989\n",
            "[Training Epoch 83] Batch 700, Loss 0.238512322306633\n",
            "[Training Epoch 83] Batch 800, Loss 0.24829310178756714\n",
            "[Training Epoch 83] Batch 900, Loss 0.26283127069473267\n",
            "[Training Epoch 83] Batch 1000, Loss 0.27336472272872925\n",
            "[Training Epoch 83] Batch 1100, Loss 0.2746526300907135\n",
            "[Training Epoch 83] Batch 1200, Loss 0.29881036281585693\n",
            "[Training Epoch 83] Batch 1300, Loss 0.26767295598983765\n",
            "[Training Epoch 83] Batch 1400, Loss 0.26211071014404297\n",
            "[Training Epoch 83] Batch 1500, Loss 0.2477874457836151\n",
            "[Training Epoch 83] Batch 1600, Loss 0.2636907994747162\n",
            "[Training Epoch 83] Batch 1700, Loss 0.2677518129348755\n",
            "[Training Epoch 83] Batch 1800, Loss 0.2939670979976654\n",
            "[Training Epoch 83] Batch 1900, Loss 0.2493165135383606\n",
            "[Training Epoch 83] Batch 2000, Loss 0.27060070633888245\n",
            "[Training Epoch 83] Batch 2100, Loss 0.23931977152824402\n",
            "[Training Epoch 83] Batch 2200, Loss 0.2608258128166199\n",
            "[Training Epoch 83] Batch 2300, Loss 0.25258558988571167\n",
            "[Training Epoch 83] Batch 2400, Loss 0.28122568130493164\n",
            "[Training Epoch 83] Batch 2500, Loss 0.2604053020477295\n",
            "[Training Epoch 83] Batch 2600, Loss 0.2966359853744507\n",
            "[Training Epoch 83] Batch 2700, Loss 0.2656968832015991\n",
            "[Training Epoch 83] Batch 2800, Loss 0.2793205976486206\n",
            "[Training Epoch 83] Batch 2900, Loss 0.2454148232936859\n",
            "[Training Epoch 83] Batch 3000, Loss 0.24000315368175507\n",
            "[Training Epoch 83] Batch 3100, Loss 0.2783287763595581\n",
            "[Training Epoch 83] Batch 3200, Loss 0.24600619077682495\n",
            "[Training Epoch 83] Batch 3300, Loss 0.2514059245586395\n",
            "[Training Epoch 83] Batch 3400, Loss 0.2806236445903778\n",
            "[Training Epoch 83] Batch 3500, Loss 0.2604886293411255\n",
            "[Training Epoch 83] Batch 3600, Loss 0.2633644938468933\n",
            "[Training Epoch 83] Batch 3700, Loss 0.2605590224266052\n",
            "[Training Epoch 83] Batch 3800, Loss 0.2898910641670227\n",
            "[Training Epoch 83] Batch 3900, Loss 0.24696850776672363\n",
            "[Training Epoch 83] Batch 4000, Loss 0.24631965160369873\n",
            "[Training Epoch 83] Batch 4100, Loss 0.2817532420158386\n",
            "[Training Epoch 83] Batch 4200, Loss 0.28402960300445557\n",
            "[Training Epoch 83] Batch 4300, Loss 0.2699340581893921\n",
            "[Training Epoch 83] Batch 4400, Loss 0.2750406563282013\n",
            "[Training Epoch 83] Batch 4500, Loss 0.2919155955314636\n",
            "[Training Epoch 83] Batch 4600, Loss 0.271298348903656\n",
            "[Training Epoch 83] Batch 4700, Loss 0.29149967432022095\n",
            "[Training Epoch 83] Batch 4800, Loss 0.26944783329963684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 83] HR = 0.6364, NDCG = 0.3688\n",
            "Epoch 84 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 84] Batch 0, Loss 0.27077099680900574\n",
            "[Training Epoch 84] Batch 100, Loss 0.25953948497772217\n",
            "[Training Epoch 84] Batch 200, Loss 0.28801587224006653\n",
            "[Training Epoch 84] Batch 300, Loss 0.2342153638601303\n",
            "[Training Epoch 84] Batch 400, Loss 0.24338990449905396\n",
            "[Training Epoch 84] Batch 500, Loss 0.2542421817779541\n",
            "[Training Epoch 84] Batch 600, Loss 0.26759058237075806\n",
            "[Training Epoch 84] Batch 700, Loss 0.2513183057308197\n",
            "[Training Epoch 84] Batch 800, Loss 0.27792972326278687\n",
            "[Training Epoch 84] Batch 900, Loss 0.2701408863067627\n",
            "[Training Epoch 84] Batch 1000, Loss 0.27074742317199707\n",
            "[Training Epoch 84] Batch 1100, Loss 0.2935482859611511\n",
            "[Training Epoch 84] Batch 1200, Loss 0.21739046275615692\n",
            "[Training Epoch 84] Batch 1300, Loss 0.2785249352455139\n",
            "[Training Epoch 84] Batch 1400, Loss 0.23676081001758575\n",
            "[Training Epoch 84] Batch 1500, Loss 0.26505500078201294\n",
            "[Training Epoch 84] Batch 1600, Loss 0.24864187836647034\n",
            "[Training Epoch 84] Batch 1700, Loss 0.275859534740448\n",
            "[Training Epoch 84] Batch 1800, Loss 0.2633078098297119\n",
            "[Training Epoch 84] Batch 1900, Loss 0.2630577087402344\n",
            "[Training Epoch 84] Batch 2000, Loss 0.27651432156562805\n",
            "[Training Epoch 84] Batch 2100, Loss 0.2597380578517914\n",
            "[Training Epoch 84] Batch 2200, Loss 0.2420411854982376\n",
            "[Training Epoch 84] Batch 2300, Loss 0.279154896736145\n",
            "[Training Epoch 84] Batch 2400, Loss 0.28415220975875854\n",
            "[Training Epoch 84] Batch 2500, Loss 0.2802566885948181\n",
            "[Training Epoch 84] Batch 2600, Loss 0.2667345106601715\n",
            "[Training Epoch 84] Batch 2700, Loss 0.2830456495285034\n",
            "[Training Epoch 84] Batch 2800, Loss 0.24888616800308228\n",
            "[Training Epoch 84] Batch 2900, Loss 0.2781004011631012\n",
            "[Training Epoch 84] Batch 3000, Loss 0.23465196788311005\n",
            "[Training Epoch 84] Batch 3100, Loss 0.23338264226913452\n",
            "[Training Epoch 84] Batch 3200, Loss 0.2540048062801361\n",
            "[Training Epoch 84] Batch 3300, Loss 0.24029640853405\n",
            "[Training Epoch 84] Batch 3400, Loss 0.2578725218772888\n",
            "[Training Epoch 84] Batch 3500, Loss 0.28591352701187134\n",
            "[Training Epoch 84] Batch 3600, Loss 0.26397618651390076\n",
            "[Training Epoch 84] Batch 3700, Loss 0.279646635055542\n",
            "[Training Epoch 84] Batch 3800, Loss 0.28374606370925903\n",
            "[Training Epoch 84] Batch 3900, Loss 0.2666319012641907\n",
            "[Training Epoch 84] Batch 4000, Loss 0.26237624883651733\n",
            "[Training Epoch 84] Batch 4100, Loss 0.2509975731372833\n",
            "[Training Epoch 84] Batch 4200, Loss 0.2466152459383011\n",
            "[Training Epoch 84] Batch 4300, Loss 0.2670404613018036\n",
            "[Training Epoch 84] Batch 4400, Loss 0.2506885230541229\n",
            "[Training Epoch 84] Batch 4500, Loss 0.2449578046798706\n",
            "[Training Epoch 84] Batch 4600, Loss 0.27446281909942627\n",
            "[Training Epoch 84] Batch 4700, Loss 0.2640419602394104\n",
            "[Training Epoch 84] Batch 4800, Loss 0.26259103417396545\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 84] HR = 0.6349, NDCG = 0.3674\n",
            "Epoch 85 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 85] Batch 0, Loss 0.3083983063697815\n",
            "[Training Epoch 85] Batch 100, Loss 0.27024877071380615\n",
            "[Training Epoch 85] Batch 200, Loss 0.2705840766429901\n",
            "[Training Epoch 85] Batch 300, Loss 0.2900546193122864\n",
            "[Training Epoch 85] Batch 400, Loss 0.22590720653533936\n",
            "[Training Epoch 85] Batch 500, Loss 0.2322324812412262\n",
            "[Training Epoch 85] Batch 600, Loss 0.26490166783332825\n",
            "[Training Epoch 85] Batch 700, Loss 0.24532970786094666\n",
            "[Training Epoch 85] Batch 800, Loss 0.2679014801979065\n",
            "[Training Epoch 85] Batch 900, Loss 0.25335168838500977\n",
            "[Training Epoch 85] Batch 1000, Loss 0.2881127595901489\n",
            "[Training Epoch 85] Batch 1100, Loss 0.26214170455932617\n",
            "[Training Epoch 85] Batch 1200, Loss 0.26985305547714233\n",
            "[Training Epoch 85] Batch 1300, Loss 0.24917906522750854\n",
            "[Training Epoch 85] Batch 1400, Loss 0.29964351654052734\n",
            "[Training Epoch 85] Batch 1500, Loss 0.25507503747940063\n",
            "[Training Epoch 85] Batch 1600, Loss 0.2750300168991089\n",
            "[Training Epoch 85] Batch 1700, Loss 0.27178215980529785\n",
            "[Training Epoch 85] Batch 1800, Loss 0.27799510955810547\n",
            "[Training Epoch 85] Batch 1900, Loss 0.27901792526245117\n",
            "[Training Epoch 85] Batch 2000, Loss 0.2582109570503235\n",
            "[Training Epoch 85] Batch 2100, Loss 0.254026859998703\n",
            "[Training Epoch 85] Batch 2200, Loss 0.2921540439128876\n",
            "[Training Epoch 85] Batch 2300, Loss 0.25603002309799194\n",
            "[Training Epoch 85] Batch 2400, Loss 0.2793682813644409\n",
            "[Training Epoch 85] Batch 2500, Loss 0.2610507011413574\n",
            "[Training Epoch 85] Batch 2600, Loss 0.24117271602153778\n",
            "[Training Epoch 85] Batch 2700, Loss 0.2900579869747162\n",
            "[Training Epoch 85] Batch 2800, Loss 0.23931658267974854\n",
            "[Training Epoch 85] Batch 2900, Loss 0.2634945511817932\n",
            "[Training Epoch 85] Batch 3000, Loss 0.2390548288822174\n",
            "[Training Epoch 85] Batch 3100, Loss 0.267150342464447\n",
            "[Training Epoch 85] Batch 3200, Loss 0.2683563232421875\n",
            "[Training Epoch 85] Batch 3300, Loss 0.27468281984329224\n",
            "[Training Epoch 85] Batch 3400, Loss 0.28438547253608704\n",
            "[Training Epoch 85] Batch 3500, Loss 0.2636573314666748\n",
            "[Training Epoch 85] Batch 3600, Loss 0.24214836955070496\n",
            "[Training Epoch 85] Batch 3700, Loss 0.26574528217315674\n",
            "[Training Epoch 85] Batch 3800, Loss 0.26301729679107666\n",
            "[Training Epoch 85] Batch 3900, Loss 0.29529276490211487\n",
            "[Training Epoch 85] Batch 4000, Loss 0.25681114196777344\n",
            "[Training Epoch 85] Batch 4100, Loss 0.2476576715707779\n",
            "[Training Epoch 85] Batch 4200, Loss 0.28474873304367065\n",
            "[Training Epoch 85] Batch 4300, Loss 0.23766253888607025\n",
            "[Training Epoch 85] Batch 4400, Loss 0.25173431634902954\n",
            "[Training Epoch 85] Batch 4500, Loss 0.24121060967445374\n",
            "[Training Epoch 85] Batch 4600, Loss 0.2673547863960266\n",
            "[Training Epoch 85] Batch 4700, Loss 0.26397180557250977\n",
            "[Training Epoch 85] Batch 4800, Loss 0.2771850526332855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 85] HR = 0.6366, NDCG = 0.3680\n",
            "Epoch 86 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 86] Batch 0, Loss 0.2713313698768616\n",
            "[Training Epoch 86] Batch 100, Loss 0.26514554023742676\n",
            "[Training Epoch 86] Batch 200, Loss 0.2651597261428833\n",
            "[Training Epoch 86] Batch 300, Loss 0.29794323444366455\n",
            "[Training Epoch 86] Batch 400, Loss 0.2425895482301712\n",
            "[Training Epoch 86] Batch 500, Loss 0.2753154933452606\n",
            "[Training Epoch 86] Batch 600, Loss 0.26770344376564026\n",
            "[Training Epoch 86] Batch 700, Loss 0.28401315212249756\n",
            "[Training Epoch 86] Batch 800, Loss 0.2572292685508728\n",
            "[Training Epoch 86] Batch 900, Loss 0.28120946884155273\n",
            "[Training Epoch 86] Batch 1000, Loss 0.25194883346557617\n",
            "[Training Epoch 86] Batch 1100, Loss 0.27561378479003906\n",
            "[Training Epoch 86] Batch 1200, Loss 0.2576577067375183\n",
            "[Training Epoch 86] Batch 1300, Loss 0.25298529863357544\n",
            "[Training Epoch 86] Batch 1400, Loss 0.25023066997528076\n",
            "[Training Epoch 86] Batch 1500, Loss 0.27439484000205994\n",
            "[Training Epoch 86] Batch 1600, Loss 0.2528504729270935\n",
            "[Training Epoch 86] Batch 1700, Loss 0.2764762341976166\n",
            "[Training Epoch 86] Batch 1800, Loss 0.2518937885761261\n",
            "[Training Epoch 86] Batch 1900, Loss 0.2930709719657898\n",
            "[Training Epoch 86] Batch 2000, Loss 0.26817482709884644\n",
            "[Training Epoch 86] Batch 2100, Loss 0.2674795389175415\n",
            "[Training Epoch 86] Batch 2200, Loss 0.2520013153553009\n",
            "[Training Epoch 86] Batch 2300, Loss 0.26958268880844116\n",
            "[Training Epoch 86] Batch 2400, Loss 0.2500155568122864\n",
            "[Training Epoch 86] Batch 2500, Loss 0.2820163369178772\n",
            "[Training Epoch 86] Batch 2600, Loss 0.23244833946228027\n",
            "[Training Epoch 86] Batch 2700, Loss 0.27456533908843994\n",
            "[Training Epoch 86] Batch 2800, Loss 0.2513731122016907\n",
            "[Training Epoch 86] Batch 2900, Loss 0.2668488621711731\n",
            "[Training Epoch 86] Batch 3000, Loss 0.2553013563156128\n",
            "[Training Epoch 86] Batch 3100, Loss 0.27655935287475586\n",
            "[Training Epoch 86] Batch 3200, Loss 0.25930118560791016\n",
            "[Training Epoch 86] Batch 3300, Loss 0.25778746604919434\n",
            "[Training Epoch 86] Batch 3400, Loss 0.2671750485897064\n",
            "[Training Epoch 86] Batch 3500, Loss 0.256084144115448\n",
            "[Training Epoch 86] Batch 3600, Loss 0.24320146441459656\n",
            "[Training Epoch 86] Batch 3700, Loss 0.25590914487838745\n",
            "[Training Epoch 86] Batch 3800, Loss 0.2865521311759949\n",
            "[Training Epoch 86] Batch 3900, Loss 0.27987295389175415\n",
            "[Training Epoch 86] Batch 4000, Loss 0.25460994243621826\n",
            "[Training Epoch 86] Batch 4100, Loss 0.28809142112731934\n",
            "[Training Epoch 86] Batch 4200, Loss 0.23603299260139465\n",
            "[Training Epoch 86] Batch 4300, Loss 0.26259031891822815\n",
            "[Training Epoch 86] Batch 4400, Loss 0.2548690438270569\n",
            "[Training Epoch 86] Batch 4500, Loss 0.27323710918426514\n",
            "[Training Epoch 86] Batch 4600, Loss 0.2856411635875702\n",
            "[Training Epoch 86] Batch 4700, Loss 0.25333455204963684\n",
            "[Training Epoch 86] Batch 4800, Loss 0.2491515427827835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 86] HR = 0.6373, NDCG = 0.3675\n",
            "Epoch 87 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 87] Batch 0, Loss 0.27417606115341187\n",
            "[Training Epoch 87] Batch 100, Loss 0.266227662563324\n",
            "[Training Epoch 87] Batch 200, Loss 0.2527265250682831\n",
            "[Training Epoch 87] Batch 300, Loss 0.2776423692703247\n",
            "[Training Epoch 87] Batch 400, Loss 0.24380144476890564\n",
            "[Training Epoch 87] Batch 500, Loss 0.2530863285064697\n",
            "[Training Epoch 87] Batch 600, Loss 0.25688397884368896\n",
            "[Training Epoch 87] Batch 700, Loss 0.25134003162384033\n",
            "[Training Epoch 87] Batch 800, Loss 0.2645840048789978\n",
            "[Training Epoch 87] Batch 900, Loss 0.2665703296661377\n",
            "[Training Epoch 87] Batch 1000, Loss 0.2935792803764343\n",
            "[Training Epoch 87] Batch 1100, Loss 0.2653507590293884\n",
            "[Training Epoch 87] Batch 1200, Loss 0.2674175500869751\n",
            "[Training Epoch 87] Batch 1300, Loss 0.27018851041793823\n",
            "[Training Epoch 87] Batch 1400, Loss 0.2609792947769165\n",
            "[Training Epoch 87] Batch 1500, Loss 0.24704881012439728\n",
            "[Training Epoch 87] Batch 1600, Loss 0.26396968960762024\n",
            "[Training Epoch 87] Batch 1700, Loss 0.2840246856212616\n",
            "[Training Epoch 87] Batch 1800, Loss 0.26005637645721436\n",
            "[Training Epoch 87] Batch 1900, Loss 0.2703506648540497\n",
            "[Training Epoch 87] Batch 2000, Loss 0.2737080156803131\n",
            "[Training Epoch 87] Batch 2100, Loss 0.2848811745643616\n",
            "[Training Epoch 87] Batch 2200, Loss 0.25685441493988037\n",
            "[Training Epoch 87] Batch 2300, Loss 0.2873793840408325\n",
            "[Training Epoch 87] Batch 2400, Loss 0.2715260982513428\n",
            "[Training Epoch 87] Batch 2500, Loss 0.27436965703964233\n",
            "[Training Epoch 87] Batch 2600, Loss 0.2749921381473541\n",
            "[Training Epoch 87] Batch 2700, Loss 0.2762186527252197\n",
            "[Training Epoch 87] Batch 2800, Loss 0.27273672819137573\n",
            "[Training Epoch 87] Batch 2900, Loss 0.24019919335842133\n",
            "[Training Epoch 87] Batch 3000, Loss 0.22191298007965088\n",
            "[Training Epoch 87] Batch 3100, Loss 0.2801111340522766\n",
            "[Training Epoch 87] Batch 3200, Loss 0.24578098952770233\n",
            "[Training Epoch 87] Batch 3300, Loss 0.2658892869949341\n",
            "[Training Epoch 87] Batch 3400, Loss 0.26627734303474426\n",
            "[Training Epoch 87] Batch 3500, Loss 0.257791668176651\n",
            "[Training Epoch 87] Batch 3600, Loss 0.2360377013683319\n",
            "[Training Epoch 87] Batch 3700, Loss 0.26656606793403625\n",
            "[Training Epoch 87] Batch 3800, Loss 0.24890831112861633\n",
            "[Training Epoch 87] Batch 3900, Loss 0.25209468603134155\n",
            "[Training Epoch 87] Batch 4000, Loss 0.2572961449623108\n",
            "[Training Epoch 87] Batch 4100, Loss 0.2879291772842407\n",
            "[Training Epoch 87] Batch 4200, Loss 0.22306285798549652\n",
            "[Training Epoch 87] Batch 4300, Loss 0.29581761360168457\n",
            "[Training Epoch 87] Batch 4400, Loss 0.25717806816101074\n",
            "[Training Epoch 87] Batch 4500, Loss 0.26602160930633545\n",
            "[Training Epoch 87] Batch 4600, Loss 0.2525692582130432\n",
            "[Training Epoch 87] Batch 4700, Loss 0.2857917547225952\n",
            "[Training Epoch 87] Batch 4800, Loss 0.27201560139656067\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 87] HR = 0.6382, NDCG = 0.3696\n",
            "Epoch 88 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 88] Batch 0, Loss 0.2617730498313904\n",
            "[Training Epoch 88] Batch 100, Loss 0.25078755617141724\n",
            "[Training Epoch 88] Batch 200, Loss 0.24846218526363373\n",
            "[Training Epoch 88] Batch 300, Loss 0.25697433948516846\n",
            "[Training Epoch 88] Batch 400, Loss 0.27597177028656006\n",
            "[Training Epoch 88] Batch 500, Loss 0.2722451686859131\n",
            "[Training Epoch 88] Batch 600, Loss 0.25095289945602417\n",
            "[Training Epoch 88] Batch 700, Loss 0.2643758952617645\n",
            "[Training Epoch 88] Batch 800, Loss 0.23904423415660858\n",
            "[Training Epoch 88] Batch 900, Loss 0.2526578903198242\n",
            "[Training Epoch 88] Batch 1000, Loss 0.2762150764465332\n",
            "[Training Epoch 88] Batch 1100, Loss 0.24442942440509796\n",
            "[Training Epoch 88] Batch 1200, Loss 0.2747790515422821\n",
            "[Training Epoch 88] Batch 1300, Loss 0.2628898024559021\n",
            "[Training Epoch 88] Batch 1400, Loss 0.2880898118019104\n",
            "[Training Epoch 88] Batch 1500, Loss 0.23753923177719116\n",
            "[Training Epoch 88] Batch 1600, Loss 0.25827544927597046\n",
            "[Training Epoch 88] Batch 1700, Loss 0.2746197581291199\n",
            "[Training Epoch 88] Batch 1800, Loss 0.2647217810153961\n",
            "[Training Epoch 88] Batch 1900, Loss 0.24970513582229614\n",
            "[Training Epoch 88] Batch 2000, Loss 0.249180868268013\n",
            "[Training Epoch 88] Batch 2100, Loss 0.25881659984588623\n",
            "[Training Epoch 88] Batch 2200, Loss 0.24485164880752563\n",
            "[Training Epoch 88] Batch 2300, Loss 0.24275431036949158\n",
            "[Training Epoch 88] Batch 2400, Loss 0.263219952583313\n",
            "[Training Epoch 88] Batch 2500, Loss 0.24099931120872498\n",
            "[Training Epoch 88] Batch 2600, Loss 0.29684293270111084\n",
            "[Training Epoch 88] Batch 2700, Loss 0.22841686010360718\n",
            "[Training Epoch 88] Batch 2800, Loss 0.3111696243286133\n",
            "[Training Epoch 88] Batch 2900, Loss 0.27815937995910645\n",
            "[Training Epoch 88] Batch 3000, Loss 0.2798088788986206\n",
            "[Training Epoch 88] Batch 3100, Loss 0.2830060124397278\n",
            "[Training Epoch 88] Batch 3200, Loss 0.2812427878379822\n",
            "[Training Epoch 88] Batch 3300, Loss 0.2680954933166504\n",
            "[Training Epoch 88] Batch 3400, Loss 0.26950740814208984\n",
            "[Training Epoch 88] Batch 3500, Loss 0.24400672316551208\n",
            "[Training Epoch 88] Batch 3600, Loss 0.28254663944244385\n",
            "[Training Epoch 88] Batch 3700, Loss 0.25795209407806396\n",
            "[Training Epoch 88] Batch 3800, Loss 0.26171639561653137\n",
            "[Training Epoch 88] Batch 3900, Loss 0.2654426693916321\n",
            "[Training Epoch 88] Batch 4000, Loss 0.264707088470459\n",
            "[Training Epoch 88] Batch 4100, Loss 0.24053838849067688\n",
            "[Training Epoch 88] Batch 4200, Loss 0.2623922526836395\n",
            "[Training Epoch 88] Batch 4300, Loss 0.2406798154115677\n",
            "[Training Epoch 88] Batch 4400, Loss 0.23759455978870392\n",
            "[Training Epoch 88] Batch 4500, Loss 0.2680022716522217\n",
            "[Training Epoch 88] Batch 4600, Loss 0.2834232449531555\n",
            "[Training Epoch 88] Batch 4700, Loss 0.2534206807613373\n",
            "[Training Epoch 88] Batch 4800, Loss 0.2587164342403412\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 88] HR = 0.6366, NDCG = 0.3683\n",
            "Epoch 89 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 89] Batch 0, Loss 0.254994660615921\n",
            "[Training Epoch 89] Batch 100, Loss 0.26956358551979065\n",
            "[Training Epoch 89] Batch 200, Loss 0.24688959121704102\n",
            "[Training Epoch 89] Batch 300, Loss 0.23362796008586884\n",
            "[Training Epoch 89] Batch 400, Loss 0.24878139793872833\n",
            "[Training Epoch 89] Batch 500, Loss 0.236044779419899\n",
            "[Training Epoch 89] Batch 600, Loss 0.2567482590675354\n",
            "[Training Epoch 89] Batch 700, Loss 0.27442821860313416\n",
            "[Training Epoch 89] Batch 800, Loss 0.26207756996154785\n",
            "[Training Epoch 89] Batch 900, Loss 0.2781796455383301\n",
            "[Training Epoch 89] Batch 1000, Loss 0.2491617202758789\n",
            "[Training Epoch 89] Batch 1100, Loss 0.2418970912694931\n",
            "[Training Epoch 89] Batch 1200, Loss 0.2643923759460449\n",
            "[Training Epoch 89] Batch 1300, Loss 0.26344987750053406\n",
            "[Training Epoch 89] Batch 1400, Loss 0.2580391466617584\n",
            "[Training Epoch 89] Batch 1500, Loss 0.26290208101272583\n",
            "[Training Epoch 89] Batch 1600, Loss 0.282292902469635\n",
            "[Training Epoch 89] Batch 1700, Loss 0.27814778685569763\n",
            "[Training Epoch 89] Batch 1800, Loss 0.2874819040298462\n",
            "[Training Epoch 89] Batch 1900, Loss 0.27372539043426514\n",
            "[Training Epoch 89] Batch 2000, Loss 0.23579531908035278\n",
            "[Training Epoch 89] Batch 2100, Loss 0.24119976162910461\n",
            "[Training Epoch 89] Batch 2200, Loss 0.24370157718658447\n",
            "[Training Epoch 89] Batch 2300, Loss 0.26482221484184265\n",
            "[Training Epoch 89] Batch 2400, Loss 0.24576106667518616\n",
            "[Training Epoch 89] Batch 2500, Loss 0.2575995922088623\n",
            "[Training Epoch 89] Batch 2600, Loss 0.27043119072914124\n",
            "[Training Epoch 89] Batch 2700, Loss 0.25235283374786377\n",
            "[Training Epoch 89] Batch 2800, Loss 0.2571708559989929\n",
            "[Training Epoch 89] Batch 2900, Loss 0.26762712001800537\n",
            "[Training Epoch 89] Batch 3000, Loss 0.24395152926445007\n",
            "[Training Epoch 89] Batch 3100, Loss 0.2658521234989166\n",
            "[Training Epoch 89] Batch 3200, Loss 0.28723645210266113\n",
            "[Training Epoch 89] Batch 3300, Loss 0.25390321016311646\n",
            "[Training Epoch 89] Batch 3400, Loss 0.2618069052696228\n",
            "[Training Epoch 89] Batch 3500, Loss 0.26565128564834595\n",
            "[Training Epoch 89] Batch 3600, Loss 0.26785439252853394\n",
            "[Training Epoch 89] Batch 3700, Loss 0.258550226688385\n",
            "[Training Epoch 89] Batch 3800, Loss 0.27757132053375244\n",
            "[Training Epoch 89] Batch 3900, Loss 0.28477686643600464\n",
            "[Training Epoch 89] Batch 4000, Loss 0.25853103399276733\n",
            "[Training Epoch 89] Batch 4100, Loss 0.24245503544807434\n",
            "[Training Epoch 89] Batch 4200, Loss 0.2623569369316101\n",
            "[Training Epoch 89] Batch 4300, Loss 0.24670131504535675\n",
            "[Training Epoch 89] Batch 4400, Loss 0.28140872716903687\n",
            "[Training Epoch 89] Batch 4500, Loss 0.28124213218688965\n",
            "[Training Epoch 89] Batch 4600, Loss 0.2524111270904541\n",
            "[Training Epoch 89] Batch 4700, Loss 0.2652931809425354\n",
            "[Training Epoch 89] Batch 4800, Loss 0.2855464220046997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 89] HR = 0.6359, NDCG = 0.3684\n",
            "Epoch 90 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 90] Batch 0, Loss 0.274946391582489\n",
            "[Training Epoch 90] Batch 100, Loss 0.2562805116176605\n",
            "[Training Epoch 90] Batch 200, Loss 0.27463263273239136\n",
            "[Training Epoch 90] Batch 300, Loss 0.27535027265548706\n",
            "[Training Epoch 90] Batch 400, Loss 0.23897038400173187\n",
            "[Training Epoch 90] Batch 500, Loss 0.2555345296859741\n",
            "[Training Epoch 90] Batch 600, Loss 0.2798052430152893\n",
            "[Training Epoch 90] Batch 700, Loss 0.25152361392974854\n",
            "[Training Epoch 90] Batch 800, Loss 0.26517176628112793\n",
            "[Training Epoch 90] Batch 900, Loss 0.25043821334838867\n",
            "[Training Epoch 90] Batch 1000, Loss 0.26211464405059814\n",
            "[Training Epoch 90] Batch 1100, Loss 0.2732054591178894\n",
            "[Training Epoch 90] Batch 1200, Loss 0.2830519378185272\n",
            "[Training Epoch 90] Batch 1300, Loss 0.25533241033554077\n",
            "[Training Epoch 90] Batch 1400, Loss 0.2954665720462799\n",
            "[Training Epoch 90] Batch 1500, Loss 0.26655900478363037\n",
            "[Training Epoch 90] Batch 1600, Loss 0.28021252155303955\n",
            "[Training Epoch 90] Batch 1700, Loss 0.2531043291091919\n",
            "[Training Epoch 90] Batch 1800, Loss 0.2416648119688034\n",
            "[Training Epoch 90] Batch 1900, Loss 0.2509234547615051\n",
            "[Training Epoch 90] Batch 2000, Loss 0.2729567885398865\n",
            "[Training Epoch 90] Batch 2100, Loss 0.2683842182159424\n",
            "[Training Epoch 90] Batch 2200, Loss 0.269342303276062\n",
            "[Training Epoch 90] Batch 2300, Loss 0.2666519582271576\n",
            "[Training Epoch 90] Batch 2400, Loss 0.24968820810317993\n",
            "[Training Epoch 90] Batch 2500, Loss 0.25675123929977417\n",
            "[Training Epoch 90] Batch 2600, Loss 0.2846911549568176\n",
            "[Training Epoch 90] Batch 2700, Loss 0.27200639247894287\n",
            "[Training Epoch 90] Batch 2800, Loss 0.26873865723609924\n",
            "[Training Epoch 90] Batch 2900, Loss 0.2591913938522339\n",
            "[Training Epoch 90] Batch 3000, Loss 0.2753123641014099\n",
            "[Training Epoch 90] Batch 3100, Loss 0.2791408896446228\n",
            "[Training Epoch 90] Batch 3200, Loss 0.2882227301597595\n",
            "[Training Epoch 90] Batch 3300, Loss 0.26176702976226807\n",
            "[Training Epoch 90] Batch 3400, Loss 0.2739148736000061\n",
            "[Training Epoch 90] Batch 3500, Loss 0.241947203874588\n",
            "[Training Epoch 90] Batch 3600, Loss 0.24361759424209595\n",
            "[Training Epoch 90] Batch 3700, Loss 0.23193670809268951\n",
            "[Training Epoch 90] Batch 3800, Loss 0.25904637575149536\n",
            "[Training Epoch 90] Batch 3900, Loss 0.2639240026473999\n",
            "[Training Epoch 90] Batch 4000, Loss 0.2529616951942444\n",
            "[Training Epoch 90] Batch 4100, Loss 0.2735480070114136\n",
            "[Training Epoch 90] Batch 4200, Loss 0.2450147569179535\n",
            "[Training Epoch 90] Batch 4300, Loss 0.25147539377212524\n",
            "[Training Epoch 90] Batch 4400, Loss 0.24829041957855225\n",
            "[Training Epoch 90] Batch 4500, Loss 0.2526929974555969\n",
            "[Training Epoch 90] Batch 4600, Loss 0.2792194187641144\n",
            "[Training Epoch 90] Batch 4700, Loss 0.258796751499176\n",
            "[Training Epoch 90] Batch 4800, Loss 0.23337200284004211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 90] HR = 0.6368, NDCG = 0.3672\n",
            "Epoch 91 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 91] Batch 0, Loss 0.26181983947753906\n",
            "[Training Epoch 91] Batch 100, Loss 0.2299385368824005\n",
            "[Training Epoch 91] Batch 200, Loss 0.2653656005859375\n",
            "[Training Epoch 91] Batch 300, Loss 0.2860756814479828\n",
            "[Training Epoch 91] Batch 400, Loss 0.2741497755050659\n",
            "[Training Epoch 91] Batch 500, Loss 0.25797998905181885\n",
            "[Training Epoch 91] Batch 600, Loss 0.241623193025589\n",
            "[Training Epoch 91] Batch 700, Loss 0.2695462703704834\n",
            "[Training Epoch 91] Batch 800, Loss 0.26799216866493225\n",
            "[Training Epoch 91] Batch 900, Loss 0.24879606068134308\n",
            "[Training Epoch 91] Batch 1000, Loss 0.26081663370132446\n",
            "[Training Epoch 91] Batch 1100, Loss 0.30761051177978516\n",
            "[Training Epoch 91] Batch 1200, Loss 0.23708420991897583\n",
            "[Training Epoch 91] Batch 1300, Loss 0.2906203866004944\n",
            "[Training Epoch 91] Batch 1400, Loss 0.27720338106155396\n",
            "[Training Epoch 91] Batch 1500, Loss 0.2513173222541809\n",
            "[Training Epoch 91] Batch 1600, Loss 0.2628381550312042\n",
            "[Training Epoch 91] Batch 1700, Loss 0.2775195837020874\n",
            "[Training Epoch 91] Batch 1800, Loss 0.2812100648880005\n",
            "[Training Epoch 91] Batch 1900, Loss 0.26535922288894653\n",
            "[Training Epoch 91] Batch 2000, Loss 0.27930521965026855\n",
            "[Training Epoch 91] Batch 2100, Loss 0.2635968327522278\n",
            "[Training Epoch 91] Batch 2200, Loss 0.270496666431427\n",
            "[Training Epoch 91] Batch 2300, Loss 0.25356966257095337\n",
            "[Training Epoch 91] Batch 2400, Loss 0.28212565183639526\n",
            "[Training Epoch 91] Batch 2500, Loss 0.2772665023803711\n",
            "[Training Epoch 91] Batch 2600, Loss 0.25764745473861694\n",
            "[Training Epoch 91] Batch 2700, Loss 0.29537370800971985\n",
            "[Training Epoch 91] Batch 2800, Loss 0.25611788034439087\n",
            "[Training Epoch 91] Batch 2900, Loss 0.24387264251708984\n",
            "[Training Epoch 91] Batch 3000, Loss 0.28147995471954346\n",
            "[Training Epoch 91] Batch 3100, Loss 0.2648739218711853\n",
            "[Training Epoch 91] Batch 3200, Loss 0.29793643951416016\n",
            "[Training Epoch 91] Batch 3300, Loss 0.2588781714439392\n",
            "[Training Epoch 91] Batch 3400, Loss 0.26492106914520264\n",
            "[Training Epoch 91] Batch 3500, Loss 0.2703899145126343\n",
            "[Training Epoch 91] Batch 3600, Loss 0.2704731822013855\n",
            "[Training Epoch 91] Batch 3700, Loss 0.27957841753959656\n",
            "[Training Epoch 91] Batch 3800, Loss 0.27055105566978455\n",
            "[Training Epoch 91] Batch 3900, Loss 0.28159117698669434\n",
            "[Training Epoch 91] Batch 4000, Loss 0.26301127672195435\n",
            "[Training Epoch 91] Batch 4100, Loss 0.2608017921447754\n",
            "[Training Epoch 91] Batch 4200, Loss 0.2364073097705841\n",
            "[Training Epoch 91] Batch 4300, Loss 0.28077763319015503\n",
            "[Training Epoch 91] Batch 4400, Loss 0.2725754976272583\n",
            "[Training Epoch 91] Batch 4500, Loss 0.27163130044937134\n",
            "[Training Epoch 91] Batch 4600, Loss 0.2614024877548218\n",
            "[Training Epoch 91] Batch 4700, Loss 0.25943267345428467\n",
            "[Training Epoch 91] Batch 4800, Loss 0.25710630416870117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 91] HR = 0.6356, NDCG = 0.3673\n",
            "Epoch 92 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 92] Batch 0, Loss 0.2461114376783371\n",
            "[Training Epoch 92] Batch 100, Loss 0.24897721409797668\n",
            "[Training Epoch 92] Batch 200, Loss 0.28171712160110474\n",
            "[Training Epoch 92] Batch 300, Loss 0.27718859910964966\n",
            "[Training Epoch 92] Batch 400, Loss 0.25709348917007446\n",
            "[Training Epoch 92] Batch 500, Loss 0.2567135691642761\n",
            "[Training Epoch 92] Batch 600, Loss 0.2881094813346863\n",
            "[Training Epoch 92] Batch 700, Loss 0.27705782651901245\n",
            "[Training Epoch 92] Batch 800, Loss 0.27642956376075745\n",
            "[Training Epoch 92] Batch 900, Loss 0.263464093208313\n",
            "[Training Epoch 92] Batch 1000, Loss 0.29757505655288696\n",
            "[Training Epoch 92] Batch 1100, Loss 0.26878732442855835\n",
            "[Training Epoch 92] Batch 1200, Loss 0.2936810255050659\n",
            "[Training Epoch 92] Batch 1300, Loss 0.298617959022522\n",
            "[Training Epoch 92] Batch 1400, Loss 0.2558930516242981\n",
            "[Training Epoch 92] Batch 1500, Loss 0.23391811549663544\n",
            "[Training Epoch 92] Batch 1600, Loss 0.2571141719818115\n",
            "[Training Epoch 92] Batch 1700, Loss 0.2644132077693939\n",
            "[Training Epoch 92] Batch 1800, Loss 0.2610877454280853\n",
            "[Training Epoch 92] Batch 1900, Loss 0.2708902359008789\n",
            "[Training Epoch 92] Batch 2000, Loss 0.2751862406730652\n",
            "[Training Epoch 92] Batch 2100, Loss 0.27503830194473267\n",
            "[Training Epoch 92] Batch 2200, Loss 0.2740767002105713\n",
            "[Training Epoch 92] Batch 2300, Loss 0.246686652302742\n",
            "[Training Epoch 92] Batch 2400, Loss 0.2608424127101898\n",
            "[Training Epoch 92] Batch 2500, Loss 0.24796371161937714\n",
            "[Training Epoch 92] Batch 2600, Loss 0.244878888130188\n",
            "[Training Epoch 92] Batch 2700, Loss 0.2660753130912781\n",
            "[Training Epoch 92] Batch 2800, Loss 0.26160433888435364\n",
            "[Training Epoch 92] Batch 2900, Loss 0.26016199588775635\n",
            "[Training Epoch 92] Batch 3000, Loss 0.2582971155643463\n",
            "[Training Epoch 92] Batch 3100, Loss 0.24866747856140137\n",
            "[Training Epoch 92] Batch 3200, Loss 0.2647370994091034\n",
            "[Training Epoch 92] Batch 3300, Loss 0.24743199348449707\n",
            "[Training Epoch 92] Batch 3400, Loss 0.26565074920654297\n",
            "[Training Epoch 92] Batch 3500, Loss 0.28526800870895386\n",
            "[Training Epoch 92] Batch 3600, Loss 0.2362198531627655\n",
            "[Training Epoch 92] Batch 3700, Loss 0.26735353469848633\n",
            "[Training Epoch 92] Batch 3800, Loss 0.2749050259590149\n",
            "[Training Epoch 92] Batch 3900, Loss 0.28048840165138245\n",
            "[Training Epoch 92] Batch 4000, Loss 0.3003504276275635\n",
            "[Training Epoch 92] Batch 4100, Loss 0.2589160203933716\n",
            "[Training Epoch 92] Batch 4200, Loss 0.26395827531814575\n",
            "[Training Epoch 92] Batch 4300, Loss 0.2701205015182495\n",
            "[Training Epoch 92] Batch 4400, Loss 0.2788550853729248\n",
            "[Training Epoch 92] Batch 4500, Loss 0.27817943692207336\n",
            "[Training Epoch 92] Batch 4600, Loss 0.2721298933029175\n",
            "[Training Epoch 92] Batch 4700, Loss 0.25956007838249207\n",
            "[Training Epoch 92] Batch 4800, Loss 0.2676699757575989\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 92] HR = 0.6349, NDCG = 0.3675\n",
            "Epoch 93 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 93] Batch 0, Loss 0.24507524073123932\n",
            "[Training Epoch 93] Batch 100, Loss 0.25860899686813354\n",
            "[Training Epoch 93] Batch 200, Loss 0.258990079164505\n",
            "[Training Epoch 93] Batch 300, Loss 0.24471978843212128\n",
            "[Training Epoch 93] Batch 400, Loss 0.2922203242778778\n",
            "[Training Epoch 93] Batch 500, Loss 0.2788149416446686\n",
            "[Training Epoch 93] Batch 600, Loss 0.24583908915519714\n",
            "[Training Epoch 93] Batch 700, Loss 0.30877453088760376\n",
            "[Training Epoch 93] Batch 800, Loss 0.2458401918411255\n",
            "[Training Epoch 93] Batch 900, Loss 0.2695200443267822\n",
            "[Training Epoch 93] Batch 1000, Loss 0.2967900335788727\n",
            "[Training Epoch 93] Batch 1100, Loss 0.24127262830734253\n",
            "[Training Epoch 93] Batch 1200, Loss 0.26845583319664\n",
            "[Training Epoch 93] Batch 1300, Loss 0.24956375360488892\n",
            "[Training Epoch 93] Batch 1400, Loss 0.266973614692688\n",
            "[Training Epoch 93] Batch 1500, Loss 0.27552470564842224\n",
            "[Training Epoch 93] Batch 1600, Loss 0.27229738235473633\n",
            "[Training Epoch 93] Batch 1700, Loss 0.28141504526138306\n",
            "[Training Epoch 93] Batch 1800, Loss 0.2650730609893799\n",
            "[Training Epoch 93] Batch 1900, Loss 0.261308878660202\n",
            "[Training Epoch 93] Batch 2000, Loss 0.26945340633392334\n",
            "[Training Epoch 93] Batch 2100, Loss 0.25574153661727905\n",
            "[Training Epoch 93] Batch 2200, Loss 0.270532488822937\n",
            "[Training Epoch 93] Batch 2300, Loss 0.2356759011745453\n",
            "[Training Epoch 93] Batch 2400, Loss 0.24053488671779633\n",
            "[Training Epoch 93] Batch 2500, Loss 0.24303746223449707\n",
            "[Training Epoch 93] Batch 2600, Loss 0.27245330810546875\n",
            "[Training Epoch 93] Batch 2700, Loss 0.2601039409637451\n",
            "[Training Epoch 93] Batch 2800, Loss 0.229507714509964\n",
            "[Training Epoch 93] Batch 2900, Loss 0.268335223197937\n",
            "[Training Epoch 93] Batch 3000, Loss 0.30296918749809265\n",
            "[Training Epoch 93] Batch 3100, Loss 0.24485591053962708\n",
            "[Training Epoch 93] Batch 3200, Loss 0.29602718353271484\n",
            "[Training Epoch 93] Batch 3300, Loss 0.28011223673820496\n",
            "[Training Epoch 93] Batch 3400, Loss 0.27946916222572327\n",
            "[Training Epoch 93] Batch 3500, Loss 0.2529231011867523\n",
            "[Training Epoch 93] Batch 3600, Loss 0.30536460876464844\n",
            "[Training Epoch 93] Batch 3700, Loss 0.2658737301826477\n",
            "[Training Epoch 93] Batch 3800, Loss 0.2438330352306366\n",
            "[Training Epoch 93] Batch 3900, Loss 0.26737239956855774\n",
            "[Training Epoch 93] Batch 4000, Loss 0.2457921802997589\n",
            "[Training Epoch 93] Batch 4100, Loss 0.2718907296657562\n",
            "[Training Epoch 93] Batch 4200, Loss 0.23504894971847534\n",
            "[Training Epoch 93] Batch 4300, Loss 0.2722066342830658\n",
            "[Training Epoch 93] Batch 4400, Loss 0.2624990940093994\n",
            "[Training Epoch 93] Batch 4500, Loss 0.2892124056816101\n",
            "[Training Epoch 93] Batch 4600, Loss 0.25549644231796265\n",
            "[Training Epoch 93] Batch 4700, Loss 0.27419644594192505\n",
            "[Training Epoch 93] Batch 4800, Loss 0.28074419498443604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 93] HR = 0.6376, NDCG = 0.3692\n",
            "Epoch 94 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 94] Batch 0, Loss 0.25677067041397095\n",
            "[Training Epoch 94] Batch 100, Loss 0.24556675553321838\n",
            "[Training Epoch 94] Batch 200, Loss 0.281318724155426\n",
            "[Training Epoch 94] Batch 300, Loss 0.2679682970046997\n",
            "[Training Epoch 94] Batch 400, Loss 0.24365608394145966\n",
            "[Training Epoch 94] Batch 500, Loss 0.284011572599411\n",
            "[Training Epoch 94] Batch 600, Loss 0.26332029700279236\n",
            "[Training Epoch 94] Batch 700, Loss 0.2710159122943878\n",
            "[Training Epoch 94] Batch 800, Loss 0.23964159190654755\n",
            "[Training Epoch 94] Batch 900, Loss 0.25776177644729614\n",
            "[Training Epoch 94] Batch 1000, Loss 0.26151180267333984\n",
            "[Training Epoch 94] Batch 1100, Loss 0.27362680435180664\n",
            "[Training Epoch 94] Batch 1200, Loss 0.27123576402664185\n",
            "[Training Epoch 94] Batch 1300, Loss 0.2819400727748871\n",
            "[Training Epoch 94] Batch 1400, Loss 0.28395146131515503\n",
            "[Training Epoch 94] Batch 1500, Loss 0.26317161321640015\n",
            "[Training Epoch 94] Batch 1600, Loss 0.24889345467090607\n",
            "[Training Epoch 94] Batch 1700, Loss 0.2624981105327606\n",
            "[Training Epoch 94] Batch 1800, Loss 0.27144867181777954\n",
            "[Training Epoch 94] Batch 1900, Loss 0.25661012530326843\n",
            "[Training Epoch 94] Batch 2000, Loss 0.2765175700187683\n",
            "[Training Epoch 94] Batch 2100, Loss 0.2741548418998718\n",
            "[Training Epoch 94] Batch 2200, Loss 0.27068281173706055\n",
            "[Training Epoch 94] Batch 2300, Loss 0.2567955553531647\n",
            "[Training Epoch 94] Batch 2400, Loss 0.26508694887161255\n",
            "[Training Epoch 94] Batch 2500, Loss 0.2551369369029999\n",
            "[Training Epoch 94] Batch 2600, Loss 0.2659724950790405\n",
            "[Training Epoch 94] Batch 2700, Loss 0.2633146643638611\n",
            "[Training Epoch 94] Batch 2800, Loss 0.24424204230308533\n",
            "[Training Epoch 94] Batch 2900, Loss 0.24843402206897736\n",
            "[Training Epoch 94] Batch 3000, Loss 0.2780795693397522\n",
            "[Training Epoch 94] Batch 3100, Loss 0.29733985662460327\n",
            "[Training Epoch 94] Batch 3200, Loss 0.26017341017723083\n",
            "[Training Epoch 94] Batch 3300, Loss 0.2772514224052429\n",
            "[Training Epoch 94] Batch 3400, Loss 0.2671969532966614\n",
            "[Training Epoch 94] Batch 3500, Loss 0.2694333791732788\n",
            "[Training Epoch 94] Batch 3600, Loss 0.26850900053977966\n",
            "[Training Epoch 94] Batch 3700, Loss 0.3015292286872864\n",
            "[Training Epoch 94] Batch 3800, Loss 0.2510892450809479\n",
            "[Training Epoch 94] Batch 3900, Loss 0.24786607921123505\n",
            "[Training Epoch 94] Batch 4000, Loss 0.23305824398994446\n",
            "[Training Epoch 94] Batch 4100, Loss 0.2562500238418579\n",
            "[Training Epoch 94] Batch 4200, Loss 0.24204692244529724\n",
            "[Training Epoch 94] Batch 4300, Loss 0.2642132043838501\n",
            "[Training Epoch 94] Batch 4400, Loss 0.243671253323555\n",
            "[Training Epoch 94] Batch 4500, Loss 0.2679187059402466\n",
            "[Training Epoch 94] Batch 4600, Loss 0.2698914110660553\n",
            "[Training Epoch 94] Batch 4700, Loss 0.28819549083709717\n",
            "[Training Epoch 94] Batch 4800, Loss 0.26353180408477783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 94] HR = 0.6374, NDCG = 0.3684\n",
            "Epoch 95 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 95] Batch 0, Loss 0.28208106756210327\n",
            "[Training Epoch 95] Batch 100, Loss 0.25391989946365356\n",
            "[Training Epoch 95] Batch 200, Loss 0.2699336111545563\n",
            "[Training Epoch 95] Batch 300, Loss 0.26583707332611084\n",
            "[Training Epoch 95] Batch 400, Loss 0.27225667238235474\n",
            "[Training Epoch 95] Batch 500, Loss 0.2398023009300232\n",
            "[Training Epoch 95] Batch 600, Loss 0.24030442535877228\n",
            "[Training Epoch 95] Batch 700, Loss 0.24295255541801453\n",
            "[Training Epoch 95] Batch 800, Loss 0.258004754781723\n",
            "[Training Epoch 95] Batch 900, Loss 0.2571064829826355\n",
            "[Training Epoch 95] Batch 1000, Loss 0.27997487783432007\n",
            "[Training Epoch 95] Batch 1100, Loss 0.25580623745918274\n",
            "[Training Epoch 95] Batch 1200, Loss 0.26059746742248535\n",
            "[Training Epoch 95] Batch 1300, Loss 0.25547170639038086\n",
            "[Training Epoch 95] Batch 1400, Loss 0.2865186929702759\n",
            "[Training Epoch 95] Batch 1500, Loss 0.2707135081291199\n",
            "[Training Epoch 95] Batch 1600, Loss 0.2902914583683014\n",
            "[Training Epoch 95] Batch 1700, Loss 0.2835678458213806\n",
            "[Training Epoch 95] Batch 1800, Loss 0.27445176243782043\n",
            "[Training Epoch 95] Batch 1900, Loss 0.24780821800231934\n",
            "[Training Epoch 95] Batch 2000, Loss 0.2763625383377075\n",
            "[Training Epoch 95] Batch 2100, Loss 0.27093711495399475\n",
            "[Training Epoch 95] Batch 2200, Loss 0.2680884301662445\n",
            "[Training Epoch 95] Batch 2300, Loss 0.2716517448425293\n",
            "[Training Epoch 95] Batch 2400, Loss 0.2449900209903717\n",
            "[Training Epoch 95] Batch 2500, Loss 0.23712801933288574\n",
            "[Training Epoch 95] Batch 2600, Loss 0.2482510805130005\n",
            "[Training Epoch 95] Batch 2700, Loss 0.2808718681335449\n",
            "[Training Epoch 95] Batch 2800, Loss 0.2564379572868347\n",
            "[Training Epoch 95] Batch 2900, Loss 0.24880090355873108\n",
            "[Training Epoch 95] Batch 3000, Loss 0.2508889436721802\n",
            "[Training Epoch 95] Batch 3100, Loss 0.24129575490951538\n",
            "[Training Epoch 95] Batch 3200, Loss 0.2562703490257263\n",
            "[Training Epoch 95] Batch 3300, Loss 0.26374855637550354\n",
            "[Training Epoch 95] Batch 3400, Loss 0.2695838510990143\n",
            "[Training Epoch 95] Batch 3500, Loss 0.2607475519180298\n",
            "[Training Epoch 95] Batch 3600, Loss 0.2811585068702698\n",
            "[Training Epoch 95] Batch 3700, Loss 0.28821900486946106\n",
            "[Training Epoch 95] Batch 3800, Loss 0.2829754948616028\n",
            "[Training Epoch 95] Batch 3900, Loss 0.259973406791687\n",
            "[Training Epoch 95] Batch 4000, Loss 0.2406100332736969\n",
            "[Training Epoch 95] Batch 4100, Loss 0.26869893074035645\n",
            "[Training Epoch 95] Batch 4200, Loss 0.25041261315345764\n",
            "[Training Epoch 95] Batch 4300, Loss 0.257183313369751\n",
            "[Training Epoch 95] Batch 4400, Loss 0.2624719738960266\n",
            "[Training Epoch 95] Batch 4500, Loss 0.29121655225753784\n",
            "[Training Epoch 95] Batch 4600, Loss 0.26922327280044556\n",
            "[Training Epoch 95] Batch 4700, Loss 0.2511987090110779\n",
            "[Training Epoch 95] Batch 4800, Loss 0.2704196572303772\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 95] HR = 0.6386, NDCG = 0.3678\n",
            "Epoch 96 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 96] Batch 0, Loss 0.2394440770149231\n",
            "[Training Epoch 96] Batch 100, Loss 0.26792508363723755\n",
            "[Training Epoch 96] Batch 200, Loss 0.27526724338531494\n",
            "[Training Epoch 96] Batch 300, Loss 0.28415602445602417\n",
            "[Training Epoch 96] Batch 400, Loss 0.2786049246788025\n",
            "[Training Epoch 96] Batch 500, Loss 0.2605602741241455\n",
            "[Training Epoch 96] Batch 600, Loss 0.28028589487075806\n",
            "[Training Epoch 96] Batch 700, Loss 0.246265709400177\n",
            "[Training Epoch 96] Batch 800, Loss 0.25904232263565063\n",
            "[Training Epoch 96] Batch 900, Loss 0.2562817931175232\n",
            "[Training Epoch 96] Batch 1000, Loss 0.2725592255592346\n",
            "[Training Epoch 96] Batch 1100, Loss 0.2548011541366577\n",
            "[Training Epoch 96] Batch 1200, Loss 0.23801571130752563\n",
            "[Training Epoch 96] Batch 1300, Loss 0.27400413155555725\n",
            "[Training Epoch 96] Batch 1400, Loss 0.28631845116615295\n",
            "[Training Epoch 96] Batch 1500, Loss 0.2861480712890625\n",
            "[Training Epoch 96] Batch 1600, Loss 0.2728714942932129\n",
            "[Training Epoch 96] Batch 1700, Loss 0.2448216676712036\n",
            "[Training Epoch 96] Batch 1800, Loss 0.25400716066360474\n",
            "[Training Epoch 96] Batch 1900, Loss 0.2761421203613281\n",
            "[Training Epoch 96] Batch 2000, Loss 0.27639156579971313\n",
            "[Training Epoch 96] Batch 2100, Loss 0.2839754819869995\n",
            "[Training Epoch 96] Batch 2200, Loss 0.2755512595176697\n",
            "[Training Epoch 96] Batch 2300, Loss 0.2516266405582428\n",
            "[Training Epoch 96] Batch 2400, Loss 0.2498217076063156\n",
            "[Training Epoch 96] Batch 2500, Loss 0.25845590233802795\n",
            "[Training Epoch 96] Batch 2600, Loss 0.2522233724594116\n",
            "[Training Epoch 96] Batch 2700, Loss 0.26496246457099915\n",
            "[Training Epoch 96] Batch 2800, Loss 0.28943148255348206\n",
            "[Training Epoch 96] Batch 2900, Loss 0.2822684347629547\n",
            "[Training Epoch 96] Batch 3000, Loss 0.25979089736938477\n",
            "[Training Epoch 96] Batch 3100, Loss 0.26722991466522217\n",
            "[Training Epoch 96] Batch 3200, Loss 0.23803958296775818\n",
            "[Training Epoch 96] Batch 3300, Loss 0.2556547522544861\n",
            "[Training Epoch 96] Batch 3400, Loss 0.25025197863578796\n",
            "[Training Epoch 96] Batch 3500, Loss 0.2803136110305786\n",
            "[Training Epoch 96] Batch 3600, Loss 0.29292771220207214\n",
            "[Training Epoch 96] Batch 3700, Loss 0.27360016107559204\n",
            "[Training Epoch 96] Batch 3800, Loss 0.2736964225769043\n",
            "[Training Epoch 96] Batch 3900, Loss 0.2720336318016052\n",
            "[Training Epoch 96] Batch 4000, Loss 0.23804351687431335\n",
            "[Training Epoch 96] Batch 4100, Loss 0.25360167026519775\n",
            "[Training Epoch 96] Batch 4200, Loss 0.2744362950325012\n",
            "[Training Epoch 96] Batch 4300, Loss 0.28167080879211426\n",
            "[Training Epoch 96] Batch 4400, Loss 0.25573018193244934\n",
            "[Training Epoch 96] Batch 4500, Loss 0.28243905305862427\n",
            "[Training Epoch 96] Batch 4600, Loss 0.2882760167121887\n",
            "[Training Epoch 96] Batch 4700, Loss 0.28660908341407776\n",
            "[Training Epoch 96] Batch 4800, Loss 0.2738838493824005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 96] HR = 0.6374, NDCG = 0.3686\n",
            "Epoch 97 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 97] Batch 0, Loss 0.21873719990253448\n",
            "[Training Epoch 97] Batch 100, Loss 0.2669084072113037\n",
            "[Training Epoch 97] Batch 200, Loss 0.2520306706428528\n",
            "[Training Epoch 97] Batch 300, Loss 0.26941776275634766\n",
            "[Training Epoch 97] Batch 400, Loss 0.2692861557006836\n",
            "[Training Epoch 97] Batch 500, Loss 0.2589230537414551\n",
            "[Training Epoch 97] Batch 600, Loss 0.26173093914985657\n",
            "[Training Epoch 97] Batch 700, Loss 0.28034913539886475\n",
            "[Training Epoch 97] Batch 800, Loss 0.2673909068107605\n",
            "[Training Epoch 97] Batch 900, Loss 0.24901282787322998\n",
            "[Training Epoch 97] Batch 1000, Loss 0.2592605650424957\n",
            "[Training Epoch 97] Batch 1100, Loss 0.2640008330345154\n",
            "[Training Epoch 97] Batch 1200, Loss 0.2771078646183014\n",
            "[Training Epoch 97] Batch 1300, Loss 0.2519478499889374\n",
            "[Training Epoch 97] Batch 1400, Loss 0.2605917751789093\n",
            "[Training Epoch 97] Batch 1500, Loss 0.2828938663005829\n",
            "[Training Epoch 97] Batch 1600, Loss 0.23124390840530396\n",
            "[Training Epoch 97] Batch 1700, Loss 0.25625139474868774\n",
            "[Training Epoch 97] Batch 1800, Loss 0.2537885904312134\n",
            "[Training Epoch 97] Batch 1900, Loss 0.2546125054359436\n",
            "[Training Epoch 97] Batch 2000, Loss 0.27720779180526733\n",
            "[Training Epoch 97] Batch 2100, Loss 0.23136521875858307\n",
            "[Training Epoch 97] Batch 2200, Loss 0.27275896072387695\n",
            "[Training Epoch 97] Batch 2300, Loss 0.26513591408729553\n",
            "[Training Epoch 97] Batch 2400, Loss 0.2821996510028839\n",
            "[Training Epoch 97] Batch 2500, Loss 0.27066588401794434\n",
            "[Training Epoch 97] Batch 2600, Loss 0.27467891573905945\n",
            "[Training Epoch 97] Batch 2700, Loss 0.25744855403900146\n",
            "[Training Epoch 97] Batch 2800, Loss 0.28121358156204224\n",
            "[Training Epoch 97] Batch 2900, Loss 0.26296597719192505\n",
            "[Training Epoch 97] Batch 3000, Loss 0.24655456840991974\n",
            "[Training Epoch 97] Batch 3100, Loss 0.27166038751602173\n",
            "[Training Epoch 97] Batch 3200, Loss 0.25338226556777954\n",
            "[Training Epoch 97] Batch 3300, Loss 0.24530667066574097\n",
            "[Training Epoch 97] Batch 3400, Loss 0.28015637397766113\n",
            "[Training Epoch 97] Batch 3500, Loss 0.2708764970302582\n",
            "[Training Epoch 97] Batch 3600, Loss 0.2673603892326355\n",
            "[Training Epoch 97] Batch 3700, Loss 0.27960896492004395\n",
            "[Training Epoch 97] Batch 3800, Loss 0.2670130133628845\n",
            "[Training Epoch 97] Batch 3900, Loss 0.2475006878376007\n",
            "[Training Epoch 97] Batch 4000, Loss 0.22803860902786255\n",
            "[Training Epoch 97] Batch 4100, Loss 0.2565736174583435\n",
            "[Training Epoch 97] Batch 4200, Loss 0.25475597381591797\n",
            "[Training Epoch 97] Batch 4300, Loss 0.2705889940261841\n",
            "[Training Epoch 97] Batch 4400, Loss 0.268930047750473\n",
            "[Training Epoch 97] Batch 4500, Loss 0.2627307176589966\n",
            "[Training Epoch 97] Batch 4600, Loss 0.258101224899292\n",
            "[Training Epoch 97] Batch 4700, Loss 0.26656126976013184\n",
            "[Training Epoch 97] Batch 4800, Loss 0.2714415192604065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 97] HR = 0.6387, NDCG = 0.3680\n",
            "Epoch 98 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 98] Batch 0, Loss 0.26735907793045044\n",
            "[Training Epoch 98] Batch 100, Loss 0.2366325557231903\n",
            "[Training Epoch 98] Batch 200, Loss 0.2244817465543747\n",
            "[Training Epoch 98] Batch 300, Loss 0.2565042972564697\n",
            "[Training Epoch 98] Batch 400, Loss 0.2594450116157532\n",
            "[Training Epoch 98] Batch 500, Loss 0.251799076795578\n",
            "[Training Epoch 98] Batch 600, Loss 0.2683757543563843\n",
            "[Training Epoch 98] Batch 700, Loss 0.2717466950416565\n",
            "[Training Epoch 98] Batch 800, Loss 0.2548721432685852\n",
            "[Training Epoch 98] Batch 900, Loss 0.2482992708683014\n",
            "[Training Epoch 98] Batch 1000, Loss 0.273985356092453\n",
            "[Training Epoch 98] Batch 1100, Loss 0.2748774588108063\n",
            "[Training Epoch 98] Batch 1200, Loss 0.25978410243988037\n",
            "[Training Epoch 98] Batch 1300, Loss 0.2636946737766266\n",
            "[Training Epoch 98] Batch 1400, Loss 0.25191015005111694\n",
            "[Training Epoch 98] Batch 1500, Loss 0.23332002758979797\n",
            "[Training Epoch 98] Batch 1600, Loss 0.25451093912124634\n",
            "[Training Epoch 98] Batch 1700, Loss 0.2618947923183441\n",
            "[Training Epoch 98] Batch 1800, Loss 0.2750605642795563\n",
            "[Training Epoch 98] Batch 1900, Loss 0.26462215185165405\n",
            "[Training Epoch 98] Batch 2000, Loss 0.2949172556400299\n",
            "[Training Epoch 98] Batch 2100, Loss 0.24653246998786926\n",
            "[Training Epoch 98] Batch 2200, Loss 0.2867105305194855\n",
            "[Training Epoch 98] Batch 2300, Loss 0.2636537551879883\n",
            "[Training Epoch 98] Batch 2400, Loss 0.28431668877601624\n",
            "[Training Epoch 98] Batch 2500, Loss 0.2595100998878479\n",
            "[Training Epoch 98] Batch 2600, Loss 0.22137606143951416\n",
            "[Training Epoch 98] Batch 2700, Loss 0.29369938373565674\n",
            "[Training Epoch 98] Batch 2800, Loss 0.27877599000930786\n",
            "[Training Epoch 98] Batch 2900, Loss 0.25678151845932007\n",
            "[Training Epoch 98] Batch 3000, Loss 0.2679500877857208\n",
            "[Training Epoch 98] Batch 3100, Loss 0.2646985948085785\n",
            "[Training Epoch 98] Batch 3200, Loss 0.2811034321784973\n",
            "[Training Epoch 98] Batch 3300, Loss 0.2949463129043579\n",
            "[Training Epoch 98] Batch 3400, Loss 0.2489745169878006\n",
            "[Training Epoch 98] Batch 3500, Loss 0.2676875591278076\n",
            "[Training Epoch 98] Batch 3600, Loss 0.2636372447013855\n",
            "[Training Epoch 98] Batch 3700, Loss 0.2902815043926239\n",
            "[Training Epoch 98] Batch 3800, Loss 0.24647559225559235\n",
            "[Training Epoch 98] Batch 3900, Loss 0.2784363031387329\n",
            "[Training Epoch 98] Batch 4000, Loss 0.27947360277175903\n",
            "[Training Epoch 98] Batch 4100, Loss 0.24676230549812317\n",
            "[Training Epoch 98] Batch 4200, Loss 0.2628028392791748\n",
            "[Training Epoch 98] Batch 4300, Loss 0.29273802042007446\n",
            "[Training Epoch 98] Batch 4400, Loss 0.25824660062789917\n",
            "[Training Epoch 98] Batch 4500, Loss 0.2958614230155945\n",
            "[Training Epoch 98] Batch 4600, Loss 0.27862226963043213\n",
            "[Training Epoch 98] Batch 4700, Loss 0.25660258531570435\n",
            "[Training Epoch 98] Batch 4800, Loss 0.2689554691314697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 98] HR = 0.6341, NDCG = 0.3677\n",
            "Epoch 99 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 99] Batch 0, Loss 0.2595362067222595\n",
            "[Training Epoch 99] Batch 100, Loss 0.2664507329463959\n",
            "[Training Epoch 99] Batch 200, Loss 0.2652985453605652\n",
            "[Training Epoch 99] Batch 300, Loss 0.2735425531864166\n",
            "[Training Epoch 99] Batch 400, Loss 0.24741926789283752\n",
            "[Training Epoch 99] Batch 500, Loss 0.2553277313709259\n",
            "[Training Epoch 99] Batch 600, Loss 0.28026431798934937\n",
            "[Training Epoch 99] Batch 700, Loss 0.2792074382305145\n",
            "[Training Epoch 99] Batch 800, Loss 0.23994353413581848\n",
            "[Training Epoch 99] Batch 900, Loss 0.23533840477466583\n",
            "[Training Epoch 99] Batch 1000, Loss 0.2555273771286011\n",
            "[Training Epoch 99] Batch 1100, Loss 0.2759961783885956\n",
            "[Training Epoch 99] Batch 1200, Loss 0.24231119453907013\n",
            "[Training Epoch 99] Batch 1300, Loss 0.2598022222518921\n",
            "[Training Epoch 99] Batch 1400, Loss 0.2546187937259674\n",
            "[Training Epoch 99] Batch 1500, Loss 0.2530438303947449\n",
            "[Training Epoch 99] Batch 1600, Loss 0.2608533501625061\n",
            "[Training Epoch 99] Batch 1700, Loss 0.2520645260810852\n",
            "[Training Epoch 99] Batch 1800, Loss 0.27248626947402954\n",
            "[Training Epoch 99] Batch 1900, Loss 0.27397364377975464\n",
            "[Training Epoch 99] Batch 2000, Loss 0.26452454924583435\n",
            "[Training Epoch 99] Batch 2100, Loss 0.2514932453632355\n",
            "[Training Epoch 99] Batch 2200, Loss 0.2900625169277191\n",
            "[Training Epoch 99] Batch 2300, Loss 0.25073787569999695\n",
            "[Training Epoch 99] Batch 2400, Loss 0.278986394405365\n",
            "[Training Epoch 99] Batch 2500, Loss 0.24852457642555237\n",
            "[Training Epoch 99] Batch 2600, Loss 0.3021864891052246\n",
            "[Training Epoch 99] Batch 2700, Loss 0.23874980211257935\n",
            "[Training Epoch 99] Batch 2800, Loss 0.2719630002975464\n",
            "[Training Epoch 99] Batch 2900, Loss 0.26580122113227844\n",
            "[Training Epoch 99] Batch 3000, Loss 0.24274972081184387\n",
            "[Training Epoch 99] Batch 3100, Loss 0.2782313823699951\n",
            "[Training Epoch 99] Batch 3200, Loss 0.24655354022979736\n",
            "[Training Epoch 99] Batch 3300, Loss 0.27232831716537476\n",
            "[Training Epoch 99] Batch 3400, Loss 0.26514148712158203\n",
            "[Training Epoch 99] Batch 3500, Loss 0.2727396786212921\n",
            "[Training Epoch 99] Batch 3600, Loss 0.2691267728805542\n",
            "[Training Epoch 99] Batch 3700, Loss 0.25743332505226135\n",
            "[Training Epoch 99] Batch 3800, Loss 0.2553965449333191\n",
            "[Training Epoch 99] Batch 3900, Loss 0.25242191553115845\n",
            "[Training Epoch 99] Batch 4000, Loss 0.26891887187957764\n",
            "[Training Epoch 99] Batch 4100, Loss 0.2561805248260498\n",
            "[Training Epoch 99] Batch 4200, Loss 0.2710920572280884\n",
            "[Training Epoch 99] Batch 4300, Loss 0.26772621273994446\n",
            "[Training Epoch 99] Batch 4400, Loss 0.27338188886642456\n",
            "[Training Epoch 99] Batch 4500, Loss 0.2637752890586853\n",
            "[Training Epoch 99] Batch 4600, Loss 0.25925683975219727\n",
            "[Training Epoch 99] Batch 4700, Loss 0.2903604507446289\n",
            "[Training Epoch 99] Batch 4800, Loss 0.27999353408813477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 99] HR = 0.6373, NDCG = 0.3694\n",
            "Epoch 100 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 100] Batch 0, Loss 0.26540595293045044\n",
            "[Training Epoch 100] Batch 100, Loss 0.2618761360645294\n",
            "[Training Epoch 100] Batch 200, Loss 0.26329299807548523\n",
            "[Training Epoch 100] Batch 300, Loss 0.28985926508903503\n",
            "[Training Epoch 100] Batch 400, Loss 0.2905718684196472\n",
            "[Training Epoch 100] Batch 500, Loss 0.2790607810020447\n",
            "[Training Epoch 100] Batch 600, Loss 0.2638874650001526\n",
            "[Training Epoch 100] Batch 700, Loss 0.23956280946731567\n",
            "[Training Epoch 100] Batch 800, Loss 0.260459840297699\n",
            "[Training Epoch 100] Batch 900, Loss 0.24511238932609558\n",
            "[Training Epoch 100] Batch 1000, Loss 0.27875351905822754\n",
            "[Training Epoch 100] Batch 1100, Loss 0.27989062666893005\n",
            "[Training Epoch 100] Batch 1200, Loss 0.26838400959968567\n",
            "[Training Epoch 100] Batch 1300, Loss 0.28130674362182617\n",
            "[Training Epoch 100] Batch 1400, Loss 0.2481960952281952\n",
            "[Training Epoch 100] Batch 1500, Loss 0.254213809967041\n",
            "[Training Epoch 100] Batch 1600, Loss 0.27310866117477417\n",
            "[Training Epoch 100] Batch 1700, Loss 0.25732046365737915\n",
            "[Training Epoch 100] Batch 1800, Loss 0.2584102749824524\n",
            "[Training Epoch 100] Batch 1900, Loss 0.2649099826812744\n",
            "[Training Epoch 100] Batch 2000, Loss 0.2620834708213806\n",
            "[Training Epoch 100] Batch 2100, Loss 0.23876507580280304\n",
            "[Training Epoch 100] Batch 2200, Loss 0.25350555777549744\n",
            "[Training Epoch 100] Batch 2300, Loss 0.27306056022644043\n",
            "[Training Epoch 100] Batch 2400, Loss 0.2529856264591217\n",
            "[Training Epoch 100] Batch 2500, Loss 0.24338798224925995\n",
            "[Training Epoch 100] Batch 2600, Loss 0.29661470651626587\n",
            "[Training Epoch 100] Batch 2700, Loss 0.2498769760131836\n",
            "[Training Epoch 100] Batch 2800, Loss 0.2656062841415405\n",
            "[Training Epoch 100] Batch 2900, Loss 0.26256680488586426\n",
            "[Training Epoch 100] Batch 3000, Loss 0.26063522696495056\n",
            "[Training Epoch 100] Batch 3100, Loss 0.25047048926353455\n",
            "[Training Epoch 100] Batch 3200, Loss 0.27658534049987793\n",
            "[Training Epoch 100] Batch 3300, Loss 0.27342110872268677\n",
            "[Training Epoch 100] Batch 3400, Loss 0.2582879066467285\n",
            "[Training Epoch 100] Batch 3500, Loss 0.2693575322628021\n",
            "[Training Epoch 100] Batch 3600, Loss 0.2364407181739807\n",
            "[Training Epoch 100] Batch 3700, Loss 0.26166391372680664\n",
            "[Training Epoch 100] Batch 3800, Loss 0.25995486974716187\n",
            "[Training Epoch 100] Batch 3900, Loss 0.2563430666923523\n",
            "[Training Epoch 100] Batch 4000, Loss 0.2501349449157715\n",
            "[Training Epoch 100] Batch 4100, Loss 0.2649689316749573\n",
            "[Training Epoch 100] Batch 4200, Loss 0.27534550428390503\n",
            "[Training Epoch 100] Batch 4300, Loss 0.2666812539100647\n",
            "[Training Epoch 100] Batch 4400, Loss 0.2738507091999054\n",
            "[Training Epoch 100] Batch 4500, Loss 0.27419620752334595\n",
            "[Training Epoch 100] Batch 4600, Loss 0.2723390460014343\n",
            "[Training Epoch 100] Batch 4700, Loss 0.2689260244369507\n",
            "[Training Epoch 100] Batch 4800, Loss 0.28064990043640137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 100] HR = 0.6374, NDCG = 0.3678\n",
            "Epoch 101 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 101] Batch 0, Loss 0.2876758873462677\n",
            "[Training Epoch 101] Batch 100, Loss 0.2479952573776245\n",
            "[Training Epoch 101] Batch 200, Loss 0.25103941559791565\n",
            "[Training Epoch 101] Batch 300, Loss 0.2766733467578888\n",
            "[Training Epoch 101] Batch 400, Loss 0.23643723130226135\n",
            "[Training Epoch 101] Batch 500, Loss 0.2639453709125519\n",
            "[Training Epoch 101] Batch 600, Loss 0.230026975274086\n",
            "[Training Epoch 101] Batch 700, Loss 0.26294803619384766\n",
            "[Training Epoch 101] Batch 800, Loss 0.258792519569397\n",
            "[Training Epoch 101] Batch 900, Loss 0.26844289898872375\n",
            "[Training Epoch 101] Batch 1000, Loss 0.2810205817222595\n",
            "[Training Epoch 101] Batch 1100, Loss 0.27525603771209717\n",
            "[Training Epoch 101] Batch 1200, Loss 0.25786787271499634\n",
            "[Training Epoch 101] Batch 1300, Loss 0.2373524010181427\n",
            "[Training Epoch 101] Batch 1400, Loss 0.25986963510513306\n",
            "[Training Epoch 101] Batch 1500, Loss 0.2621161639690399\n",
            "[Training Epoch 101] Batch 1600, Loss 0.27634143829345703\n",
            "[Training Epoch 101] Batch 1700, Loss 0.2638415992259979\n",
            "[Training Epoch 101] Batch 1800, Loss 0.23710937798023224\n",
            "[Training Epoch 101] Batch 1900, Loss 0.2556836009025574\n",
            "[Training Epoch 101] Batch 2000, Loss 0.2720184624195099\n",
            "[Training Epoch 101] Batch 2100, Loss 0.24724140763282776\n",
            "[Training Epoch 101] Batch 2200, Loss 0.26984521746635437\n",
            "[Training Epoch 101] Batch 2300, Loss 0.2753640413284302\n",
            "[Training Epoch 101] Batch 2400, Loss 0.26103609800338745\n",
            "[Training Epoch 101] Batch 2500, Loss 0.2794796824455261\n",
            "[Training Epoch 101] Batch 2600, Loss 0.2865748405456543\n",
            "[Training Epoch 101] Batch 2700, Loss 0.2622952163219452\n",
            "[Training Epoch 101] Batch 2800, Loss 0.27641761302948\n",
            "[Training Epoch 101] Batch 2900, Loss 0.2715539038181305\n",
            "[Training Epoch 101] Batch 3000, Loss 0.29663121700286865\n",
            "[Training Epoch 101] Batch 3100, Loss 0.27594077587127686\n",
            "[Training Epoch 101] Batch 3200, Loss 0.2713887691497803\n",
            "[Training Epoch 101] Batch 3300, Loss 0.28258606791496277\n",
            "[Training Epoch 101] Batch 3400, Loss 0.25507503747940063\n",
            "[Training Epoch 101] Batch 3500, Loss 0.21859689056873322\n",
            "[Training Epoch 101] Batch 3600, Loss 0.2717466950416565\n",
            "[Training Epoch 101] Batch 3700, Loss 0.2759850025177002\n",
            "[Training Epoch 101] Batch 3800, Loss 0.2959655821323395\n",
            "[Training Epoch 101] Batch 3900, Loss 0.27089667320251465\n",
            "[Training Epoch 101] Batch 4000, Loss 0.2717670202255249\n",
            "[Training Epoch 101] Batch 4100, Loss 0.2971818447113037\n",
            "[Training Epoch 101] Batch 4200, Loss 0.2332107424736023\n",
            "[Training Epoch 101] Batch 4300, Loss 0.275412917137146\n",
            "[Training Epoch 101] Batch 4400, Loss 0.3074440658092499\n",
            "[Training Epoch 101] Batch 4500, Loss 0.2521893084049225\n",
            "[Training Epoch 101] Batch 4600, Loss 0.2760293483734131\n",
            "[Training Epoch 101] Batch 4700, Loss 0.29871684312820435\n",
            "[Training Epoch 101] Batch 4800, Loss 0.26992589235305786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 101] HR = 0.6379, NDCG = 0.3690\n",
            "Epoch 102 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 102] Batch 0, Loss 0.2715090215206146\n",
            "[Training Epoch 102] Batch 100, Loss 0.25066113471984863\n",
            "[Training Epoch 102] Batch 200, Loss 0.2679358720779419\n",
            "[Training Epoch 102] Batch 300, Loss 0.25309738516807556\n",
            "[Training Epoch 102] Batch 400, Loss 0.23482105135917664\n",
            "[Training Epoch 102] Batch 500, Loss 0.27085912227630615\n",
            "[Training Epoch 102] Batch 600, Loss 0.25921815633773804\n",
            "[Training Epoch 102] Batch 700, Loss 0.26184722781181335\n",
            "[Training Epoch 102] Batch 800, Loss 0.2790491282939911\n",
            "[Training Epoch 102] Batch 900, Loss 0.2720610499382019\n",
            "[Training Epoch 102] Batch 1000, Loss 0.2841717004776001\n",
            "[Training Epoch 102] Batch 1100, Loss 0.26472222805023193\n",
            "[Training Epoch 102] Batch 1200, Loss 0.24720355868339539\n",
            "[Training Epoch 102] Batch 1300, Loss 0.2743799388408661\n",
            "[Training Epoch 102] Batch 1400, Loss 0.26907578110694885\n",
            "[Training Epoch 102] Batch 1500, Loss 0.2403988540172577\n",
            "[Training Epoch 102] Batch 1600, Loss 0.22836649417877197\n",
            "[Training Epoch 102] Batch 1700, Loss 0.29783058166503906\n",
            "[Training Epoch 102] Batch 1800, Loss 0.2873820960521698\n",
            "[Training Epoch 102] Batch 1900, Loss 0.285664439201355\n",
            "[Training Epoch 102] Batch 2000, Loss 0.28906917572021484\n",
            "[Training Epoch 102] Batch 2100, Loss 0.28522104024887085\n",
            "[Training Epoch 102] Batch 2200, Loss 0.24154779314994812\n",
            "[Training Epoch 102] Batch 2300, Loss 0.24902525544166565\n",
            "[Training Epoch 102] Batch 2400, Loss 0.2660178542137146\n",
            "[Training Epoch 102] Batch 2500, Loss 0.27687132358551025\n",
            "[Training Epoch 102] Batch 2600, Loss 0.27391672134399414\n",
            "[Training Epoch 102] Batch 2700, Loss 0.22493547201156616\n",
            "[Training Epoch 102] Batch 2800, Loss 0.26812535524368286\n",
            "[Training Epoch 102] Batch 2900, Loss 0.24672603607177734\n",
            "[Training Epoch 102] Batch 3000, Loss 0.2568299174308777\n",
            "[Training Epoch 102] Batch 3100, Loss 0.27463334798812866\n",
            "[Training Epoch 102] Batch 3200, Loss 0.26263004541397095\n",
            "[Training Epoch 102] Batch 3300, Loss 0.26471859216690063\n",
            "[Training Epoch 102] Batch 3400, Loss 0.238925039768219\n",
            "[Training Epoch 102] Batch 3500, Loss 0.2561914324760437\n",
            "[Training Epoch 102] Batch 3600, Loss 0.26066672801971436\n",
            "[Training Epoch 102] Batch 3700, Loss 0.2837860882282257\n",
            "[Training Epoch 102] Batch 3800, Loss 0.2491905391216278\n",
            "[Training Epoch 102] Batch 3900, Loss 0.2544177174568176\n",
            "[Training Epoch 102] Batch 4000, Loss 0.24446754157543182\n",
            "[Training Epoch 102] Batch 4100, Loss 0.29572275280952454\n",
            "[Training Epoch 102] Batch 4200, Loss 0.26372358202934265\n",
            "[Training Epoch 102] Batch 4300, Loss 0.263262540102005\n",
            "[Training Epoch 102] Batch 4400, Loss 0.2655211389064789\n",
            "[Training Epoch 102] Batch 4500, Loss 0.24557796120643616\n",
            "[Training Epoch 102] Batch 4600, Loss 0.22653183341026306\n",
            "[Training Epoch 102] Batch 4700, Loss 0.25296759605407715\n",
            "[Training Epoch 102] Batch 4800, Loss 0.24128209054470062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 102] HR = 0.6384, NDCG = 0.3686\n",
            "Epoch 103 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 103] Batch 0, Loss 0.28275737166404724\n",
            "[Training Epoch 103] Batch 100, Loss 0.25676989555358887\n",
            "[Training Epoch 103] Batch 200, Loss 0.25412851572036743\n",
            "[Training Epoch 103] Batch 300, Loss 0.27768513560295105\n",
            "[Training Epoch 103] Batch 400, Loss 0.24869705736637115\n",
            "[Training Epoch 103] Batch 500, Loss 0.269748330116272\n",
            "[Training Epoch 103] Batch 600, Loss 0.29644691944122314\n",
            "[Training Epoch 103] Batch 700, Loss 0.2848889231681824\n",
            "[Training Epoch 103] Batch 800, Loss 0.24206474423408508\n",
            "[Training Epoch 103] Batch 900, Loss 0.2489396631717682\n",
            "[Training Epoch 103] Batch 1000, Loss 0.26206448674201965\n",
            "[Training Epoch 103] Batch 1100, Loss 0.26561665534973145\n",
            "[Training Epoch 103] Batch 1200, Loss 0.26254624128341675\n",
            "[Training Epoch 103] Batch 1300, Loss 0.26532796025276184\n",
            "[Training Epoch 103] Batch 1400, Loss 0.30171525478363037\n",
            "[Training Epoch 103] Batch 1500, Loss 0.27342426776885986\n",
            "[Training Epoch 103] Batch 1600, Loss 0.2526693344116211\n",
            "[Training Epoch 103] Batch 1700, Loss 0.27066969871520996\n",
            "[Training Epoch 103] Batch 1800, Loss 0.2632792592048645\n",
            "[Training Epoch 103] Batch 1900, Loss 0.2807207405567169\n",
            "[Training Epoch 103] Batch 2000, Loss 0.28005748987197876\n",
            "[Training Epoch 103] Batch 2100, Loss 0.27342867851257324\n",
            "[Training Epoch 103] Batch 2200, Loss 0.29490524530410767\n",
            "[Training Epoch 103] Batch 2300, Loss 0.2657967507839203\n",
            "[Training Epoch 103] Batch 2400, Loss 0.2508053779602051\n",
            "[Training Epoch 103] Batch 2500, Loss 0.250247597694397\n",
            "[Training Epoch 103] Batch 2600, Loss 0.2580152153968811\n",
            "[Training Epoch 103] Batch 2700, Loss 0.25530722737312317\n",
            "[Training Epoch 103] Batch 2800, Loss 0.2780936360359192\n",
            "[Training Epoch 103] Batch 2900, Loss 0.2833797335624695\n",
            "[Training Epoch 103] Batch 3000, Loss 0.2718554437160492\n",
            "[Training Epoch 103] Batch 3100, Loss 0.2871757745742798\n",
            "[Training Epoch 103] Batch 3200, Loss 0.28175145387649536\n",
            "[Training Epoch 103] Batch 3300, Loss 0.2778316140174866\n",
            "[Training Epoch 103] Batch 3400, Loss 0.2686071991920471\n",
            "[Training Epoch 103] Batch 3500, Loss 0.25576549768447876\n",
            "[Training Epoch 103] Batch 3600, Loss 0.2614915370941162\n",
            "[Training Epoch 103] Batch 3700, Loss 0.2695170044898987\n",
            "[Training Epoch 103] Batch 3800, Loss 0.2553247809410095\n",
            "[Training Epoch 103] Batch 3900, Loss 0.2756083607673645\n",
            "[Training Epoch 103] Batch 4000, Loss 0.2834945321083069\n",
            "[Training Epoch 103] Batch 4100, Loss 0.2611265182495117\n",
            "[Training Epoch 103] Batch 4200, Loss 0.24592560529708862\n",
            "[Training Epoch 103] Batch 4300, Loss 0.2569064497947693\n",
            "[Training Epoch 103] Batch 4400, Loss 0.25350427627563477\n",
            "[Training Epoch 103] Batch 4500, Loss 0.25455451011657715\n",
            "[Training Epoch 103] Batch 4600, Loss 0.2185133844614029\n",
            "[Training Epoch 103] Batch 4700, Loss 0.27170902490615845\n",
            "[Training Epoch 103] Batch 4800, Loss 0.26733994483947754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 103] HR = 0.6396, NDCG = 0.3693\n",
            "Epoch 104 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 104] Batch 0, Loss 0.27146920561790466\n",
            "[Training Epoch 104] Batch 100, Loss 0.2847795784473419\n",
            "[Training Epoch 104] Batch 200, Loss 0.2688411772251129\n",
            "[Training Epoch 104] Batch 300, Loss 0.26901763677597046\n",
            "[Training Epoch 104] Batch 400, Loss 0.2742193937301636\n",
            "[Training Epoch 104] Batch 500, Loss 0.2872616648674011\n",
            "[Training Epoch 104] Batch 600, Loss 0.26616010069847107\n",
            "[Training Epoch 104] Batch 700, Loss 0.24124187231063843\n",
            "[Training Epoch 104] Batch 800, Loss 0.23453985154628754\n",
            "[Training Epoch 104] Batch 900, Loss 0.25350746512413025\n",
            "[Training Epoch 104] Batch 1000, Loss 0.25511831045150757\n",
            "[Training Epoch 104] Batch 1100, Loss 0.2603188455104828\n",
            "[Training Epoch 104] Batch 1200, Loss 0.3006635904312134\n",
            "[Training Epoch 104] Batch 1300, Loss 0.2596571445465088\n",
            "[Training Epoch 104] Batch 1400, Loss 0.2599148154258728\n",
            "[Training Epoch 104] Batch 1500, Loss 0.2525709569454193\n",
            "[Training Epoch 104] Batch 1600, Loss 0.263799786567688\n",
            "[Training Epoch 104] Batch 1700, Loss 0.2593793570995331\n",
            "[Training Epoch 104] Batch 1800, Loss 0.25998982787132263\n",
            "[Training Epoch 104] Batch 1900, Loss 0.26546189188957214\n",
            "[Training Epoch 104] Batch 2000, Loss 0.2541905343532562\n",
            "[Training Epoch 104] Batch 2100, Loss 0.2561416029930115\n",
            "[Training Epoch 104] Batch 2200, Loss 0.23314698040485382\n",
            "[Training Epoch 104] Batch 2300, Loss 0.26213598251342773\n",
            "[Training Epoch 104] Batch 2400, Loss 0.23391348123550415\n",
            "[Training Epoch 104] Batch 2500, Loss 0.27634087204933167\n",
            "[Training Epoch 104] Batch 2600, Loss 0.2523362636566162\n",
            "[Training Epoch 104] Batch 2700, Loss 0.2829880714416504\n",
            "[Training Epoch 104] Batch 2800, Loss 0.2510893940925598\n",
            "[Training Epoch 104] Batch 2900, Loss 0.26300084590911865\n",
            "[Training Epoch 104] Batch 3000, Loss 0.26406577229499817\n",
            "[Training Epoch 104] Batch 3100, Loss 0.24744871258735657\n",
            "[Training Epoch 104] Batch 3200, Loss 0.2561017870903015\n",
            "[Training Epoch 104] Batch 3300, Loss 0.242350772023201\n",
            "[Training Epoch 104] Batch 3400, Loss 0.2469179630279541\n",
            "[Training Epoch 104] Batch 3500, Loss 0.2683591842651367\n",
            "[Training Epoch 104] Batch 3600, Loss 0.2664982080459595\n",
            "[Training Epoch 104] Batch 3700, Loss 0.25131213665008545\n",
            "[Training Epoch 104] Batch 3800, Loss 0.26182615756988525\n",
            "[Training Epoch 104] Batch 3900, Loss 0.26060763001441956\n",
            "[Training Epoch 104] Batch 4000, Loss 0.23839539289474487\n",
            "[Training Epoch 104] Batch 4100, Loss 0.25973719358444214\n",
            "[Training Epoch 104] Batch 4200, Loss 0.26474231481552124\n",
            "[Training Epoch 104] Batch 4300, Loss 0.27932101488113403\n",
            "[Training Epoch 104] Batch 4400, Loss 0.2400379776954651\n",
            "[Training Epoch 104] Batch 4500, Loss 0.2515624165534973\n",
            "[Training Epoch 104] Batch 4600, Loss 0.2642565965652466\n",
            "[Training Epoch 104] Batch 4700, Loss 0.2555047869682312\n",
            "[Training Epoch 104] Batch 4800, Loss 0.2663142681121826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 104] HR = 0.6392, NDCG = 0.3690\n",
            "Epoch 105 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 105] Batch 0, Loss 0.272666871547699\n",
            "[Training Epoch 105] Batch 100, Loss 0.2527744174003601\n",
            "[Training Epoch 105] Batch 200, Loss 0.2626975178718567\n",
            "[Training Epoch 105] Batch 300, Loss 0.23339098691940308\n",
            "[Training Epoch 105] Batch 400, Loss 0.24033021926879883\n",
            "[Training Epoch 105] Batch 500, Loss 0.23547238111495972\n",
            "[Training Epoch 105] Batch 600, Loss 0.25839412212371826\n",
            "[Training Epoch 105] Batch 700, Loss 0.23610709607601166\n",
            "[Training Epoch 105] Batch 800, Loss 0.2395896017551422\n",
            "[Training Epoch 105] Batch 900, Loss 0.2796443700790405\n",
            "[Training Epoch 105] Batch 1000, Loss 0.26635128259658813\n",
            "[Training Epoch 105] Batch 1100, Loss 0.24450059235095978\n",
            "[Training Epoch 105] Batch 1200, Loss 0.2590826153755188\n",
            "[Training Epoch 105] Batch 1300, Loss 0.2587067782878876\n",
            "[Training Epoch 105] Batch 1400, Loss 0.28404057025909424\n",
            "[Training Epoch 105] Batch 1500, Loss 0.27184927463531494\n",
            "[Training Epoch 105] Batch 1600, Loss 0.25541606545448303\n",
            "[Training Epoch 105] Batch 1700, Loss 0.2721152901649475\n",
            "[Training Epoch 105] Batch 1800, Loss 0.2732834815979004\n",
            "[Training Epoch 105] Batch 1900, Loss 0.29664355516433716\n",
            "[Training Epoch 105] Batch 2000, Loss 0.2783084511756897\n",
            "[Training Epoch 105] Batch 2100, Loss 0.2294032871723175\n",
            "[Training Epoch 105] Batch 2200, Loss 0.25406432151794434\n",
            "[Training Epoch 105] Batch 2300, Loss 0.24595907330513\n",
            "[Training Epoch 105] Batch 2400, Loss 0.23131364583969116\n",
            "[Training Epoch 105] Batch 2500, Loss 0.2829034626483917\n",
            "[Training Epoch 105] Batch 2600, Loss 0.2628086805343628\n",
            "[Training Epoch 105] Batch 2700, Loss 0.2538210153579712\n",
            "[Training Epoch 105] Batch 2800, Loss 0.2661583423614502\n",
            "[Training Epoch 105] Batch 2900, Loss 0.2720130383968353\n",
            "[Training Epoch 105] Batch 3000, Loss 0.25315117835998535\n",
            "[Training Epoch 105] Batch 3100, Loss 0.24341605603694916\n",
            "[Training Epoch 105] Batch 3200, Loss 0.2667232155799866\n",
            "[Training Epoch 105] Batch 3300, Loss 0.25568437576293945\n",
            "[Training Epoch 105] Batch 3400, Loss 0.2766830325126648\n",
            "[Training Epoch 105] Batch 3500, Loss 0.22413721680641174\n",
            "[Training Epoch 105] Batch 3600, Loss 0.24838286638259888\n",
            "[Training Epoch 105] Batch 3700, Loss 0.25768113136291504\n",
            "[Training Epoch 105] Batch 3800, Loss 0.3039030432701111\n",
            "[Training Epoch 105] Batch 3900, Loss 0.27150729298591614\n",
            "[Training Epoch 105] Batch 4000, Loss 0.279031366109848\n",
            "[Training Epoch 105] Batch 4100, Loss 0.25507500767707825\n",
            "[Training Epoch 105] Batch 4200, Loss 0.261014461517334\n",
            "[Training Epoch 105] Batch 4300, Loss 0.24447885155677795\n",
            "[Training Epoch 105] Batch 4400, Loss 0.2714667320251465\n",
            "[Training Epoch 105] Batch 4500, Loss 0.2530202269554138\n",
            "[Training Epoch 105] Batch 4600, Loss 0.29807618260383606\n",
            "[Training Epoch 105] Batch 4700, Loss 0.2688262164592743\n",
            "[Training Epoch 105] Batch 4800, Loss 0.28645122051239014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 105] HR = 0.6376, NDCG = 0.3692\n",
            "Epoch 106 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 106] Batch 0, Loss 0.25784388184547424\n",
            "[Training Epoch 106] Batch 100, Loss 0.25057053565979004\n",
            "[Training Epoch 106] Batch 200, Loss 0.2514418065547943\n",
            "[Training Epoch 106] Batch 300, Loss 0.25847291946411133\n",
            "[Training Epoch 106] Batch 400, Loss 0.26411163806915283\n",
            "[Training Epoch 106] Batch 500, Loss 0.26779046654701233\n",
            "[Training Epoch 106] Batch 600, Loss 0.2552224099636078\n",
            "[Training Epoch 106] Batch 700, Loss 0.2839913070201874\n",
            "[Training Epoch 106] Batch 800, Loss 0.24523374438285828\n",
            "[Training Epoch 106] Batch 900, Loss 0.2587605118751526\n",
            "[Training Epoch 106] Batch 1000, Loss 0.26821017265319824\n",
            "[Training Epoch 106] Batch 1100, Loss 0.2997286915779114\n",
            "[Training Epoch 106] Batch 1200, Loss 0.2545199394226074\n",
            "[Training Epoch 106] Batch 1300, Loss 0.2611946165561676\n",
            "[Training Epoch 106] Batch 1400, Loss 0.2939470708370209\n",
            "[Training Epoch 106] Batch 1500, Loss 0.24911165237426758\n",
            "[Training Epoch 106] Batch 1600, Loss 0.2687452435493469\n",
            "[Training Epoch 106] Batch 1700, Loss 0.2381964474916458\n",
            "[Training Epoch 106] Batch 1800, Loss 0.2759203314781189\n",
            "[Training Epoch 106] Batch 1900, Loss 0.26747846603393555\n",
            "[Training Epoch 106] Batch 2000, Loss 0.24422037601470947\n",
            "[Training Epoch 106] Batch 2100, Loss 0.2643393278121948\n",
            "[Training Epoch 106] Batch 2200, Loss 0.2630090117454529\n",
            "[Training Epoch 106] Batch 2300, Loss 0.2553769648075104\n",
            "[Training Epoch 106] Batch 2400, Loss 0.2946898937225342\n",
            "[Training Epoch 106] Batch 2500, Loss 0.2804918587207794\n",
            "[Training Epoch 106] Batch 2600, Loss 0.2509523332118988\n",
            "[Training Epoch 106] Batch 2700, Loss 0.2637781500816345\n",
            "[Training Epoch 106] Batch 2800, Loss 0.2615848779678345\n",
            "[Training Epoch 106] Batch 2900, Loss 0.2642837166786194\n",
            "[Training Epoch 106] Batch 3000, Loss 0.2527121305465698\n",
            "[Training Epoch 106] Batch 3100, Loss 0.2947366237640381\n",
            "[Training Epoch 106] Batch 3200, Loss 0.25529709458351135\n",
            "[Training Epoch 106] Batch 3300, Loss 0.27120086550712585\n",
            "[Training Epoch 106] Batch 3400, Loss 0.2642134130001068\n",
            "[Training Epoch 106] Batch 3500, Loss 0.25794702768325806\n",
            "[Training Epoch 106] Batch 3600, Loss 0.251672625541687\n",
            "[Training Epoch 106] Batch 3700, Loss 0.27850931882858276\n",
            "[Training Epoch 106] Batch 3800, Loss 0.24648433923721313\n",
            "[Training Epoch 106] Batch 3900, Loss 0.2662135362625122\n",
            "[Training Epoch 106] Batch 4000, Loss 0.2764953374862671\n",
            "[Training Epoch 106] Batch 4100, Loss 0.26218855381011963\n",
            "[Training Epoch 106] Batch 4200, Loss 0.26148539781570435\n",
            "[Training Epoch 106] Batch 4300, Loss 0.27885696291923523\n",
            "[Training Epoch 106] Batch 4400, Loss 0.26122793555259705\n",
            "[Training Epoch 106] Batch 4500, Loss 0.24047411978244781\n",
            "[Training Epoch 106] Batch 4600, Loss 0.2615125775337219\n",
            "[Training Epoch 106] Batch 4700, Loss 0.2628138065338135\n",
            "[Training Epoch 106] Batch 4800, Loss 0.26527056097984314\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 106] HR = 0.6397, NDCG = 0.3691\n",
            "Epoch 107 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 107] Batch 0, Loss 0.27659475803375244\n",
            "[Training Epoch 107] Batch 100, Loss 0.29040491580963135\n",
            "[Training Epoch 107] Batch 200, Loss 0.2423143982887268\n",
            "[Training Epoch 107] Batch 300, Loss 0.24832262098789215\n",
            "[Training Epoch 107] Batch 400, Loss 0.22769685089588165\n",
            "[Training Epoch 107] Batch 500, Loss 0.2580137252807617\n",
            "[Training Epoch 107] Batch 600, Loss 0.2542586624622345\n",
            "[Training Epoch 107] Batch 700, Loss 0.25706222653388977\n",
            "[Training Epoch 107] Batch 800, Loss 0.25235795974731445\n",
            "[Training Epoch 107] Batch 900, Loss 0.25105762481689453\n",
            "[Training Epoch 107] Batch 1000, Loss 0.2691401243209839\n",
            "[Training Epoch 107] Batch 1100, Loss 0.2904868721961975\n",
            "[Training Epoch 107] Batch 1200, Loss 0.270419716835022\n",
            "[Training Epoch 107] Batch 1300, Loss 0.2767689824104309\n",
            "[Training Epoch 107] Batch 1400, Loss 0.27622753381729126\n",
            "[Training Epoch 107] Batch 1500, Loss 0.2410534918308258\n",
            "[Training Epoch 107] Batch 1600, Loss 0.27093443274497986\n",
            "[Training Epoch 107] Batch 1700, Loss 0.23728877305984497\n",
            "[Training Epoch 107] Batch 1800, Loss 0.24945765733718872\n",
            "[Training Epoch 107] Batch 1900, Loss 0.29173678159713745\n",
            "[Training Epoch 107] Batch 2000, Loss 0.2465846836566925\n",
            "[Training Epoch 107] Batch 2100, Loss 0.24734251201152802\n",
            "[Training Epoch 107] Batch 2200, Loss 0.2822284400463104\n",
            "[Training Epoch 107] Batch 2300, Loss 0.2904476225376129\n",
            "[Training Epoch 107] Batch 2400, Loss 0.30845963954925537\n",
            "[Training Epoch 107] Batch 2500, Loss 0.257601797580719\n",
            "[Training Epoch 107] Batch 2600, Loss 0.2606533169746399\n",
            "[Training Epoch 107] Batch 2700, Loss 0.2669863700866699\n",
            "[Training Epoch 107] Batch 2800, Loss 0.27314233779907227\n",
            "[Training Epoch 107] Batch 2900, Loss 0.2682725787162781\n",
            "[Training Epoch 107] Batch 3000, Loss 0.2506752908229828\n",
            "[Training Epoch 107] Batch 3100, Loss 0.26062458753585815\n",
            "[Training Epoch 107] Batch 3200, Loss 0.26151740550994873\n",
            "[Training Epoch 107] Batch 3300, Loss 0.27604609727859497\n",
            "[Training Epoch 107] Batch 3400, Loss 0.2532082796096802\n",
            "[Training Epoch 107] Batch 3500, Loss 0.25067365169525146\n",
            "[Training Epoch 107] Batch 3600, Loss 0.26081582903862\n",
            "[Training Epoch 107] Batch 3700, Loss 0.24801932275295258\n",
            "[Training Epoch 107] Batch 3800, Loss 0.267751544713974\n",
            "[Training Epoch 107] Batch 3900, Loss 0.2773648500442505\n",
            "[Training Epoch 107] Batch 4000, Loss 0.2507060170173645\n",
            "[Training Epoch 107] Batch 4100, Loss 0.2629569172859192\n",
            "[Training Epoch 107] Batch 4200, Loss 0.2707093358039856\n",
            "[Training Epoch 107] Batch 4300, Loss 0.25838732719421387\n",
            "[Training Epoch 107] Batch 4400, Loss 0.2623593509197235\n",
            "[Training Epoch 107] Batch 4500, Loss 0.2869136333465576\n",
            "[Training Epoch 107] Batch 4600, Loss 0.26333773136138916\n",
            "[Training Epoch 107] Batch 4700, Loss 0.26737308502197266\n",
            "[Training Epoch 107] Batch 4800, Loss 0.2452213168144226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 107] HR = 0.6353, NDCG = 0.3679\n",
            "Epoch 108 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 108] Batch 0, Loss 0.2539600133895874\n",
            "[Training Epoch 108] Batch 100, Loss 0.2651585042476654\n",
            "[Training Epoch 108] Batch 200, Loss 0.2585359811782837\n",
            "[Training Epoch 108] Batch 300, Loss 0.3019440770149231\n",
            "[Training Epoch 108] Batch 400, Loss 0.2928692698478699\n",
            "[Training Epoch 108] Batch 500, Loss 0.2649269998073578\n",
            "[Training Epoch 108] Batch 600, Loss 0.24076446890830994\n",
            "[Training Epoch 108] Batch 700, Loss 0.24952733516693115\n",
            "[Training Epoch 108] Batch 800, Loss 0.2862458825111389\n",
            "[Training Epoch 108] Batch 900, Loss 0.26620492339134216\n",
            "[Training Epoch 108] Batch 1000, Loss 0.24593320488929749\n",
            "[Training Epoch 108] Batch 1100, Loss 0.2719472050666809\n",
            "[Training Epoch 108] Batch 1200, Loss 0.2403292953968048\n",
            "[Training Epoch 108] Batch 1300, Loss 0.25987935066223145\n",
            "[Training Epoch 108] Batch 1400, Loss 0.2598492503166199\n",
            "[Training Epoch 108] Batch 1500, Loss 0.26795536279678345\n",
            "[Training Epoch 108] Batch 1600, Loss 0.2938746511936188\n",
            "[Training Epoch 108] Batch 1700, Loss 0.28926175832748413\n",
            "[Training Epoch 108] Batch 1800, Loss 0.27704551815986633\n",
            "[Training Epoch 108] Batch 1900, Loss 0.2663915753364563\n",
            "[Training Epoch 108] Batch 2000, Loss 0.2878072261810303\n",
            "[Training Epoch 108] Batch 2100, Loss 0.28937625885009766\n",
            "[Training Epoch 108] Batch 2200, Loss 0.26845788955688477\n",
            "[Training Epoch 108] Batch 2300, Loss 0.29600298404693604\n",
            "[Training Epoch 108] Batch 2400, Loss 0.27040258049964905\n",
            "[Training Epoch 108] Batch 2500, Loss 0.2700396776199341\n",
            "[Training Epoch 108] Batch 2600, Loss 0.29034560918807983\n",
            "[Training Epoch 108] Batch 2700, Loss 0.273556649684906\n",
            "[Training Epoch 108] Batch 2800, Loss 0.23844097554683685\n",
            "[Training Epoch 108] Batch 2900, Loss 0.26877158880233765\n",
            "[Training Epoch 108] Batch 3000, Loss 0.2721034288406372\n",
            "[Training Epoch 108] Batch 3100, Loss 0.23918157815933228\n",
            "[Training Epoch 108] Batch 3200, Loss 0.25726091861724854\n",
            "[Training Epoch 108] Batch 3300, Loss 0.26182979345321655\n",
            "[Training Epoch 108] Batch 3400, Loss 0.2746831774711609\n",
            "[Training Epoch 108] Batch 3500, Loss 0.2421739548444748\n",
            "[Training Epoch 108] Batch 3600, Loss 0.2764856815338135\n",
            "[Training Epoch 108] Batch 3700, Loss 0.23974096775054932\n",
            "[Training Epoch 108] Batch 3800, Loss 0.27706378698349\n",
            "[Training Epoch 108] Batch 3900, Loss 0.2693519592285156\n",
            "[Training Epoch 108] Batch 4000, Loss 0.2821040749549866\n",
            "[Training Epoch 108] Batch 4100, Loss 0.25531041622161865\n",
            "[Training Epoch 108] Batch 4200, Loss 0.24528416991233826\n",
            "[Training Epoch 108] Batch 4300, Loss 0.25683149695396423\n",
            "[Training Epoch 108] Batch 4400, Loss 0.26772162318229675\n",
            "[Training Epoch 108] Batch 4500, Loss 0.22902128100395203\n",
            "[Training Epoch 108] Batch 4600, Loss 0.2698444128036499\n",
            "[Training Epoch 108] Batch 4700, Loss 0.2598881721496582\n",
            "[Training Epoch 108] Batch 4800, Loss 0.27185288071632385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 108] HR = 0.6353, NDCG = 0.3691\n",
            "Epoch 109 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 109] Batch 0, Loss 0.2593475580215454\n",
            "[Training Epoch 109] Batch 100, Loss 0.26464277505874634\n",
            "[Training Epoch 109] Batch 200, Loss 0.27081236243247986\n",
            "[Training Epoch 109] Batch 300, Loss 0.286682665348053\n",
            "[Training Epoch 109] Batch 400, Loss 0.2557808756828308\n",
            "[Training Epoch 109] Batch 500, Loss 0.26130837202072144\n",
            "[Training Epoch 109] Batch 600, Loss 0.259311705827713\n",
            "[Training Epoch 109] Batch 700, Loss 0.27427083253860474\n",
            "[Training Epoch 109] Batch 800, Loss 0.2663941979408264\n",
            "[Training Epoch 109] Batch 900, Loss 0.25772345066070557\n",
            "[Training Epoch 109] Batch 1000, Loss 0.25001004338264465\n",
            "[Training Epoch 109] Batch 1100, Loss 0.25434502959251404\n",
            "[Training Epoch 109] Batch 1200, Loss 0.248592346906662\n",
            "[Training Epoch 109] Batch 1300, Loss 0.24365480244159698\n",
            "[Training Epoch 109] Batch 1400, Loss 0.26145169138908386\n",
            "[Training Epoch 109] Batch 1500, Loss 0.2549007534980774\n",
            "[Training Epoch 109] Batch 1600, Loss 0.26632168889045715\n",
            "[Training Epoch 109] Batch 1700, Loss 0.28502732515335083\n",
            "[Training Epoch 109] Batch 1800, Loss 0.24242433905601501\n",
            "[Training Epoch 109] Batch 1900, Loss 0.2718534767627716\n",
            "[Training Epoch 109] Batch 2000, Loss 0.24986401200294495\n",
            "[Training Epoch 109] Batch 2100, Loss 0.2788652777671814\n",
            "[Training Epoch 109] Batch 2200, Loss 0.2678324580192566\n",
            "[Training Epoch 109] Batch 2300, Loss 0.258158415555954\n",
            "[Training Epoch 109] Batch 2400, Loss 0.2545223832130432\n",
            "[Training Epoch 109] Batch 2500, Loss 0.2549305856227875\n",
            "[Training Epoch 109] Batch 2600, Loss 0.2830688953399658\n",
            "[Training Epoch 109] Batch 2700, Loss 0.2531149685382843\n",
            "[Training Epoch 109] Batch 2800, Loss 0.25145190954208374\n",
            "[Training Epoch 109] Batch 2900, Loss 0.2633565664291382\n",
            "[Training Epoch 109] Batch 3000, Loss 0.24892514944076538\n",
            "[Training Epoch 109] Batch 3100, Loss 0.2538580894470215\n",
            "[Training Epoch 109] Batch 3200, Loss 0.28840699791908264\n",
            "[Training Epoch 109] Batch 3300, Loss 0.27137312293052673\n",
            "[Training Epoch 109] Batch 3400, Loss 0.25903546810150146\n",
            "[Training Epoch 109] Batch 3500, Loss 0.28196313977241516\n",
            "[Training Epoch 109] Batch 3600, Loss 0.29223793745040894\n",
            "[Training Epoch 109] Batch 3700, Loss 0.2686395049095154\n",
            "[Training Epoch 109] Batch 3800, Loss 0.2618563175201416\n",
            "[Training Epoch 109] Batch 3900, Loss 0.2509503960609436\n",
            "[Training Epoch 109] Batch 4000, Loss 0.2960077226161957\n",
            "[Training Epoch 109] Batch 4100, Loss 0.26259520649909973\n",
            "[Training Epoch 109] Batch 4200, Loss 0.24262677133083344\n",
            "[Training Epoch 109] Batch 4300, Loss 0.2755146622657776\n",
            "[Training Epoch 109] Batch 4400, Loss 0.25245124101638794\n",
            "[Training Epoch 109] Batch 4500, Loss 0.25222572684288025\n",
            "[Training Epoch 109] Batch 4600, Loss 0.24643395841121674\n",
            "[Training Epoch 109] Batch 4700, Loss 0.2768457531929016\n",
            "[Training Epoch 109] Batch 4800, Loss 0.26843443512916565\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 109] HR = 0.6354, NDCG = 0.3678\n",
            "Epoch 110 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 110] Batch 0, Loss 0.26783615350723267\n",
            "[Training Epoch 110] Batch 100, Loss 0.27312421798706055\n",
            "[Training Epoch 110] Batch 200, Loss 0.26671454310417175\n",
            "[Training Epoch 110] Batch 300, Loss 0.2616468071937561\n",
            "[Training Epoch 110] Batch 400, Loss 0.2573060989379883\n",
            "[Training Epoch 110] Batch 500, Loss 0.2934305667877197\n",
            "[Training Epoch 110] Batch 600, Loss 0.272702157497406\n",
            "[Training Epoch 110] Batch 700, Loss 0.26290857791900635\n",
            "[Training Epoch 110] Batch 800, Loss 0.2489623725414276\n",
            "[Training Epoch 110] Batch 900, Loss 0.26957714557647705\n",
            "[Training Epoch 110] Batch 1000, Loss 0.30337756872177124\n",
            "[Training Epoch 110] Batch 1100, Loss 0.25852420926094055\n",
            "[Training Epoch 110] Batch 1200, Loss 0.28273332118988037\n",
            "[Training Epoch 110] Batch 1300, Loss 0.23882076144218445\n",
            "[Training Epoch 110] Batch 1400, Loss 0.2533283829689026\n",
            "[Training Epoch 110] Batch 1500, Loss 0.29000064730644226\n",
            "[Training Epoch 110] Batch 1600, Loss 0.2694888412952423\n",
            "[Training Epoch 110] Batch 1700, Loss 0.2764410376548767\n",
            "[Training Epoch 110] Batch 1800, Loss 0.2610607445240021\n",
            "[Training Epoch 110] Batch 1900, Loss 0.27013227343559265\n",
            "[Training Epoch 110] Batch 2000, Loss 0.2713772654533386\n",
            "[Training Epoch 110] Batch 2100, Loss 0.2599187195301056\n",
            "[Training Epoch 110] Batch 2200, Loss 0.28016990423202515\n",
            "[Training Epoch 110] Batch 2300, Loss 0.261910080909729\n",
            "[Training Epoch 110] Batch 2400, Loss 0.26703131198883057\n",
            "[Training Epoch 110] Batch 2500, Loss 0.2577735185623169\n",
            "[Training Epoch 110] Batch 2600, Loss 0.2354458123445511\n",
            "[Training Epoch 110] Batch 2700, Loss 0.2764049172401428\n",
            "[Training Epoch 110] Batch 2800, Loss 0.25025033950805664\n",
            "[Training Epoch 110] Batch 2900, Loss 0.2394045889377594\n",
            "[Training Epoch 110] Batch 3000, Loss 0.25267499685287476\n",
            "[Training Epoch 110] Batch 3100, Loss 0.2701414227485657\n",
            "[Training Epoch 110] Batch 3200, Loss 0.2947580814361572\n",
            "[Training Epoch 110] Batch 3300, Loss 0.26794514060020447\n",
            "[Training Epoch 110] Batch 3400, Loss 0.2729406952857971\n",
            "[Training Epoch 110] Batch 3500, Loss 0.2553824782371521\n",
            "[Training Epoch 110] Batch 3600, Loss 0.25840699672698975\n",
            "[Training Epoch 110] Batch 3700, Loss 0.253815621137619\n",
            "[Training Epoch 110] Batch 3800, Loss 0.26577287912368774\n",
            "[Training Epoch 110] Batch 3900, Loss 0.2982499897480011\n",
            "[Training Epoch 110] Batch 4000, Loss 0.27769917249679565\n",
            "[Training Epoch 110] Batch 4100, Loss 0.252544105052948\n",
            "[Training Epoch 110] Batch 4200, Loss 0.24658656120300293\n",
            "[Training Epoch 110] Batch 4300, Loss 0.26304522156715393\n",
            "[Training Epoch 110] Batch 4400, Loss 0.27367061376571655\n",
            "[Training Epoch 110] Batch 4500, Loss 0.25193342566490173\n",
            "[Training Epoch 110] Batch 4600, Loss 0.24138978123664856\n",
            "[Training Epoch 110] Batch 4700, Loss 0.26154977083206177\n",
            "[Training Epoch 110] Batch 4800, Loss 0.282060831785202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 110] HR = 0.6379, NDCG = 0.3688\n",
            "Epoch 111 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 111] Batch 0, Loss 0.30266010761260986\n",
            "[Training Epoch 111] Batch 100, Loss 0.29411888122558594\n",
            "[Training Epoch 111] Batch 200, Loss 0.2636532783508301\n",
            "[Training Epoch 111] Batch 300, Loss 0.2637861371040344\n",
            "[Training Epoch 111] Batch 400, Loss 0.27134984731674194\n",
            "[Training Epoch 111] Batch 500, Loss 0.2682318687438965\n",
            "[Training Epoch 111] Batch 600, Loss 0.2557099461555481\n",
            "[Training Epoch 111] Batch 700, Loss 0.2625776529312134\n",
            "[Training Epoch 111] Batch 800, Loss 0.24481633305549622\n",
            "[Training Epoch 111] Batch 900, Loss 0.25045129656791687\n",
            "[Training Epoch 111] Batch 1000, Loss 0.22433537244796753\n",
            "[Training Epoch 111] Batch 1100, Loss 0.28557366132736206\n",
            "[Training Epoch 111] Batch 1200, Loss 0.29019343852996826\n",
            "[Training Epoch 111] Batch 1300, Loss 0.24815824627876282\n",
            "[Training Epoch 111] Batch 1400, Loss 0.27447670698165894\n",
            "[Training Epoch 111] Batch 1500, Loss 0.2586595416069031\n",
            "[Training Epoch 111] Batch 1600, Loss 0.23975950479507446\n",
            "[Training Epoch 111] Batch 1700, Loss 0.26627105474472046\n",
            "[Training Epoch 111] Batch 1800, Loss 0.2547234892845154\n",
            "[Training Epoch 111] Batch 1900, Loss 0.2733740508556366\n",
            "[Training Epoch 111] Batch 2000, Loss 0.27099311351776123\n",
            "[Training Epoch 111] Batch 2100, Loss 0.25748884677886963\n",
            "[Training Epoch 111] Batch 2200, Loss 0.2677020728588104\n",
            "[Training Epoch 111] Batch 2300, Loss 0.24991042912006378\n",
            "[Training Epoch 111] Batch 2400, Loss 0.25290411710739136\n",
            "[Training Epoch 111] Batch 2500, Loss 0.26848581433296204\n",
            "[Training Epoch 111] Batch 2600, Loss 0.23573404550552368\n",
            "[Training Epoch 111] Batch 2700, Loss 0.2825457751750946\n",
            "[Training Epoch 111] Batch 2800, Loss 0.27483952045440674\n",
            "[Training Epoch 111] Batch 2900, Loss 0.23966175317764282\n",
            "[Training Epoch 111] Batch 3000, Loss 0.2408493459224701\n",
            "[Training Epoch 111] Batch 3100, Loss 0.24460065364837646\n",
            "[Training Epoch 111] Batch 3200, Loss 0.2544747591018677\n",
            "[Training Epoch 111] Batch 3300, Loss 0.24800987541675568\n",
            "[Training Epoch 111] Batch 3400, Loss 0.2799840569496155\n",
            "[Training Epoch 111] Batch 3500, Loss 0.2482585310935974\n",
            "[Training Epoch 111] Batch 3600, Loss 0.23383373022079468\n",
            "[Training Epoch 111] Batch 3700, Loss 0.25513842701911926\n",
            "[Training Epoch 111] Batch 3800, Loss 0.2526623606681824\n",
            "[Training Epoch 111] Batch 3900, Loss 0.2635326683521271\n",
            "[Training Epoch 111] Batch 4000, Loss 0.28764647245407104\n",
            "[Training Epoch 111] Batch 4100, Loss 0.2682305574417114\n",
            "[Training Epoch 111] Batch 4200, Loss 0.24104401469230652\n",
            "[Training Epoch 111] Batch 4300, Loss 0.2576731741428375\n",
            "[Training Epoch 111] Batch 4400, Loss 0.26062944531440735\n",
            "[Training Epoch 111] Batch 4500, Loss 0.2486170530319214\n",
            "[Training Epoch 111] Batch 4600, Loss 0.24802248179912567\n",
            "[Training Epoch 111] Batch 4700, Loss 0.2680799067020416\n",
            "[Training Epoch 111] Batch 4800, Loss 0.2551889419555664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 111] HR = 0.6354, NDCG = 0.3680\n",
            "Epoch 112 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 112] Batch 0, Loss 0.2658211588859558\n",
            "[Training Epoch 112] Batch 100, Loss 0.24132056534290314\n",
            "[Training Epoch 112] Batch 200, Loss 0.28942227363586426\n",
            "[Training Epoch 112] Batch 300, Loss 0.2590656280517578\n",
            "[Training Epoch 112] Batch 400, Loss 0.2757672667503357\n",
            "[Training Epoch 112] Batch 500, Loss 0.25612205266952515\n",
            "[Training Epoch 112] Batch 600, Loss 0.2603578567504883\n",
            "[Training Epoch 112] Batch 700, Loss 0.26355963945388794\n",
            "[Training Epoch 112] Batch 800, Loss 0.2977634072303772\n",
            "[Training Epoch 112] Batch 900, Loss 0.25262925028800964\n",
            "[Training Epoch 112] Batch 1000, Loss 0.2688397467136383\n",
            "[Training Epoch 112] Batch 1100, Loss 0.23602430522441864\n",
            "[Training Epoch 112] Batch 1200, Loss 0.27103888988494873\n",
            "[Training Epoch 112] Batch 1300, Loss 0.24431759119033813\n",
            "[Training Epoch 112] Batch 1400, Loss 0.24919439852237701\n",
            "[Training Epoch 112] Batch 1500, Loss 0.25937312841415405\n",
            "[Training Epoch 112] Batch 1600, Loss 0.2821861505508423\n",
            "[Training Epoch 112] Batch 1700, Loss 0.26507139205932617\n",
            "[Training Epoch 112] Batch 1800, Loss 0.255601704120636\n",
            "[Training Epoch 112] Batch 1900, Loss 0.23151159286499023\n",
            "[Training Epoch 112] Batch 2000, Loss 0.28924626111984253\n",
            "[Training Epoch 112] Batch 2100, Loss 0.266478955745697\n",
            "[Training Epoch 112] Batch 2200, Loss 0.2607983648777008\n",
            "[Training Epoch 112] Batch 2300, Loss 0.24059438705444336\n",
            "[Training Epoch 112] Batch 2400, Loss 0.279732346534729\n",
            "[Training Epoch 112] Batch 2500, Loss 0.2421911656856537\n",
            "[Training Epoch 112] Batch 2600, Loss 0.2597469687461853\n",
            "[Training Epoch 112] Batch 2700, Loss 0.3017193675041199\n",
            "[Training Epoch 112] Batch 2800, Loss 0.2613471746444702\n",
            "[Training Epoch 112] Batch 2900, Loss 0.2436358630657196\n",
            "[Training Epoch 112] Batch 3000, Loss 0.2870938181877136\n",
            "[Training Epoch 112] Batch 3100, Loss 0.24277806282043457\n",
            "[Training Epoch 112] Batch 3200, Loss 0.26449185609817505\n",
            "[Training Epoch 112] Batch 3300, Loss 0.23806320130825043\n",
            "[Training Epoch 112] Batch 3400, Loss 0.28885018825531006\n",
            "[Training Epoch 112] Batch 3500, Loss 0.2684033513069153\n",
            "[Training Epoch 112] Batch 3600, Loss 0.22016361355781555\n",
            "[Training Epoch 112] Batch 3700, Loss 0.24355657398700714\n",
            "[Training Epoch 112] Batch 3800, Loss 0.27705952525138855\n",
            "[Training Epoch 112] Batch 3900, Loss 0.2801562249660492\n",
            "[Training Epoch 112] Batch 4000, Loss 0.2501121759414673\n",
            "[Training Epoch 112] Batch 4100, Loss 0.27583417296409607\n",
            "[Training Epoch 112] Batch 4200, Loss 0.25997307896614075\n",
            "[Training Epoch 112] Batch 4300, Loss 0.2701457142829895\n",
            "[Training Epoch 112] Batch 4400, Loss 0.25877541303634644\n",
            "[Training Epoch 112] Batch 4500, Loss 0.27735698223114014\n",
            "[Training Epoch 112] Batch 4600, Loss 0.2647073268890381\n",
            "[Training Epoch 112] Batch 4700, Loss 0.2911134362220764\n",
            "[Training Epoch 112] Batch 4800, Loss 0.28401806950569153\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 112] HR = 0.6363, NDCG = 0.3686\n",
            "Epoch 113 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 113] Batch 0, Loss 0.2399122565984726\n",
            "[Training Epoch 113] Batch 100, Loss 0.24239583313465118\n",
            "[Training Epoch 113] Batch 200, Loss 0.2726580500602722\n",
            "[Training Epoch 113] Batch 300, Loss 0.26201188564300537\n",
            "[Training Epoch 113] Batch 400, Loss 0.2774079442024231\n",
            "[Training Epoch 113] Batch 500, Loss 0.23675572872161865\n",
            "[Training Epoch 113] Batch 600, Loss 0.2693183422088623\n",
            "[Training Epoch 113] Batch 700, Loss 0.26640984416007996\n",
            "[Training Epoch 113] Batch 800, Loss 0.28009068965911865\n",
            "[Training Epoch 113] Batch 900, Loss 0.24623863399028778\n",
            "[Training Epoch 113] Batch 1000, Loss 0.2603025436401367\n",
            "[Training Epoch 113] Batch 1100, Loss 0.2608226239681244\n",
            "[Training Epoch 113] Batch 1200, Loss 0.2523462772369385\n",
            "[Training Epoch 113] Batch 1300, Loss 0.25505512952804565\n",
            "[Training Epoch 113] Batch 1400, Loss 0.25237295031547546\n",
            "[Training Epoch 113] Batch 1500, Loss 0.27374255657196045\n",
            "[Training Epoch 113] Batch 1600, Loss 0.2962726950645447\n",
            "[Training Epoch 113] Batch 1700, Loss 0.2699524760246277\n",
            "[Training Epoch 113] Batch 1800, Loss 0.24542778730392456\n",
            "[Training Epoch 113] Batch 1900, Loss 0.23344548046588898\n",
            "[Training Epoch 113] Batch 2000, Loss 0.2519378960132599\n",
            "[Training Epoch 113] Batch 2100, Loss 0.26196926832199097\n",
            "[Training Epoch 113] Batch 2200, Loss 0.2736600637435913\n",
            "[Training Epoch 113] Batch 2300, Loss 0.23149651288986206\n",
            "[Training Epoch 113] Batch 2400, Loss 0.291504442691803\n",
            "[Training Epoch 113] Batch 2500, Loss 0.29958000779151917\n",
            "[Training Epoch 113] Batch 2600, Loss 0.27667236328125\n",
            "[Training Epoch 113] Batch 2700, Loss 0.2484172135591507\n",
            "[Training Epoch 113] Batch 2800, Loss 0.2514440417289734\n",
            "[Training Epoch 113] Batch 2900, Loss 0.2497546374797821\n",
            "[Training Epoch 113] Batch 3000, Loss 0.27748924493789673\n",
            "[Training Epoch 113] Batch 3100, Loss 0.26632583141326904\n",
            "[Training Epoch 113] Batch 3200, Loss 0.2538799047470093\n",
            "[Training Epoch 113] Batch 3300, Loss 0.23631519079208374\n",
            "[Training Epoch 113] Batch 3400, Loss 0.29164671897888184\n",
            "[Training Epoch 113] Batch 3500, Loss 0.2906544506549835\n",
            "[Training Epoch 113] Batch 3600, Loss 0.2977730631828308\n",
            "[Training Epoch 113] Batch 3700, Loss 0.25258463621139526\n",
            "[Training Epoch 113] Batch 3800, Loss 0.294249951839447\n",
            "[Training Epoch 113] Batch 3900, Loss 0.26556289196014404\n",
            "[Training Epoch 113] Batch 4000, Loss 0.2919960916042328\n",
            "[Training Epoch 113] Batch 4100, Loss 0.2702534794807434\n",
            "[Training Epoch 113] Batch 4200, Loss 0.2647530138492584\n",
            "[Training Epoch 113] Batch 4300, Loss 0.2687256932258606\n",
            "[Training Epoch 113] Batch 4400, Loss 0.2513974905014038\n",
            "[Training Epoch 113] Batch 4500, Loss 0.2438955456018448\n",
            "[Training Epoch 113] Batch 4600, Loss 0.303092896938324\n",
            "[Training Epoch 113] Batch 4700, Loss 0.24955540895462036\n",
            "[Training Epoch 113] Batch 4800, Loss 0.26356756687164307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 113] HR = 0.6377, NDCG = 0.3693\n",
            "Epoch 114 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 114] Batch 0, Loss 0.25299474596977234\n",
            "[Training Epoch 114] Batch 100, Loss 0.2431427240371704\n",
            "[Training Epoch 114] Batch 200, Loss 0.2663946747779846\n",
            "[Training Epoch 114] Batch 300, Loss 0.259980708360672\n",
            "[Training Epoch 114] Batch 400, Loss 0.264661967754364\n",
            "[Training Epoch 114] Batch 500, Loss 0.2434033900499344\n",
            "[Training Epoch 114] Batch 600, Loss 0.25790899991989136\n",
            "[Training Epoch 114] Batch 700, Loss 0.2420543134212494\n",
            "[Training Epoch 114] Batch 800, Loss 0.25577080249786377\n",
            "[Training Epoch 114] Batch 900, Loss 0.28430837392807007\n",
            "[Training Epoch 114] Batch 1000, Loss 0.26485389471054077\n",
            "[Training Epoch 114] Batch 1100, Loss 0.26802924275398254\n",
            "[Training Epoch 114] Batch 1200, Loss 0.2727797329425812\n",
            "[Training Epoch 114] Batch 1300, Loss 0.28170067071914673\n",
            "[Training Epoch 114] Batch 1400, Loss 0.27701336145401\n",
            "[Training Epoch 114] Batch 1500, Loss 0.24435795843601227\n",
            "[Training Epoch 114] Batch 1600, Loss 0.25022995471954346\n",
            "[Training Epoch 114] Batch 1700, Loss 0.24276532232761383\n",
            "[Training Epoch 114] Batch 1800, Loss 0.273493230342865\n",
            "[Training Epoch 114] Batch 1900, Loss 0.2522773742675781\n",
            "[Training Epoch 114] Batch 2000, Loss 0.2676571011543274\n",
            "[Training Epoch 114] Batch 2100, Loss 0.2571154236793518\n",
            "[Training Epoch 114] Batch 2200, Loss 0.27661460638046265\n",
            "[Training Epoch 114] Batch 2300, Loss 0.2587546408176422\n",
            "[Training Epoch 114] Batch 2400, Loss 0.2741846740245819\n",
            "[Training Epoch 114] Batch 2500, Loss 0.24755756556987762\n",
            "[Training Epoch 114] Batch 2600, Loss 0.27168434858322144\n",
            "[Training Epoch 114] Batch 2700, Loss 0.27769649028778076\n",
            "[Training Epoch 114] Batch 2800, Loss 0.25370728969573975\n",
            "[Training Epoch 114] Batch 2900, Loss 0.2600294053554535\n",
            "[Training Epoch 114] Batch 3000, Loss 0.2550511360168457\n",
            "[Training Epoch 114] Batch 3100, Loss 0.27902042865753174\n",
            "[Training Epoch 114] Batch 3200, Loss 0.26025307178497314\n",
            "[Training Epoch 114] Batch 3300, Loss 0.2374444305896759\n",
            "[Training Epoch 114] Batch 3400, Loss 0.30475398898124695\n",
            "[Training Epoch 114] Batch 3500, Loss 0.26850658655166626\n",
            "[Training Epoch 114] Batch 3600, Loss 0.2616910934448242\n",
            "[Training Epoch 114] Batch 3700, Loss 0.2587146759033203\n",
            "[Training Epoch 114] Batch 3800, Loss 0.2706905007362366\n",
            "[Training Epoch 114] Batch 3900, Loss 0.27194029092788696\n",
            "[Training Epoch 114] Batch 4000, Loss 0.26594045758247375\n",
            "[Training Epoch 114] Batch 4100, Loss 0.27505260705947876\n",
            "[Training Epoch 114] Batch 4200, Loss 0.28431937098503113\n",
            "[Training Epoch 114] Batch 4300, Loss 0.25104689598083496\n",
            "[Training Epoch 114] Batch 4400, Loss 0.25141414999961853\n",
            "[Training Epoch 114] Batch 4500, Loss 0.27812808752059937\n",
            "[Training Epoch 114] Batch 4600, Loss 0.29642167687416077\n",
            "[Training Epoch 114] Batch 4700, Loss 0.28859981894493103\n",
            "[Training Epoch 114] Batch 4800, Loss 0.2808002829551697\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 114] HR = 0.6373, NDCG = 0.3695\n",
            "Epoch 115 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 115] Batch 0, Loss 0.2602922022342682\n",
            "[Training Epoch 115] Batch 100, Loss 0.2592974603176117\n",
            "[Training Epoch 115] Batch 200, Loss 0.25879448652267456\n",
            "[Training Epoch 115] Batch 300, Loss 0.23881153762340546\n",
            "[Training Epoch 115] Batch 400, Loss 0.29256635904312134\n",
            "[Training Epoch 115] Batch 500, Loss 0.24916253983974457\n",
            "[Training Epoch 115] Batch 600, Loss 0.24292388558387756\n",
            "[Training Epoch 115] Batch 700, Loss 0.24847012758255005\n",
            "[Training Epoch 115] Batch 800, Loss 0.27266478538513184\n",
            "[Training Epoch 115] Batch 900, Loss 0.2643132507801056\n",
            "[Training Epoch 115] Batch 1000, Loss 0.2712641656398773\n",
            "[Training Epoch 115] Batch 1100, Loss 0.2677420973777771\n",
            "[Training Epoch 115] Batch 1200, Loss 0.2726967930793762\n",
            "[Training Epoch 115] Batch 1300, Loss 0.2525799572467804\n",
            "[Training Epoch 115] Batch 1400, Loss 0.24022671580314636\n",
            "[Training Epoch 115] Batch 1500, Loss 0.252775102853775\n",
            "[Training Epoch 115] Batch 1600, Loss 0.26768165826797485\n",
            "[Training Epoch 115] Batch 1700, Loss 0.26512402296066284\n",
            "[Training Epoch 115] Batch 1800, Loss 0.24586142599582672\n",
            "[Training Epoch 115] Batch 1900, Loss 0.2599913775920868\n",
            "[Training Epoch 115] Batch 2000, Loss 0.2616598308086395\n",
            "[Training Epoch 115] Batch 2100, Loss 0.2954747676849365\n",
            "[Training Epoch 115] Batch 2200, Loss 0.2440219670534134\n",
            "[Training Epoch 115] Batch 2300, Loss 0.24604038894176483\n",
            "[Training Epoch 115] Batch 2400, Loss 0.268554151058197\n",
            "[Training Epoch 115] Batch 2500, Loss 0.24754303693771362\n",
            "[Training Epoch 115] Batch 2600, Loss 0.24263176321983337\n",
            "[Training Epoch 115] Batch 2700, Loss 0.2728755474090576\n",
            "[Training Epoch 115] Batch 2800, Loss 0.26438653469085693\n",
            "[Training Epoch 115] Batch 2900, Loss 0.2785157561302185\n",
            "[Training Epoch 115] Batch 3000, Loss 0.2459242045879364\n",
            "[Training Epoch 115] Batch 3100, Loss 0.2541082501411438\n",
            "[Training Epoch 115] Batch 3200, Loss 0.2636606693267822\n",
            "[Training Epoch 115] Batch 3300, Loss 0.25771188735961914\n",
            "[Training Epoch 115] Batch 3400, Loss 0.26816412806510925\n",
            "[Training Epoch 115] Batch 3500, Loss 0.2663472294807434\n",
            "[Training Epoch 115] Batch 3600, Loss 0.2555791139602661\n",
            "[Training Epoch 115] Batch 3700, Loss 0.28143417835235596\n",
            "[Training Epoch 115] Batch 3800, Loss 0.26950982213020325\n",
            "[Training Epoch 115] Batch 3900, Loss 0.26293686032295227\n",
            "[Training Epoch 115] Batch 4000, Loss 0.2721515893936157\n",
            "[Training Epoch 115] Batch 4100, Loss 0.252322793006897\n",
            "[Training Epoch 115] Batch 4200, Loss 0.2763547897338867\n",
            "[Training Epoch 115] Batch 4300, Loss 0.2645696997642517\n",
            "[Training Epoch 115] Batch 4400, Loss 0.2539163827896118\n",
            "[Training Epoch 115] Batch 4500, Loss 0.257041871547699\n",
            "[Training Epoch 115] Batch 4600, Loss 0.28063279390335083\n",
            "[Training Epoch 115] Batch 4700, Loss 0.24320438504219055\n",
            "[Training Epoch 115] Batch 4800, Loss 0.2659415602684021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 115] HR = 0.6368, NDCG = 0.3693\n",
            "Epoch 116 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 116] Batch 0, Loss 0.23109674453735352\n",
            "[Training Epoch 116] Batch 100, Loss 0.25982511043548584\n",
            "[Training Epoch 116] Batch 200, Loss 0.23324793577194214\n",
            "[Training Epoch 116] Batch 300, Loss 0.26863688230514526\n",
            "[Training Epoch 116] Batch 400, Loss 0.25163453817367554\n",
            "[Training Epoch 116] Batch 500, Loss 0.2733514606952667\n",
            "[Training Epoch 116] Batch 600, Loss 0.2864565849304199\n",
            "[Training Epoch 116] Batch 700, Loss 0.2306230366230011\n",
            "[Training Epoch 116] Batch 800, Loss 0.2767055332660675\n",
            "[Training Epoch 116] Batch 900, Loss 0.2636867165565491\n",
            "[Training Epoch 116] Batch 1000, Loss 0.26605260372161865\n",
            "[Training Epoch 116] Batch 1100, Loss 0.28573858737945557\n",
            "[Training Epoch 116] Batch 1200, Loss 0.242080420255661\n",
            "[Training Epoch 116] Batch 1300, Loss 0.2375829517841339\n",
            "[Training Epoch 116] Batch 1400, Loss 0.2597528398036957\n",
            "[Training Epoch 116] Batch 1500, Loss 0.24696135520935059\n",
            "[Training Epoch 116] Batch 1600, Loss 0.2579314112663269\n",
            "[Training Epoch 116] Batch 1700, Loss 0.2508782148361206\n",
            "[Training Epoch 116] Batch 1800, Loss 0.2721285820007324\n",
            "[Training Epoch 116] Batch 1900, Loss 0.2968793511390686\n",
            "[Training Epoch 116] Batch 2000, Loss 0.23921585083007812\n",
            "[Training Epoch 116] Batch 2100, Loss 0.24489398300647736\n",
            "[Training Epoch 116] Batch 2200, Loss 0.24313263595104218\n",
            "[Training Epoch 116] Batch 2300, Loss 0.23939946293830872\n",
            "[Training Epoch 116] Batch 2400, Loss 0.2612379193305969\n",
            "[Training Epoch 116] Batch 2500, Loss 0.25783684849739075\n",
            "[Training Epoch 116] Batch 2600, Loss 0.2838296890258789\n",
            "[Training Epoch 116] Batch 2700, Loss 0.2533559799194336\n",
            "[Training Epoch 116] Batch 2800, Loss 0.2387261986732483\n",
            "[Training Epoch 116] Batch 2900, Loss 0.2735591530799866\n",
            "[Training Epoch 116] Batch 3000, Loss 0.2697758376598358\n",
            "[Training Epoch 116] Batch 3100, Loss 0.2457638829946518\n",
            "[Training Epoch 116] Batch 3200, Loss 0.26666873693466187\n",
            "[Training Epoch 116] Batch 3300, Loss 0.2507832646369934\n",
            "[Training Epoch 116] Batch 3400, Loss 0.2737194001674652\n",
            "[Training Epoch 116] Batch 3500, Loss 0.2830188274383545\n",
            "[Training Epoch 116] Batch 3600, Loss 0.26723653078079224\n",
            "[Training Epoch 116] Batch 3700, Loss 0.26647448539733887\n",
            "[Training Epoch 116] Batch 3800, Loss 0.2539304792881012\n",
            "[Training Epoch 116] Batch 3900, Loss 0.2910517752170563\n",
            "[Training Epoch 116] Batch 4000, Loss 0.27188894152641296\n",
            "[Training Epoch 116] Batch 4100, Loss 0.2511173188686371\n",
            "[Training Epoch 116] Batch 4200, Loss 0.2564167380332947\n",
            "[Training Epoch 116] Batch 4300, Loss 0.2883239686489105\n",
            "[Training Epoch 116] Batch 4400, Loss 0.2828580141067505\n",
            "[Training Epoch 116] Batch 4500, Loss 0.2496873140335083\n",
            "[Training Epoch 116] Batch 4600, Loss 0.2667098045349121\n",
            "[Training Epoch 116] Batch 4700, Loss 0.228884756565094\n",
            "[Training Epoch 116] Batch 4800, Loss 0.28838562965393066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 116] HR = 0.6376, NDCG = 0.3683\n",
            "Epoch 117 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 117] Batch 0, Loss 0.2773566246032715\n",
            "[Training Epoch 117] Batch 100, Loss 0.23946523666381836\n",
            "[Training Epoch 117] Batch 200, Loss 0.2522885799407959\n",
            "[Training Epoch 117] Batch 300, Loss 0.24750131368637085\n",
            "[Training Epoch 117] Batch 400, Loss 0.2624289393424988\n",
            "[Training Epoch 117] Batch 500, Loss 0.2714700400829315\n",
            "[Training Epoch 117] Batch 600, Loss 0.267536461353302\n",
            "[Training Epoch 117] Batch 700, Loss 0.27014634013175964\n",
            "[Training Epoch 117] Batch 800, Loss 0.25732719898223877\n",
            "[Training Epoch 117] Batch 900, Loss 0.2634425759315491\n",
            "[Training Epoch 117] Batch 1000, Loss 0.27141058444976807\n",
            "[Training Epoch 117] Batch 1100, Loss 0.2794959545135498\n",
            "[Training Epoch 117] Batch 1200, Loss 0.2710303068161011\n",
            "[Training Epoch 117] Batch 1300, Loss 0.25381746888160706\n",
            "[Training Epoch 117] Batch 1400, Loss 0.26597195863723755\n",
            "[Training Epoch 117] Batch 1500, Loss 0.264237642288208\n",
            "[Training Epoch 117] Batch 1600, Loss 0.2283487766981125\n",
            "[Training Epoch 117] Batch 1700, Loss 0.3005709648132324\n",
            "[Training Epoch 117] Batch 1800, Loss 0.27793774008750916\n",
            "[Training Epoch 117] Batch 1900, Loss 0.27825018763542175\n",
            "[Training Epoch 117] Batch 2000, Loss 0.274222731590271\n",
            "[Training Epoch 117] Batch 2100, Loss 0.2924080193042755\n",
            "[Training Epoch 117] Batch 2200, Loss 0.2777329087257385\n",
            "[Training Epoch 117] Batch 2300, Loss 0.25138577818870544\n",
            "[Training Epoch 117] Batch 2400, Loss 0.25070667266845703\n",
            "[Training Epoch 117] Batch 2500, Loss 0.24960049986839294\n",
            "[Training Epoch 117] Batch 2600, Loss 0.26351064443588257\n",
            "[Training Epoch 117] Batch 2700, Loss 0.2874033451080322\n",
            "[Training Epoch 117] Batch 2800, Loss 0.25766193866729736\n",
            "[Training Epoch 117] Batch 2900, Loss 0.25585681200027466\n",
            "[Training Epoch 117] Batch 3000, Loss 0.2658378481864929\n",
            "[Training Epoch 117] Batch 3100, Loss 0.26133087277412415\n",
            "[Training Epoch 117] Batch 3200, Loss 0.24349962174892426\n",
            "[Training Epoch 117] Batch 3300, Loss 0.27097630500793457\n",
            "[Training Epoch 117] Batch 3400, Loss 0.2616073489189148\n",
            "[Training Epoch 117] Batch 3500, Loss 0.24652332067489624\n",
            "[Training Epoch 117] Batch 3600, Loss 0.2404618114233017\n",
            "[Training Epoch 117] Batch 3700, Loss 0.2591213285923004\n",
            "[Training Epoch 117] Batch 3800, Loss 0.27194729447364807\n",
            "[Training Epoch 117] Batch 3900, Loss 0.28565964102745056\n",
            "[Training Epoch 117] Batch 4000, Loss 0.2666780948638916\n",
            "[Training Epoch 117] Batch 4100, Loss 0.256368488073349\n",
            "[Training Epoch 117] Batch 4200, Loss 0.2723119854927063\n",
            "[Training Epoch 117] Batch 4300, Loss 0.24374240636825562\n",
            "[Training Epoch 117] Batch 4400, Loss 0.25796958804130554\n",
            "[Training Epoch 117] Batch 4500, Loss 0.2680988311767578\n",
            "[Training Epoch 117] Batch 4600, Loss 0.26204413175582886\n",
            "[Training Epoch 117] Batch 4700, Loss 0.26104485988616943\n",
            "[Training Epoch 117] Batch 4800, Loss 0.2413049042224884\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 117] HR = 0.6376, NDCG = 0.3697\n",
            "Epoch 118 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 118] Batch 0, Loss 0.2513652443885803\n",
            "[Training Epoch 118] Batch 100, Loss 0.26707881689071655\n",
            "[Training Epoch 118] Batch 200, Loss 0.26378777623176575\n",
            "[Training Epoch 118] Batch 300, Loss 0.26592880487442017\n",
            "[Training Epoch 118] Batch 400, Loss 0.24314571917057037\n",
            "[Training Epoch 118] Batch 500, Loss 0.26900023221969604\n",
            "[Training Epoch 118] Batch 600, Loss 0.2626745104789734\n",
            "[Training Epoch 118] Batch 700, Loss 0.26795846223831177\n",
            "[Training Epoch 118] Batch 800, Loss 0.2715670168399811\n",
            "[Training Epoch 118] Batch 900, Loss 0.25869256258010864\n",
            "[Training Epoch 118] Batch 1000, Loss 0.2531023621559143\n",
            "[Training Epoch 118] Batch 1100, Loss 0.29179877042770386\n",
            "[Training Epoch 118] Batch 1200, Loss 0.29914677143096924\n",
            "[Training Epoch 118] Batch 1300, Loss 0.26210957765579224\n",
            "[Training Epoch 118] Batch 1400, Loss 0.2644679546356201\n",
            "[Training Epoch 118] Batch 1500, Loss 0.28476178646087646\n",
            "[Training Epoch 118] Batch 1600, Loss 0.263234406709671\n",
            "[Training Epoch 118] Batch 1700, Loss 0.2506125271320343\n",
            "[Training Epoch 118] Batch 1800, Loss 0.2656322419643402\n",
            "[Training Epoch 118] Batch 1900, Loss 0.2789618670940399\n",
            "[Training Epoch 118] Batch 2000, Loss 0.2642737627029419\n",
            "[Training Epoch 118] Batch 2100, Loss 0.25942766666412354\n",
            "[Training Epoch 118] Batch 2200, Loss 0.2534736394882202\n",
            "[Training Epoch 118] Batch 2300, Loss 0.259628564119339\n",
            "[Training Epoch 118] Batch 2400, Loss 0.2863265872001648\n",
            "[Training Epoch 118] Batch 2500, Loss 0.2615604102611542\n",
            "[Training Epoch 118] Batch 2600, Loss 0.2859926223754883\n",
            "[Training Epoch 118] Batch 2700, Loss 0.2533503770828247\n",
            "[Training Epoch 118] Batch 2800, Loss 0.24259692430496216\n",
            "[Training Epoch 118] Batch 2900, Loss 0.2681407928466797\n",
            "[Training Epoch 118] Batch 3000, Loss 0.23759230971336365\n",
            "[Training Epoch 118] Batch 3100, Loss 0.26557421684265137\n",
            "[Training Epoch 118] Batch 3200, Loss 0.23781195282936096\n",
            "[Training Epoch 118] Batch 3300, Loss 0.24382953345775604\n",
            "[Training Epoch 118] Batch 3400, Loss 0.26710689067840576\n",
            "[Training Epoch 118] Batch 3500, Loss 0.2541082799434662\n",
            "[Training Epoch 118] Batch 3600, Loss 0.2667602300643921\n",
            "[Training Epoch 118] Batch 3700, Loss 0.2813704311847687\n",
            "[Training Epoch 118] Batch 3800, Loss 0.26372748613357544\n",
            "[Training Epoch 118] Batch 3900, Loss 0.2533676326274872\n",
            "[Training Epoch 118] Batch 4000, Loss 0.31040453910827637\n",
            "[Training Epoch 118] Batch 4100, Loss 0.288200318813324\n",
            "[Training Epoch 118] Batch 4200, Loss 0.26000675559043884\n",
            "[Training Epoch 118] Batch 4300, Loss 0.25673431158065796\n",
            "[Training Epoch 118] Batch 4400, Loss 0.26629120111465454\n",
            "[Training Epoch 118] Batch 4500, Loss 0.26650935411453247\n",
            "[Training Epoch 118] Batch 4600, Loss 0.287330687046051\n",
            "[Training Epoch 118] Batch 4700, Loss 0.26552826166152954\n",
            "[Training Epoch 118] Batch 4800, Loss 0.288089781999588\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 118] HR = 0.6361, NDCG = 0.3692\n",
            "Epoch 119 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 119] Batch 0, Loss 0.2685038447380066\n",
            "[Training Epoch 119] Batch 100, Loss 0.23705190420150757\n",
            "[Training Epoch 119] Batch 200, Loss 0.27627551555633545\n",
            "[Training Epoch 119] Batch 300, Loss 0.23909899592399597\n",
            "[Training Epoch 119] Batch 400, Loss 0.2829977869987488\n",
            "[Training Epoch 119] Batch 500, Loss 0.22548340260982513\n",
            "[Training Epoch 119] Batch 600, Loss 0.26127469539642334\n",
            "[Training Epoch 119] Batch 700, Loss 0.2719806432723999\n",
            "[Training Epoch 119] Batch 800, Loss 0.26172634959220886\n",
            "[Training Epoch 119] Batch 900, Loss 0.2594281733036041\n",
            "[Training Epoch 119] Batch 1000, Loss 0.2631630599498749\n",
            "[Training Epoch 119] Batch 1100, Loss 0.26648128032684326\n",
            "[Training Epoch 119] Batch 1200, Loss 0.2733899652957916\n",
            "[Training Epoch 119] Batch 1300, Loss 0.23383483290672302\n",
            "[Training Epoch 119] Batch 1400, Loss 0.2728363871574402\n",
            "[Training Epoch 119] Batch 1500, Loss 0.30087709426879883\n",
            "[Training Epoch 119] Batch 1600, Loss 0.2577458620071411\n",
            "[Training Epoch 119] Batch 1700, Loss 0.2918326258659363\n",
            "[Training Epoch 119] Batch 1800, Loss 0.2535461187362671\n",
            "[Training Epoch 119] Batch 1900, Loss 0.30753040313720703\n",
            "[Training Epoch 119] Batch 2000, Loss 0.28377658128738403\n",
            "[Training Epoch 119] Batch 2100, Loss 0.2843969464302063\n",
            "[Training Epoch 119] Batch 2200, Loss 0.25371992588043213\n",
            "[Training Epoch 119] Batch 2300, Loss 0.25474947690963745\n",
            "[Training Epoch 119] Batch 2400, Loss 0.29024186730384827\n",
            "[Training Epoch 119] Batch 2500, Loss 0.2640317678451538\n",
            "[Training Epoch 119] Batch 2600, Loss 0.26393237709999084\n",
            "[Training Epoch 119] Batch 2700, Loss 0.23680606484413147\n",
            "[Training Epoch 119] Batch 2800, Loss 0.2711801826953888\n",
            "[Training Epoch 119] Batch 2900, Loss 0.27161040902137756\n",
            "[Training Epoch 119] Batch 3000, Loss 0.2726057469844818\n",
            "[Training Epoch 119] Batch 3100, Loss 0.2673056721687317\n",
            "[Training Epoch 119] Batch 3200, Loss 0.270041823387146\n",
            "[Training Epoch 119] Batch 3300, Loss 0.259552538394928\n",
            "[Training Epoch 119] Batch 3400, Loss 0.24049779772758484\n",
            "[Training Epoch 119] Batch 3500, Loss 0.2943674623966217\n",
            "[Training Epoch 119] Batch 3600, Loss 0.2539723515510559\n",
            "[Training Epoch 119] Batch 3700, Loss 0.26727819442749023\n",
            "[Training Epoch 119] Batch 3800, Loss 0.254010409116745\n",
            "[Training Epoch 119] Batch 3900, Loss 0.2792857885360718\n",
            "[Training Epoch 119] Batch 4000, Loss 0.250887930393219\n",
            "[Training Epoch 119] Batch 4100, Loss 0.2520594596862793\n",
            "[Training Epoch 119] Batch 4200, Loss 0.31492894887924194\n",
            "[Training Epoch 119] Batch 4300, Loss 0.2972099184989929\n",
            "[Training Epoch 119] Batch 4400, Loss 0.2816316783428192\n",
            "[Training Epoch 119] Batch 4500, Loss 0.28832805156707764\n",
            "[Training Epoch 119] Batch 4600, Loss 0.2881177067756653\n",
            "[Training Epoch 119] Batch 4700, Loss 0.26601406931877136\n",
            "[Training Epoch 119] Batch 4800, Loss 0.24619069695472717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 119] HR = 0.6384, NDCG = 0.3685\n",
            "Epoch 120 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 120] Batch 0, Loss 0.2798670530319214\n",
            "[Training Epoch 120] Batch 100, Loss 0.3072414994239807\n",
            "[Training Epoch 120] Batch 200, Loss 0.2790048122406006\n",
            "[Training Epoch 120] Batch 300, Loss 0.2513349652290344\n",
            "[Training Epoch 120] Batch 400, Loss 0.24858781695365906\n",
            "[Training Epoch 120] Batch 500, Loss 0.2657816410064697\n",
            "[Training Epoch 120] Batch 600, Loss 0.27451658248901367\n",
            "[Training Epoch 120] Batch 700, Loss 0.2634592652320862\n",
            "[Training Epoch 120] Batch 800, Loss 0.23621076345443726\n",
            "[Training Epoch 120] Batch 900, Loss 0.26800239086151123\n",
            "[Training Epoch 120] Batch 1000, Loss 0.24738284945487976\n",
            "[Training Epoch 120] Batch 1100, Loss 0.2710742652416229\n",
            "[Training Epoch 120] Batch 1200, Loss 0.27115923166275024\n",
            "[Training Epoch 120] Batch 1300, Loss 0.249614879488945\n",
            "[Training Epoch 120] Batch 1400, Loss 0.25725406408309937\n",
            "[Training Epoch 120] Batch 1500, Loss 0.25663334131240845\n",
            "[Training Epoch 120] Batch 1600, Loss 0.28641730546951294\n",
            "[Training Epoch 120] Batch 1700, Loss 0.2708629369735718\n",
            "[Training Epoch 120] Batch 1800, Loss 0.263980507850647\n",
            "[Training Epoch 120] Batch 1900, Loss 0.24784953892230988\n",
            "[Training Epoch 120] Batch 2000, Loss 0.25327324867248535\n",
            "[Training Epoch 120] Batch 2100, Loss 0.22309549152851105\n",
            "[Training Epoch 120] Batch 2200, Loss 0.23468166589736938\n",
            "[Training Epoch 120] Batch 2300, Loss 0.3267590403556824\n",
            "[Training Epoch 120] Batch 2400, Loss 0.25059449672698975\n",
            "[Training Epoch 120] Batch 2500, Loss 0.25094667077064514\n",
            "[Training Epoch 120] Batch 2600, Loss 0.2615581750869751\n",
            "[Training Epoch 120] Batch 2700, Loss 0.2585703134536743\n",
            "[Training Epoch 120] Batch 2800, Loss 0.2541138827800751\n",
            "[Training Epoch 120] Batch 2900, Loss 0.27181127667427063\n",
            "[Training Epoch 120] Batch 3000, Loss 0.2922647297382355\n",
            "[Training Epoch 120] Batch 3100, Loss 0.27603238821029663\n",
            "[Training Epoch 120] Batch 3200, Loss 0.2739417254924774\n",
            "[Training Epoch 120] Batch 3300, Loss 0.2540850043296814\n",
            "[Training Epoch 120] Batch 3400, Loss 0.24944254755973816\n",
            "[Training Epoch 120] Batch 3500, Loss 0.2904536724090576\n",
            "[Training Epoch 120] Batch 3600, Loss 0.23926809430122375\n",
            "[Training Epoch 120] Batch 3700, Loss 0.2817150354385376\n",
            "[Training Epoch 120] Batch 3800, Loss 0.2668876647949219\n",
            "[Training Epoch 120] Batch 3900, Loss 0.27016347646713257\n",
            "[Training Epoch 120] Batch 4000, Loss 0.24025991559028625\n",
            "[Training Epoch 120] Batch 4100, Loss 0.26388129591941833\n",
            "[Training Epoch 120] Batch 4200, Loss 0.2552267909049988\n",
            "[Training Epoch 120] Batch 4300, Loss 0.27920252084732056\n",
            "[Training Epoch 120] Batch 4400, Loss 0.26572489738464355\n",
            "[Training Epoch 120] Batch 4500, Loss 0.26340141892433167\n",
            "[Training Epoch 120] Batch 4600, Loss 0.2767522931098938\n",
            "[Training Epoch 120] Batch 4700, Loss 0.2593507170677185\n",
            "[Training Epoch 120] Batch 4800, Loss 0.2860841453075409\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 120] HR = 0.6346, NDCG = 0.3682\n",
            "Epoch 121 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 121] Batch 0, Loss 0.25532519817352295\n",
            "[Training Epoch 121] Batch 100, Loss 0.27719539403915405\n",
            "[Training Epoch 121] Batch 200, Loss 0.2635912299156189\n",
            "[Training Epoch 121] Batch 300, Loss 0.25014668703079224\n",
            "[Training Epoch 121] Batch 400, Loss 0.2550812065601349\n",
            "[Training Epoch 121] Batch 500, Loss 0.2566620707511902\n",
            "[Training Epoch 121] Batch 600, Loss 0.27024877071380615\n",
            "[Training Epoch 121] Batch 700, Loss 0.2456073760986328\n",
            "[Training Epoch 121] Batch 800, Loss 0.24419422447681427\n",
            "[Training Epoch 121] Batch 900, Loss 0.29616308212280273\n",
            "[Training Epoch 121] Batch 1000, Loss 0.24495992064476013\n",
            "[Training Epoch 121] Batch 1100, Loss 0.25547605752944946\n",
            "[Training Epoch 121] Batch 1200, Loss 0.28222838044166565\n",
            "[Training Epoch 121] Batch 1300, Loss 0.297169953584671\n",
            "[Training Epoch 121] Batch 1400, Loss 0.28414949774742126\n",
            "[Training Epoch 121] Batch 1500, Loss 0.2702212333679199\n",
            "[Training Epoch 121] Batch 1600, Loss 0.24869073927402496\n",
            "[Training Epoch 121] Batch 1700, Loss 0.25530266761779785\n",
            "[Training Epoch 121] Batch 1800, Loss 0.2782696485519409\n",
            "[Training Epoch 121] Batch 1900, Loss 0.24224135279655457\n",
            "[Training Epoch 121] Batch 2000, Loss 0.24990451335906982\n",
            "[Training Epoch 121] Batch 2100, Loss 0.27143973112106323\n",
            "[Training Epoch 121] Batch 2200, Loss 0.24295157194137573\n",
            "[Training Epoch 121] Batch 2300, Loss 0.25059807300567627\n",
            "[Training Epoch 121] Batch 2400, Loss 0.2502293288707733\n",
            "[Training Epoch 121] Batch 2500, Loss 0.27049344778060913\n",
            "[Training Epoch 121] Batch 2600, Loss 0.264556348323822\n",
            "[Training Epoch 121] Batch 2700, Loss 0.29010042548179626\n",
            "[Training Epoch 121] Batch 2800, Loss 0.2849552631378174\n",
            "[Training Epoch 121] Batch 2900, Loss 0.2717645764350891\n",
            "[Training Epoch 121] Batch 3000, Loss 0.2632254958152771\n",
            "[Training Epoch 121] Batch 3100, Loss 0.2601110339164734\n",
            "[Training Epoch 121] Batch 3200, Loss 0.2412649393081665\n",
            "[Training Epoch 121] Batch 3300, Loss 0.2582852244377136\n",
            "[Training Epoch 121] Batch 3400, Loss 0.2840512990951538\n",
            "[Training Epoch 121] Batch 3500, Loss 0.27834352850914\n",
            "[Training Epoch 121] Batch 3600, Loss 0.286915123462677\n",
            "[Training Epoch 121] Batch 3700, Loss 0.2584616541862488\n",
            "[Training Epoch 121] Batch 3800, Loss 0.25810927152633667\n",
            "[Training Epoch 121] Batch 3900, Loss 0.26322200894355774\n",
            "[Training Epoch 121] Batch 4000, Loss 0.26489508152008057\n",
            "[Training Epoch 121] Batch 4100, Loss 0.25018414855003357\n",
            "[Training Epoch 121] Batch 4200, Loss 0.24014325439929962\n",
            "[Training Epoch 121] Batch 4300, Loss 0.2562495470046997\n",
            "[Training Epoch 121] Batch 4400, Loss 0.2588852345943451\n",
            "[Training Epoch 121] Batch 4500, Loss 0.255299836397171\n",
            "[Training Epoch 121] Batch 4600, Loss 0.24375389516353607\n",
            "[Training Epoch 121] Batch 4700, Loss 0.2803555130958557\n",
            "[Training Epoch 121] Batch 4800, Loss 0.28037694096565247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 121] HR = 0.6346, NDCG = 0.3681\n",
            "Epoch 122 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 122] Batch 0, Loss 0.2858220338821411\n",
            "[Training Epoch 122] Batch 100, Loss 0.27015793323516846\n",
            "[Training Epoch 122] Batch 200, Loss 0.27963870763778687\n",
            "[Training Epoch 122] Batch 300, Loss 0.289753794670105\n",
            "[Training Epoch 122] Batch 400, Loss 0.25735020637512207\n",
            "[Training Epoch 122] Batch 500, Loss 0.22745630145072937\n",
            "[Training Epoch 122] Batch 600, Loss 0.251901775598526\n",
            "[Training Epoch 122] Batch 700, Loss 0.2901118993759155\n",
            "[Training Epoch 122] Batch 800, Loss 0.27387189865112305\n",
            "[Training Epoch 122] Batch 900, Loss 0.25842276215553284\n",
            "[Training Epoch 122] Batch 1000, Loss 0.2797176241874695\n",
            "[Training Epoch 122] Batch 1100, Loss 0.28426241874694824\n",
            "[Training Epoch 122] Batch 1200, Loss 0.2709263563156128\n",
            "[Training Epoch 122] Batch 1300, Loss 0.2684786915779114\n",
            "[Training Epoch 122] Batch 1400, Loss 0.236915722489357\n",
            "[Training Epoch 122] Batch 1500, Loss 0.24267780780792236\n",
            "[Training Epoch 122] Batch 1600, Loss 0.2752847671508789\n",
            "[Training Epoch 122] Batch 1700, Loss 0.2531311511993408\n",
            "[Training Epoch 122] Batch 1800, Loss 0.2687944173812866\n",
            "[Training Epoch 122] Batch 1900, Loss 0.25462788343429565\n",
            "[Training Epoch 122] Batch 2000, Loss 0.26212552189826965\n",
            "[Training Epoch 122] Batch 2100, Loss 0.25772547721862793\n",
            "[Training Epoch 122] Batch 2200, Loss 0.27372869849205017\n",
            "[Training Epoch 122] Batch 2300, Loss 0.2363547384738922\n",
            "[Training Epoch 122] Batch 2400, Loss 0.2850441336631775\n",
            "[Training Epoch 122] Batch 2500, Loss 0.21803626418113708\n",
            "[Training Epoch 122] Batch 2600, Loss 0.2605668306350708\n",
            "[Training Epoch 122] Batch 2700, Loss 0.24594196677207947\n",
            "[Training Epoch 122] Batch 2800, Loss 0.227093905210495\n",
            "[Training Epoch 122] Batch 2900, Loss 0.2558113634586334\n",
            "[Training Epoch 122] Batch 3000, Loss 0.26122617721557617\n",
            "[Training Epoch 122] Batch 3100, Loss 0.2830578684806824\n",
            "[Training Epoch 122] Batch 3200, Loss 0.2764080762863159\n",
            "[Training Epoch 122] Batch 3300, Loss 0.2718120515346527\n",
            "[Training Epoch 122] Batch 3400, Loss 0.26063281297683716\n",
            "[Training Epoch 122] Batch 3500, Loss 0.25827473402023315\n",
            "[Training Epoch 122] Batch 3600, Loss 0.26165419816970825\n",
            "[Training Epoch 122] Batch 3700, Loss 0.26316744089126587\n",
            "[Training Epoch 122] Batch 3800, Loss 0.2685571610927582\n",
            "[Training Epoch 122] Batch 3900, Loss 0.26775676012039185\n",
            "[Training Epoch 122] Batch 4000, Loss 0.25656402111053467\n",
            "[Training Epoch 122] Batch 4100, Loss 0.27147507667541504\n",
            "[Training Epoch 122] Batch 4200, Loss 0.29554253816604614\n",
            "[Training Epoch 122] Batch 4300, Loss 0.2819410562515259\n",
            "[Training Epoch 122] Batch 4400, Loss 0.2402414083480835\n",
            "[Training Epoch 122] Batch 4500, Loss 0.2913298010826111\n",
            "[Training Epoch 122] Batch 4600, Loss 0.27550554275512695\n",
            "[Training Epoch 122] Batch 4700, Loss 0.2582128047943115\n",
            "[Training Epoch 122] Batch 4800, Loss 0.28783875703811646\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 122] HR = 0.6371, NDCG = 0.3686\n",
            "Epoch 123 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 123] Batch 0, Loss 0.3029467761516571\n",
            "[Training Epoch 123] Batch 100, Loss 0.2577075958251953\n",
            "[Training Epoch 123] Batch 200, Loss 0.27780476212501526\n",
            "[Training Epoch 123] Batch 300, Loss 0.2715657353401184\n",
            "[Training Epoch 123] Batch 400, Loss 0.2713015079498291\n",
            "[Training Epoch 123] Batch 500, Loss 0.25743746757507324\n",
            "[Training Epoch 123] Batch 600, Loss 0.24907955527305603\n",
            "[Training Epoch 123] Batch 700, Loss 0.27717918157577515\n",
            "[Training Epoch 123] Batch 800, Loss 0.2781444787979126\n",
            "[Training Epoch 123] Batch 900, Loss 0.21911481022834778\n",
            "[Training Epoch 123] Batch 1000, Loss 0.22844445705413818\n",
            "[Training Epoch 123] Batch 1100, Loss 0.2641744911670685\n",
            "[Training Epoch 123] Batch 1200, Loss 0.25867754220962524\n",
            "[Training Epoch 123] Batch 1300, Loss 0.26513028144836426\n",
            "[Training Epoch 123] Batch 1400, Loss 0.2720511257648468\n",
            "[Training Epoch 123] Batch 1500, Loss 0.26444387435913086\n",
            "[Training Epoch 123] Batch 1600, Loss 0.30818089842796326\n",
            "[Training Epoch 123] Batch 1700, Loss 0.2782849669456482\n",
            "[Training Epoch 123] Batch 1800, Loss 0.2604692578315735\n",
            "[Training Epoch 123] Batch 1900, Loss 0.25742071866989136\n",
            "[Training Epoch 123] Batch 2000, Loss 0.25188925862312317\n",
            "[Training Epoch 123] Batch 2100, Loss 0.26621782779693604\n",
            "[Training Epoch 123] Batch 2200, Loss 0.25711363554000854\n",
            "[Training Epoch 123] Batch 2300, Loss 0.24549809098243713\n",
            "[Training Epoch 123] Batch 2400, Loss 0.26200342178344727\n",
            "[Training Epoch 123] Batch 2500, Loss 0.2636289596557617\n",
            "[Training Epoch 123] Batch 2600, Loss 0.2930503785610199\n",
            "[Training Epoch 123] Batch 2700, Loss 0.28004658222198486\n",
            "[Training Epoch 123] Batch 2800, Loss 0.2605273127555847\n",
            "[Training Epoch 123] Batch 2900, Loss 0.2545698583126068\n",
            "[Training Epoch 123] Batch 3000, Loss 0.2823041081428528\n",
            "[Training Epoch 123] Batch 3100, Loss 0.2534729242324829\n",
            "[Training Epoch 123] Batch 3200, Loss 0.28362247347831726\n",
            "[Training Epoch 123] Batch 3300, Loss 0.23449717462062836\n",
            "[Training Epoch 123] Batch 3400, Loss 0.3014926612377167\n",
            "[Training Epoch 123] Batch 3500, Loss 0.25849828124046326\n",
            "[Training Epoch 123] Batch 3600, Loss 0.2550683915615082\n",
            "[Training Epoch 123] Batch 3700, Loss 0.2781580090522766\n",
            "[Training Epoch 123] Batch 3800, Loss 0.2983139157295227\n",
            "[Training Epoch 123] Batch 3900, Loss 0.25105878710746765\n",
            "[Training Epoch 123] Batch 4000, Loss 0.25712913274765015\n",
            "[Training Epoch 123] Batch 4100, Loss 0.2701811194419861\n",
            "[Training Epoch 123] Batch 4200, Loss 0.2841658592224121\n",
            "[Training Epoch 123] Batch 4300, Loss 0.24553164839744568\n",
            "[Training Epoch 123] Batch 4400, Loss 0.28556668758392334\n",
            "[Training Epoch 123] Batch 4500, Loss 0.25269776582717896\n",
            "[Training Epoch 123] Batch 4600, Loss 0.30246031284332275\n",
            "[Training Epoch 123] Batch 4700, Loss 0.2564067840576172\n",
            "[Training Epoch 123] Batch 4800, Loss 0.2817872166633606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 123] HR = 0.6386, NDCG = 0.3692\n",
            "Epoch 124 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 124] Batch 0, Loss 0.26708048582077026\n",
            "[Training Epoch 124] Batch 100, Loss 0.2753153443336487\n",
            "[Training Epoch 124] Batch 200, Loss 0.25923630595207214\n",
            "[Training Epoch 124] Batch 300, Loss 0.25640571117401123\n",
            "[Training Epoch 124] Batch 400, Loss 0.2647983729839325\n",
            "[Training Epoch 124] Batch 500, Loss 0.21995490789413452\n",
            "[Training Epoch 124] Batch 600, Loss 0.2478310912847519\n",
            "[Training Epoch 124] Batch 700, Loss 0.2628178596496582\n",
            "[Training Epoch 124] Batch 800, Loss 0.2560436725616455\n",
            "[Training Epoch 124] Batch 900, Loss 0.27346712350845337\n",
            "[Training Epoch 124] Batch 1000, Loss 0.2493363320827484\n",
            "[Training Epoch 124] Batch 1100, Loss 0.2794410288333893\n",
            "[Training Epoch 124] Batch 1200, Loss 0.2712134122848511\n",
            "[Training Epoch 124] Batch 1300, Loss 0.2707083225250244\n",
            "[Training Epoch 124] Batch 1400, Loss 0.2649739682674408\n",
            "[Training Epoch 124] Batch 1500, Loss 0.25227320194244385\n",
            "[Training Epoch 124] Batch 1600, Loss 0.25400352478027344\n",
            "[Training Epoch 124] Batch 1700, Loss 0.2438320815563202\n",
            "[Training Epoch 124] Batch 1800, Loss 0.25009357929229736\n",
            "[Training Epoch 124] Batch 1900, Loss 0.2629839777946472\n",
            "[Training Epoch 124] Batch 2000, Loss 0.2602028548717499\n",
            "[Training Epoch 124] Batch 2100, Loss 0.2670193314552307\n",
            "[Training Epoch 124] Batch 2200, Loss 0.2583191394805908\n",
            "[Training Epoch 124] Batch 2300, Loss 0.23400266468524933\n",
            "[Training Epoch 124] Batch 2400, Loss 0.27487581968307495\n",
            "[Training Epoch 124] Batch 2500, Loss 0.2632829546928406\n",
            "[Training Epoch 124] Batch 2600, Loss 0.2677362561225891\n",
            "[Training Epoch 124] Batch 2700, Loss 0.2719321548938751\n",
            "[Training Epoch 124] Batch 2800, Loss 0.24435469508171082\n",
            "[Training Epoch 124] Batch 2900, Loss 0.2745434641838074\n",
            "[Training Epoch 124] Batch 3000, Loss 0.2836596667766571\n",
            "[Training Epoch 124] Batch 3100, Loss 0.26833564043045044\n",
            "[Training Epoch 124] Batch 3200, Loss 0.2586422264575958\n",
            "[Training Epoch 124] Batch 3300, Loss 0.3094003200531006\n",
            "[Training Epoch 124] Batch 3400, Loss 0.31059402227401733\n",
            "[Training Epoch 124] Batch 3500, Loss 0.25155991315841675\n",
            "[Training Epoch 124] Batch 3600, Loss 0.27891990542411804\n",
            "[Training Epoch 124] Batch 3700, Loss 0.2720325291156769\n",
            "[Training Epoch 124] Batch 3800, Loss 0.27854523062705994\n",
            "[Training Epoch 124] Batch 3900, Loss 0.26811838150024414\n",
            "[Training Epoch 124] Batch 4000, Loss 0.2442743331193924\n",
            "[Training Epoch 124] Batch 4100, Loss 0.2774140238761902\n",
            "[Training Epoch 124] Batch 4200, Loss 0.2675412595272064\n",
            "[Training Epoch 124] Batch 4300, Loss 0.2838808298110962\n",
            "[Training Epoch 124] Batch 4400, Loss 0.2786688208580017\n",
            "[Training Epoch 124] Batch 4500, Loss 0.269594669342041\n",
            "[Training Epoch 124] Batch 4600, Loss 0.22325342893600464\n",
            "[Training Epoch 124] Batch 4700, Loss 0.27254408597946167\n",
            "[Training Epoch 124] Batch 4800, Loss 0.27267420291900635\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 124] HR = 0.6382, NDCG = 0.3697\n",
            "Epoch 125 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 125] Batch 0, Loss 0.2771207094192505\n",
            "[Training Epoch 125] Batch 100, Loss 0.2684817910194397\n",
            "[Training Epoch 125] Batch 200, Loss 0.25142702460289\n",
            "[Training Epoch 125] Batch 300, Loss 0.24408356845378876\n",
            "[Training Epoch 125] Batch 400, Loss 0.2594060003757477\n",
            "[Training Epoch 125] Batch 500, Loss 0.2910234332084656\n",
            "[Training Epoch 125] Batch 600, Loss 0.2950841784477234\n",
            "[Training Epoch 125] Batch 700, Loss 0.248611718416214\n",
            "[Training Epoch 125] Batch 800, Loss 0.25297701358795166\n",
            "[Training Epoch 125] Batch 900, Loss 0.2561286687850952\n",
            "[Training Epoch 125] Batch 1000, Loss 0.2597460150718689\n",
            "[Training Epoch 125] Batch 1100, Loss 0.2710705101490021\n",
            "[Training Epoch 125] Batch 1200, Loss 0.276424765586853\n",
            "[Training Epoch 125] Batch 1300, Loss 0.25275784730911255\n",
            "[Training Epoch 125] Batch 1400, Loss 0.27931708097457886\n",
            "[Training Epoch 125] Batch 1500, Loss 0.258958101272583\n",
            "[Training Epoch 125] Batch 1600, Loss 0.26123857498168945\n",
            "[Training Epoch 125] Batch 1700, Loss 0.2707376480102539\n",
            "[Training Epoch 125] Batch 1800, Loss 0.24065615236759186\n",
            "[Training Epoch 125] Batch 1900, Loss 0.28172987699508667\n",
            "[Training Epoch 125] Batch 2000, Loss 0.25468865036964417\n",
            "[Training Epoch 125] Batch 2100, Loss 0.2711734473705292\n",
            "[Training Epoch 125] Batch 2200, Loss 0.2604082226753235\n",
            "[Training Epoch 125] Batch 2300, Loss 0.23915508389472961\n",
            "[Training Epoch 125] Batch 2400, Loss 0.2546953558921814\n",
            "[Training Epoch 125] Batch 2500, Loss 0.25451064109802246\n",
            "[Training Epoch 125] Batch 2600, Loss 0.28175845742225647\n",
            "[Training Epoch 125] Batch 2700, Loss 0.2792017161846161\n",
            "[Training Epoch 125] Batch 2800, Loss 0.24979408085346222\n",
            "[Training Epoch 125] Batch 2900, Loss 0.27305158972740173\n",
            "[Training Epoch 125] Batch 3000, Loss 0.26262736320495605\n",
            "[Training Epoch 125] Batch 3100, Loss 0.2739727795124054\n",
            "[Training Epoch 125] Batch 3200, Loss 0.25712013244628906\n",
            "[Training Epoch 125] Batch 3300, Loss 0.24359531700611115\n",
            "[Training Epoch 125] Batch 3400, Loss 0.236386239528656\n",
            "[Training Epoch 125] Batch 3500, Loss 0.28874874114990234\n",
            "[Training Epoch 125] Batch 3600, Loss 0.24186649918556213\n",
            "[Training Epoch 125] Batch 3700, Loss 0.28935307264328003\n",
            "[Training Epoch 125] Batch 3800, Loss 0.2607309818267822\n",
            "[Training Epoch 125] Batch 3900, Loss 0.28778284788131714\n",
            "[Training Epoch 125] Batch 4000, Loss 0.27564528584480286\n",
            "[Training Epoch 125] Batch 4100, Loss 0.29626694321632385\n",
            "[Training Epoch 125] Batch 4200, Loss 0.29325103759765625\n",
            "[Training Epoch 125] Batch 4300, Loss 0.2857654094696045\n",
            "[Training Epoch 125] Batch 4400, Loss 0.253903329372406\n",
            "[Training Epoch 125] Batch 4500, Loss 0.24166861176490784\n",
            "[Training Epoch 125] Batch 4600, Loss 0.24466578662395477\n",
            "[Training Epoch 125] Batch 4700, Loss 0.2659529745578766\n",
            "[Training Epoch 125] Batch 4800, Loss 0.26427483558654785\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 125] HR = 0.6346, NDCG = 0.3676\n",
            "Epoch 126 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 126] Batch 0, Loss 0.25454115867614746\n",
            "[Training Epoch 126] Batch 100, Loss 0.2648320198059082\n",
            "[Training Epoch 126] Batch 200, Loss 0.25991207361221313\n",
            "[Training Epoch 126] Batch 300, Loss 0.2528653144836426\n",
            "[Training Epoch 126] Batch 400, Loss 0.2543247640132904\n",
            "[Training Epoch 126] Batch 500, Loss 0.24521267414093018\n",
            "[Training Epoch 126] Batch 600, Loss 0.27880871295928955\n",
            "[Training Epoch 126] Batch 700, Loss 0.25184881687164307\n",
            "[Training Epoch 126] Batch 800, Loss 0.2572668194770813\n",
            "[Training Epoch 126] Batch 900, Loss 0.272260844707489\n",
            "[Training Epoch 126] Batch 1000, Loss 0.2459825575351715\n",
            "[Training Epoch 126] Batch 1100, Loss 0.2765628397464752\n",
            "[Training Epoch 126] Batch 1200, Loss 0.25953859090805054\n",
            "[Training Epoch 126] Batch 1300, Loss 0.26246166229248047\n",
            "[Training Epoch 126] Batch 1400, Loss 0.2543039917945862\n",
            "[Training Epoch 126] Batch 1500, Loss 0.2787334620952606\n",
            "[Training Epoch 126] Batch 1600, Loss 0.25155630707740784\n",
            "[Training Epoch 126] Batch 1700, Loss 0.267103910446167\n",
            "[Training Epoch 126] Batch 1800, Loss 0.2553161084651947\n",
            "[Training Epoch 126] Batch 1900, Loss 0.26322293281555176\n",
            "[Training Epoch 126] Batch 2000, Loss 0.2519627809524536\n",
            "[Training Epoch 126] Batch 2100, Loss 0.2299041450023651\n",
            "[Training Epoch 126] Batch 2200, Loss 0.26389557123184204\n",
            "[Training Epoch 126] Batch 2300, Loss 0.24237215518951416\n",
            "[Training Epoch 126] Batch 2400, Loss 0.2629283666610718\n",
            "[Training Epoch 126] Batch 2500, Loss 0.26304394006729126\n",
            "[Training Epoch 126] Batch 2600, Loss 0.25607502460479736\n",
            "[Training Epoch 126] Batch 2700, Loss 0.26863786578178406\n",
            "[Training Epoch 126] Batch 2800, Loss 0.2602003216743469\n",
            "[Training Epoch 126] Batch 2900, Loss 0.23844465613365173\n",
            "[Training Epoch 126] Batch 3000, Loss 0.27614492177963257\n",
            "[Training Epoch 126] Batch 3100, Loss 0.2524569034576416\n",
            "[Training Epoch 126] Batch 3200, Loss 0.269034743309021\n",
            "[Training Epoch 126] Batch 3300, Loss 0.20982977747917175\n",
            "[Training Epoch 126] Batch 3400, Loss 0.26362961530685425\n",
            "[Training Epoch 126] Batch 3500, Loss 0.2269754409790039\n",
            "[Training Epoch 126] Batch 3600, Loss 0.2767544686794281\n",
            "[Training Epoch 126] Batch 3700, Loss 0.27659672498703003\n",
            "[Training Epoch 126] Batch 3800, Loss 0.2742331027984619\n",
            "[Training Epoch 126] Batch 3900, Loss 0.23815244436264038\n",
            "[Training Epoch 126] Batch 4000, Loss 0.23951339721679688\n",
            "[Training Epoch 126] Batch 4100, Loss 0.28507328033447266\n",
            "[Training Epoch 126] Batch 4200, Loss 0.2794678807258606\n",
            "[Training Epoch 126] Batch 4300, Loss 0.284249484539032\n",
            "[Training Epoch 126] Batch 4400, Loss 0.2866191864013672\n",
            "[Training Epoch 126] Batch 4500, Loss 0.2986183166503906\n",
            "[Training Epoch 126] Batch 4600, Loss 0.2619958817958832\n",
            "[Training Epoch 126] Batch 4700, Loss 0.26070308685302734\n",
            "[Training Epoch 126] Batch 4800, Loss 0.25201156735420227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 126] HR = 0.6373, NDCG = 0.3686\n",
            "Epoch 127 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 127] Batch 0, Loss 0.2785400152206421\n",
            "[Training Epoch 127] Batch 100, Loss 0.2850700616836548\n",
            "[Training Epoch 127] Batch 200, Loss 0.2589392066001892\n",
            "[Training Epoch 127] Batch 300, Loss 0.29257479310035706\n",
            "[Training Epoch 127] Batch 400, Loss 0.2640353739261627\n",
            "[Training Epoch 127] Batch 500, Loss 0.23640790581703186\n",
            "[Training Epoch 127] Batch 600, Loss 0.26259222626686096\n",
            "[Training Epoch 127] Batch 700, Loss 0.2491973638534546\n",
            "[Training Epoch 127] Batch 800, Loss 0.2507975399494171\n",
            "[Training Epoch 127] Batch 900, Loss 0.2824239730834961\n",
            "[Training Epoch 127] Batch 1000, Loss 0.2289230227470398\n",
            "[Training Epoch 127] Batch 1100, Loss 0.25708961486816406\n",
            "[Training Epoch 127] Batch 1200, Loss 0.2649681568145752\n",
            "[Training Epoch 127] Batch 1300, Loss 0.2688513994216919\n",
            "[Training Epoch 127] Batch 1400, Loss 0.2699447274208069\n",
            "[Training Epoch 127] Batch 1500, Loss 0.23036354780197144\n",
            "[Training Epoch 127] Batch 1600, Loss 0.26452624797821045\n",
            "[Training Epoch 127] Batch 1700, Loss 0.25547540187835693\n",
            "[Training Epoch 127] Batch 1800, Loss 0.27935153245925903\n",
            "[Training Epoch 127] Batch 1900, Loss 0.2554641366004944\n",
            "[Training Epoch 127] Batch 2000, Loss 0.25948622822761536\n",
            "[Training Epoch 127] Batch 2100, Loss 0.25968846678733826\n",
            "[Training Epoch 127] Batch 2200, Loss 0.2609741985797882\n",
            "[Training Epoch 127] Batch 2300, Loss 0.2412729114294052\n",
            "[Training Epoch 127] Batch 2400, Loss 0.2739330530166626\n",
            "[Training Epoch 127] Batch 2500, Loss 0.2695377469062805\n",
            "[Training Epoch 127] Batch 2600, Loss 0.2211315631866455\n",
            "[Training Epoch 127] Batch 2700, Loss 0.25805002450942993\n",
            "[Training Epoch 127] Batch 2800, Loss 0.27660036087036133\n",
            "[Training Epoch 127] Batch 2900, Loss 0.25953930616378784\n",
            "[Training Epoch 127] Batch 3000, Loss 0.2643325924873352\n",
            "[Training Epoch 127] Batch 3100, Loss 0.25375670194625854\n",
            "[Training Epoch 127] Batch 3200, Loss 0.25953930616378784\n",
            "[Training Epoch 127] Batch 3300, Loss 0.2754940390586853\n",
            "[Training Epoch 127] Batch 3400, Loss 0.28498953580856323\n",
            "[Training Epoch 127] Batch 3500, Loss 0.24948954582214355\n",
            "[Training Epoch 127] Batch 3600, Loss 0.25777867436408997\n",
            "[Training Epoch 127] Batch 3700, Loss 0.27987024188041687\n",
            "[Training Epoch 127] Batch 3800, Loss 0.2694762349128723\n",
            "[Training Epoch 127] Batch 3900, Loss 0.24583661556243896\n",
            "[Training Epoch 127] Batch 4000, Loss 0.27286475896835327\n",
            "[Training Epoch 127] Batch 4100, Loss 0.25896763801574707\n",
            "[Training Epoch 127] Batch 4200, Loss 0.2600765824317932\n",
            "[Training Epoch 127] Batch 4300, Loss 0.24979865550994873\n",
            "[Training Epoch 127] Batch 4400, Loss 0.2775524854660034\n",
            "[Training Epoch 127] Batch 4500, Loss 0.29609352350234985\n",
            "[Training Epoch 127] Batch 4600, Loss 0.2679769694805145\n",
            "[Training Epoch 127] Batch 4700, Loss 0.259357213973999\n",
            "[Training Epoch 127] Batch 4800, Loss 0.23704323172569275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 127] HR = 0.6366, NDCG = 0.3694\n",
            "Epoch 128 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 128] Batch 0, Loss 0.2866496741771698\n",
            "[Training Epoch 128] Batch 100, Loss 0.25824618339538574\n",
            "[Training Epoch 128] Batch 200, Loss 0.2640572488307953\n",
            "[Training Epoch 128] Batch 300, Loss 0.2550027072429657\n",
            "[Training Epoch 128] Batch 400, Loss 0.2742964029312134\n",
            "[Training Epoch 128] Batch 500, Loss 0.24174508452415466\n",
            "[Training Epoch 128] Batch 600, Loss 0.31052762269973755\n",
            "[Training Epoch 128] Batch 700, Loss 0.25460943579673767\n",
            "[Training Epoch 128] Batch 800, Loss 0.24891270697116852\n",
            "[Training Epoch 128] Batch 900, Loss 0.2913455069065094\n",
            "[Training Epoch 128] Batch 1000, Loss 0.24289187788963318\n",
            "[Training Epoch 128] Batch 1100, Loss 0.22855336964130402\n",
            "[Training Epoch 128] Batch 1200, Loss 0.26387518644332886\n",
            "[Training Epoch 128] Batch 1300, Loss 0.2761450409889221\n",
            "[Training Epoch 128] Batch 1400, Loss 0.27072790265083313\n",
            "[Training Epoch 128] Batch 1500, Loss 0.27645134925842285\n",
            "[Training Epoch 128] Batch 1600, Loss 0.2399679273366928\n",
            "[Training Epoch 128] Batch 1700, Loss 0.2602726221084595\n",
            "[Training Epoch 128] Batch 1800, Loss 0.2783033549785614\n",
            "[Training Epoch 128] Batch 1900, Loss 0.26881474256515503\n",
            "[Training Epoch 128] Batch 2000, Loss 0.26932039856910706\n",
            "[Training Epoch 128] Batch 2100, Loss 0.2780253291130066\n",
            "[Training Epoch 128] Batch 2200, Loss 0.24356979131698608\n",
            "[Training Epoch 128] Batch 2300, Loss 0.261788547039032\n",
            "[Training Epoch 128] Batch 2400, Loss 0.25963300466537476\n",
            "[Training Epoch 128] Batch 2500, Loss 0.27619779109954834\n",
            "[Training Epoch 128] Batch 2600, Loss 0.2698589861392975\n",
            "[Training Epoch 128] Batch 2700, Loss 0.26299965381622314\n",
            "[Training Epoch 128] Batch 2800, Loss 0.24024033546447754\n",
            "[Training Epoch 128] Batch 2900, Loss 0.26822370290756226\n",
            "[Training Epoch 128] Batch 3000, Loss 0.27529627084732056\n",
            "[Training Epoch 128] Batch 3100, Loss 0.2734607458114624\n",
            "[Training Epoch 128] Batch 3200, Loss 0.26706814765930176\n",
            "[Training Epoch 128] Batch 3300, Loss 0.24925866723060608\n",
            "[Training Epoch 128] Batch 3400, Loss 0.2530977725982666\n",
            "[Training Epoch 128] Batch 3500, Loss 0.2658764123916626\n",
            "[Training Epoch 128] Batch 3600, Loss 0.2463606745004654\n",
            "[Training Epoch 128] Batch 3700, Loss 0.23682640492916107\n",
            "[Training Epoch 128] Batch 3800, Loss 0.27609384059906006\n",
            "[Training Epoch 128] Batch 3900, Loss 0.27097129821777344\n",
            "[Training Epoch 128] Batch 4000, Loss 0.2776143550872803\n",
            "[Training Epoch 128] Batch 4100, Loss 0.2553291916847229\n",
            "[Training Epoch 128] Batch 4200, Loss 0.24987053871154785\n",
            "[Training Epoch 128] Batch 4300, Loss 0.28685086965560913\n",
            "[Training Epoch 128] Batch 4400, Loss 0.26144182682037354\n",
            "[Training Epoch 128] Batch 4500, Loss 0.2472529411315918\n",
            "[Training Epoch 128] Batch 4600, Loss 0.28211987018585205\n",
            "[Training Epoch 128] Batch 4700, Loss 0.25428885221481323\n",
            "[Training Epoch 128] Batch 4800, Loss 0.2526487112045288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 128] HR = 0.6361, NDCG = 0.3681\n",
            "Epoch 129 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 129] Batch 0, Loss 0.24974000453948975\n",
            "[Training Epoch 129] Batch 100, Loss 0.2834821939468384\n",
            "[Training Epoch 129] Batch 200, Loss 0.2558749318122864\n",
            "[Training Epoch 129] Batch 300, Loss 0.2565654516220093\n",
            "[Training Epoch 129] Batch 400, Loss 0.25324565172195435\n",
            "[Training Epoch 129] Batch 500, Loss 0.2654895782470703\n",
            "[Training Epoch 129] Batch 600, Loss 0.23175236582756042\n",
            "[Training Epoch 129] Batch 700, Loss 0.2863517701625824\n",
            "[Training Epoch 129] Batch 800, Loss 0.27050405740737915\n",
            "[Training Epoch 129] Batch 900, Loss 0.2427261918783188\n",
            "[Training Epoch 129] Batch 1000, Loss 0.2674766480922699\n",
            "[Training Epoch 129] Batch 1100, Loss 0.2705892324447632\n",
            "[Training Epoch 129] Batch 1200, Loss 0.27852749824523926\n",
            "[Training Epoch 129] Batch 1300, Loss 0.24285119771957397\n",
            "[Training Epoch 129] Batch 1400, Loss 0.27202773094177246\n",
            "[Training Epoch 129] Batch 1500, Loss 0.2547420263290405\n",
            "[Training Epoch 129] Batch 1600, Loss 0.2662360370159149\n",
            "[Training Epoch 129] Batch 1700, Loss 0.2386852204799652\n",
            "[Training Epoch 129] Batch 1800, Loss 0.28309786319732666\n",
            "[Training Epoch 129] Batch 1900, Loss 0.3091723322868347\n",
            "[Training Epoch 129] Batch 2000, Loss 0.26497167348861694\n",
            "[Training Epoch 129] Batch 2100, Loss 0.28653842210769653\n",
            "[Training Epoch 129] Batch 2200, Loss 0.25001955032348633\n",
            "[Training Epoch 129] Batch 2300, Loss 0.26255589723587036\n",
            "[Training Epoch 129] Batch 2400, Loss 0.26454395055770874\n",
            "[Training Epoch 129] Batch 2500, Loss 0.3095529079437256\n",
            "[Training Epoch 129] Batch 2600, Loss 0.301744282245636\n",
            "[Training Epoch 129] Batch 2700, Loss 0.2556980848312378\n",
            "[Training Epoch 129] Batch 2800, Loss 0.23636946082115173\n",
            "[Training Epoch 129] Batch 2900, Loss 0.2531889081001282\n",
            "[Training Epoch 129] Batch 3000, Loss 0.26838237047195435\n",
            "[Training Epoch 129] Batch 3100, Loss 0.27254366874694824\n",
            "[Training Epoch 129] Batch 3200, Loss 0.2463650405406952\n",
            "[Training Epoch 129] Batch 3300, Loss 0.2667589783668518\n",
            "[Training Epoch 129] Batch 3400, Loss 0.24906142055988312\n",
            "[Training Epoch 129] Batch 3500, Loss 0.2637123465538025\n",
            "[Training Epoch 129] Batch 3600, Loss 0.2606614828109741\n",
            "[Training Epoch 129] Batch 3700, Loss 0.3051341772079468\n",
            "[Training Epoch 129] Batch 3800, Loss 0.2532021403312683\n",
            "[Training Epoch 129] Batch 3900, Loss 0.2213553935289383\n",
            "[Training Epoch 129] Batch 4000, Loss 0.27405497431755066\n",
            "[Training Epoch 129] Batch 4100, Loss 0.24141745269298553\n",
            "[Training Epoch 129] Batch 4200, Loss 0.2560444474220276\n",
            "[Training Epoch 129] Batch 4300, Loss 0.2654893696308136\n",
            "[Training Epoch 129] Batch 4400, Loss 0.23381054401397705\n",
            "[Training Epoch 129] Batch 4500, Loss 0.28373149037361145\n",
            "[Training Epoch 129] Batch 4600, Loss 0.2599143981933594\n",
            "[Training Epoch 129] Batch 4700, Loss 0.2704232931137085\n",
            "[Training Epoch 129] Batch 4800, Loss 0.27964818477630615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 129] HR = 0.6387, NDCG = 0.3699\n",
            "Epoch 130 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 130] Batch 0, Loss 0.2875330448150635\n",
            "[Training Epoch 130] Batch 100, Loss 0.21671681106090546\n",
            "[Training Epoch 130] Batch 200, Loss 0.25244277715682983\n",
            "[Training Epoch 130] Batch 300, Loss 0.25759589672088623\n",
            "[Training Epoch 130] Batch 400, Loss 0.2442493736743927\n",
            "[Training Epoch 130] Batch 500, Loss 0.27890366315841675\n",
            "[Training Epoch 130] Batch 600, Loss 0.2798461616039276\n",
            "[Training Epoch 130] Batch 700, Loss 0.2749229669570923\n",
            "[Training Epoch 130] Batch 800, Loss 0.27355581521987915\n",
            "[Training Epoch 130] Batch 900, Loss 0.24541831016540527\n",
            "[Training Epoch 130] Batch 1000, Loss 0.25724244117736816\n",
            "[Training Epoch 130] Batch 1100, Loss 0.26324430108070374\n",
            "[Training Epoch 130] Batch 1200, Loss 0.2705962359905243\n",
            "[Training Epoch 130] Batch 1300, Loss 0.260434627532959\n",
            "[Training Epoch 130] Batch 1400, Loss 0.28910666704177856\n",
            "[Training Epoch 130] Batch 1500, Loss 0.27754318714141846\n",
            "[Training Epoch 130] Batch 1600, Loss 0.2587588429450989\n",
            "[Training Epoch 130] Batch 1700, Loss 0.2559646666049957\n",
            "[Training Epoch 130] Batch 1800, Loss 0.26849284768104553\n",
            "[Training Epoch 130] Batch 1900, Loss 0.251107394695282\n",
            "[Training Epoch 130] Batch 2000, Loss 0.25046396255493164\n",
            "[Training Epoch 130] Batch 2100, Loss 0.28673607110977173\n",
            "[Training Epoch 130] Batch 2200, Loss 0.24940980970859528\n",
            "[Training Epoch 130] Batch 2300, Loss 0.2747027277946472\n",
            "[Training Epoch 130] Batch 2400, Loss 0.2874130606651306\n",
            "[Training Epoch 130] Batch 2500, Loss 0.2521670460700989\n",
            "[Training Epoch 130] Batch 2600, Loss 0.27726107835769653\n",
            "[Training Epoch 130] Batch 2700, Loss 0.24865424633026123\n",
            "[Training Epoch 130] Batch 2800, Loss 0.25194019079208374\n",
            "[Training Epoch 130] Batch 2900, Loss 0.2651249170303345\n",
            "[Training Epoch 130] Batch 3000, Loss 0.2605224847793579\n",
            "[Training Epoch 130] Batch 3100, Loss 0.26455944776535034\n",
            "[Training Epoch 130] Batch 3200, Loss 0.27106788754463196\n",
            "[Training Epoch 130] Batch 3300, Loss 0.2621200680732727\n",
            "[Training Epoch 130] Batch 3400, Loss 0.2611960172653198\n",
            "[Training Epoch 130] Batch 3500, Loss 0.25703972578048706\n",
            "[Training Epoch 130] Batch 3600, Loss 0.26100048422813416\n",
            "[Training Epoch 130] Batch 3700, Loss 0.2493431121110916\n",
            "[Training Epoch 130] Batch 3800, Loss 0.28080350160598755\n",
            "[Training Epoch 130] Batch 3900, Loss 0.27122536301612854\n",
            "[Training Epoch 130] Batch 4000, Loss 0.28883305191993713\n",
            "[Training Epoch 130] Batch 4100, Loss 0.2585206925868988\n",
            "[Training Epoch 130] Batch 4200, Loss 0.29675716161727905\n",
            "[Training Epoch 130] Batch 4300, Loss 0.2687370181083679\n",
            "[Training Epoch 130] Batch 4400, Loss 0.2736988067626953\n",
            "[Training Epoch 130] Batch 4500, Loss 0.2427213490009308\n",
            "[Training Epoch 130] Batch 4600, Loss 0.2378254234790802\n",
            "[Training Epoch 130] Batch 4700, Loss 0.263062983751297\n",
            "[Training Epoch 130] Batch 4800, Loss 0.29162532091140747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 130] HR = 0.6391, NDCG = 0.3687\n",
            "Epoch 131 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 131] Batch 0, Loss 0.2688136696815491\n",
            "[Training Epoch 131] Batch 100, Loss 0.2645316421985626\n",
            "[Training Epoch 131] Batch 200, Loss 0.2701547145843506\n",
            "[Training Epoch 131] Batch 300, Loss 0.23713810741901398\n",
            "[Training Epoch 131] Batch 400, Loss 0.24497008323669434\n",
            "[Training Epoch 131] Batch 500, Loss 0.27633363008499146\n",
            "[Training Epoch 131] Batch 600, Loss 0.26775816082954407\n",
            "[Training Epoch 131] Batch 700, Loss 0.2661263644695282\n",
            "[Training Epoch 131] Batch 800, Loss 0.23049302399158478\n",
            "[Training Epoch 131] Batch 900, Loss 0.24550238251686096\n",
            "[Training Epoch 131] Batch 1000, Loss 0.2692644000053406\n",
            "[Training Epoch 131] Batch 1100, Loss 0.2257099449634552\n",
            "[Training Epoch 131] Batch 1200, Loss 0.28316861391067505\n",
            "[Training Epoch 131] Batch 1300, Loss 0.26547253131866455\n",
            "[Training Epoch 131] Batch 1400, Loss 0.2595374584197998\n",
            "[Training Epoch 131] Batch 1500, Loss 0.2801441550254822\n",
            "[Training Epoch 131] Batch 1600, Loss 0.2652219533920288\n",
            "[Training Epoch 131] Batch 1700, Loss 0.2718786895275116\n",
            "[Training Epoch 131] Batch 1800, Loss 0.2976483106613159\n",
            "[Training Epoch 131] Batch 1900, Loss 0.2789199650287628\n",
            "[Training Epoch 131] Batch 2000, Loss 0.27669814229011536\n",
            "[Training Epoch 131] Batch 2100, Loss 0.24007444083690643\n",
            "[Training Epoch 131] Batch 2200, Loss 0.2714177966117859\n",
            "[Training Epoch 131] Batch 2300, Loss 0.268510103225708\n",
            "[Training Epoch 131] Batch 2400, Loss 0.2795640826225281\n",
            "[Training Epoch 131] Batch 2500, Loss 0.2406274974346161\n",
            "[Training Epoch 131] Batch 2600, Loss 0.2507225573062897\n",
            "[Training Epoch 131] Batch 2700, Loss 0.2568395733833313\n",
            "[Training Epoch 131] Batch 2800, Loss 0.28182482719421387\n",
            "[Training Epoch 131] Batch 2900, Loss 0.25110140442848206\n",
            "[Training Epoch 131] Batch 3000, Loss 0.2697873115539551\n",
            "[Training Epoch 131] Batch 3100, Loss 0.2834455370903015\n",
            "[Training Epoch 131] Batch 3200, Loss 0.2686905860900879\n",
            "[Training Epoch 131] Batch 3300, Loss 0.2533974051475525\n",
            "[Training Epoch 131] Batch 3400, Loss 0.27564334869384766\n",
            "[Training Epoch 131] Batch 3500, Loss 0.26878923177719116\n",
            "[Training Epoch 131] Batch 3600, Loss 0.29688599705696106\n",
            "[Training Epoch 131] Batch 3700, Loss 0.24249237775802612\n",
            "[Training Epoch 131] Batch 3800, Loss 0.250335693359375\n",
            "[Training Epoch 131] Batch 3900, Loss 0.26583293080329895\n",
            "[Training Epoch 131] Batch 4000, Loss 0.24227295815944672\n",
            "[Training Epoch 131] Batch 4100, Loss 0.25589731335639954\n",
            "[Training Epoch 131] Batch 4200, Loss 0.2560255527496338\n",
            "[Training Epoch 131] Batch 4300, Loss 0.2718944549560547\n",
            "[Training Epoch 131] Batch 4400, Loss 0.25927281379699707\n",
            "[Training Epoch 131] Batch 4500, Loss 0.29042840003967285\n",
            "[Training Epoch 131] Batch 4600, Loss 0.2615424394607544\n",
            "[Training Epoch 131] Batch 4700, Loss 0.24205954372882843\n",
            "[Training Epoch 131] Batch 4800, Loss 0.2505114674568176\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 131] HR = 0.6384, NDCG = 0.3680\n",
            "Epoch 132 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 132] Batch 0, Loss 0.25012367963790894\n",
            "[Training Epoch 132] Batch 100, Loss 0.24670886993408203\n",
            "[Training Epoch 132] Batch 200, Loss 0.24072249233722687\n",
            "[Training Epoch 132] Batch 300, Loss 0.28019243478775024\n",
            "[Training Epoch 132] Batch 400, Loss 0.23029610514640808\n",
            "[Training Epoch 132] Batch 500, Loss 0.2755109667778015\n",
            "[Training Epoch 132] Batch 600, Loss 0.2737298011779785\n",
            "[Training Epoch 132] Batch 700, Loss 0.24977678060531616\n",
            "[Training Epoch 132] Batch 800, Loss 0.27647125720977783\n",
            "[Training Epoch 132] Batch 900, Loss 0.2464509755373001\n",
            "[Training Epoch 132] Batch 1000, Loss 0.2903141379356384\n",
            "[Training Epoch 132] Batch 1100, Loss 0.2602119445800781\n",
            "[Training Epoch 132] Batch 1200, Loss 0.268794983625412\n",
            "[Training Epoch 132] Batch 1300, Loss 0.26575955748558044\n",
            "[Training Epoch 132] Batch 1400, Loss 0.2605874538421631\n",
            "[Training Epoch 132] Batch 1500, Loss 0.2568201422691345\n",
            "[Training Epoch 132] Batch 1600, Loss 0.27845215797424316\n",
            "[Training Epoch 132] Batch 1700, Loss 0.23537249863147736\n",
            "[Training Epoch 132] Batch 1800, Loss 0.2747699022293091\n",
            "[Training Epoch 132] Batch 1900, Loss 0.2568313181400299\n",
            "[Training Epoch 132] Batch 2000, Loss 0.2688450813293457\n",
            "[Training Epoch 132] Batch 2100, Loss 0.30344778299331665\n",
            "[Training Epoch 132] Batch 2200, Loss 0.27218079566955566\n",
            "[Training Epoch 132] Batch 2300, Loss 0.27930110692977905\n",
            "[Training Epoch 132] Batch 2400, Loss 0.2539981007575989\n",
            "[Training Epoch 132] Batch 2500, Loss 0.28385797142982483\n",
            "[Training Epoch 132] Batch 2600, Loss 0.29339104890823364\n",
            "[Training Epoch 132] Batch 2700, Loss 0.24571119248867035\n",
            "[Training Epoch 132] Batch 2800, Loss 0.25861671566963196\n",
            "[Training Epoch 132] Batch 2900, Loss 0.25673460960388184\n",
            "[Training Epoch 132] Batch 3000, Loss 0.2607100009918213\n",
            "[Training Epoch 132] Batch 3100, Loss 0.24754992127418518\n",
            "[Training Epoch 132] Batch 3200, Loss 0.25999733805656433\n",
            "[Training Epoch 132] Batch 3300, Loss 0.2755998373031616\n",
            "[Training Epoch 132] Batch 3400, Loss 0.2775449752807617\n",
            "[Training Epoch 132] Batch 3500, Loss 0.2588381767272949\n",
            "[Training Epoch 132] Batch 3600, Loss 0.27411338686943054\n",
            "[Training Epoch 132] Batch 3700, Loss 0.2518623173236847\n",
            "[Training Epoch 132] Batch 3800, Loss 0.27056729793548584\n",
            "[Training Epoch 132] Batch 3900, Loss 0.28525587916374207\n",
            "[Training Epoch 132] Batch 4000, Loss 0.2878757119178772\n",
            "[Training Epoch 132] Batch 4100, Loss 0.26807674765586853\n",
            "[Training Epoch 132] Batch 4200, Loss 0.26511961221694946\n",
            "[Training Epoch 132] Batch 4300, Loss 0.27970993518829346\n",
            "[Training Epoch 132] Batch 4400, Loss 0.24264328181743622\n",
            "[Training Epoch 132] Batch 4500, Loss 0.2762686610221863\n",
            "[Training Epoch 132] Batch 4600, Loss 0.28960931301116943\n",
            "[Training Epoch 132] Batch 4700, Loss 0.2777562141418457\n",
            "[Training Epoch 132] Batch 4800, Loss 0.27483734488487244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 132] HR = 0.6373, NDCG = 0.3698\n",
            "Epoch 133 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 133] Batch 0, Loss 0.27824440598487854\n",
            "[Training Epoch 133] Batch 100, Loss 0.2797345519065857\n",
            "[Training Epoch 133] Batch 200, Loss 0.2863953411579132\n",
            "[Training Epoch 133] Batch 300, Loss 0.26465457677841187\n",
            "[Training Epoch 133] Batch 400, Loss 0.3004024028778076\n",
            "[Training Epoch 133] Batch 500, Loss 0.25313037633895874\n",
            "[Training Epoch 133] Batch 600, Loss 0.255405068397522\n",
            "[Training Epoch 133] Batch 700, Loss 0.2457142025232315\n",
            "[Training Epoch 133] Batch 800, Loss 0.264579713344574\n",
            "[Training Epoch 133] Batch 900, Loss 0.22456501424312592\n",
            "[Training Epoch 133] Batch 1000, Loss 0.25183409452438354\n",
            "[Training Epoch 133] Batch 1100, Loss 0.24895334243774414\n",
            "[Training Epoch 133] Batch 1200, Loss 0.2680314779281616\n",
            "[Training Epoch 133] Batch 1300, Loss 0.27850744128227234\n",
            "[Training Epoch 133] Batch 1400, Loss 0.2572290897369385\n",
            "[Training Epoch 133] Batch 1500, Loss 0.26078468561172485\n",
            "[Training Epoch 133] Batch 1600, Loss 0.2721700668334961\n",
            "[Training Epoch 133] Batch 1700, Loss 0.2617166042327881\n",
            "[Training Epoch 133] Batch 1800, Loss 0.25417560338974\n",
            "[Training Epoch 133] Batch 1900, Loss 0.24130146205425262\n",
            "[Training Epoch 133] Batch 2000, Loss 0.2789536714553833\n",
            "[Training Epoch 133] Batch 2100, Loss 0.27920666337013245\n",
            "[Training Epoch 133] Batch 2200, Loss 0.27203789353370667\n",
            "[Training Epoch 133] Batch 2300, Loss 0.28155046701431274\n",
            "[Training Epoch 133] Batch 2400, Loss 0.27685418725013733\n",
            "[Training Epoch 133] Batch 2500, Loss 0.26119381189346313\n",
            "[Training Epoch 133] Batch 2600, Loss 0.2561849057674408\n",
            "[Training Epoch 133] Batch 2700, Loss 0.2917340397834778\n",
            "[Training Epoch 133] Batch 2800, Loss 0.2754301428794861\n",
            "[Training Epoch 133] Batch 2900, Loss 0.25969550013542175\n",
            "[Training Epoch 133] Batch 3000, Loss 0.26502761244773865\n",
            "[Training Epoch 133] Batch 3100, Loss 0.2761397957801819\n",
            "[Training Epoch 133] Batch 3200, Loss 0.266391396522522\n",
            "[Training Epoch 133] Batch 3300, Loss 0.2710650861263275\n",
            "[Training Epoch 133] Batch 3400, Loss 0.2519407868385315\n",
            "[Training Epoch 133] Batch 3500, Loss 0.25358468294143677\n",
            "[Training Epoch 133] Batch 3600, Loss 0.27805066108703613\n",
            "[Training Epoch 133] Batch 3700, Loss 0.29374027252197266\n",
            "[Training Epoch 133] Batch 3800, Loss 0.25734126567840576\n",
            "[Training Epoch 133] Batch 3900, Loss 0.25975388288497925\n",
            "[Training Epoch 133] Batch 4000, Loss 0.25849461555480957\n",
            "[Training Epoch 133] Batch 4100, Loss 0.25507789850234985\n",
            "[Training Epoch 133] Batch 4200, Loss 0.29427629709243774\n",
            "[Training Epoch 133] Batch 4300, Loss 0.26458853483200073\n",
            "[Training Epoch 133] Batch 4400, Loss 0.2436823695898056\n",
            "[Training Epoch 133] Batch 4500, Loss 0.2604995369911194\n",
            "[Training Epoch 133] Batch 4600, Loss 0.27300477027893066\n",
            "[Training Epoch 133] Batch 4700, Loss 0.2794311046600342\n",
            "[Training Epoch 133] Batch 4800, Loss 0.2712046504020691\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 133] HR = 0.6366, NDCG = 0.3682\n",
            "Epoch 134 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 134] Batch 0, Loss 0.26570773124694824\n",
            "[Training Epoch 134] Batch 100, Loss 0.2534741759300232\n",
            "[Training Epoch 134] Batch 200, Loss 0.2572544515132904\n",
            "[Training Epoch 134] Batch 300, Loss 0.283363401889801\n",
            "[Training Epoch 134] Batch 400, Loss 0.2654896378517151\n",
            "[Training Epoch 134] Batch 500, Loss 0.27287253737449646\n",
            "[Training Epoch 134] Batch 600, Loss 0.25177720189094543\n",
            "[Training Epoch 134] Batch 700, Loss 0.2511952519416809\n",
            "[Training Epoch 134] Batch 800, Loss 0.2706339955329895\n",
            "[Training Epoch 134] Batch 900, Loss 0.27490752935409546\n",
            "[Training Epoch 134] Batch 1000, Loss 0.2621259391307831\n",
            "[Training Epoch 134] Batch 1100, Loss 0.23842395842075348\n",
            "[Training Epoch 134] Batch 1200, Loss 0.2810283601284027\n",
            "[Training Epoch 134] Batch 1300, Loss 0.24018600583076477\n",
            "[Training Epoch 134] Batch 1400, Loss 0.2752474248409271\n",
            "[Training Epoch 134] Batch 1500, Loss 0.24333861470222473\n",
            "[Training Epoch 134] Batch 1600, Loss 0.2882622480392456\n",
            "[Training Epoch 134] Batch 1700, Loss 0.2591787576675415\n",
            "[Training Epoch 134] Batch 1800, Loss 0.28541046380996704\n",
            "[Training Epoch 134] Batch 1900, Loss 0.2745400071144104\n",
            "[Training Epoch 134] Batch 2000, Loss 0.26012909412384033\n",
            "[Training Epoch 134] Batch 2100, Loss 0.2675144374370575\n",
            "[Training Epoch 134] Batch 2200, Loss 0.26046091318130493\n",
            "[Training Epoch 134] Batch 2300, Loss 0.24181106686592102\n",
            "[Training Epoch 134] Batch 2400, Loss 0.2574358582496643\n",
            "[Training Epoch 134] Batch 2500, Loss 0.31219741702079773\n",
            "[Training Epoch 134] Batch 2600, Loss 0.2862570583820343\n",
            "[Training Epoch 134] Batch 2700, Loss 0.26207301020622253\n",
            "[Training Epoch 134] Batch 2800, Loss 0.29413360357284546\n",
            "[Training Epoch 134] Batch 2900, Loss 0.2699073851108551\n",
            "[Training Epoch 134] Batch 3000, Loss 0.2501199245452881\n",
            "[Training Epoch 134] Batch 3100, Loss 0.22621434926986694\n",
            "[Training Epoch 134] Batch 3200, Loss 0.24263615906238556\n",
            "[Training Epoch 134] Batch 3300, Loss 0.2699968218803406\n",
            "[Training Epoch 134] Batch 3400, Loss 0.27401429414749146\n",
            "[Training Epoch 134] Batch 3500, Loss 0.26002299785614014\n",
            "[Training Epoch 134] Batch 3600, Loss 0.2663446068763733\n",
            "[Training Epoch 134] Batch 3700, Loss 0.2938576340675354\n",
            "[Training Epoch 134] Batch 3800, Loss 0.2563229501247406\n",
            "[Training Epoch 134] Batch 3900, Loss 0.26565665006637573\n",
            "[Training Epoch 134] Batch 4000, Loss 0.25213897228240967\n",
            "[Training Epoch 134] Batch 4100, Loss 0.27915018796920776\n",
            "[Training Epoch 134] Batch 4200, Loss 0.27502375841140747\n",
            "[Training Epoch 134] Batch 4300, Loss 0.2708680033683777\n",
            "[Training Epoch 134] Batch 4400, Loss 0.26142579317092896\n",
            "[Training Epoch 134] Batch 4500, Loss 0.2846650779247284\n",
            "[Training Epoch 134] Batch 4600, Loss 0.2718804180622101\n",
            "[Training Epoch 134] Batch 4700, Loss 0.23473837971687317\n",
            "[Training Epoch 134] Batch 4800, Loss 0.22275446355342865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 134] HR = 0.6359, NDCG = 0.3676\n",
            "Epoch 135 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 135] Batch 0, Loss 0.24974694848060608\n",
            "[Training Epoch 135] Batch 100, Loss 0.26978784799575806\n",
            "[Training Epoch 135] Batch 200, Loss 0.26671847701072693\n",
            "[Training Epoch 135] Batch 300, Loss 0.27407240867614746\n",
            "[Training Epoch 135] Batch 400, Loss 0.2834818959236145\n",
            "[Training Epoch 135] Batch 500, Loss 0.25419002771377563\n",
            "[Training Epoch 135] Batch 600, Loss 0.26387694478034973\n",
            "[Training Epoch 135] Batch 700, Loss 0.26213085651397705\n",
            "[Training Epoch 135] Batch 800, Loss 0.2883901000022888\n",
            "[Training Epoch 135] Batch 900, Loss 0.24783506989479065\n",
            "[Training Epoch 135] Batch 1000, Loss 0.2603885531425476\n",
            "[Training Epoch 135] Batch 1100, Loss 0.2684473991394043\n",
            "[Training Epoch 135] Batch 1200, Loss 0.2661464214324951\n",
            "[Training Epoch 135] Batch 1300, Loss 0.2554248869419098\n",
            "[Training Epoch 135] Batch 1400, Loss 0.23268350958824158\n",
            "[Training Epoch 135] Batch 1500, Loss 0.2689884901046753\n",
            "[Training Epoch 135] Batch 1600, Loss 0.2670944929122925\n",
            "[Training Epoch 135] Batch 1700, Loss 0.28950995206832886\n",
            "[Training Epoch 135] Batch 1800, Loss 0.24227376282215118\n",
            "[Training Epoch 135] Batch 1900, Loss 0.22378036379814148\n",
            "[Training Epoch 135] Batch 2000, Loss 0.2536763548851013\n",
            "[Training Epoch 135] Batch 2100, Loss 0.2601211667060852\n",
            "[Training Epoch 135] Batch 2200, Loss 0.29246968030929565\n",
            "[Training Epoch 135] Batch 2300, Loss 0.25730013847351074\n",
            "[Training Epoch 135] Batch 2400, Loss 0.2889563739299774\n",
            "[Training Epoch 135] Batch 2500, Loss 0.27161073684692383\n",
            "[Training Epoch 135] Batch 2600, Loss 0.25243544578552246\n",
            "[Training Epoch 135] Batch 2700, Loss 0.2492314577102661\n",
            "[Training Epoch 135] Batch 2800, Loss 0.2730337083339691\n",
            "[Training Epoch 135] Batch 2900, Loss 0.2686905264854431\n",
            "[Training Epoch 135] Batch 3000, Loss 0.31751465797424316\n",
            "[Training Epoch 135] Batch 3100, Loss 0.271761953830719\n",
            "[Training Epoch 135] Batch 3200, Loss 0.23813676834106445\n",
            "[Training Epoch 135] Batch 3300, Loss 0.2615804374217987\n",
            "[Training Epoch 135] Batch 3400, Loss 0.2794415354728699\n",
            "[Training Epoch 135] Batch 3500, Loss 0.2627313733100891\n",
            "[Training Epoch 135] Batch 3600, Loss 0.23329897224903107\n",
            "[Training Epoch 135] Batch 3700, Loss 0.27692312002182007\n",
            "[Training Epoch 135] Batch 3800, Loss 0.2451125681400299\n",
            "[Training Epoch 135] Batch 3900, Loss 0.2746890187263489\n",
            "[Training Epoch 135] Batch 4000, Loss 0.2936469614505768\n",
            "[Training Epoch 135] Batch 4100, Loss 0.2584049105644226\n",
            "[Training Epoch 135] Batch 4200, Loss 0.2911267876625061\n",
            "[Training Epoch 135] Batch 4300, Loss 0.2759643495082855\n",
            "[Training Epoch 135] Batch 4400, Loss 0.27725833654403687\n",
            "[Training Epoch 135] Batch 4500, Loss 0.26499873399734497\n",
            "[Training Epoch 135] Batch 4600, Loss 0.24570083618164062\n",
            "[Training Epoch 135] Batch 4700, Loss 0.285604327917099\n",
            "[Training Epoch 135] Batch 4800, Loss 0.2474033534526825\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 135] HR = 0.6379, NDCG = 0.3684\n",
            "Epoch 136 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 136] Batch 0, Loss 0.27438807487487793\n",
            "[Training Epoch 136] Batch 100, Loss 0.29163944721221924\n",
            "[Training Epoch 136] Batch 200, Loss 0.24246662855148315\n",
            "[Training Epoch 136] Batch 300, Loss 0.26941269636154175\n",
            "[Training Epoch 136] Batch 400, Loss 0.2876291871070862\n",
            "[Training Epoch 136] Batch 500, Loss 0.29781267046928406\n",
            "[Training Epoch 136] Batch 600, Loss 0.27009546756744385\n",
            "[Training Epoch 136] Batch 700, Loss 0.2767619490623474\n",
            "[Training Epoch 136] Batch 800, Loss 0.2682320773601532\n",
            "[Training Epoch 136] Batch 900, Loss 0.24045564234256744\n",
            "[Training Epoch 136] Batch 1000, Loss 0.25933945178985596\n",
            "[Training Epoch 136] Batch 1100, Loss 0.24925735592842102\n",
            "[Training Epoch 136] Batch 1200, Loss 0.29239606857299805\n",
            "[Training Epoch 136] Batch 1300, Loss 0.251065731048584\n",
            "[Training Epoch 136] Batch 1400, Loss 0.27428901195526123\n",
            "[Training Epoch 136] Batch 1500, Loss 0.2519906163215637\n",
            "[Training Epoch 136] Batch 1600, Loss 0.276409775018692\n",
            "[Training Epoch 136] Batch 1700, Loss 0.23582693934440613\n",
            "[Training Epoch 136] Batch 1800, Loss 0.23031774163246155\n",
            "[Training Epoch 136] Batch 1900, Loss 0.2713702619075775\n",
            "[Training Epoch 136] Batch 2000, Loss 0.2405020296573639\n",
            "[Training Epoch 136] Batch 2100, Loss 0.25501835346221924\n",
            "[Training Epoch 136] Batch 2200, Loss 0.26179495453834534\n",
            "[Training Epoch 136] Batch 2300, Loss 0.248292475938797\n",
            "[Training Epoch 136] Batch 2400, Loss 0.24821484088897705\n",
            "[Training Epoch 136] Batch 2500, Loss 0.27578645944595337\n",
            "[Training Epoch 136] Batch 2600, Loss 0.254491925239563\n",
            "[Training Epoch 136] Batch 2700, Loss 0.2685816287994385\n",
            "[Training Epoch 136] Batch 2800, Loss 0.24980539083480835\n",
            "[Training Epoch 136] Batch 2900, Loss 0.26573866605758667\n",
            "[Training Epoch 136] Batch 3000, Loss 0.2501243054866791\n",
            "[Training Epoch 136] Batch 3100, Loss 0.2539674639701843\n",
            "[Training Epoch 136] Batch 3200, Loss 0.2807575464248657\n",
            "[Training Epoch 136] Batch 3300, Loss 0.2620856761932373\n",
            "[Training Epoch 136] Batch 3400, Loss 0.24116672575473785\n",
            "[Training Epoch 136] Batch 3500, Loss 0.2752477824687958\n",
            "[Training Epoch 136] Batch 3600, Loss 0.2750703692436218\n",
            "[Training Epoch 136] Batch 3700, Loss 0.24650141596794128\n",
            "[Training Epoch 136] Batch 3800, Loss 0.26227742433547974\n",
            "[Training Epoch 136] Batch 3900, Loss 0.25928813219070435\n",
            "[Training Epoch 136] Batch 4000, Loss 0.2623942494392395\n",
            "[Training Epoch 136] Batch 4100, Loss 0.2808234691619873\n",
            "[Training Epoch 136] Batch 4200, Loss 0.22355583310127258\n",
            "[Training Epoch 136] Batch 4300, Loss 0.2673051357269287\n",
            "[Training Epoch 136] Batch 4400, Loss 0.2644449472427368\n",
            "[Training Epoch 136] Batch 4500, Loss 0.28840935230255127\n",
            "[Training Epoch 136] Batch 4600, Loss 0.2746392488479614\n",
            "[Training Epoch 136] Batch 4700, Loss 0.26636284589767456\n",
            "[Training Epoch 136] Batch 4800, Loss 0.26142793893814087\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 136] HR = 0.6379, NDCG = 0.3682\n",
            "Epoch 137 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 137] Batch 0, Loss 0.2595188617706299\n",
            "[Training Epoch 137] Batch 100, Loss 0.26997363567352295\n",
            "[Training Epoch 137] Batch 200, Loss 0.25233516097068787\n",
            "[Training Epoch 137] Batch 300, Loss 0.2557271718978882\n",
            "[Training Epoch 137] Batch 400, Loss 0.25735920667648315\n",
            "[Training Epoch 137] Batch 500, Loss 0.2647392749786377\n",
            "[Training Epoch 137] Batch 600, Loss 0.27325373888015747\n",
            "[Training Epoch 137] Batch 700, Loss 0.2650422155857086\n",
            "[Training Epoch 137] Batch 800, Loss 0.27375155687332153\n",
            "[Training Epoch 137] Batch 900, Loss 0.27943742275238037\n",
            "[Training Epoch 137] Batch 1000, Loss 0.24383333325386047\n",
            "[Training Epoch 137] Batch 1100, Loss 0.25635379552841187\n",
            "[Training Epoch 137] Batch 1200, Loss 0.26640599966049194\n",
            "[Training Epoch 137] Batch 1300, Loss 0.26260411739349365\n",
            "[Training Epoch 137] Batch 1400, Loss 0.26333481073379517\n",
            "[Training Epoch 137] Batch 1500, Loss 0.24658043682575226\n",
            "[Training Epoch 137] Batch 1600, Loss 0.24204126000404358\n",
            "[Training Epoch 137] Batch 1700, Loss 0.2618474066257477\n",
            "[Training Epoch 137] Batch 1800, Loss 0.25848788022994995\n",
            "[Training Epoch 137] Batch 1900, Loss 0.30180487036705017\n",
            "[Training Epoch 137] Batch 2000, Loss 0.24391718208789825\n",
            "[Training Epoch 137] Batch 2100, Loss 0.25285500288009644\n",
            "[Training Epoch 137] Batch 2200, Loss 0.25790318846702576\n",
            "[Training Epoch 137] Batch 2300, Loss 0.2469913214445114\n",
            "[Training Epoch 137] Batch 2400, Loss 0.266144722700119\n",
            "[Training Epoch 137] Batch 2500, Loss 0.2692001163959503\n",
            "[Training Epoch 137] Batch 2600, Loss 0.27175867557525635\n",
            "[Training Epoch 137] Batch 2700, Loss 0.2498023360967636\n",
            "[Training Epoch 137] Batch 2800, Loss 0.28532031178474426\n",
            "[Training Epoch 137] Batch 2900, Loss 0.28203415870666504\n",
            "[Training Epoch 137] Batch 3000, Loss 0.26434481143951416\n",
            "[Training Epoch 137] Batch 3100, Loss 0.2837083637714386\n",
            "[Training Epoch 137] Batch 3200, Loss 0.2612960934638977\n",
            "[Training Epoch 137] Batch 3300, Loss 0.25896376371383667\n",
            "[Training Epoch 137] Batch 3400, Loss 0.261635959148407\n",
            "[Training Epoch 137] Batch 3500, Loss 0.27922171354293823\n",
            "[Training Epoch 137] Batch 3600, Loss 0.25394314527511597\n",
            "[Training Epoch 137] Batch 3700, Loss 0.25185081362724304\n",
            "[Training Epoch 137] Batch 3800, Loss 0.27238449454307556\n",
            "[Training Epoch 137] Batch 3900, Loss 0.23581133782863617\n",
            "[Training Epoch 137] Batch 4000, Loss 0.2528494596481323\n",
            "[Training Epoch 137] Batch 4100, Loss 0.264522910118103\n",
            "[Training Epoch 137] Batch 4200, Loss 0.26902875304222107\n",
            "[Training Epoch 137] Batch 4300, Loss 0.25397318601608276\n",
            "[Training Epoch 137] Batch 4400, Loss 0.2402629256248474\n",
            "[Training Epoch 137] Batch 4500, Loss 0.28048819303512573\n",
            "[Training Epoch 137] Batch 4600, Loss 0.2450990527868271\n",
            "[Training Epoch 137] Batch 4700, Loss 0.23328736424446106\n",
            "[Training Epoch 137] Batch 4800, Loss 0.2735782563686371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 137] HR = 0.6374, NDCG = 0.3684\n",
            "Epoch 138 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 138] Batch 0, Loss 0.2949085235595703\n",
            "[Training Epoch 138] Batch 100, Loss 0.27087897062301636\n",
            "[Training Epoch 138] Batch 200, Loss 0.24608783423900604\n",
            "[Training Epoch 138] Batch 300, Loss 0.2640432119369507\n",
            "[Training Epoch 138] Batch 400, Loss 0.25247251987457275\n",
            "[Training Epoch 138] Batch 500, Loss 0.26159244775772095\n",
            "[Training Epoch 138] Batch 600, Loss 0.23499923944473267\n",
            "[Training Epoch 138] Batch 700, Loss 0.29803287982940674\n",
            "[Training Epoch 138] Batch 800, Loss 0.25700700283050537\n",
            "[Training Epoch 138] Batch 900, Loss 0.2736659049987793\n",
            "[Training Epoch 138] Batch 1000, Loss 0.26876300573349\n",
            "[Training Epoch 138] Batch 1100, Loss 0.26543301343917847\n",
            "[Training Epoch 138] Batch 1200, Loss 0.23939168453216553\n",
            "[Training Epoch 138] Batch 1300, Loss 0.25649523735046387\n",
            "[Training Epoch 138] Batch 1400, Loss 0.26150164008140564\n",
            "[Training Epoch 138] Batch 1500, Loss 0.28352025151252747\n",
            "[Training Epoch 138] Batch 1600, Loss 0.26538538932800293\n",
            "[Training Epoch 138] Batch 1700, Loss 0.27916377782821655\n",
            "[Training Epoch 138] Batch 1800, Loss 0.28301119804382324\n",
            "[Training Epoch 138] Batch 1900, Loss 0.2671564817428589\n",
            "[Training Epoch 138] Batch 2000, Loss 0.2577303946018219\n",
            "[Training Epoch 138] Batch 2100, Loss 0.26148074865341187\n",
            "[Training Epoch 138] Batch 2200, Loss 0.29615142941474915\n",
            "[Training Epoch 138] Batch 2300, Loss 0.2586119472980499\n",
            "[Training Epoch 138] Batch 2400, Loss 0.2515573799610138\n",
            "[Training Epoch 138] Batch 2500, Loss 0.256115198135376\n",
            "[Training Epoch 138] Batch 2600, Loss 0.29943257570266724\n",
            "[Training Epoch 138] Batch 2700, Loss 0.3070589601993561\n",
            "[Training Epoch 138] Batch 2800, Loss 0.24397829174995422\n",
            "[Training Epoch 138] Batch 2900, Loss 0.24825525283813477\n",
            "[Training Epoch 138] Batch 3000, Loss 0.28086817264556885\n",
            "[Training Epoch 138] Batch 3100, Loss 0.25897839665412903\n",
            "[Training Epoch 138] Batch 3200, Loss 0.2772119641304016\n",
            "[Training Epoch 138] Batch 3300, Loss 0.22191233932971954\n",
            "[Training Epoch 138] Batch 3400, Loss 0.27620071172714233\n",
            "[Training Epoch 138] Batch 3500, Loss 0.2796105742454529\n",
            "[Training Epoch 138] Batch 3600, Loss 0.24222007393836975\n",
            "[Training Epoch 138] Batch 3700, Loss 0.26951348781585693\n",
            "[Training Epoch 138] Batch 3800, Loss 0.2570466995239258\n",
            "[Training Epoch 138] Batch 3900, Loss 0.23601053655147552\n",
            "[Training Epoch 138] Batch 4000, Loss 0.29934829473495483\n",
            "[Training Epoch 138] Batch 4100, Loss 0.2678946554660797\n",
            "[Training Epoch 138] Batch 4200, Loss 0.2652977705001831\n",
            "[Training Epoch 138] Batch 4300, Loss 0.27937328815460205\n",
            "[Training Epoch 138] Batch 4400, Loss 0.2696675956249237\n",
            "[Training Epoch 138] Batch 4500, Loss 0.2673257291316986\n",
            "[Training Epoch 138] Batch 4600, Loss 0.2767590284347534\n",
            "[Training Epoch 138] Batch 4700, Loss 0.2863207459449768\n",
            "[Training Epoch 138] Batch 4800, Loss 0.29197922348976135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 138] HR = 0.6364, NDCG = 0.3677\n",
            "Epoch 139 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 139] Batch 0, Loss 0.2587744891643524\n",
            "[Training Epoch 139] Batch 100, Loss 0.2962048053741455\n",
            "[Training Epoch 139] Batch 200, Loss 0.26315832138061523\n",
            "[Training Epoch 139] Batch 300, Loss 0.26091617345809937\n",
            "[Training Epoch 139] Batch 400, Loss 0.2450333535671234\n",
            "[Training Epoch 139] Batch 500, Loss 0.250849187374115\n",
            "[Training Epoch 139] Batch 600, Loss 0.2685377895832062\n",
            "[Training Epoch 139] Batch 700, Loss 0.28542840480804443\n",
            "[Training Epoch 139] Batch 800, Loss 0.2802557945251465\n",
            "[Training Epoch 139] Batch 900, Loss 0.2533811330795288\n",
            "[Training Epoch 139] Batch 1000, Loss 0.25411590933799744\n",
            "[Training Epoch 139] Batch 1100, Loss 0.2653007209300995\n",
            "[Training Epoch 139] Batch 1200, Loss 0.26784807443618774\n",
            "[Training Epoch 139] Batch 1300, Loss 0.26556485891342163\n",
            "[Training Epoch 139] Batch 1400, Loss 0.26415133476257324\n",
            "[Training Epoch 139] Batch 1500, Loss 0.2907329201698303\n",
            "[Training Epoch 139] Batch 1600, Loss 0.26489898562431335\n",
            "[Training Epoch 139] Batch 1700, Loss 0.2709786295890808\n",
            "[Training Epoch 139] Batch 1800, Loss 0.265985906124115\n",
            "[Training Epoch 139] Batch 1900, Loss 0.2561500668525696\n",
            "[Training Epoch 139] Batch 2000, Loss 0.25945454835891724\n",
            "[Training Epoch 139] Batch 2100, Loss 0.24212774634361267\n",
            "[Training Epoch 139] Batch 2200, Loss 0.25444579124450684\n",
            "[Training Epoch 139] Batch 2300, Loss 0.2712561786174774\n",
            "[Training Epoch 139] Batch 2400, Loss 0.27434396743774414\n",
            "[Training Epoch 139] Batch 2500, Loss 0.29816514253616333\n",
            "[Training Epoch 139] Batch 2600, Loss 0.28090018033981323\n",
            "[Training Epoch 139] Batch 2700, Loss 0.27556005120277405\n",
            "[Training Epoch 139] Batch 2800, Loss 0.2634996771812439\n",
            "[Training Epoch 139] Batch 2900, Loss 0.27997156977653503\n",
            "[Training Epoch 139] Batch 3000, Loss 0.2718322277069092\n",
            "[Training Epoch 139] Batch 3100, Loss 0.26556622982025146\n",
            "[Training Epoch 139] Batch 3200, Loss 0.25347113609313965\n",
            "[Training Epoch 139] Batch 3300, Loss 0.27298617362976074\n",
            "[Training Epoch 139] Batch 3400, Loss 0.2831343412399292\n",
            "[Training Epoch 139] Batch 3500, Loss 0.24379612505435944\n",
            "[Training Epoch 139] Batch 3600, Loss 0.2652241587638855\n",
            "[Training Epoch 139] Batch 3700, Loss 0.282965749502182\n",
            "[Training Epoch 139] Batch 3800, Loss 0.26650470495224\n",
            "[Training Epoch 139] Batch 3900, Loss 0.2666783928871155\n",
            "[Training Epoch 139] Batch 4000, Loss 0.26307451725006104\n",
            "[Training Epoch 139] Batch 4100, Loss 0.25274571776390076\n",
            "[Training Epoch 139] Batch 4200, Loss 0.2576492428779602\n",
            "[Training Epoch 139] Batch 4300, Loss 0.27406078577041626\n",
            "[Training Epoch 139] Batch 4400, Loss 0.2581484913825989\n",
            "[Training Epoch 139] Batch 4500, Loss 0.22967924177646637\n",
            "[Training Epoch 139] Batch 4600, Loss 0.2372281700372696\n",
            "[Training Epoch 139] Batch 4700, Loss 0.258243203163147\n",
            "[Training Epoch 139] Batch 4800, Loss 0.25102531909942627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 139] HR = 0.6376, NDCG = 0.3697\n",
            "Epoch 140 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 140] Batch 0, Loss 0.2581464648246765\n",
            "[Training Epoch 140] Batch 100, Loss 0.28314441442489624\n",
            "[Training Epoch 140] Batch 200, Loss 0.23978932201862335\n",
            "[Training Epoch 140] Batch 300, Loss 0.2494352012872696\n",
            "[Training Epoch 140] Batch 400, Loss 0.259507954120636\n",
            "[Training Epoch 140] Batch 500, Loss 0.27717217803001404\n",
            "[Training Epoch 140] Batch 600, Loss 0.25598961114883423\n",
            "[Training Epoch 140] Batch 700, Loss 0.2877974510192871\n",
            "[Training Epoch 140] Batch 800, Loss 0.2685950994491577\n",
            "[Training Epoch 140] Batch 900, Loss 0.25811105966567993\n",
            "[Training Epoch 140] Batch 1000, Loss 0.24060338735580444\n",
            "[Training Epoch 140] Batch 1100, Loss 0.2512880265712738\n",
            "[Training Epoch 140] Batch 1200, Loss 0.2598075866699219\n",
            "[Training Epoch 140] Batch 1300, Loss 0.2627243995666504\n",
            "[Training Epoch 140] Batch 1400, Loss 0.28810906410217285\n",
            "[Training Epoch 140] Batch 1500, Loss 0.27711546421051025\n",
            "[Training Epoch 140] Batch 1600, Loss 0.28637537360191345\n",
            "[Training Epoch 140] Batch 1700, Loss 0.263753205537796\n",
            "[Training Epoch 140] Batch 1800, Loss 0.28483670949935913\n",
            "[Training Epoch 140] Batch 1900, Loss 0.25596824288368225\n",
            "[Training Epoch 140] Batch 2000, Loss 0.23783253133296967\n",
            "[Training Epoch 140] Batch 2100, Loss 0.2548508942127228\n",
            "[Training Epoch 140] Batch 2200, Loss 0.26083701848983765\n",
            "[Training Epoch 140] Batch 2300, Loss 0.25072985887527466\n",
            "[Training Epoch 140] Batch 2400, Loss 0.2711065411567688\n",
            "[Training Epoch 140] Batch 2500, Loss 0.24757622182369232\n",
            "[Training Epoch 140] Batch 2600, Loss 0.3031378388404846\n",
            "[Training Epoch 140] Batch 2700, Loss 0.2460913062095642\n",
            "[Training Epoch 140] Batch 2800, Loss 0.27249813079833984\n",
            "[Training Epoch 140] Batch 2900, Loss 0.26969045400619507\n",
            "[Training Epoch 140] Batch 3000, Loss 0.2814726233482361\n",
            "[Training Epoch 140] Batch 3100, Loss 0.29243719577789307\n",
            "[Training Epoch 140] Batch 3200, Loss 0.23969510197639465\n",
            "[Training Epoch 140] Batch 3300, Loss 0.2426540106534958\n",
            "[Training Epoch 140] Batch 3400, Loss 0.2564978003501892\n",
            "[Training Epoch 140] Batch 3500, Loss 0.2887440025806427\n",
            "[Training Epoch 140] Batch 3600, Loss 0.26045146584510803\n",
            "[Training Epoch 140] Batch 3700, Loss 0.2707265317440033\n",
            "[Training Epoch 140] Batch 3800, Loss 0.2656877934932709\n",
            "[Training Epoch 140] Batch 3900, Loss 0.2902826964855194\n",
            "[Training Epoch 140] Batch 4000, Loss 0.25935930013656616\n",
            "[Training Epoch 140] Batch 4100, Loss 0.27191704511642456\n",
            "[Training Epoch 140] Batch 4200, Loss 0.27537012100219727\n",
            "[Training Epoch 140] Batch 4300, Loss 0.2800055146217346\n",
            "[Training Epoch 140] Batch 4400, Loss 0.2598458528518677\n",
            "[Training Epoch 140] Batch 4500, Loss 0.27602529525756836\n",
            "[Training Epoch 140] Batch 4600, Loss 0.28030648827552795\n",
            "[Training Epoch 140] Batch 4700, Loss 0.26532599329948425\n",
            "[Training Epoch 140] Batch 4800, Loss 0.25822216272354126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 140] HR = 0.6384, NDCG = 0.3681\n",
            "Epoch 141 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 141] Batch 0, Loss 0.2605668604373932\n",
            "[Training Epoch 141] Batch 100, Loss 0.2546679973602295\n",
            "[Training Epoch 141] Batch 200, Loss 0.267955482006073\n",
            "[Training Epoch 141] Batch 300, Loss 0.2840561270713806\n",
            "[Training Epoch 141] Batch 400, Loss 0.2769979238510132\n",
            "[Training Epoch 141] Batch 500, Loss 0.2879217565059662\n",
            "[Training Epoch 141] Batch 600, Loss 0.229533389210701\n",
            "[Training Epoch 141] Batch 700, Loss 0.29952484369277954\n",
            "[Training Epoch 141] Batch 800, Loss 0.2771919369697571\n",
            "[Training Epoch 141] Batch 900, Loss 0.26758480072021484\n",
            "[Training Epoch 141] Batch 1000, Loss 0.25079023838043213\n",
            "[Training Epoch 141] Batch 1100, Loss 0.2555493712425232\n",
            "[Training Epoch 141] Batch 1200, Loss 0.2831459641456604\n",
            "[Training Epoch 141] Batch 1300, Loss 0.2862972915172577\n",
            "[Training Epoch 141] Batch 1400, Loss 0.23261401057243347\n",
            "[Training Epoch 141] Batch 1500, Loss 0.28114885091781616\n",
            "[Training Epoch 141] Batch 1600, Loss 0.273162305355072\n",
            "[Training Epoch 141] Batch 1700, Loss 0.29510560631752014\n",
            "[Training Epoch 141] Batch 1800, Loss 0.2733927071094513\n",
            "[Training Epoch 141] Batch 1900, Loss 0.24488383531570435\n",
            "[Training Epoch 141] Batch 2000, Loss 0.24661460518836975\n",
            "[Training Epoch 141] Batch 2100, Loss 0.27721530199050903\n",
            "[Training Epoch 141] Batch 2200, Loss 0.23702372610569\n",
            "[Training Epoch 141] Batch 2300, Loss 0.26495683193206787\n",
            "[Training Epoch 141] Batch 2400, Loss 0.23582583665847778\n",
            "[Training Epoch 141] Batch 2500, Loss 0.2825441360473633\n",
            "[Training Epoch 141] Batch 2600, Loss 0.2688567042350769\n",
            "[Training Epoch 141] Batch 2700, Loss 0.27822911739349365\n",
            "[Training Epoch 141] Batch 2800, Loss 0.28469574451446533\n",
            "[Training Epoch 141] Batch 2900, Loss 0.2527362108230591\n",
            "[Training Epoch 141] Batch 3000, Loss 0.2516096830368042\n",
            "[Training Epoch 141] Batch 3100, Loss 0.2319888025522232\n",
            "[Training Epoch 141] Batch 3200, Loss 0.2858593165874481\n",
            "[Training Epoch 141] Batch 3300, Loss 0.26367491483688354\n",
            "[Training Epoch 141] Batch 3400, Loss 0.2585497796535492\n",
            "[Training Epoch 141] Batch 3500, Loss 0.27699950337409973\n",
            "[Training Epoch 141] Batch 3600, Loss 0.2553088963031769\n",
            "[Training Epoch 141] Batch 3700, Loss 0.24655374884605408\n",
            "[Training Epoch 141] Batch 3800, Loss 0.2375084012746811\n",
            "[Training Epoch 141] Batch 3900, Loss 0.27648892998695374\n",
            "[Training Epoch 141] Batch 4000, Loss 0.24879911541938782\n",
            "[Training Epoch 141] Batch 4100, Loss 0.2577595114707947\n",
            "[Training Epoch 141] Batch 4200, Loss 0.3022819757461548\n",
            "[Training Epoch 141] Batch 4300, Loss 0.261129766702652\n",
            "[Training Epoch 141] Batch 4400, Loss 0.2851153612136841\n",
            "[Training Epoch 141] Batch 4500, Loss 0.2640058696269989\n",
            "[Training Epoch 141] Batch 4600, Loss 0.2461095154285431\n",
            "[Training Epoch 141] Batch 4700, Loss 0.2538987398147583\n",
            "[Training Epoch 141] Batch 4800, Loss 0.26497817039489746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 141] HR = 0.6384, NDCG = 0.3684\n",
            "Epoch 142 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 142] Batch 0, Loss 0.2447630763053894\n",
            "[Training Epoch 142] Batch 100, Loss 0.28190210461616516\n",
            "[Training Epoch 142] Batch 200, Loss 0.25623565912246704\n",
            "[Training Epoch 142] Batch 300, Loss 0.2737511396408081\n",
            "[Training Epoch 142] Batch 400, Loss 0.2604069113731384\n",
            "[Training Epoch 142] Batch 500, Loss 0.24963416159152985\n",
            "[Training Epoch 142] Batch 600, Loss 0.3007170557975769\n",
            "[Training Epoch 142] Batch 700, Loss 0.2763793468475342\n",
            "[Training Epoch 142] Batch 800, Loss 0.2562220096588135\n",
            "[Training Epoch 142] Batch 900, Loss 0.26857686042785645\n",
            "[Training Epoch 142] Batch 1000, Loss 0.27258387207984924\n",
            "[Training Epoch 142] Batch 1100, Loss 0.26883167028427124\n",
            "[Training Epoch 142] Batch 1200, Loss 0.25668367743492126\n",
            "[Training Epoch 142] Batch 1300, Loss 0.25864383578300476\n",
            "[Training Epoch 142] Batch 1400, Loss 0.27398234605789185\n",
            "[Training Epoch 142] Batch 1500, Loss 0.25864556431770325\n",
            "[Training Epoch 142] Batch 1600, Loss 0.2688233256340027\n",
            "[Training Epoch 142] Batch 1700, Loss 0.2792844772338867\n",
            "[Training Epoch 142] Batch 1800, Loss 0.24801723659038544\n",
            "[Training Epoch 142] Batch 1900, Loss 0.2590726613998413\n",
            "[Training Epoch 142] Batch 2000, Loss 0.2810738682746887\n",
            "[Training Epoch 142] Batch 2100, Loss 0.27130764722824097\n",
            "[Training Epoch 142] Batch 2200, Loss 0.2917841672897339\n",
            "[Training Epoch 142] Batch 2300, Loss 0.2788977026939392\n",
            "[Training Epoch 142] Batch 2400, Loss 0.23033058643341064\n",
            "[Training Epoch 142] Batch 2500, Loss 0.24762719869613647\n",
            "[Training Epoch 142] Batch 2600, Loss 0.2687157094478607\n",
            "[Training Epoch 142] Batch 2700, Loss 0.27744096517562866\n",
            "[Training Epoch 142] Batch 2800, Loss 0.2636306881904602\n",
            "[Training Epoch 142] Batch 2900, Loss 0.27215132117271423\n",
            "[Training Epoch 142] Batch 3000, Loss 0.25238341093063354\n",
            "[Training Epoch 142] Batch 3100, Loss 0.2647334635257721\n",
            "[Training Epoch 142] Batch 3200, Loss 0.2487529218196869\n",
            "[Training Epoch 142] Batch 3300, Loss 0.27677100896835327\n",
            "[Training Epoch 142] Batch 3400, Loss 0.26877498626708984\n",
            "[Training Epoch 142] Batch 3500, Loss 0.26277869939804077\n",
            "[Training Epoch 142] Batch 3600, Loss 0.27064403891563416\n",
            "[Training Epoch 142] Batch 3700, Loss 0.2508889436721802\n",
            "[Training Epoch 142] Batch 3800, Loss 0.26329541206359863\n",
            "[Training Epoch 142] Batch 3900, Loss 0.28596001863479614\n",
            "[Training Epoch 142] Batch 4000, Loss 0.2837427258491516\n",
            "[Training Epoch 142] Batch 4100, Loss 0.256023108959198\n",
            "[Training Epoch 142] Batch 4200, Loss 0.2664526104927063\n",
            "[Training Epoch 142] Batch 4300, Loss 0.25029322504997253\n",
            "[Training Epoch 142] Batch 4400, Loss 0.2742491364479065\n",
            "[Training Epoch 142] Batch 4500, Loss 0.2501378059387207\n",
            "[Training Epoch 142] Batch 4600, Loss 0.2551097273826599\n",
            "[Training Epoch 142] Batch 4700, Loss 0.27068790793418884\n",
            "[Training Epoch 142] Batch 4800, Loss 0.27284812927246094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 142] HR = 0.6356, NDCG = 0.3680\n",
            "Epoch 143 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 143] Batch 0, Loss 0.2785516381263733\n",
            "[Training Epoch 143] Batch 100, Loss 0.2982036769390106\n",
            "[Training Epoch 143] Batch 200, Loss 0.2513384222984314\n",
            "[Training Epoch 143] Batch 300, Loss 0.28500741720199585\n",
            "[Training Epoch 143] Batch 400, Loss 0.29249048233032227\n",
            "[Training Epoch 143] Batch 500, Loss 0.211256206035614\n",
            "[Training Epoch 143] Batch 600, Loss 0.2348702847957611\n",
            "[Training Epoch 143] Batch 700, Loss 0.2640325725078583\n",
            "[Training Epoch 143] Batch 800, Loss 0.2573281526565552\n",
            "[Training Epoch 143] Batch 900, Loss 0.24246689677238464\n",
            "[Training Epoch 143] Batch 1000, Loss 0.27508461475372314\n",
            "[Training Epoch 143] Batch 1100, Loss 0.251362144947052\n",
            "[Training Epoch 143] Batch 1200, Loss 0.24935370683670044\n",
            "[Training Epoch 143] Batch 1300, Loss 0.2624247372150421\n",
            "[Training Epoch 143] Batch 1400, Loss 0.2709873914718628\n",
            "[Training Epoch 143] Batch 1500, Loss 0.24353305995464325\n",
            "[Training Epoch 143] Batch 1600, Loss 0.23058021068572998\n",
            "[Training Epoch 143] Batch 1700, Loss 0.23984090983867645\n",
            "[Training Epoch 143] Batch 1800, Loss 0.25958698987960815\n",
            "[Training Epoch 143] Batch 1900, Loss 0.27531975507736206\n",
            "[Training Epoch 143] Batch 2000, Loss 0.25293272733688354\n",
            "[Training Epoch 143] Batch 2100, Loss 0.2889271676540375\n",
            "[Training Epoch 143] Batch 2200, Loss 0.25546908378601074\n",
            "[Training Epoch 143] Batch 2300, Loss 0.27531760931015015\n",
            "[Training Epoch 143] Batch 2400, Loss 0.26845356822013855\n",
            "[Training Epoch 143] Batch 2500, Loss 0.2721286714076996\n",
            "[Training Epoch 143] Batch 2600, Loss 0.24426479637622833\n",
            "[Training Epoch 143] Batch 2700, Loss 0.2711503505706787\n",
            "[Training Epoch 143] Batch 2800, Loss 0.2640218734741211\n",
            "[Training Epoch 143] Batch 2900, Loss 0.2644852101802826\n",
            "[Training Epoch 143] Batch 3000, Loss 0.2888711094856262\n",
            "[Training Epoch 143] Batch 3100, Loss 0.2833300232887268\n",
            "[Training Epoch 143] Batch 3200, Loss 0.2714430093765259\n",
            "[Training Epoch 143] Batch 3300, Loss 0.24677878618240356\n",
            "[Training Epoch 143] Batch 3400, Loss 0.2612358331680298\n",
            "[Training Epoch 143] Batch 3500, Loss 0.24458098411560059\n",
            "[Training Epoch 143] Batch 3600, Loss 0.27413487434387207\n",
            "[Training Epoch 143] Batch 3700, Loss 0.2855302095413208\n",
            "[Training Epoch 143] Batch 3800, Loss 0.2598351836204529\n",
            "[Training Epoch 143] Batch 3900, Loss 0.2737587094306946\n",
            "[Training Epoch 143] Batch 4000, Loss 0.26519477367401123\n",
            "[Training Epoch 143] Batch 4100, Loss 0.23638395965099335\n",
            "[Training Epoch 143] Batch 4200, Loss 0.2696034908294678\n",
            "[Training Epoch 143] Batch 4300, Loss 0.2722783088684082\n",
            "[Training Epoch 143] Batch 4400, Loss 0.24066349864006042\n",
            "[Training Epoch 143] Batch 4500, Loss 0.2598383128643036\n",
            "[Training Epoch 143] Batch 4600, Loss 0.25713545083999634\n",
            "[Training Epoch 143] Batch 4700, Loss 0.2511112093925476\n",
            "[Training Epoch 143] Batch 4800, Loss 0.2718978226184845\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 143] HR = 0.6386, NDCG = 0.3685\n",
            "Epoch 144 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 144] Batch 0, Loss 0.2433149218559265\n",
            "[Training Epoch 144] Batch 100, Loss 0.2744438648223877\n",
            "[Training Epoch 144] Batch 200, Loss 0.24364739656448364\n",
            "[Training Epoch 144] Batch 300, Loss 0.26802268624305725\n",
            "[Training Epoch 144] Batch 400, Loss 0.2692502737045288\n",
            "[Training Epoch 144] Batch 500, Loss 0.2718302011489868\n",
            "[Training Epoch 144] Batch 600, Loss 0.2888747453689575\n",
            "[Training Epoch 144] Batch 700, Loss 0.281180739402771\n",
            "[Training Epoch 144] Batch 800, Loss 0.2775474190711975\n",
            "[Training Epoch 144] Batch 900, Loss 0.2914280891418457\n",
            "[Training Epoch 144] Batch 1000, Loss 0.267702579498291\n",
            "[Training Epoch 144] Batch 1100, Loss 0.24958012998104095\n",
            "[Training Epoch 144] Batch 1200, Loss 0.28079795837402344\n",
            "[Training Epoch 144] Batch 1300, Loss 0.28511303663253784\n",
            "[Training Epoch 144] Batch 1400, Loss 0.2811582684516907\n",
            "[Training Epoch 144] Batch 1500, Loss 0.2658894658088684\n",
            "[Training Epoch 144] Batch 1600, Loss 0.25775331258773804\n",
            "[Training Epoch 144] Batch 1700, Loss 0.25589072704315186\n",
            "[Training Epoch 144] Batch 1800, Loss 0.27427685260772705\n",
            "[Training Epoch 144] Batch 1900, Loss 0.2684284448623657\n",
            "[Training Epoch 144] Batch 2000, Loss 0.2452409565448761\n",
            "[Training Epoch 144] Batch 2100, Loss 0.2769708037376404\n",
            "[Training Epoch 144] Batch 2200, Loss 0.27130112051963806\n",
            "[Training Epoch 144] Batch 2300, Loss 0.24726668000221252\n",
            "[Training Epoch 144] Batch 2400, Loss 0.26632097363471985\n",
            "[Training Epoch 144] Batch 2500, Loss 0.26467862725257874\n",
            "[Training Epoch 144] Batch 2600, Loss 0.2329278141260147\n",
            "[Training Epoch 144] Batch 2700, Loss 0.26812756061553955\n",
            "[Training Epoch 144] Batch 2800, Loss 0.25784364342689514\n",
            "[Training Epoch 144] Batch 2900, Loss 0.27082887291908264\n",
            "[Training Epoch 144] Batch 3000, Loss 0.25855258107185364\n",
            "[Training Epoch 144] Batch 3100, Loss 0.2666730284690857\n",
            "[Training Epoch 144] Batch 3200, Loss 0.27734947204589844\n",
            "[Training Epoch 144] Batch 3300, Loss 0.26810234785079956\n",
            "[Training Epoch 144] Batch 3400, Loss 0.24477706849575043\n",
            "[Training Epoch 144] Batch 3500, Loss 0.2661939561367035\n",
            "[Training Epoch 144] Batch 3600, Loss 0.2535010576248169\n",
            "[Training Epoch 144] Batch 3700, Loss 0.25569266080856323\n",
            "[Training Epoch 144] Batch 3800, Loss 0.27332982420921326\n",
            "[Training Epoch 144] Batch 3900, Loss 0.2886549234390259\n",
            "[Training Epoch 144] Batch 4000, Loss 0.27233821153640747\n",
            "[Training Epoch 144] Batch 4100, Loss 0.29010313749313354\n",
            "[Training Epoch 144] Batch 4200, Loss 0.2474469095468521\n",
            "[Training Epoch 144] Batch 4300, Loss 0.23198144137859344\n",
            "[Training Epoch 144] Batch 4400, Loss 0.2663525938987732\n",
            "[Training Epoch 144] Batch 4500, Loss 0.29069820046424866\n",
            "[Training Epoch 144] Batch 4600, Loss 0.28933683037757874\n",
            "[Training Epoch 144] Batch 4700, Loss 0.24688883125782013\n",
            "[Training Epoch 144] Batch 4800, Loss 0.2989053726196289\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 144] HR = 0.6382, NDCG = 0.3688\n",
            "Epoch 145 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 145] Batch 0, Loss 0.28327131271362305\n",
            "[Training Epoch 145] Batch 100, Loss 0.25037407875061035\n",
            "[Training Epoch 145] Batch 200, Loss 0.25562816858291626\n",
            "[Training Epoch 145] Batch 300, Loss 0.2679031193256378\n",
            "[Training Epoch 145] Batch 400, Loss 0.2633574604988098\n",
            "[Training Epoch 145] Batch 500, Loss 0.2605437636375427\n",
            "[Training Epoch 145] Batch 600, Loss 0.2647002041339874\n",
            "[Training Epoch 145] Batch 700, Loss 0.2585006356239319\n",
            "[Training Epoch 145] Batch 800, Loss 0.22024005651474\n",
            "[Training Epoch 145] Batch 900, Loss 0.2590174674987793\n",
            "[Training Epoch 145] Batch 1000, Loss 0.261913537979126\n",
            "[Training Epoch 145] Batch 1100, Loss 0.23418113589286804\n",
            "[Training Epoch 145] Batch 1200, Loss 0.2746378779411316\n",
            "[Training Epoch 145] Batch 1300, Loss 0.2939409613609314\n",
            "[Training Epoch 145] Batch 1400, Loss 0.29504531621932983\n",
            "[Training Epoch 145] Batch 1500, Loss 0.27183473110198975\n",
            "[Training Epoch 145] Batch 1600, Loss 0.28781917691230774\n",
            "[Training Epoch 145] Batch 1700, Loss 0.2805098295211792\n",
            "[Training Epoch 145] Batch 1800, Loss 0.273280531167984\n",
            "[Training Epoch 145] Batch 1900, Loss 0.2476031631231308\n",
            "[Training Epoch 145] Batch 2000, Loss 0.2653197646141052\n",
            "[Training Epoch 145] Batch 2100, Loss 0.2782880663871765\n",
            "[Training Epoch 145] Batch 2200, Loss 0.24303242564201355\n",
            "[Training Epoch 145] Batch 2300, Loss 0.2592793107032776\n",
            "[Training Epoch 145] Batch 2400, Loss 0.26987355947494507\n",
            "[Training Epoch 145] Batch 2500, Loss 0.26397228240966797\n",
            "[Training Epoch 145] Batch 2600, Loss 0.26001089811325073\n",
            "[Training Epoch 145] Batch 2700, Loss 0.2591588795185089\n",
            "[Training Epoch 145] Batch 2800, Loss 0.259829044342041\n",
            "[Training Epoch 145] Batch 2900, Loss 0.25221073627471924\n",
            "[Training Epoch 145] Batch 3000, Loss 0.27036765217781067\n",
            "[Training Epoch 145] Batch 3100, Loss 0.26846590638160706\n",
            "[Training Epoch 145] Batch 3200, Loss 0.2934250235557556\n",
            "[Training Epoch 145] Batch 3300, Loss 0.2693905532360077\n",
            "[Training Epoch 145] Batch 3400, Loss 0.2699918746948242\n",
            "[Training Epoch 145] Batch 3500, Loss 0.2585206627845764\n",
            "[Training Epoch 145] Batch 3600, Loss 0.26000839471817017\n",
            "[Training Epoch 145] Batch 3700, Loss 0.27836334705352783\n",
            "[Training Epoch 145] Batch 3800, Loss 0.2785385251045227\n",
            "[Training Epoch 145] Batch 3900, Loss 0.2497463822364807\n",
            "[Training Epoch 145] Batch 4000, Loss 0.26785629987716675\n",
            "[Training Epoch 145] Batch 4100, Loss 0.2749708294868469\n",
            "[Training Epoch 145] Batch 4200, Loss 0.29130175709724426\n",
            "[Training Epoch 145] Batch 4300, Loss 0.2894114553928375\n",
            "[Training Epoch 145] Batch 4400, Loss 0.29137635231018066\n",
            "[Training Epoch 145] Batch 4500, Loss 0.25161105394363403\n",
            "[Training Epoch 145] Batch 4600, Loss 0.25079596042633057\n",
            "[Training Epoch 145] Batch 4700, Loss 0.2524614930152893\n",
            "[Training Epoch 145] Batch 4800, Loss 0.26742681860923767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 145] HR = 0.6404, NDCG = 0.3699\n",
            "Epoch 146 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 146] Batch 0, Loss 0.23054412007331848\n",
            "[Training Epoch 146] Batch 100, Loss 0.2599155604839325\n",
            "[Training Epoch 146] Batch 200, Loss 0.28558194637298584\n",
            "[Training Epoch 146] Batch 300, Loss 0.256491482257843\n",
            "[Training Epoch 146] Batch 400, Loss 0.2615330219268799\n",
            "[Training Epoch 146] Batch 500, Loss 0.282105028629303\n",
            "[Training Epoch 146] Batch 600, Loss 0.2488754838705063\n",
            "[Training Epoch 146] Batch 700, Loss 0.2321268767118454\n",
            "[Training Epoch 146] Batch 800, Loss 0.24030408263206482\n",
            "[Training Epoch 146] Batch 900, Loss 0.24086397886276245\n",
            "[Training Epoch 146] Batch 1000, Loss 0.2622703015804291\n",
            "[Training Epoch 146] Batch 1100, Loss 0.24565011262893677\n",
            "[Training Epoch 146] Batch 1200, Loss 0.2624000310897827\n",
            "[Training Epoch 146] Batch 1300, Loss 0.2298111766576767\n",
            "[Training Epoch 146] Batch 1400, Loss 0.21160316467285156\n",
            "[Training Epoch 146] Batch 1500, Loss 0.2884127199649811\n",
            "[Training Epoch 146] Batch 1600, Loss 0.26344501972198486\n",
            "[Training Epoch 146] Batch 1700, Loss 0.25549229979515076\n",
            "[Training Epoch 146] Batch 1800, Loss 0.2859840393066406\n",
            "[Training Epoch 146] Batch 1900, Loss 0.27757906913757324\n",
            "[Training Epoch 146] Batch 2000, Loss 0.26736488938331604\n",
            "[Training Epoch 146] Batch 2100, Loss 0.24583056569099426\n",
            "[Training Epoch 146] Batch 2200, Loss 0.27213776111602783\n",
            "[Training Epoch 146] Batch 2300, Loss 0.2498493194580078\n",
            "[Training Epoch 146] Batch 2400, Loss 0.2772391140460968\n",
            "[Training Epoch 146] Batch 2500, Loss 0.26407164335250854\n",
            "[Training Epoch 146] Batch 2600, Loss 0.23747779428958893\n",
            "[Training Epoch 146] Batch 2700, Loss 0.260850727558136\n",
            "[Training Epoch 146] Batch 2800, Loss 0.26928776502609253\n",
            "[Training Epoch 146] Batch 2900, Loss 0.2620881199836731\n",
            "[Training Epoch 146] Batch 3000, Loss 0.25166967511177063\n",
            "[Training Epoch 146] Batch 3100, Loss 0.25476717948913574\n",
            "[Training Epoch 146] Batch 3200, Loss 0.2788490056991577\n",
            "[Training Epoch 146] Batch 3300, Loss 0.2198718935251236\n",
            "[Training Epoch 146] Batch 3400, Loss 0.2618345618247986\n",
            "[Training Epoch 146] Batch 3500, Loss 0.2578354775905609\n",
            "[Training Epoch 146] Batch 3600, Loss 0.2608644962310791\n",
            "[Training Epoch 146] Batch 3700, Loss 0.2669978141784668\n",
            "[Training Epoch 146] Batch 3800, Loss 0.2612099051475525\n",
            "[Training Epoch 146] Batch 3900, Loss 0.2628043293952942\n",
            "[Training Epoch 146] Batch 4000, Loss 0.27064022421836853\n",
            "[Training Epoch 146] Batch 4100, Loss 0.2571999132633209\n",
            "[Training Epoch 146] Batch 4200, Loss 0.27988898754119873\n",
            "[Training Epoch 146] Batch 4300, Loss 0.26753634214401245\n",
            "[Training Epoch 146] Batch 4400, Loss 0.2534075975418091\n",
            "[Training Epoch 146] Batch 4500, Loss 0.28996482491493225\n",
            "[Training Epoch 146] Batch 4600, Loss 0.24381445348262787\n",
            "[Training Epoch 146] Batch 4700, Loss 0.2942372262477875\n",
            "[Training Epoch 146] Batch 4800, Loss 0.2511651813983917\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 146] HR = 0.6397, NDCG = 0.3692\n",
            "Epoch 147 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 147] Batch 0, Loss 0.2728981375694275\n",
            "[Training Epoch 147] Batch 100, Loss 0.245124951004982\n",
            "[Training Epoch 147] Batch 200, Loss 0.2601642310619354\n",
            "[Training Epoch 147] Batch 300, Loss 0.27080976963043213\n",
            "[Training Epoch 147] Batch 400, Loss 0.2492445707321167\n",
            "[Training Epoch 147] Batch 500, Loss 0.2461923509836197\n",
            "[Training Epoch 147] Batch 600, Loss 0.2533149719238281\n",
            "[Training Epoch 147] Batch 700, Loss 0.23905861377716064\n",
            "[Training Epoch 147] Batch 800, Loss 0.256037175655365\n",
            "[Training Epoch 147] Batch 900, Loss 0.2641780972480774\n",
            "[Training Epoch 147] Batch 1000, Loss 0.28650274872779846\n",
            "[Training Epoch 147] Batch 1100, Loss 0.27511608600616455\n",
            "[Training Epoch 147] Batch 1200, Loss 0.29982149600982666\n",
            "[Training Epoch 147] Batch 1300, Loss 0.2577553391456604\n",
            "[Training Epoch 147] Batch 1400, Loss 0.2508794665336609\n",
            "[Training Epoch 147] Batch 1500, Loss 0.25332698225975037\n",
            "[Training Epoch 147] Batch 1600, Loss 0.27764737606048584\n",
            "[Training Epoch 147] Batch 1700, Loss 0.26821279525756836\n",
            "[Training Epoch 147] Batch 1800, Loss 0.2727898359298706\n",
            "[Training Epoch 147] Batch 1900, Loss 0.25158601999282837\n",
            "[Training Epoch 147] Batch 2000, Loss 0.2693004608154297\n",
            "[Training Epoch 147] Batch 2100, Loss 0.269239217042923\n",
            "[Training Epoch 147] Batch 2200, Loss 0.24930016696453094\n",
            "[Training Epoch 147] Batch 2300, Loss 0.23861882090568542\n",
            "[Training Epoch 147] Batch 2400, Loss 0.2720945477485657\n",
            "[Training Epoch 147] Batch 2500, Loss 0.28444746136665344\n",
            "[Training Epoch 147] Batch 2600, Loss 0.2644243836402893\n",
            "[Training Epoch 147] Batch 2700, Loss 0.2736726403236389\n",
            "[Training Epoch 147] Batch 2800, Loss 0.2561977505683899\n",
            "[Training Epoch 147] Batch 2900, Loss 0.2566077411174774\n",
            "[Training Epoch 147] Batch 3000, Loss 0.27317097783088684\n",
            "[Training Epoch 147] Batch 3100, Loss 0.2594912052154541\n",
            "[Training Epoch 147] Batch 3200, Loss 0.26016318798065186\n",
            "[Training Epoch 147] Batch 3300, Loss 0.2622963786125183\n",
            "[Training Epoch 147] Batch 3400, Loss 0.27594614028930664\n",
            "[Training Epoch 147] Batch 3500, Loss 0.2863847017288208\n",
            "[Training Epoch 147] Batch 3600, Loss 0.29800528287887573\n",
            "[Training Epoch 147] Batch 3700, Loss 0.25226670503616333\n",
            "[Training Epoch 147] Batch 3800, Loss 0.2547034025192261\n",
            "[Training Epoch 147] Batch 3900, Loss 0.2910170555114746\n",
            "[Training Epoch 147] Batch 4000, Loss 0.2781274914741516\n",
            "[Training Epoch 147] Batch 4100, Loss 0.25266751646995544\n",
            "[Training Epoch 147] Batch 4200, Loss 0.2746009826660156\n",
            "[Training Epoch 147] Batch 4300, Loss 0.26877474784851074\n",
            "[Training Epoch 147] Batch 4400, Loss 0.25713831186294556\n",
            "[Training Epoch 147] Batch 4500, Loss 0.2579532861709595\n",
            "[Training Epoch 147] Batch 4600, Loss 0.27399057149887085\n",
            "[Training Epoch 147] Batch 4700, Loss 0.26146700978279114\n",
            "[Training Epoch 147] Batch 4800, Loss 0.2525468170642853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Evluating Epoch 147] HR = 0.6386, NDCG = 0.3672\n",
            "Epoch 148 starts !\n",
            "--------------------------------------------------------------------------------\n",
            "[Training Epoch 148] Batch 0, Loss 0.28386950492858887\n",
            "[Training Epoch 148] Batch 100, Loss 0.2980727553367615\n",
            "[Training Epoch 148] Batch 200, Loss 0.2602621018886566\n",
            "[Training Epoch 148] Batch 300, Loss 0.2421620786190033\n",
            "[Training Epoch 148] Batch 400, Loss 0.2843427062034607\n",
            "[Training Epoch 148] Batch 500, Loss 0.26240140199661255\n",
            "[Training Epoch 148] Batch 600, Loss 0.2718813121318817\n",
            "[Training Epoch 148] Batch 700, Loss 0.2593681514263153\n",
            "[Training Epoch 148] Batch 800, Loss 0.25852370262145996\n",
            "[Training Epoch 148] Batch 900, Loss 0.25290027260780334\n",
            "[Training Epoch 148] Batch 1000, Loss 0.2834629714488983\n",
            "[Training Epoch 148] Batch 1100, Loss 0.28946444392204285\n",
            "[Training Epoch 148] Batch 1200, Loss 0.25377339124679565\n",
            "[Training Epoch 148] Batch 1300, Loss 0.2657666802406311\n",
            "[Training Epoch 148] Batch 1400, Loss 0.26753664016723633\n",
            "[Training Epoch 148] Batch 1500, Loss 0.2664831876754761\n",
            "[Training Epoch 148] Batch 1600, Loss 0.2790781557559967\n",
            "[Training Epoch 148] Batch 1700, Loss 0.25698912143707275\n",
            "[Training Epoch 148] Batch 1800, Loss 0.2764705419540405\n",
            "[Training Epoch 148] Batch 1900, Loss 0.2592788338661194\n",
            "[Training Epoch 148] Batch 2000, Loss 0.25238075852394104\n",
            "[Training Epoch 148] Batch 2100, Loss 0.2538486421108246\n",
            "[Training Epoch 148] Batch 2200, Loss 0.23304376006126404\n",
            "[Training Epoch 148] Batch 2300, Loss 0.26146650314331055\n",
            "[Training Epoch 148] Batch 2400, Loss 0.25663918256759644\n",
            "[Training Epoch 148] Batch 2500, Loss 0.27612966299057007\n",
            "[Training Epoch 148] Batch 2600, Loss 0.2565563917160034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltzasbGxse_8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}