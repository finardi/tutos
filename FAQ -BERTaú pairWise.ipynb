{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Public FAQ - BERTaú - pairWise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1eGW3oHDjpOrvDYoD9dt0ZLxOJms-9ImD",
      "authorship_tag": "ABX9TyOgAiKNNRyMAGp2vwlNzjUM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/FAQ%20-BERTa%C3%BA%20pairWise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEBb_doU7m43"
      },
      "source": [
        "# BERTaú PairWise FAQ example\n",
        "> #### This notebook contains a end-to-end code to run the BERTaú pairwise FAQ task with a slice of the whole FAQ dataset. The FAQ dataset used in this experiment is public."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gwdOJe1PSm-",
        "outputId": "75883a25-ec19-4528-e983-1b1cc05b7e2b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Jan 27 23:27:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCx4ZK8aaZjX",
        "outputId": "39ba762d-e94c-4dfe-f7a6-8f2a58df7be2"
      },
      "source": [
        "! git clone https://github.com/vfcarida/bertau\n",
        "! pip install -q transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bertau'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 17 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 12.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 48.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 62.7MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYsjPf_yqSEe",
        "outputId": "904b27a7-f256-4bc0-d07d-06d35d13e9a0"
      },
      "source": [
        "import gc\n",
        "import sys\n",
        "sys.path.append('/content/bertau/')\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "from train import run\n",
        "from data import Data\n",
        "from dataset import FinalPrep, FAQDataset\n",
        "from utils import deterministic, get_device\n",
        "from BERTau_pairwise import BERTauPairwise, pairwise_loss, hinge_loss\n",
        "\n",
        "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
        "\n",
        "# Make the experiment deterministic\n",
        "deterministic = deterministic() \n",
        "\n",
        "# Device of experiment: CPU or GPU\n",
        "device = get_device()\n",
        "\n",
        "# Basic configurations of the experiment\n",
        "config = {\n",
        "    'K-CANDS': 45,\n",
        "    'TRAIN_SIZE': 0.90,\n",
        "    'BATCH_SZ': 12,\n",
        "    'MAX_LEN': 196,\n",
        "    'N_EPOCHS': 1,\n",
        "    'LR': 5e-5,\n",
        "    'TOP_K': 10,\n",
        "}\n",
        "\n",
        "# Get FAQ data ---------------------------------------\n",
        "print('Getting the FAQ data ...', end=' ')\n",
        "path = '/content/bertau/public_itau-unibanco-faq.csv'\n",
        "df = Data(path)\n",
        "df_faq = df.data_prep()\n",
        "print('DONE!')\n",
        "\n",
        "# Data Prep ------------------------------------------\n",
        "print('Preparing the data ...', end=' ')\n",
        "data = FinalPrep(df_faq)\n",
        "df_docs, qid_to_text, docid_to_text, labels = data.make_labels()\n",
        "qid_target_train, train_dataframe, labels_train, \\\n",
        "qid_target_valid, valid_dataframe, labels_valid = data.split_data(\n",
        "    train_size=config['TRAIN_SIZE'])    \n",
        "print('DONE!')\n",
        "\n",
        "# Dataset and Dataloader -----------------------------\n",
        "print('Building Datasets and Dataloaders ...', end=' ')\n",
        "\n",
        "# Path model at Hugging Face hub\n",
        "path_model = 'Itau-Unibanco/BERTau'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(path_model, lowercase=True)\n",
        "\n",
        "# Train dataset and dataloader\n",
        "train = FAQDataset()\n",
        "\n",
        "ds_train = train.make_pairwise_dataset(\n",
        "    df=qid_target_train, \n",
        "    df_docs=df_docs, \n",
        "    tokenizer=tokenizer, \n",
        "    qid_to_text=qid_to_text, \n",
        "    docid_to_text=docid_to_text, \n",
        "    max_seq_len=config['MAX_LEN'], \n",
        "    K=config['K-CANDS']\n",
        "    )\n",
        "train_loader = train.make_pairwise_dataLoader(\n",
        "    ds_train, \n",
        "    batch_size=config['BATCH_SZ'], \n",
        "    phase='train',\n",
        "    )\n",
        "\n",
        "# Valid dataset and dataloader\n",
        "valid = FAQDataset()\n",
        "\n",
        "valid_data = valid._get_cands(\n",
        "    df=qid_target_valid, \n",
        "    df_docs=df_docs, \n",
        "    K=config['K-CANDS']\n",
        "    )\n",
        "print('DONE!')\n",
        "\n",
        "# Running Train and Eval -----------------------------\n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print('\\nRunning the Train Loop ...')\n",
        "\n",
        "# Put the model on device\n",
        "model = BERTauPairwise(path_model).to(device)\n",
        "\n",
        "# Config the optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['LR'])\n",
        "\n",
        "# Number of total steps to be used in Warm Up\n",
        "total_steps = len(train_loader) * config['N_EPOCHS']\n",
        "\n",
        "# Config the Learning Rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=total_steps * 0.03,  # 3% of Warm Up\n",
        "    num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "# To perform the experiment in FP16\n",
        "scaler = amp.GradScaler()\n",
        "\n",
        "# Start train time\n",
        "start = time.time()\n",
        "\n",
        "# Call run function to perform the train and valid batches\n",
        "df_stats = run(\n",
        "    model=model, \n",
        "    loss_fn=hinge_loss, \n",
        "    train_loader=train_loader, \n",
        "    qid_target=valid_data, \n",
        "    path_vocab=path_model, \n",
        "    qid_to_text=qid_to_text, \n",
        "    docid_to_text=docid_to_text, \n",
        "    ground_truths=labels_valid, \n",
        "    max_len=config['MAX_LEN'], \n",
        "    K=config['K-CANDS'], \n",
        "    optimizer=optimizer, \n",
        "    device=device,\n",
        "    scheduler=scheduler, \n",
        "    scaler=scaler, \n",
        "    path_save=None, \n",
        "    n_epochs=config['N_EPOCHS'], \n",
        "    )\n",
        "    \n",
        "# Count the training time\n",
        "end = time.time()\n",
        "elapsed = end - start\n",
        "\n",
        "print(f'DONE! Elapsed time: {round(elapsed/60)} min.')    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting the FAQ data ... DONE!\n",
            "Preparing the data ... DONE!\n",
            "Building Datasets and Dataloaders ... DONE!\n",
            "\n",
            "Running the Train Loop ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORMuexqDtTcl"
      },
      "source": [
        "# The End"
      ]
    }
  ]
}