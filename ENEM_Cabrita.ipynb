{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1DJmsjttfsBZbxN_mJft_WCfYR6kXqogI",
      "authorship_tag": "ABX9TyP/gAzQNnpyGQjJ47t3tdfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61eae75ace35467eb8bf8ca0c3eb736c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3fd4f7e6e2e481e923473b13e14e298",
              "IPY_MODEL_3003e587e9504dbda05095b6f49cd700",
              "IPY_MODEL_9bf9ea1f617f47fd87dea0eddacae010"
            ],
            "layout": "IPY_MODEL_ab5730f47cb64cadae05c2f26b6b273b"
          }
        },
        "d3fd4f7e6e2e481e923473b13e14e298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aa4e93590af4d53b07b811b8d656476",
            "placeholder": "​",
            "style": "IPY_MODEL_f7277f8c90db4e6db92bf4f24328af52",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3003e587e9504dbda05095b6f49cd700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9a507a914dd43158192e47dca35a3fd",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e638bd3a8be4288b8a424be1d36af47",
            "value": 33
          }
        },
        "9bf9ea1f617f47fd87dea0eddacae010": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2023ac0618574165a8e17bbcc9be313b",
            "placeholder": "​",
            "style": "IPY_MODEL_ac633bc595a242d59f53cac8ccfbee6a",
            "value": " 33/33 [01:14&lt;00:00,  2.39s/it]"
          }
        },
        "ab5730f47cb64cadae05c2f26b6b273b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aa4e93590af4d53b07b811b8d656476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7277f8c90db4e6db92bf4f24328af52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9a507a914dd43158192e47dca35a3fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e638bd3a8be4288b8a424be1d36af47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2023ac0618574165a8e17bbcc9be313b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac633bc595a242d59f53cac8ccfbee6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e15bc5aa7a7489fb772301ce556ebaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e40a75781b34fa1930fdbb97cb3c6c4",
              "IPY_MODEL_643a62d5bd484cfc9694a601fd6054c4",
              "IPY_MODEL_21e65ebcd0de4ba9af6729c735ad2c4e"
            ],
            "layout": "IPY_MODEL_c90552aa32534ac1b30b10ec7753291d"
          }
        },
        "0e40a75781b34fa1930fdbb97cb3c6c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d47b10b2e1594346afecd536a9c6c286",
            "placeholder": "​",
            "style": "IPY_MODEL_92159071d5c943d8800a24de35105a07",
            "value": "100%"
          }
        },
        "643a62d5bd484cfc9694a601fd6054c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a5fca252ef34c29a9e865673ebbd163",
            "max": 118,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_097d40c6ad6f44ed920718a0e58bb7e2",
            "value": 118
          }
        },
        "21e65ebcd0de4ba9af6729c735ad2c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cce47a9643b748d6891ed2ce54752dff",
            "placeholder": "​",
            "style": "IPY_MODEL_aa5af5adee2a436ab7e07dbee5476ad7",
            "value": " 118/118 [04:54&lt;00:00,  1.66s/it]"
          }
        },
        "c90552aa32534ac1b30b10ec7753291d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d47b10b2e1594346afecd536a9c6c286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92159071d5c943d8800a24de35105a07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a5fca252ef34c29a9e865673ebbd163": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "097d40c6ad6f44ed920718a0e58bb7e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cce47a9643b748d6891ed2ce54752dff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa5af5adee2a436ab7e07dbee5476ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/ENEM_Cabrita.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai \n",
        "!pip install -q rank_bm25 \n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "id": "eL5EGALAOAey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dec232f5-31fa-43fa-adfd-8bf1996b5ed5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m  WARNING: Did not find branch or tag 'c3dc391', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------- #\n",
        "# This code is from https://github.com/piresramon/gpt-4-enem #\n",
        "# ---------------------------------------------------------- #\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import random \n",
        "import collections\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import openai\n",
        "\n",
        "import rank_bm25\n",
        "\n",
        "from peft import PeftModel\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datasets import Dataset\n",
        "\n",
        "if torch.cuda.is_available(): \n",
        "    device = 'cuda'  \n",
        "else: \n",
        "    device ='cpu'\n",
        "\n",
        "MANUAL_SEED = 2711\n",
        "rnd = random.Random()\n",
        "rnd.seed(MANUAL_SEED)\n",
        "def deterministic(rep=True, manual_seed=MANUAL_SEED):\n",
        "    if rep:\n",
        "        torch.manual_seed(manual_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(manual_seed)\n",
        "            torch.cuda.manual_seed_all(manual_seed)\n",
        "        torch.backends.cudnn.enabled = False \n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        print(f'Experimento deterministico, seed: {manual_seed}')\n",
        "        if device == 'cuda':\n",
        "            print(f'Existe {torch.cuda.device_count()} GPU\\\n",
        "            {torch.cuda.get_device_name(0)} disponível.')\n",
        "    else:\n",
        "        print('Experimento randomico')\n",
        "deterministic()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4dfyr6EmGsJ",
        "outputId": "319c1ead-c0c8-41e0-92b7-772387bfd522"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('8013'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-v100-hm-2hxu7ziw7ao97 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimento deterministico, seed: 2711\n",
            "Existe 1 GPU            Tesla V100-SXM2-16GB disponível.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Enem json and create enem_data "
      ],
      "metadata": {
        "id": "EaS1311b-ew1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ignore_question(doc):\n",
        "    filters = {\n",
        "        'IU': False,\n",
        "        # 'MR': False,  # uncomment to filter out MR\n",
        "        # 'CE': False,  # uncomment to filter out CE\n",
        "        'ML': False,\n",
        "    }\n",
        "    for k,v in filters.items():\n",
        "        if doc[k] != v:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _process_doc_cot(doc):\n",
        "    def format_example(doc, choices):\n",
        "        \"\"\"\n",
        "            Passagem: <passage>\n",
        "            Pergunta: <question>\n",
        "            Choices:\n",
        "            A. <choice1>\n",
        "            B. <choice2>\n",
        "            C. <choice3>\n",
        "            D. <choice4>\n",
        "            Answer:\n",
        "        \"\"\"\n",
        "        prompt = \"Cabeçalho: \" + doc[\"context\"] + \"\\n\"\n",
        "        prompt += \"Enunciado: \" + doc[\"question\"] + \"\\nAlternativas:\\n\"\n",
        "        for choice, option in zip(choices, doc[\"options\"]):\n",
        "            prompt += f\"{choice.upper()}. {option}\\n\"\n",
        "        \n",
        "        prompt += \"Explicação: \" + doc.get(\"explanation\", \"\")\n",
        "        return prompt.strip()\n",
        "    choices = ['a', 'b', 'c', 'd', 'e']\n",
        "    return {\n",
        "        \"query\": format_example(doc, choices),\n",
        "        \"choices\": doc[\"options\"],\n",
        "        \"gold\": choices.index(doc[\"label\"]),\n",
        "        \"id\": doc[\"id\"],\n",
        "        \"exam\": doc[\"exam\"],\n",
        "    }    \n",
        "\n",
        "dataset = collections.defaultdict(list)\n",
        "        \n",
        "data_path = \"/content/drive/MyDrive/LLMs/ENEM/ENEMdataset/2022.json\"\n",
        "with open(data_path) as f:\n",
        "    documents = json.load(f)\n",
        "\n",
        "documents = list(filter(lambda doc: not ignore_question(doc), documents))\n",
        "dataset['test'] = list(map(_process_doc_cot, documents))\n",
        "\n",
        "enem_data = {ix:doc  for ix, doc in enumerate(dataset['test'])}\n",
        "\n",
        "print(f\"tamanho dataset: {len(enem_data)}\")\n",
        "\n",
        "enem_data[0]    "
      ],
      "metadata": {
        "id": "u6NlgTBlOf3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baefae2b-2be9-418e-e6e6-5e8facccceb4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamanho dataset: 118\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Cabeçalho: A conquista da medalha de prata por Rayssa Leal, no skate street nos Jogos Olímpicos, é exemplo da representatividade feminina no esporte, avalia a âncora do jornal da rede de televisão da CNN. A apresentadora, que também anda de skate, celebrou a vitória da brasileira, que entrou para a história como a atleta mais nova a subir num pódio defendendo o Brasil. “Essa representatividade do esporte nos Jogos faz pensarmos que não temos que ficar nos encaixando em nenhum lugar. Posso gostar de passar notícia e, mesmo assim, gostar de skate, subir montanha, mergulhar, andar de bike, fazer yoga. Temos que parar de ficar enquadrando as pessoas dentro de regras. A gente vive num padrão no qual a menina ganha boneca, mas por que também não fazer um esporte de aventura? Por que o homem pode se machucar, cair de joelhos, e a menina tem que estar sempre lindinha dentro de um padrão? Acabamos limitando os talentos das pessoas”, afirmou a jornalista, sobre a prática do skate por mulheres. Disponível em: www.cnnbrasil.com.br. Acesso em: 31 out. 2021 (adaptado).\\nEnunciado: O discurso da jornalista traz questionamentos sobre a relação da conquista da skatista com a\\nAlternativas:\\nA. conciliação do jornalismo com a prática do skate.\\nB. inserção das mulheres na modalidade skate street.\\nC. desconstrução da noção do skate como modalidade masculina.\\nD. vanguarda de ser a atleta mais jovem a subir no pódio olímpico.\\nE. conquista de medalha nos Jogos Olímpicos de Tóquio.\\nExplicação:',\n",
              " 'choices': ['conciliação do jornalismo com a prática do skate.',\n",
              "  'inserção das mulheres na modalidade skate street.',\n",
              "  'desconstrução da noção do skate como modalidade masculina.',\n",
              "  'vanguarda de ser a atleta mais jovem a subir no pódio olímpico.',\n",
              "  'conquista de medalha nos Jogos Olímpicos de Tóquio.'],\n",
              " 'gold': 2,\n",
              " 'id': 'ENEM_2022_6',\n",
              " 'exam': '2022'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Prompts\n",
        "- #### Dynamic Prompt (fewshot sampled)\n",
        "- #### Dynamic Similar Prompt (bm25 rank fewshot)"
      ],
      "metadata": {
        "id": "A5qaX_aF-q_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_similar_prompt(doc_id, data, topk=3):\n",
        "    key_predict = [k for k, v in enem_data.items() if v['id'].startswith(doc_id)][0]\n",
        "    query_list = [v['query'] for k,v in data.items()]\n",
        "    tokenized_corpus = [doc.split(\" \") for doc in query_list]\n",
        "    bm25 = rank_bm25.BM25Plus(tokenized_corpus)\n",
        "\n",
        "    query = data[key_predict]['query']\n",
        "    tokenized_query = query.split(\" \")\n",
        "    \n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "    \n",
        "    # [1:] remove query with itself ---> topk+1\n",
        "    top_similar_idxs = list(doc_scores.argsort()[-(topk+1):][::-1])[1:]\n",
        "    if key_predict in top_similar_idxs: # k in topk_sim ---> must be false\n",
        "        print('Error BM25 prompt similar retrieval')\n",
        "    \n",
        "    return top_similar_idxs\n",
        "\n",
        "\n",
        "def dynamyc_fewshot_examples(num_fewshot, doc_id, sim_prompt=True, data=enem_data):\n",
        "    if sim_prompt:\n",
        "        topk_sim = dynamic_similar_prompt(doc_id, data, topk=num_fewshot)\n",
        "        fewshot_ex = [enem_data[k] for k in topk_sim]\n",
        "\n",
        "    else:\n",
        "        # filter the sample in current predict from data\n",
        "        all_possible_fewshot_keys = [k for k, v in data.items() if not v['id'].startswith(doc_id)]\n",
        "        fewshot_keys = rnd.sample(all_possible_fewshot_keys, num_fewshot)\n",
        "        fewshot_ex = [enem_data[k] for k in fewshot_keys]\n",
        "\n",
        "    return fewshot_ex\n",
        "\n",
        "\n",
        "def fewshot_context(doc, num_fewshot, similar_prompt=True, enem_data=enem_data):\n",
        "    if num_fewshot == 0 and fewshotex is None:\n",
        "        labeled_examples = \"\"\n",
        "    else:\n",
        "        fewshotex = dynamyc_fewshot_examples(\n",
        "            num_fewshot=num_fewshot, doc_id=doc['id'], sim_prompt=similar_prompt, data=enem_data\n",
        "        )\n",
        "        \n",
        "        labeled_examples = ''\n",
        "        for i, doc_ex in enumerate(fewshotex):\n",
        "            labeled_examples += f'Questão {i+1}:\\n'\n",
        "            labeled_examples += doc_ex['query'] + \" \" + ['A.', 'B.', 'C.', 'D.', 'E.'][doc_ex['gold']].upper()\n",
        "            labeled_examples += '\\n##\\n'\n",
        "        labeled_examples += f'Questão {len(fewshotex) + 1}:\\n'\n",
        "\n",
        "    example = doc['query']\n",
        "    \n",
        "    return labeled_examples + example\n",
        "\n",
        "# -------------------------------------------------------------------------------------- #\n",
        "# number of fewshots, must be > 0\n",
        "\n",
        "# create inputs ---> list with fewshot samples + question to be answer \n",
        "two_shot, three_shot = [], []\n",
        "for k,doc in enem_data.items():\n",
        "    two_shot.append(fewshot_context(doc=doc,   num_fewshot=2, similar_prompt=True, enem_data=enem_data))\n",
        "    three_shot.append(fewshot_context(doc=doc, num_fewshot=3, similar_prompt=True, enem_data=enem_data))"
      ],
      "metadata": {
        "id": "3fLuSqrEjxVc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval ENEM-2022 with ChatGPT "
      ],
      "metadata": {
        "id": "KqOWUwVD_EEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_CHATGPT = False\n",
        "\n",
        "if EVAL_CHATGPT:\n",
        "    # OPENAI_API_KEY = \"secret\"\n",
        "    # openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "    deterministic()    \n",
        "\n",
        "    trues, preds = [], []\n",
        "    loop = tqdm(two_shot, leave=True)\n",
        "\n",
        "    for ix, batch in enumerate(loop):\n",
        "\n",
        "        # to make sure the same example to predict has the correct label \n",
        "        # find the point where the question to be evaluate starts\n",
        "        point = two_shot[ix].find('Questão 3:\\n')\n",
        "        \n",
        "        # \"11+point\" ---> is the start of the query-text, so this is a naive approach\n",
        "        if enem_data[ix]['query'] == two_shot[ix][11+point:]:\n",
        "            chatGPT_response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": two_shot[ix]}])\n",
        "            \n",
        "            preds.append(chatGPT_response['choices'][0]['message']['content'])\n",
        "            trues.append(enem_data[ix]['gold'])\n",
        "        else:\n",
        "            # if the query-text is different from inputs to be evaluated \n",
        "            # we print for further investigation\n",
        "            print(point, ix)"
      ],
      "metadata": {
        "id": "OzziaZb7wN30"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(trues, preds, chatgpt=False):\n",
        "    dataframe = pd.DataFrame({'true': trues, 'pred':preds})\n",
        "\n",
        "    #get only latter\n",
        "    if chatgpt:\n",
        "        dataframe['pred'] = dataframe.pred.apply(lambda x: x[:1])\n",
        "    \n",
        "    else:\n",
        "        dataframe['pred'] = dataframe.pred.apply(lambda x: x[-1:])\n",
        "    \n",
        "    map_label = {0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\", 4:\"E\"}\n",
        "    dataframe['true'] = dataframe.true.apply(lambda x: map_label[x])\n",
        "    \n",
        "    acc = accuracy_score(dataframe['true'], dataframe['pred'])\n",
        "    \n",
        "    return dataframe, acc\n",
        "\n",
        "\n",
        "if EVAL_CHATGPT:\n",
        "    chatGPT_results, acc = get_results(trues, preds, chatgpt=True)\n",
        "\n",
        "    print(f'ACC of 2-shot chatGPT on ENEM-2022: {acc:.3}')\n",
        "    # ACC of ChatGPT: 0.771\n",
        "\n",
        "    chatGPT_results"
      ],
      "metadata": {
        "id": "JoTVDnu_o67u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Source Models\n",
        "\n",
        "- #### with HuggingFace we will evaluate GPT2/LLama & Alpaca/Bloom models."
      ],
      "metadata": {
        "id": "7M54xL-y9ga8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gpt = 'gpt2'\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(model_gpt)\n",
        "\n",
        "model_bloom = 'bigscience/bloomz-7b1-mt'\n",
        "tokenizer_bloom = AutoTokenizer.from_pretrained(model_bloom)\n",
        "\n",
        "model_llama = 'decapoda-research/llama-7b-hf'\n",
        "tokenizer_llama = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "\n",
        "def get_input_lenghts(dataset, tokenizer):\n",
        "    num_words = [len(x.split()) for x in dataset]\n",
        "    num_words = torch.tensor(num_words, dtype=torch.float)\n",
        "    tokens_lengths = [len(tokenizer.encode(seq)) for seq in dataset]\n",
        "    tokens_lengths = torch.tensor(tokens_lengths, dtype=torch.float)\n",
        "    \n",
        "    return int(torch.ceil(num_words.mean()).item()), tokens_lengths\n",
        "\n",
        "# -------------------------------------------------------------------- #\n",
        "num_words, llama_lengths = get_input_lenghts(three_shot, tokenizer_llama)\n",
        "min_llama = torch.ceil(llama_lengths.min()).item()\n",
        "max_llama = torch.ceil(llama_lengths.max()).item()\n",
        "mean_llama = torch.ceil(llama_lengths.mean()).item()\n",
        "# -------------------------------------------------------------------- #\n",
        "num_words, gpt2_lengths = get_input_lenghts(three_shot, tokenizer_gpt)\n",
        "min_gpt2 = torch.ceil(gpt2_lengths.min()).item()\n",
        "max_gpt2 = torch.ceil(gpt2_lengths.max()).item()\n",
        "mean_gpt2 = torch.ceil(gpt2_lengths.mean()).item()\n",
        "# -------------------------------------------------------------------- #\n",
        "num_words, bloom_lengths = get_input_lenghts(three_shot, tokenizer_bloom)\n",
        "min_bloom = torch.ceil(bloom_lengths.min()).item()\n",
        "max_bloom = torch.ceil(bloom_lengths.max()).item()\n",
        "mean_bloom = torch.ceil(bloom_lengths.mean()).item()\n",
        "# -------------------------------------------------------------------- #\n",
        "print('\\n')\n",
        "print('--'*33)\n",
        "print(f' Prompt 3-shot com média de {num_words} palavras em  ENEM2022 dataset')\n",
        "print('--'*33)\n",
        "print(f'GPT2 tem média de {int(mean_gpt2)} tokens em 3-shot')\n",
        "print(f'\\nBloom tem média de {int(mean_bloom)} tokens em 3-shot')\n",
        "print(f'\\nLLama tem média de {int(mean_llama)} tokens em 3-shot')\n",
        "print(f'\\n---> Bloom gasta {(int(mean_bloom)/int(mean_gpt2))*100:.3}% de tokens comparado com o GPT2 <---')\n",
        "print(f'\\n---> Bloom gasta {(int(mean_bloom)/int(mean_llama))*100:.3}% de tokens comparado com o LLama <---')\n",
        "print(f'\\n---> LLama gasta {(int(mean_llama)/int(mean_gpt2))*100:.3}% de tokens comparado com o GPT2 <---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhioXR6l0oQf",
        "outputId": "83a9996b-fbc7-4be4-c0af-4fdbcf28c826"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2863 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "------------------------------------------------------------------\n",
            " Prompt 3-shot com média de 862 palavras em  ENEM2022 dataset\n",
            "------------------------------------------------------------------\n",
            "GPT2 tem média de 2061 tokens em 3-shot\n",
            "\n",
            "Bloom tem média de 1245 tokens em 3-shot\n",
            "\n",
            "LLama tem média de 1824 tokens em 3-shot\n",
            "\n",
            "---> Bloom gasta 60.4% de tokens comparado com o GPT2 <---\n",
            "\n",
            "---> Bloom gasta 68.3% de tokens comparado com o LLama <---\n",
            "\n",
            "---> LLama gasta 88.5% de tokens comparado com o GPT2 <---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval LLama/Cabrita\n"
      ],
      "metadata": {
        "id": "XwM1oSK2AXqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Cabrita_model_base = LLaMAForCausalLM.from_pretrained(\n",
        "    \"decapoda-research/llama-7b-hf\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "Cabrita_model = PeftModel.from_pretrained(Cabrita_model_base, \"22h/cabrita-lora-v0-1\")"
      ],
      "metadata": {
        "id": "Vbxq3VZ4OH5i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "61eae75ace35467eb8bf8ca0c3eb736c",
            "d3fd4f7e6e2e481e923473b13e14e298",
            "3003e587e9504dbda05095b6f49cd700",
            "9bf9ea1f617f47fd87dea0eddacae010",
            "ab5730f47cb64cadae05c2f26b6b273b",
            "4aa4e93590af4d53b07b811b8d656476",
            "f7277f8c90db4e6db92bf4f24328af52",
            "f9a507a914dd43158192e47dca35a3fd",
            "8e638bd3a8be4288b8a424be1d36af47",
            "2023ac0618574165a8e17bbcc9be313b",
            "ac633bc595a242d59f53cac8ccfbee6a"
          ]
        },
        "outputId": "e15ab907-e51b-453e-dd8b-2b5e32b520e8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61eae75ace35467eb8bf8ca0c3eb736c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pickle_file(path, data=None):\n",
        "    import pickle\n",
        "    if data is None:\n",
        "        with open(path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    if data is not None:\n",
        "        with open(path, 'wb') as handle:\n",
        "            pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#recovering artfacts to eval in the same data points\n",
        "path = \"/content/drive/MyDrive/LLMs/ENEM/\"\n",
        "\n",
        "two_shot = pickle_file(path+'two_shot_list')\n",
        "three_shot = pickle_file(path+'three_shot_list')\n",
        "enem_data = pickle_file(path+'enem_data')"
      ],
      "metadata": {
        "id": "OHDT3kJEkvjZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_OPEN_SOURCE = True\n",
        "\n",
        "if EVAL_OPEN_SOURCE:\n",
        "    deterministic() \n",
        "\n",
        "    trues, preds = [], []\n",
        "    loop = tqdm(two_shot, leave=True)\n",
        "\n",
        "    for ix, batch in enumerate(loop):\n",
        "        # to make sure the same example to predict has the correct label \n",
        "        # find the point where the question to be evaluate starts\n",
        "        point = two_shot[ix].find('Questão 3:\\n')\n",
        "        \n",
        "        # \"11+point\" ---> is the start of the query-text, this is naive af\n",
        "        if enem_data[ix]['query'] == two_shot[ix][11+point:]:\n",
        "            inputs = tokenizer_llama(two_shot[ix], return_tensors=\"pt\")\n",
        "            input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "            outputs = Cabrita_model.generate(\n",
        "                input_ids=input_ids,\n",
        "                generation_config=GenerationConfig(temperature=0, do_sample=False),\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                max_new_tokens=20\n",
        "            )\n",
        "            \n",
        "            preds.append(tokenizer_llama.decode(outputs.sequences[0]))\n",
        "            trues.append(enem_data[ix]['gold'])\n",
        "        else:\n",
        "            # if the query-text is different from inputs to be evaluated \n",
        "            # we print for further investigation\n",
        "            print(point, ix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "3e15bc5aa7a7489fb772301ce556ebaa",
            "0e40a75781b34fa1930fdbb97cb3c6c4",
            "643a62d5bd484cfc9694a601fd6054c4",
            "21e65ebcd0de4ba9af6729c735ad2c4e",
            "c90552aa32534ac1b30b10ec7753291d",
            "d47b10b2e1594346afecd536a9c6c286",
            "92159071d5c943d8800a24de35105a07",
            "2a5fca252ef34c29a9e865673ebbd163",
            "097d40c6ad6f44ed920718a0e58bb7e2",
            "cce47a9643b748d6891ed2ce54752dff",
            "aa5af5adee2a436ab7e07dbee5476ad7"
          ]
        },
        "id": "x07ODzVDYAPI",
        "outputId": "ed7bf48b-60c0-46ab-89ba-25932961a54f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimento deterministico, seed: 2711\n",
            "Existe 1 GPU            Tesla V100-SXM2-16GB disponível.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/118 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e15bc5aa7a7489fb772301ce556ebaa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.DataFrame({'true': trues, 'pred':preds})\n",
        "dataframe['pred'] = dataframe.pred.apply(lambda x: x[-2:-1])\n",
        "\n",
        "map_label = {0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\", 4:\"E\"}\n",
        "dataframe['true'] = dataframe.true.apply(lambda x: map_label[x])\n",
        "    \n",
        "acc = accuracy_score(dataframe['true'], dataframe['pred'])\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf6RjCjo25Bu",
        "outputId": "36985ceb-5e5f-4986-b82b-9ce419011fe5"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06779661016949153"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}