{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/%5BFinal%5D_llamaindex_retrieval_chunk_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIz4Rc-nIlwF"
      },
      "source": [
        "<img alt=\"arize llama-index logos\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/arize_llamaindex.png\" width=\"400\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6NqGz_GIlwI"
      },
      "source": [
        "## LlamaIndex Chunk Size, Retrieval Method and K Eval Suite\n",
        "\n",
        "This colab provides a suite of retrieval performance tests that helps teams understand\n",
        "how to setup the retrieval system. It makes use of the Phoenix Eval options for\n",
        "Q&A (overall did it answer the question) and retrieval (did the right chunks get returned).\n",
        "\n",
        "There is a sweep of parameters that is stored in experiment_data/results_no_zero_remove,\n",
        "check that directory for results.\n",
        "\n",
        "The goal is to help teams choose a Chunk size, retireval method, K for return chunks\n",
        "\n",
        "This colab downloads the script (py) files. Those files can be run without this colab directly,\n",
        "in a code only environment (VS code for example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3eVRDzjIlwJ"
      },
      "source": [
        "### Retrieval Eval\n",
        "\n",
        "This Eval evaluates whether a retrieved chunk contains an answer to the query. Its extremely useful for evaluating retrieval systems.\n",
        "\n",
        "https://docs.arize.com/phoenix/concepts/llm-evals/retrieval-rag-relevance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8FXkgUZIlwJ"
      },
      "source": [
        "### Q&A EVal\n",
        "This Eval evaluates whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\n",
        "\n",
        "https://docs.arize.com/phoenix/concepts/llm-evals/q-and-a-on-retrieved-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfqDtAwCIlwJ"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/chunking.png\" width=800/>\n",
        "    </p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPDBwFn3IlwK"
      },
      "source": [
        "The challenge in setting up a retrieval system is having solid performance metrics that allow you to evaluate your different strategies:\n",
        "- Chunk Size\n",
        "- Retrieval Method\n",
        "- K value\n",
        "\n",
        "In setting the above variables you first need some overall Eval metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPPSoHjpIlwK"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/eval_relevance.png\" width=800/>\n",
        "    </p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZDuiEprIlwK"
      },
      "source": [
        "The above is the relevance Evaluation used to check on whether the chunk retrieved is relevant to the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JaJbBvdIlwL"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/EvalQ_A.png\" width=600/>\n",
        "    </p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pne6sDEcIlwL"
      },
      "source": [
        "The above Eval shows a Q&A Eval on the entire system Q&A /\n",
        "on the overall question and answer.\n",
        "Each is used as we sweep through parameters to detremine effectiveness of retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AIVFHNGIlwL"
      },
      "source": [
        "## Sweeping values\n",
        "The scripts sweep through K, Retrival approach and chunk size, determining the trade off on your own docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7_I53C3IlwM"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/sweep_k.png\" width=800/>\n",
        "    </p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDrlarAuIlwM"
      },
      "source": [
        "The above shows sweeping through K=4 and K=6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsxNaLS3IlwM"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/sweep_chunk.png\" width=800/>\n",
        "    </p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naGTI-lvIlwM"
      },
      "source": [
        "The above shows sweeping through Chunk Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmd234g-IlwM"
      },
      "outputs": [],
      "source": [
        "#The script below runs a test on the question set, by default we have a 170 Question set\n",
        "#That takes some time to run so you can default it lower just to test\n",
        "#Comment this out to run on full dataset\n",
        "QUESTION_SAMPLES = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "### PARAMETER SWEEPS BELOW ###\n",
        "##############################\n",
        "###chunk_sizes### to test, will sweep through values of chunk size\n",
        "chunk_sizes = [\n",
        "    100,\n",
        "    # 300,\n",
        "    # 500,\n",
        "    # 1000,\n",
        "    # 2000,\n",
        "]  # change this, perhaps experiment from 500 to 3000 in increments of 500\n",
        "\n",
        "### K ###: Sizes to test, will sweep through values of k\n",
        "k = [4, 6, 10]\n",
        "# k = [10]  # num documents to retrieve\n",
        "\n",
        "### Retrieval Approach ###: transformation to test will sweep through retrieval\n",
        "# transformations = [\"original\", \"original_rerank\",\"hyde\", \"hyde_rerank\"]\n",
        "transformations = [\"original\", \"original_rerank\"]\n",
        "#Model for Q&A\n",
        "llama_index_model = \"gpt-4\"\n",
        "# llama_index_model = \"gpt-3.5-turbo\""
      ],
      "metadata": {
        "id": "78_XDC6oaSsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8LbueAFIlwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f7b68a9-dcb5-4f91-c3e1-8e4432cbe87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.8/850.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.0/269.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.3/484.3 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q cohere matplotlib lxml openai arize-phoenix llama_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc_BSA4IIlwP"
      },
      "source": [
        "# Retrieval Eval Scripts\n",
        "The following scripts can be run directly. In the case of long test suites, we recommend running\n",
        "the python script llama_index_w_evals_and_qa directly.py directly in python. All parameters are available\n",
        "in that script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEMB2pwRIlwP"
      },
      "outputs": [],
      "source": [
        "# # Donwload scripts\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/llama_index_w_evals_and_qa.py\"\n",
        "response = requests.get(url)\n",
        "with open(\"llama_index_w_evals_and_qa.py\", \"w\") as file:\n",
        "     file.write(response.text)\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/plotresults.py\"\n",
        "response = requests.get(url)\n",
        "with open(\"plotresults.py\", \"w\") as file:\n",
        "     file.write(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjx-pOT1IlwP"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import cohere\n",
        "import openai\n",
        "import phoenix.experimental.evals.templates.default_templates as templates\n",
        "from phoenix.experimental.evals import OpenAIModel\n",
        "from llama_index import download_loader\n",
        "from llama_index_w_evals_and_qa import get_urls, plot_graphs, run_experiments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\"\n",
        "openai.api_key = \"\"\n",
        "os.environ[\"COHERE_API_KEY\"] = \"\"\n",
        "cohere.api_key = os.getenv(\"COHERE_API_KEY\")"
      ],
      "metadata": {
        "id": "rCiPCqGhrpi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6EUE2-wIlwP"
      },
      "source": [
        "# Phoenix Observabiility\n",
        "Click link below to visualize llamaIndex queries and chunking as its happening!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "LR8iF77CIlwP",
        "outputId": "3ebcd177-0133-4496-b20f-610705115076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌍 To view the Phoenix app in your browser, visit https://5stixt52j21-496ff2e9c6d22116-6060-colab.googleusercontent.com/\n",
            "📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
            "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
          ]
        }
      ],
      "source": [
        "#########################################\n",
        "### CLICK LINK BELOW FOR PHOENIX VIZ ####\n",
        "#########################################\n",
        "\n",
        "# Phoenix can display in real time the traces automatically\n",
        "# collected from your LlamaIndex application.\n",
        "import phoenix as px\n",
        "# Look for a URL in the output to open the App in a browser.\n",
        "px.launch_app()\n",
        "# The App is initially empty, but as you proceed with the steps below,\n",
        "# traces will appear automatically as your LlamaIndex application runs.\n",
        "\n",
        "import llama_index\n",
        "llama_index.set_global_handler(\"arize_phoenix\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olw5Zm-IIlwQ"
      },
      "outputs": [],
      "source": [
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "cohere.api_key = os.getenv(\"COHERE_API_KEY\")\n",
        "\n",
        "# if loading from scratch, change these below\n",
        "web_title = \"arize\"  # nickname for this website, used for saving purposes\n",
        "base_url = \"https://docs.arize.com/arize\"\n",
        "# Local files\n",
        "file_name = \"raw_documents.pkl\"\n",
        "save_base = \"./experiment_data/\"\n",
        "if not os.path.exists(save_base):\n",
        "    os.makedirs(save_base)\n",
        "\n",
        "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "save_dir = os.path.join(save_base, run_name)\n",
        "if not os.path.exists(save_dir):\n",
        "    # Create a new directory because it does not exist\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "\n",
        "questions = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/constants.csv\",\n",
        "    header=None,\n",
        ")[0].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTXpIRefIlwQ"
      },
      "outputs": [],
      "source": [
        "#This will determine run time, how many questions to pull from the data to run\n",
        "selected_questions = questions[:QUESTION_SAMPLES] if QUESTION_SAMPLES else questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P0Pudd0IlwQ",
        "outputId": "12297220-0b53-43a0-8713-f38a10f55219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: urllib3 1.26.16\n",
            "Uninstalling urllib3-1.26.16:\n",
            "  Successfully uninstalled urllib3-1.26.16\n",
            "Collecting urllib3==2.0.4\n",
            "  Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-index 0.8.37 requires urllib3<2, but you have urllib3 2.0.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed urllib3-2.0.4\n"
          ]
        }
      ],
      "source": [
        "#!pip install --force-reinstall urllib3\n",
        "!pip uninstall urllib3 -y\n",
        "!pip install urllib3==2.0.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC4o5hRwIlwQ",
        "outputId": "e7bd9be2-0e45-4afa-d2cf-806d6934b970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'./experiment_data/raw_documents.pkl' does not exists.\n",
            "LOADED 171 URLS\n",
            "GRABBING DOCUMENTS\n",
            "LOADING DOCUMENTS FROM URLS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:phoenix.experimental.evals.models.openai:gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documents saved to raw_documents.pkl\n"
          ]
        }
      ],
      "source": [
        "raw_docs_filepath = os.path.join(save_base, file_name)\n",
        "if not os.path.exists(raw_docs_filepath):\n",
        "    print(f\"'{raw_docs_filepath}' does not exists.\")\n",
        "    urls = get_urls(base_url)  # you need to - pip install lxml\n",
        "    print(f\"LOADED {len(urls)} URLS\")\n",
        "\n",
        "print(\"GRABBING DOCUMENTS\")\n",
        "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
        "# two options here, either get the documents from scratch or load one from disk\n",
        "if not os.path.exists(raw_docs_filepath):\n",
        "    print(\"LOADING DOCUMENTS FROM URLS\")\n",
        "    # You need to 'pip install lxml'\n",
        "    loader = BeautifulSoupWebReader()\n",
        "    documents = loader.load_data(urls=urls)  # may take some time\n",
        "    with open(save_base + file_name, \"wb\") as file:\n",
        "        pickle.dump(documents, file)\n",
        "    print(\"Documents saved to raw_documents.pkl\")\n",
        "else:\n",
        "    print(\"LOADING DOCUMENTS FROM FILE\")\n",
        "    print(\"Opening raw_documents.pkl\")\n",
        "    with open(save_base + file_name, \"rb\") as file:\n",
        "        documents = pickle.load(file)\n",
        "\n",
        "#Model for Evals\n",
        "eval_model = OpenAIModel(model_name=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "qa_template = templates.QA_PROMPT_TEMPLATE_STR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1f9bb7494798488ca6a72a8b6445a307",
            "fc15fd80478447119cf6f1a367d64fa5",
            "4f3909720c154596b121dbfd861446cf",
            "e1af93670fe046908b15808481380045",
            "37844ac730ed426e98635be0c7405b92",
            "f4aa920e30864230ae63a8fac85eb3ae",
            "ce7d0e9776c04446b929ef0a766dc632",
            "1ea540d8a38e423193a4619a84cad29a",
            "fae29021ee8b420faae5dbfb0a9ab9b3",
            "9217a792bb254e5193030a767225ae04",
            "8e224082051644fa99fc4619aaeacbbf"
          ]
        },
        "id": "RYeoUdfGIlwQ",
        "outputId": "11367ff2-b098-467b-cea2-83b7509a7d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/4149 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f9bb7494798488ca6a72a8b6445a307"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "Trace: query\n",
            "    |_query ->  3.442385 seconds\n",
            "      |_retrieve ->  1.241585 seconds\n",
            "      |_synthesize ->  2.200355 seconds\n",
            "        |_templating ->  7.5e-05 seconds\n",
            "        |_llm ->  2.189853 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  3.481681 seconds\n",
            "      |_retrieve ->  1.104158 seconds\n",
            "      |_synthesize ->  2.376781 seconds\n",
            "        |_templating ->  7.9e-05 seconds\n",
            "        |_llm ->  2.371367 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  2.372142 seconds\n",
            "      |_retrieve ->  0.615845 seconds\n",
            "      |_synthesize ->  1.755029 seconds\n",
            "        |_templating ->  4.2e-05 seconds\n",
            "        |_llm ->  1.750945 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  5.302048 seconds\n",
            "      |_retrieve ->  1.117908 seconds\n",
            "      |_synthesize ->  4.174141 seconds\n",
            "        |_templating ->  5.6e-05 seconds\n",
            "        |_llm ->  4.166784 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  10.282434 seconds\n",
            "      |_retrieve ->  1.4083 seconds\n",
            "      |_synthesize ->  8.873715 seconds\n",
            "        |_templating ->  6.9e-05 seconds\n",
            "        |_llm ->  8.867274 seconds\n",
            "**********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eta:2023-10-02 01:31:05.076869 |█████████████████████████████| 100.0% (5/5) [00:02<00:00,  1.68it/s]\n",
            "Eta:2023-10-02 01:31:20.439972 |███████████████████████████| 100.0% (20/20) [00:15<00:00,  1.30it/s]\n",
            "Eta:2023-10-02 01:31:23.811769 |█████████████████████████████| 100.0% (5/5) [00:03<00:00,  1.50it/s]\n",
            "Eta:2023-10-02 01:31:36.354929 |███████████████████████████| 100.0% (20/20) [00:12<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "Trace: query\n",
            "    |_query ->  2.187333 seconds\n",
            "      |_retrieve ->  0.656751 seconds\n",
            "      |_synthesize ->  1.530191 seconds\n",
            "        |_templating ->  4.4e-05 seconds\n",
            "        |_llm ->  1.525733 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  3.02726 seconds\n",
            "      |_retrieve ->  1.115723 seconds\n",
            "      |_synthesize ->  1.910754 seconds\n",
            "        |_templating ->  8.1e-05 seconds\n",
            "        |_llm ->  1.90358 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  2.159267 seconds\n",
            "      |_retrieve ->  0.836572 seconds\n",
            "      |_synthesize ->  1.322257 seconds\n",
            "        |_templating ->  6.4e-05 seconds\n",
            "        |_llm ->  1.317827 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  4.700037 seconds\n",
            "      |_retrieve ->  0.716722 seconds\n",
            "      |_synthesize ->  3.982898 seconds\n",
            "        |_templating ->  4.1e-05 seconds\n",
            "        |_llm ->  3.978504 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  2.374605 seconds\n",
            "      |_retrieve ->  0.677691 seconds\n",
            "      |_synthesize ->  1.696512 seconds\n",
            "        |_templating ->  5e-05 seconds\n",
            "        |_llm ->  1.691746 seconds\n",
            "**********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eta:2023-10-02 01:33:03.774008 |█████████████████████████████| 100.0% (5/5) [00:02<00:00,  1.69it/s]\n",
            "Eta:2023-10-02 01:33:22.261392 |███████████████████████████| 100.0% (30/30) [00:18<00:00,  1.62it/s]\n",
            "Eta:2023-10-02 01:33:25.207443 |█████████████████████████████| 100.0% (5/5) [00:02<00:00,  1.74it/s]\n",
            "Eta:2023-10-02 01:33:43.742572 |███████████████████████████| 100.0% (30/30) [00:18<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "Trace: query\n",
            "    |_query ->  1.95206 seconds\n",
            "      |_retrieve ->  0.680254 seconds\n",
            "      |_synthesize ->  1.271388 seconds\n",
            "        |_templating ->  6e-05 seconds\n",
            "        |_llm ->  1.265284 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  3.607624 seconds\n",
            "      |_retrieve ->  0.889689 seconds\n",
            "      |_synthesize ->  2.717485 seconds\n",
            "        |_templating ->  6.1e-05 seconds\n",
            "        |_llm ->  2.70872 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  2.516423 seconds\n",
            "      |_retrieve ->  1.102323 seconds\n",
            "      |_synthesize ->  1.413623 seconds\n",
            "        |_templating ->  7.9e-05 seconds\n",
            "        |_llm ->  1.403792 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  3.929991 seconds\n",
            "      |_retrieve ->  0.663254 seconds\n",
            "      |_synthesize ->  3.266321 seconds\n",
            "        |_templating ->  6.1e-05 seconds\n",
            "        |_llm ->  3.260305 seconds\n",
            "**********\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  8.987166 seconds\n",
            "      |_retrieve ->  0.684674 seconds\n",
            "      |_synthesize ->  8.302046 seconds\n",
            "        |_templating ->  4.6e-05 seconds\n",
            "        |_llm ->  8.294321 seconds\n",
            "**********\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Eta:2023-10-02 01:36:12.060839 |█████████████████████████████| 100.0% (5/5) [00:03<00:00,  1.48it/s]\n",
            "Eta:2023-10-02 01:36:43.282921 |███████████████████████████| 100.0% (50/50) [00:31<00:00,  1.60it/s]\n",
            "Eta:2023-10-02 01:36:46.788812 |█████████████████████████████| 100.0% (5/5) [00:03<00:00,  1.45it/s]\n",
            "Eta:2023-10-02 01:37:16.174384 |███████████████████████████| 100.0% (50/50) [00:29<00:00,  1.70it/s]\n"
          ]
        }
      ],
      "source": [
        "# Uncomment when testing, 3 questions are easy to run through quickly\n",
        "# questions = questions[0:3]\n",
        "all_data = run_experiments(\n",
        "    documents=documents,\n",
        "    queries=selected_questions,\n",
        "    chunk_sizes=chunk_sizes,\n",
        "    query_transformations=transformations,\n",
        "    k_values=k,\n",
        "    web_title=web_title,\n",
        "    save_dir=save_dir,\n",
        "    llama_index_model=llama_index_model,\n",
        "    eval_model=eval_model,\n",
        "    template=qa_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTVg7-ZHIlwR"
      },
      "outputs": [],
      "source": [
        "all_data_filepath = os.path.join(save_dir, f\"{web_title}_all_data.pkl\")\n",
        "with open(all_data_filepath, \"wb\") as f:\n",
        "    pickle.dump(all_data, f)\n",
        "\n",
        "# The retrievals with 0 relevant context really can't be optimized, removing gives a diff view\n",
        "plot_graphs(\n",
        "    all_data=all_data,\n",
        "    save_dir=os.path.join(save_dir, \"results_zero_removed\"),\n",
        "    show=False,\n",
        "    remove_zero=True,\n",
        ")\n",
        "plot_graphs(\n",
        "    all_data=all_data,\n",
        "    save_dir=os.path.join(save_dir, \"results_zero_not_removed\"),\n",
        "    show=False,\n",
        "    remove_zero=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3HLZtUIIlwR"
      },
      "source": [
        "## Example Results Q&A Evals (actual results in experiment_data)\n",
        "\n",
        "The **Q&A Eval** runs at the highest level of did you get the question answer correct  based on the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbn2z5BzIlwR"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/percentage_incorrect_plot.png\" />\n",
        "    </p>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfkq8YTxIlwR"
      },
      "source": [
        "## Example Results Retrieval Eval  (actual results in experiment_data)\n",
        "\n",
        "**Retrieval Eval:** Precision @ K The retrieval analysis example is below, iterates through the chunk sizes, K (4/6/10), retrieval method\n",
        "The eval checks whether the retrieved chunk is relevant and has a chance to answer the question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_as7h-u1IlwR"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/all_mean_precisions.png\" />\n",
        "    </p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cueAK6hIlwR"
      },
      "source": [
        "## Example Results Latency  (actual results in experiment_data)\n",
        "\n",
        "The latency can highly varied based on retrieval approaches, below are latency maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXKNjBjzIlwR"
      },
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/median_latency_all.png\" />\n",
        "    </p>\n",
        "</center>\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f9bb7494798488ca6a72a8b6445a307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc15fd80478447119cf6f1a367d64fa5",
              "IPY_MODEL_4f3909720c154596b121dbfd861446cf",
              "IPY_MODEL_e1af93670fe046908b15808481380045"
            ],
            "layout": "IPY_MODEL_37844ac730ed426e98635be0c7405b92"
          }
        },
        "fc15fd80478447119cf6f1a367d64fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4aa920e30864230ae63a8fac85eb3ae",
            "placeholder": "​",
            "style": "IPY_MODEL_ce7d0e9776c04446b929ef0a766dc632",
            "value": "Generating embeddings: 100%"
          }
        },
        "4f3909720c154596b121dbfd861446cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ea540d8a38e423193a4619a84cad29a",
            "max": 4149,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fae29021ee8b420faae5dbfb0a9ab9b3",
            "value": 4149
          }
        },
        "e1af93670fe046908b15808481380045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9217a792bb254e5193030a767225ae04",
            "placeholder": "​",
            "style": "IPY_MODEL_8e224082051644fa99fc4619aaeacbbf",
            "value": " 4149/4149 [01:39&lt;00:00, 41.50it/s]"
          }
        },
        "37844ac730ed426e98635be0c7405b92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4aa920e30864230ae63a8fac85eb3ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce7d0e9776c04446b929ef0a766dc632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ea540d8a38e423193a4619a84cad29a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fae29021ee8b420faae5dbfb0a9ab9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9217a792bb254e5193030a767225ae04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e224082051644fa99fc4619aaeacbbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}