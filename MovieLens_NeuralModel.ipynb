{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MovieLens - NeuralModel.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1hue4Yth075HjbJV_wIpgeP0twQE5Oepa",
      "authorship_tag": "ABX9TyPUGcL0YyBEyYLdW2gir0GL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/MovieLens_NeuralModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrBOrdTs5wof"
      },
      "source": [
        "From here: https://github.com/HarshdeepGupta/recommender_pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LUrXwT-2hj-"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QmWpCmu2mmJ"
      },
      "source": [
        "# Description: A file for implementing the Dataset interface of PyTorch\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "np.random.seed(7)\n",
        "\n",
        "\n",
        "class MovieLensDataset(Dataset):\n",
        "    'Characterizes the dataset for PyTorch, and feeds the (user,item) pairs for training'\n",
        "\n",
        "    def __init__(self, file_name, num_negatives_train=5, num_negatives_test=100):\n",
        "        'Load the datasets from disk, and store them in appropriate structures'\n",
        "\n",
        "        self.trainMatrix = self.load_rating_file_as_matrix(\n",
        "            file_name + \".train.rating\")\n",
        "        self.num_users, self.num_items = self.trainMatrix.shape\n",
        "        # make training set with negative sampling\n",
        "        self.user_input, self.item_input, self.ratings = self.get_train_instances(\n",
        "            self.trainMatrix, num_negatives_train)\n",
        "        # make testing set with negative sampling\n",
        "        self.testRatings = self.load_rating_file_as_list(\n",
        "            file_name + \".test.rating\")\n",
        "        self.testNegatives = self.create_negative_file(\n",
        "            num_samples=num_negatives_test)\n",
        "        assert len(self.testRatings) == len(self.testNegatives)\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of rating in test set'\n",
        "        return len(self.user_input)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "\n",
        "        # get the train data\n",
        "        user_id = self.user_input[index]\n",
        "        item_id = self.item_input[index]\n",
        "        rating = self.ratings[index]\n",
        "\n",
        "        return {'user_id': user_id,\n",
        "                'item_id': item_id,\n",
        "                'rating': rating}\n",
        "\n",
        "    def get_train_instances(self, train, num_negatives):\n",
        "        user_input, item_input, ratings = [], [], []\n",
        "        num_users, num_items = train.shape\n",
        "        for (u, i) in train.keys():\n",
        "            # positive instance\n",
        "            user_input.append(u)\n",
        "            item_input.append(i)\n",
        "            ratings.append(1)\n",
        "            # negative instances\n",
        "            for _ in range(num_negatives):\n",
        "                j = np.random.randint(1, num_items)\n",
        "                # while train.has_key((u, j)):\n",
        "                while (u, j) in train:\n",
        "                    j = np.random.randint(1, num_items)\n",
        "                user_input.append(u)\n",
        "                item_input.append(j)\n",
        "                ratings.append(0)\n",
        "        return user_input, item_input, ratings\n",
        "\n",
        "    def load_rating_file_as_list(self, filename):\n",
        "        ratingList = []\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item = int(arr[0]), int(arr[1])\n",
        "                ratingList.append([user, item])\n",
        "                line = f.readline()\n",
        "        return ratingList\n",
        "\n",
        "    def create_negative_file(self, num_samples=100):\n",
        "        negativeList = []\n",
        "        for user_item_pair in self.testRatings:\n",
        "            user = user_item_pair[0]\n",
        "            item = user_item_pair[1]\n",
        "            negatives = []\n",
        "            for t in range(num_samples):\n",
        "                j = np.random.randint(1, self.num_items)\n",
        "                while (user, j) in self.trainMatrix or j == item:\n",
        "                    j = np.random.randint(1, self.num_items)\n",
        "                negatives.append(j)\n",
        "            negativeList.append(negatives)\n",
        "        return negativeList\n",
        "\n",
        "    def load_rating_file_as_matrix(self, filename):\n",
        "        '''\n",
        "        Read .rating file and Return dok matrix.\n",
        "        The first line of .rating file is: num_users\\t num_items\n",
        "        '''\n",
        "        # Get number of users and items\n",
        "        num_users, num_items = 0, 0\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                u, i = int(arr[0]), int(arr[1])\n",
        "                num_users = max(num_users, u)\n",
        "                num_items = max(num_items, i)\n",
        "                line = f.readline()\n",
        "        # Construct matrix\n",
        "        mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
        "        with open(filename, \"r\") as f:\n",
        "            line = f.readline()\n",
        "            while line != None and line != \"\":\n",
        "                arr = line.split(\"\\t\")\n",
        "                user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "                if (rating > 0):\n",
        "                    mat[user, item] = 1.0\n",
        "                line = f.readline()\n",
        "        return mat"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uMYzfAD3Ro0"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSS1aiqH3Q9s"
      },
      "source": [
        "# Description: A file for providing utility functions\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "import torch\n",
        "\n",
        "# workspace imports\n",
        "# from evaluate import evaluate_model\n",
        "# from Dataset import MovieLensDataset\n",
        "\n",
        "def train_one_epoch(model, data_loader, loss_fn, optimizer, epoch_no, device, verbose = 1):\n",
        "    'trains the model for one epoch and returns the loss'\n",
        "    print(\"Epoch = {}\".format(epoch_no))\n",
        "    # Training\n",
        "    # get user, item and rating data\n",
        "    t1 = time()\n",
        "    epoch_loss = []\n",
        "    # put the model in train mode before training\n",
        "    model.train()\n",
        "    # transfer the data to GPU\n",
        "    for feed_dict in data_loader:\n",
        "        for key in feed_dict:\n",
        "            if type(feed_dict[key]) != type(None):\n",
        "                feed_dict[key] = feed_dict[key].to(dtype = torch.long, device = device)\n",
        "        # get the predictions\n",
        "        prediction = model(feed_dict)\n",
        "        # print(prediction.shape)\n",
        "        # get the actual targets\n",
        "        rating = feed_dict['rating']\n",
        "        \n",
        "      \n",
        "        # convert to float and change dim from [batch_size] to [batch_size,1]\n",
        "        rating = rating.float().view(prediction.size())  \n",
        "        loss = loss_fn(prediction, rating)\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # backpropagate\n",
        "        loss.backward()\n",
        "        # update weights\n",
        "        optimizer.step()\n",
        "        # accumulate the loss for monitoring\n",
        "        epoch_loss.append(loss.item())\n",
        "    epoch_loss = np.mean(epoch_loss)\n",
        "    if verbose:\n",
        "        print(\"Epoch completed {:.1f} s\".format(time() - t1))\n",
        "        print(\"Train Loss: {}\".format(epoch_loss))\n",
        "    return epoch_loss\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "def test(model, full_dataset : MovieLensDataset, topK):\n",
        "    'Test the HR and NDCG for the model @topK'\n",
        "    # put the model in eval mode before testing\n",
        "    if hasattr(model,'eval'):\n",
        "        # print(\"Putting the model in eval mode\")\n",
        "        model.eval()\n",
        "    t1 = time()\n",
        "    (hits, ndcgs) = evaluate_model(model, full_dataset, topK)\n",
        "    hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
        "    print('Eval: HR = %.4f, NDCG = %.4f [%.1f s]' % (hr, ndcg, time()-t1))\n",
        "    return hr, ndcg\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def plot_statistics(hr_list, ndcg_list, loss_list, model_alias, path):\n",
        "    'plots and saves the figures to a local directory'\n",
        "    plt.figure()\n",
        "    hr = np.array(hr_list)\n",
        "    ndcg = np.array(ndcg_list)\n",
        "    loss = np.array(loss_list)\n",
        "    plt.plot(hr[:,0], hr[:,1],linestyle='-', marker='o', label = \"HR\")\n",
        "    plt.plot(ndcg[:,0], ndcg[:,1],linestyle='-', marker='v', label = \"NDCG\")\n",
        "    plt.plot(loss[:,0], loss[:,1],linestyle='-', marker='s', label = \"Loss\")\n",
        "\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.savefig(path+model_alias+\".jpg\")\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_items_interacted(user_id, interaction_df):\n",
        "    # returns a set of items the user has interacted with\n",
        "    userid_mask = interaction_df['userid'] == user_id\n",
        "    interacted_items = interaction_df.loc[userid_mask].courseid\n",
        "    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])\n",
        "\n",
        "\n",
        "\n",
        "def save_to_csv(df,path, header = False, index = False, sep = '\\t', verbose = False):\n",
        "    if verbose:\n",
        "        print(\"Saving df to path: {}\".format(path))\n",
        "        print(\"Columns in df are: {}\".format(df.columns.tolist()))\n",
        "\n",
        "    df.to_csv(path, header = header, index = index, sep = sep)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pxkvcgt3Ba5"
      },
      "source": [
        "# Train / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCuh74e93aw7",
        "outputId": "9bac6fa2-5e6d-4c37-dce2-047574c5a8ec"
      },
      "source": [
        "!unzip /content/drive/MyDrive/Colab\\ Notebooks/RecSys/movielens_datasets/ml100k/ml-100k.zip\n",
        "!mkdir Data\n",
        "\n",
        "INPUT_PATH = 'ml-100k/u.data'\n",
        "\n",
        "OUTPUT_PATH_TRAIN = 'Data/movielens.train.rating'\n",
        "OUTPUT_PATH_TEST = 'Data/movielens.test.rating'\n",
        "USER_FIELD = 'userID'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks/RecSys/movielens_datasets/ml100k/ml-100k.zip\n",
            "   creating: ml-100k/\n",
            "  inflating: ml-100k/allbut.pl       \n",
            "  inflating: ml-100k/mku.sh          \n",
            "  inflating: ml-100k/README          \n",
            "  inflating: ml-100k/u.data          \n",
            "  inflating: ml-100k/u.genre         \n",
            "  inflating: ml-100k/u.info          \n",
            "  inflating: ml-100k/u.item          \n",
            "  inflating: ml-100k/u.occupation    \n",
            "  inflating: ml-100k/u.user          \n",
            "  inflating: ml-100k/u1.base         \n",
            "  inflating: ml-100k/u1.test         \n",
            "  inflating: ml-100k/u2.base         \n",
            "  inflating: ml-100k/u2.test         \n",
            "  inflating: ml-100k/u3.base         \n",
            "  inflating: ml-100k/u3.test         \n",
            "  inflating: ml-100k/u4.base         \n",
            "  inflating: ml-100k/u4.test         \n",
            "  inflating: ml-100k/u5.base         \n",
            "  inflating: ml-100k/u5.test         \n",
            "  inflating: ml-100k/ua.base         \n",
            "  inflating: ml-100k/ua.test         \n",
            "  inflating: ml-100k/ub.base         \n",
            "  inflating: ml-100k/ub.test         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy01aIIy3BOv",
        "outputId": "aa2102bb-4eb7-4b7a-9bca-f3e43af27e26"
      },
      "source": [
        "# Description: Splits the data into train and test using the leave the latest one out strategy \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# from utils import save_to_csv\n",
        "\n",
        "\n",
        "def get_train_test_df(transactions):\n",
        "    '''\n",
        "    return train and test dataframe, with leave the latest one out strategy\n",
        "\n",
        "    Args:\n",
        "        transactions: the entire df of user/item transactions\n",
        "    '''\n",
        "\n",
        "    print(\"Size of the entire dataset:{}\".format(transactions.shape))\n",
        "    transactions.sort_values(by = ['timestamp'], inplace = True)\n",
        "    last_transaction_mask = transactions.duplicated(subset = {USER_FIELD}, keep = \"last\")\n",
        "    # The last transaction mask has all the latest items of people\n",
        "    # We want for the test dataset, items marked with a False\n",
        "    train_df = transactions[last_transaction_mask]\n",
        "    test_df = transactions[~last_transaction_mask]\n",
        "    \n",
        "    train_df.sort_values(by=[\"userID\", 'timestamp'], inplace = True)\n",
        "    test_df.sort_values(by=[\"userID\", 'timestamp'], inplace = True)\n",
        "    return train_df, test_df\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "def report_stats(transactions, train_df, test_df):\n",
        "    whole_size = transactions.shape[0]*1.0\n",
        "    train_size = train_df.shape[0]\n",
        "    test_size = test_df.shape[0]\n",
        "    print(\"Total No. of Records = {}\".format(whole_size))\n",
        "    print(\"Train size = {}, Test size = {}\".format(train_size, test_size))\n",
        "    print(\"Train % = {}, Test % ={}\".format(train_size/whole_size, test_size/whole_size))\n",
        "\n",
        "\n",
        "def execute():\n",
        "\n",
        "    transactions = pd.read_csv(INPUT_PATH, sep=\"\\t\", names = ['userID', 'movieID', 'rating', 'timestamp'], engine = 'python')\n",
        "    # print(transactions.head())\n",
        "\n",
        "    # convert to implicit scenario\n",
        "    transactions['rating'] = 1\n",
        "    \n",
        "    # make the dataset\n",
        "    train_df, test_df = get_train_test_df(transactions)\n",
        "    save_to_csv(train_df, OUTPUT_PATH_TRAIN, header = False,index = False, verbose = 1)\n",
        "    save_to_csv(test_df, OUTPUT_PATH_TEST,header = False,index = False, verbose = 1)\n",
        "    report_stats(transactions, train_df, test_df)\n",
        "    return 0\n",
        "\n",
        "execute()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the entire dataset:(100000, 4)\n",
            "Saving df to path: Data/movielens.train.rating\n",
            "Columns in df are: ['userID', 'movieID', 'rating', 'timestamp']\n",
            "Saving df to path: Data/movielens.test.rating\n",
            "Columns in df are: ['userID', 'movieID', 'rating', 'timestamp']\n",
            "Total No. of Records = 100000.0\n",
            "Train size = 99057, Test size = 943\n",
            "Train % = 0.99057, Test % =0.00943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6zYeB8n2dlB"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si7bwTDC2diY"
      },
      "source": [
        "'''\n",
        "Evaluate the performance of Top-K recommendation:\n",
        "    Protocol: leave-1-out evaluation\n",
        "    Measures: Hit Ratio and NDCG\n",
        "    (more details are in: Xiangnan He, et al. Fast Matrix Factorization for \n",
        "    Online Recommendation with Implicit Feedback. SIGIR'16)\n",
        "\n",
        "'''\n",
        "import math\n",
        "import heapq  # for retrieval topK\n",
        "import numpy as np\n",
        "# from numba import jit, autojit\n",
        "# from Dataset import MovieLensDataset\n",
        "\n",
        "# Global variables that are shared across processes\n",
        "_model = None\n",
        "_testRatings = None\n",
        "_testNegatives = None\n",
        "_topk = None\n",
        "\n",
        "\n",
        "def evaluate_model(model, full_dataset: MovieLensDataset, topK: int):\n",
        "    \"\"\"\n",
        "    Evaluate the performance (Hit_Ratio, NDCG) of top-K recommendation\n",
        "    Return: score of each test rating.\n",
        "    \"\"\"\n",
        "    global _model\n",
        "    global _testRatings\n",
        "    global _testNegatives\n",
        "    global _topk\n",
        "    _model = model\n",
        "    _testRatings = full_dataset.testRatings\n",
        "    _testNegatives = full_dataset.testNegatives\n",
        "    _topk = topK\n",
        "\n",
        "    hits, ndcgs = [], []\n",
        "    for idx in range(len(_testRatings)):\n",
        "        (hr, ndcg) = eval_one_rating(idx, full_dataset)\n",
        "        hits.append(hr)\n",
        "        ndcgs.append(ndcg)\n",
        "    return (hits, ndcgs)\n",
        "\n",
        "\n",
        "def eval_one_rating(idx, full_dataset: MovieLensDataset):\n",
        "    rating = _testRatings[idx]\n",
        "    items = _testNegatives[idx]\n",
        "    u = rating[0]\n",
        "\n",
        "    gtItem = rating[1]\n",
        "    items.append(gtItem)\n",
        "    # Get prediction scores\n",
        "    map_item_score = {}\n",
        "    users = np.full(len(items), u, dtype='int32')\n",
        "\n",
        "    feed_dict = {\n",
        "        'user_id': users,\n",
        "        'item_id': np.array(items),\n",
        "    }\n",
        "    predictions = _model.predict(feed_dict)\n",
        "    for i in range(len(items)):\n",
        "        item = items[i]\n",
        "        map_item_score[item] = predictions[i]\n",
        "\n",
        "    # Evaluate top rank list\n",
        "    ranklist = heapq.nlargest(_topk, map_item_score, key=map_item_score.get)\n",
        "    hr = getHitRatio(ranklist, gtItem)\n",
        "    ndcg = getNDCG(ranklist, gtItem)\n",
        "    return (hr, ndcg)\n",
        "\n",
        "\n",
        "def getHitRatio(ranklist, gtItem):\n",
        "    for item in ranklist:\n",
        "        if item == gtItem:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def getNDCG(ranklist, gtItem):\n",
        "    for i in range(len(ranklist)):\n",
        "        item = ranklist[i]\n",
        "        if item == gtItem:\n",
        "            return math.log(2) / math.log(i+2)\n",
        "    return 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1XQtmT32dgM"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbE1EjiX2ddy",
        "outputId": "6494d1cc-2b6e-48bf-e103-032ea42455bb"
      },
      "source": [
        "# Description: A PyTorch implementation of Xiangnan's work , implementing the MLP model\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Workspace imports\n",
        "# from evaluate import evaluate_model\n",
        "# from Dataset import MovieLensDataset\n",
        "# from utils import train_one_epoch, test, plot_statistics\n",
        "\n",
        "# Python imports\n",
        "import argparse\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "# cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Run MLP.\")\n",
        "    parser.add_argument('--path', nargs='?', default='Data/',\n",
        "                        help='Input data path.')\n",
        "    parser.add_argument('--dataset', nargs='?', default='movielens',\n",
        "                        help='Choose a dataset.')\n",
        "    parser.add_argument('--epochs', type=int, default=30,\n",
        "                        help='Number of epochs.')\n",
        "    parser.add_argument('--batch_size', type=int, default=256,\n",
        "                        help='Batch size.')\n",
        "    parser.add_argument('--layers', nargs='?', default='[16,32,16,8]',\n",
        "                        help=\"Size of each layer. Note that the first layer is the concatenation of user and item embeddings. So layers[0]/2 is the embedding size.\")\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.00001,\n",
        "                        help=\"Regularization for each layer\")\n",
        "    parser.add_argument('--num_neg_train', type=int, default=4,\n",
        "                        help='Number of negative instances to pair with a positive instance while training')\n",
        "    parser.add_argument('--num_neg_test', type=int, default=100,\n",
        "                        help='Number of negative instances to pair with a positive instance while testing')\n",
        "    parser.add_argument('--lr', type=float, default=0.001,\n",
        "                        help='Learning rate.')\n",
        "    parser.add_argument('--dropout', type=float, default=0,\n",
        "                        help='Add dropout layer after each dense layer, with p = dropout_prob')\n",
        "    parser.add_argument('--learner', nargs='?', default='adam',\n",
        "                        help='Specify an optimizer: adagrad, adam, rmsprop, sgd')\n",
        "    parser.add_argument('--verbose', type=int, default=1,\n",
        "                        help='Show performance per X iterations')\n",
        "    parser.add_argument('--out', type=int, default=1,\n",
        "                        help='Whether to save the trained model.')\n",
        "    return parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, n_users, n_items, layers=[16, 8], dropout=False):\n",
        "        \"\"\"\n",
        "        Simple Feedforward network with Embeddings for users and items\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert (layers[0] % 2 == 0), \"layers[0] must be an even number\"\n",
        "        self.__alias__ = \"MLP {}\".format(layers)\n",
        "        self.__dropout__ = dropout\n",
        "\n",
        "        # user and item embedding layers\n",
        "        embedding_dim = int(layers[0]/2)\n",
        "        self.user_embedding = torch.nn.Embedding(n_users, embedding_dim)\n",
        "        self.item_embedding = torch.nn.Embedding(n_items, embedding_dim)\n",
        "\n",
        "        # list of weight matrices\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        # hidden dense layers\n",
        "        for _, (in_size, out_size) in enumerate(zip(layers[:-1], layers[1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "        # final prediction layer\n",
        "        self.output_layer = torch.nn.Linear(layers[-1], 1)\n",
        "\n",
        "    def forward(self, feed_dict):\n",
        "        users = feed_dict['user_id']\n",
        "        items = feed_dict['item_id']\n",
        "        user_embedding = self.user_embedding(users)\n",
        "        item_embedding = self.item_embedding(items)\n",
        "        # concatenate user and item embeddings to form input\n",
        "        x = torch.cat([user_embedding, item_embedding], 1)\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            x = self.fc_layers[idx](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x,  p=self.__dropout__, training=self.training)\n",
        "        logit = self.output_layer(x)\n",
        "        rating = torch.sigmoid(logit)\n",
        "        return rating\n",
        "\n",
        "    def predict(self, feed_dict):\n",
        "        # return the score, inputs and outputs are numpy arrays\n",
        "        for key in feed_dict:\n",
        "            if type(feed_dict[key]) != type(None):\n",
        "                feed_dict[key] = torch.from_numpy(\n",
        "                    feed_dict[key]).to(dtype=torch.long, device=device)\n",
        "        output_scores = self.forward(feed_dict)\n",
        "        return output_scores.cpu().detach().numpy()\n",
        "\n",
        "    def get_alias(self):\n",
        "        return self.__alias__\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    args = parse_args()\n",
        "    path = args.path\n",
        "    dataset = args.dataset\n",
        "    layers = eval(args.layers)\n",
        "    weight_decay = args.weight_decay\n",
        "    num_negatives_train = args.num_neg_train\n",
        "    num_negatives_test = args.num_neg_test\n",
        "    dropout = args.dropout\n",
        "    learner = args.learner\n",
        "    learning_rate = args.lr\n",
        "    batch_size = args.batch_size\n",
        "    epochs = args.epochs\n",
        "    verbose = args.verbose\n",
        "\n",
        "    topK = 10\n",
        "    print(\"MLP arguments: %s \" % (args))\n",
        "    # model_out_file = 'Pretrain/%s_MLP_%s_%d.h5' %(args.dataset, args.layers, time())\n",
        "\n",
        "    # Load data\n",
        "\n",
        "    t1 = time()\n",
        "    full_dataset = MovieLensDataset(\n",
        "        path + dataset, num_negatives_train=num_negatives_train, num_negatives_test=num_negatives_test)\n",
        "    train, testRatings, testNegatives = full_dataset.trainMatrix, full_dataset.testRatings, full_dataset.testNegatives\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\"\n",
        "          % (time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
        "\n",
        "    training_data_generator = DataLoader(\n",
        "        full_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "    # Build model\n",
        "    model = MLP(num_users, num_items, layers=layers, dropout=dropout)\n",
        "    # Transfer the model to GPU, if one is available\n",
        "    model.to(device)\n",
        "    if verbose:\n",
        "        print(model)\n",
        "\n",
        "    loss_fn = torch.nn.BCELoss()\n",
        "    # Use Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay)\n",
        "\n",
        "    # Record performance\n",
        "    hr_list = []\n",
        "    ndcg_list = []\n",
        "    BCE_loss_list = []\n",
        "\n",
        "    # Check Init performance\n",
        "    hr, ndcg = test(model, full_dataset, topK)\n",
        "    hr_list.append(hr)\n",
        "    ndcg_list.append(ndcg)\n",
        "    BCE_loss_list.append(1)\n",
        "    # do the epochs now\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = train_one_epoch( model, training_data_generator, loss_fn, optimizer, epoch, device)\n",
        "\n",
        "        if epoch % verbose == 0:\n",
        "            hr, ndcg = test(model, full_dataset, topK)\n",
        "            hr_list.append(hr)\n",
        "            ndcg_list.append(ndcg)\n",
        "            BCE_loss_list.append(epoch_loss)\n",
        "            # if hr > best_hr:\n",
        "            #     best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
        "            #     if args.out > 0:\n",
        "            #         model.save(model_out_file, overwrite=True)\n",
        "    print(\"hr for epochs: \", hr_list)\n",
        "    print(\"ndcg for epochs: \", ndcg_list)\n",
        "    print(\"loss for epochs: \", BCE_loss_list)\n",
        "    # plot_statistics(hr_list, ndcg_list, BCE_loss_list,model.get_alias(), \"./figs\")\n",
        "    # with open(\"metrics\", 'wb') as fp:\n",
        "    #     pickle.dump(hr_list, fp)\n",
        "    #     pickle.dump(ndcg_list, fp)\n",
        "\n",
        "    best_iter = np.argmax(np.array(hr_list))\n",
        "    best_hr = hr_list[best_iter]\n",
        "    best_ndcg = ndcg_list[best_iter]\n",
        "    print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %\n",
        "          (best_iter, best_hr, best_ndcg))\n",
        "    # if args.out > 0:\n",
        "    #     print(\"The best MLP model is saved to %s\" %(model_out_file))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Device available: {}\".format(device))\n",
        "    main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device available: cuda:0\n",
            "MLP arguments: Namespace(batch_size=256, dataset='movielens', dropout=0, epochs=30, layers='[16,32,16,8]', learner='adam', lr=0.001, num_neg_test=100, num_neg_train=4, out=1, path='Data/', verbose=1, weight_decay=1e-05) \n",
            "Load data done [3.7 s]. #user=944, #item=1683, #train=99057, #test=943\n",
            "MLP(\n",
            "  (user_embedding): Embedding(944, 8)\n",
            "  (item_embedding): Embedding(1683, 8)\n",
            "  (fc_layers): ModuleList(\n",
            "    (0): Linear(in_features=16, out_features=32, bias=True)\n",
            "    (1): Linear(in_features=32, out_features=16, bias=True)\n",
            "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
            "  )\n",
            "  (output_layer): Linear(in_features=8, out_features=1, bias=True)\n",
            ")\n",
            "Eval: HR = 0.0965, NDCG = 0.0424 [1.0 s]\n",
            "Epoch = 0\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.4398874054275434\n",
            "Eval: HR = 0.4040, NDCG = 0.2081 [0.9 s]\n",
            "Epoch = 1\n",
            "Epoch completed 6.9 s\n",
            "Train Loss: 0.363216784496332\n",
            "Eval: HR = 0.3839, NDCG = 0.2121 [0.9 s]\n",
            "Epoch = 2\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.35540102081089364\n",
            "Eval: HR = 0.3998, NDCG = 0.2201 [0.9 s]\n",
            "Epoch = 3\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.35174689799624204\n",
            "Eval: HR = 0.4051, NDCG = 0.2203 [0.9 s]\n",
            "Epoch = 4\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.349118277830979\n",
            "Eval: HR = 0.4030, NDCG = 0.2189 [0.9 s]\n",
            "Epoch = 5\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.34687670399976334\n",
            "Eval: HR = 0.4199, NDCG = 0.2291 [0.9 s]\n",
            "Epoch = 6\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.3442449106880553\n",
            "Eval: HR = 0.4242, NDCG = 0.2382 [0.9 s]\n",
            "Epoch = 7\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.34041780905668123\n",
            "Eval: HR = 0.4422, NDCG = 0.2516 [0.9 s]\n",
            "Epoch = 8\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.33558834146621613\n",
            "Eval: HR = 0.4624, NDCG = 0.2614 [0.9 s]\n",
            "Epoch = 9\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.3297082264679039\n",
            "Eval: HR = 0.4825, NDCG = 0.2736 [0.9 s]\n",
            "Epoch = 10\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.323103608017744\n",
            "Eval: HR = 0.4899, NDCG = 0.2761 [0.9 s]\n",
            "Epoch = 11\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.31614459493517566\n",
            "Eval: HR = 0.5207, NDCG = 0.2873 [0.9 s]\n",
            "Epoch = 12\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.3099072543764607\n",
            "Eval: HR = 0.5323, NDCG = 0.2934 [0.9 s]\n",
            "Epoch = 13\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.30477262916336995\n",
            "Eval: HR = 0.5419, NDCG = 0.3000 [0.9 s]\n",
            "Epoch = 14\n",
            "Epoch completed 7.2 s\n",
            "Train Loss: 0.30033716202860344\n",
            "Eval: HR = 0.5620, NDCG = 0.3033 [0.9 s]\n",
            "Epoch = 15\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.2966918150506894\n",
            "Eval: HR = 0.5589, NDCG = 0.3059 [0.9 s]\n",
            "Epoch = 16\n",
            "Epoch completed 6.9 s\n",
            "Train Loss: 0.29384668343905024\n",
            "Eval: HR = 0.5557, NDCG = 0.3064 [0.9 s]\n",
            "Epoch = 17\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.29099676238628014\n",
            "Eval: HR = 0.5642, NDCG = 0.3098 [0.9 s]\n",
            "Epoch = 18\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.2886513389897285\n",
            "Eval: HR = 0.5610, NDCG = 0.3107 [0.9 s]\n",
            "Epoch = 19\n",
            "Epoch completed 6.9 s\n",
            "Train Loss: 0.28640858784356477\n",
            "Eval: HR = 0.5599, NDCG = 0.3100 [0.9 s]\n",
            "Epoch = 20\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.2844094593241541\n",
            "Eval: HR = 0.5790, NDCG = 0.3184 [0.9 s]\n",
            "Epoch = 21\n",
            "Epoch completed 6.9 s\n",
            "Train Loss: 0.2824994821727122\n",
            "Eval: HR = 0.5652, NDCG = 0.3133 [0.9 s]\n",
            "Epoch = 22\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.28078453265881353\n",
            "Eval: HR = 0.5610, NDCG = 0.3075 [0.9 s]\n",
            "Epoch = 23\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.27936925690919545\n",
            "Eval: HR = 0.5663, NDCG = 0.3080 [0.9 s]\n",
            "Epoch = 24\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.27769750593860637\n",
            "Eval: HR = 0.5652, NDCG = 0.3136 [0.9 s]\n",
            "Epoch = 25\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.27631383710745383\n",
            "Eval: HR = 0.5737, NDCG = 0.3133 [0.9 s]\n",
            "Epoch = 26\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.2752475519512975\n",
            "Eval: HR = 0.5652, NDCG = 0.3120 [0.9 s]\n",
            "Epoch = 27\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.27384773501284054\n",
            "Eval: HR = 0.5854, NDCG = 0.3195 [0.9 s]\n",
            "Epoch = 28\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.27286094256149707\n",
            "Eval: HR = 0.5832, NDCG = 0.3223 [0.9 s]\n",
            "Epoch = 29\n",
            "Epoch completed 7.0 s\n",
            "Train Loss: 0.27183321448721626\n",
            "Eval: HR = 0.5928, NDCG = 0.3245 [0.9 s]\n",
            "hr for epochs:  [0.09650053022269353, 0.40402969247083775, 0.383881230116649, 0.3997879109225875, 0.4050901378579003, 0.4029692470837752, 0.41993637327677624, 0.4241781548250265, 0.44220572640509015, 0.4623541887592789, 0.48250265111346763, 0.4899257688229056, 0.5206786850477201, 0.5323435843054083, 0.5418875927889714, 0.5620360551431601, 0.5588547189819725, 0.5556733828207847, 0.5641569459172853, 0.5609756097560976, 0.559915164369035, 0.5790031813361611, 0.5652173913043478, 0.5609756097560976, 0.5662778366914104, 0.5652173913043478, 0.5737009544008483, 0.5652173913043478, 0.5853658536585366, 0.5832449628844114, 0.5927889713679746]\n",
            "ndcg for epochs:  [0.0423747654806099, 0.20813466112904463, 0.21206882649761757, 0.22008809955160985, 0.2203076752069988, 0.21889769033876624, 0.2291047072693353, 0.2382236366236304, 0.2516359618020243, 0.26140519830472403, 0.2736089489120055, 0.27614371867092574, 0.2873314436717197, 0.2934492755194322, 0.30001533925970275, 0.30329837327497206, 0.3059305212178939, 0.3063997228546366, 0.3097623231836917, 0.3107495679132645, 0.30998047486957736, 0.3184473622648194, 0.31333414055880643, 0.3075043709128433, 0.30798035611172225, 0.31361909622468326, 0.31328330495397194, 0.31198426437567894, 0.3194881493309601, 0.32227476585380377, 0.3244508365794286]\n",
            "loss for epochs:  [1, 0.4398874054275434, 0.363216784496332, 0.35540102081089364, 0.35174689799624204, 0.349118277830979, 0.34687670399976334, 0.3442449106880553, 0.34041780905668123, 0.33558834146621613, 0.3297082264679039, 0.323103608017744, 0.31614459493517566, 0.3099072543764607, 0.30477262916336995, 0.30033716202860344, 0.2966918150506894, 0.29384668343905024, 0.29099676238628014, 0.2886513389897285, 0.28640858784356477, 0.2844094593241541, 0.2824994821727122, 0.28078453265881353, 0.27936925690919545, 0.27769750593860637, 0.27631383710745383, 0.2752475519512975, 0.27384773501284054, 0.27286094256149707, 0.27183321448721626]\n",
            "End. Best Iteration 30:  HR = 0.5928, NDCG = 0.3245. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twDpehmg2Oiq"
      },
      "source": [
        "#Item Popularity Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AijHlIy2OTG",
        "outputId": "3d3c9853-e068-4255-d8ec-9d387eb7b65c"
      },
      "source": [
        "# Description: Implements the item popularity model for recommendations\n",
        "\n",
        "\n",
        "# Workspace imports\n",
        "# from evaluate import evaluate_model\n",
        "# from utils import test, plot_statistics\n",
        "# from Dataset import MovieLensDataset\n",
        "\n",
        "# Python imports\n",
        "import argparse\n",
        "from time import time\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Run ItemPop\")\n",
        "    parser.add_argument('--path', nargs='?', default='Data/',\n",
        "                        help='Input data path.')\n",
        "    parser.add_argument('--dataset', nargs='?', default='movielens',\n",
        "                        help='Choose a dataset.')\n",
        "    parser.add_argument('--num_neg_test', type=int, default=100,\n",
        "                        help='Number of negative instances to pair with a positive instance while testing')\n",
        "    \n",
        "    return parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "class ItemPop():\n",
        "    def __init__(self, train_interaction_matrix: sp.dok_matrix):\n",
        "        \"\"\"\n",
        "        Simple popularity based recommender system\n",
        "        \"\"\"\n",
        "        self.__alias__ = \"Item Popularity without metadata\"\n",
        "        # Sum the occurences of each item to get is popularity, convert to array and \n",
        "        # lose the extra dimension\n",
        "        self.item_ratings = np.array(train_interaction_matrix.sum(axis=0, dtype=int)).flatten()\n",
        "\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    def predict(self, feeddict) -> np.array:\n",
        "        # returns the prediction score for each (user,item) pair in the input\n",
        "        items = feeddict['item_id']\n",
        "        output_scores = [self.item_ratings[itemid] for itemid in items]\n",
        "        return np.array(output_scores)\n",
        "\n",
        "    def get_alias(self):\n",
        "        return self.__alias__\n",
        "       \n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    path = args.path\n",
        "    dataset = args.dataset\n",
        "    num_negatives_test = args.num_neg_test\n",
        "    print(\"Model arguments: %s \" %(args))\n",
        "\n",
        "    topK = 10\n",
        "\n",
        "    # Load data\n",
        "\n",
        "    t1 = time()\n",
        "    full_dataset = MovieLensDataset(path + dataset, num_negatives_test= num_negatives_test)\n",
        "    train, testRatings, testNegatives = full_dataset.trainMatrix, full_dataset.testRatings, full_dataset.testNegatives\n",
        "    num_users, num_items = train.shape\n",
        "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\"\n",
        "          % (time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
        "\n",
        "    model = ItemPop(train)\n",
        "    test(model, full_dataset, topK)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model arguments: Namespace(dataset='movielens', num_neg_test=100, path='Data/') \n",
            "Load data done [3.9 s]. #user=944, #item=1683, #train=99057, #test=943\n",
            "Eval: HR = 0.4040, NDCG = 0.2185 [0.1 s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRbxitXg59Yr"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}