{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "15cA9Uu-kMlUu3Q-GX5Sr9EMPiBIiOiOD",
      "authorship_tag": "ABX9TyNjD5OGkyfBvxhhme06MjBZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9a2e43370f2476b86db3fb13d160829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a70096c40c34895afcb71b2816b7d32",
              "IPY_MODEL_a2799410adf24289b6cc2e4e0f2bbf93",
              "IPY_MODEL_1b8b6bfef3014282834f2cdfff2355a5"
            ],
            "layout": "IPY_MODEL_090c9183f8f146a2b15977026154538b"
          }
        },
        "8a70096c40c34895afcb71b2816b7d32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29f8f735532f40d4a6a683f635d16501",
            "placeholder": "​",
            "style": "IPY_MODEL_cd3afaf94be64e2d8fdd653578ee24e0",
            "value": "100%"
          }
        },
        "a2799410adf24289b6cc2e4e0f2bbf93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91d1e4c0ef8a41ae97820bf732099881",
            "max": 118,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_487c00cf6f4241259dbcce5f33902c0f",
            "value": 118
          }
        },
        "1b8b6bfef3014282834f2cdfff2355a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe5e6b1108754a8ba4209861b6c87a89",
            "placeholder": "​",
            "style": "IPY_MODEL_95003f1be83c40169804dcc82d914bd4",
            "value": " 118/118 [07:23&lt;00:00, 12.06s/it]"
          }
        },
        "090c9183f8f146a2b15977026154538b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29f8f735532f40d4a6a683f635d16501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd3afaf94be64e2d8fdd653578ee24e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91d1e4c0ef8a41ae97820bf732099881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "487c00cf6f4241259dbcce5f33902c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe5e6b1108754a8ba4209861b6c87a89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95003f1be83c40169804dcc82d914bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d2a450ea3ed40808cdb22349b853fc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_978abb3530ec4fb7b4fe3c71ef1b88b0",
              "IPY_MODEL_3ef1297dcb784d3ab6885dd4ec189037",
              "IPY_MODEL_ec8dead56bfc4d809292b12c050943d1"
            ],
            "layout": "IPY_MODEL_09bc457ac1054112a631bc738969bee2"
          }
        },
        "978abb3530ec4fb7b4fe3c71ef1b88b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2bc770e98cb4848a044812eb40e8f3c",
            "placeholder": "​",
            "style": "IPY_MODEL_ed4fad09f35844daa2c4bfb52c11fe19",
            "value": "100%"
          }
        },
        "3ef1297dcb784d3ab6885dd4ec189037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd5d2110864941c9b9e4657a3f2c29d0",
            "max": 118,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dadd5af2545b4540b764ef047ebe925c",
            "value": 118
          }
        },
        "ec8dead56bfc4d809292b12c050943d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b41cfee5fc341b2b3b39040e6cfeae8",
            "placeholder": "​",
            "style": "IPY_MODEL_ac8a3ed3b9ae4d908ebdabebfc35fd2c",
            "value": " 118/118 [01:54&lt;00:00,  1.37it/s]"
          }
        },
        "09bc457ac1054112a631bc738969bee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2bc770e98cb4848a044812eb40e8f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed4fad09f35844daa2c4bfb52c11fe19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd5d2110864941c9b9e4657a3f2c29d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dadd5af2545b4540b764ef047ebe925c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b41cfee5fc341b2b3b39040e6cfeae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac8a3ed3b9ae4d908ebdabebfc35fd2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/eval_ENEM_2022_with_HF_and_chatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets transformers rank_bm25 openai accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "eL5EGALAOAey"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------- #\n",
        "# This code is from https://github.com/piresramon/gpt-4-enem #\n",
        "# ---------------------------------------------------------- #\n",
        "\n",
        "import json\n",
        "import torch\n",
        "import random \n",
        "import collections\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', 200)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import openai\n",
        "\n",
        "import rank_bm25\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datasets import Dataset\n",
        "\n",
        "if torch.cuda.is_available(): \n",
        "    device = 'cuda'  \n",
        "else: \n",
        "    device ='cpu'\n",
        "\n",
        "MANUAL_SEED = 2711\n",
        "rnd = random.Random()\n",
        "rnd.seed(MANUAL_SEED)\n",
        "def deterministic(rep=True, manual_seed=MANUAL_SEED):\n",
        "    if rep:\n",
        "        torch.manual_seed(manual_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(manual_seed)\n",
        "            torch.cuda.manual_seed_all(manual_seed)\n",
        "        torch.backends.cudnn.enabled = False \n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        print(f'Experimento deterministico, seed: {manual_seed}')\n",
        "        if device == 'cuda':\n",
        "            print(f'Existe {torch.cuda.device_count()} GPU\\\n",
        "            {torch.cuda.get_device_name(0)} disponível.')\n",
        "    else:\n",
        "        print('Experimento randomico')\n",
        "deterministic()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4dfyr6EmGsJ",
        "outputId": "6a1ec5e6-7328-4115-9a0d-8c15c388d365"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimento deterministico, seed: 2711\n",
            "Existe 1 GPU            Tesla T4 disponível.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Enem json and create enem_data "
      ],
      "metadata": {
        "id": "EaS1311b-ew1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ignore_question(doc):\n",
        "    filters = {\n",
        "        'IU': False,\n",
        "        # 'MR': False,  # uncomment to filter out MR\n",
        "        # 'CE': False,  # uncomment to filter out CE\n",
        "        'ML': False,\n",
        "    }\n",
        "    for k,v in filters.items():\n",
        "        if doc[k] != v:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _process_doc_cot(doc):\n",
        "    def format_example(doc, choices):\n",
        "        \"\"\"\n",
        "            Passagem: <passage>\n",
        "            Pergunta: <question>\n",
        "            Choices:\n",
        "            A. <choice1>\n",
        "            B. <choice2>\n",
        "            C. <choice3>\n",
        "            D. <choice4>\n",
        "            Answer:\n",
        "        \"\"\"\n",
        "        prompt = \"Cabeçalho: \" + doc[\"context\"] + \"\\n\"\n",
        "        prompt += \"Enunciado: \" + doc[\"question\"] + \"\\nAlternativas:\\n\"\n",
        "        for choice, option in zip(choices, doc[\"options\"]):\n",
        "            prompt += f\"{choice.upper()}. {option}\\n\"\n",
        "        \n",
        "        prompt += \"Explicação: \" + doc.get(\"explanation\", \"\")\n",
        "        return prompt.strip()\n",
        "    choices = ['a', 'b', 'c', 'd', 'e']\n",
        "    return {\n",
        "        \"query\": format_example(doc, choices),\n",
        "        \"choices\": doc[\"options\"],\n",
        "        \"gold\": choices.index(doc[\"label\"]),\n",
        "        \"id\": doc[\"id\"],\n",
        "        \"exam\": doc[\"exam\"],\n",
        "    }    \n",
        "\n",
        "dataset = collections.defaultdict(list)\n",
        "        \n",
        "data_path = \"/content/drive/MyDrive/LLMs/ENEM/ENEMdataset/2022.json\"\n",
        "with open(data_path) as f:\n",
        "    documents = json.load(f)\n",
        "\n",
        "documents = list(filter(lambda doc: not ignore_question(doc), documents))\n",
        "dataset['test'] = list(map(_process_doc_cot, documents))\n",
        "\n",
        "enem_data = {ix:doc  for ix, doc in enumerate(dataset['test'])}\n",
        "\n",
        "print(f\"tamanho dataset: {len(enem_data)}\")\n",
        "\n",
        "enem_data[0]    "
      ],
      "metadata": {
        "id": "u6NlgTBlOf3F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c90e75-deb9-4fd8-9cbb-9202e6a11fdc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tamanho dataset: 118\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Cabeçalho: A conquista da medalha de prata por Rayssa Leal, no skate street nos Jogos Olímpicos, é exemplo da representatividade feminina no esporte, avalia a âncora do jornal da rede de televisão da CNN. A apresentadora, que também anda de skate, celebrou a vitória da brasileira, que entrou para a história como a atleta mais nova a subir num pódio defendendo o Brasil. “Essa representatividade do esporte nos Jogos faz pensarmos que não temos que ficar nos encaixando em nenhum lugar. Posso gostar de passar notícia e, mesmo assim, gostar de skate, subir montanha, mergulhar, andar de bike, fazer yoga. Temos que parar de ficar enquadrando as pessoas dentro de regras. A gente vive num padrão no qual a menina ganha boneca, mas por que também não fazer um esporte de aventura? Por que o homem pode se machucar, cair de joelhos, e a menina tem que estar sempre lindinha dentro de um padrão? Acabamos limitando os talentos das pessoas”, afirmou a jornalista, sobre a prática do skate por mulheres. Disponível em: www.cnnbrasil.com.br. Acesso em: 31 out. 2021 (adaptado).\\nEnunciado: O discurso da jornalista traz questionamentos sobre a relação da conquista da skatista com a\\nAlternativas:\\nA. conciliação do jornalismo com a prática do skate.\\nB. inserção das mulheres na modalidade skate street.\\nC. desconstrução da noção do skate como modalidade masculina.\\nD. vanguarda de ser a atleta mais jovem a subir no pódio olímpico.\\nE. conquista de medalha nos Jogos Olímpicos de Tóquio.\\nExplicação:',\n",
              " 'choices': ['conciliação do jornalismo com a prática do skate.',\n",
              "  'inserção das mulheres na modalidade skate street.',\n",
              "  'desconstrução da noção do skate como modalidade masculina.',\n",
              "  'vanguarda de ser a atleta mais jovem a subir no pódio olímpico.',\n",
              "  'conquista de medalha nos Jogos Olímpicos de Tóquio.'],\n",
              " 'gold': 2,\n",
              " 'id': 'ENEM_2022_6',\n",
              " 'exam': '2022'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Prompts\n",
        "- #### Dynamic Prompt (fewshot sampled)\n",
        "- #### Dynamic Similar Prompt (bm25 rank fewshot)"
      ],
      "metadata": {
        "id": "A5qaX_aF-q_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dynamic_similar_prompt(doc_id, data, topk=3):\n",
        "    key_predict = [k for k, v in enem_data.items() if v['id'].startswith(doc_id)][0]\n",
        "    query_list = [v['query'] for k,v in data.items()]\n",
        "    tokenized_corpus = [doc.split(\" \") for doc in query_list]\n",
        "    bm25 = rank_bm25.BM25Plus(tokenized_corpus)\n",
        "\n",
        "    query = data[key_predict]['query']\n",
        "    tokenized_query = query.split(\" \")\n",
        "    \n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "    \n",
        "    # [1:] remove query with itself ---> topk+1\n",
        "    top_similar_idxs = list(doc_scores.argsort()[-(topk+1):][::-1])[1:]\n",
        "    if key_predict in top_similar_idxs: # k in topk_sim ---> must be false\n",
        "        print('Error BM25 prompt similar retrieval')\n",
        "    \n",
        "    return top_similar_idxs\n",
        "\n",
        "\n",
        "def dynamyc_fewshot_examples(num_fewshot, doc_id, sim_prompt=True, data=enem_data):\n",
        "    if sim_prompt:\n",
        "        topk_sim = dynamic_similar_prompt(doc_id, data, topk=num_fewshot)\n",
        "        fewshot_ex = [enem_data[k] for k in topk_sim]\n",
        "\n",
        "    else:\n",
        "        # filter the sample in current predict from data\n",
        "        all_possible_fewshot_keys = [k for k, v in data.items() if not v['id'].startswith(doc_id)]\n",
        "        fewshot_keys = rnd.sample(all_possible_fewshot_keys, num_fewshot)\n",
        "        fewshot_ex = [enem_data[k] for k in fewshot_keys]\n",
        "\n",
        "    return fewshot_ex\n",
        "\n",
        "\n",
        "def fewshot_context(doc, num_fewshot, similar_prompt=True, enem_data=enem_data):\n",
        "    if num_fewshot == 0 and fewshotex is None:\n",
        "        labeled_examples = \"\"\n",
        "    else:\n",
        "        fewshotex = dynamyc_fewshot_examples(\n",
        "            num_fewshot=num_fewshot, doc_id=doc['id'], sim_prompt=similar_prompt, data=enem_data\n",
        "        )\n",
        "        \n",
        "        labeled_examples = ''\n",
        "        for i, doc_ex in enumerate(fewshotex):\n",
        "            labeled_examples += f'Questão {i+1}:\\n'\n",
        "            labeled_examples += doc_ex['query'] + \" \" + ['A.', 'B.', 'C.', 'D.', 'E.'][doc_ex['gold']].upper()\n",
        "            labeled_examples += '\\n##\\n'\n",
        "        labeled_examples += f'Questão {len(fewshotex) + 1}:\\n'\n",
        "\n",
        "    example = doc['query']\n",
        "    \n",
        "    return labeled_examples + example\n",
        "\n",
        "# -------------------------------------------------------------------------------------- #\n",
        "# number of fewshots, must be > 0\n",
        "\n",
        "# create inputs ---> list with fewshot samples + question to be answer \n",
        "two_shot, three_shot = [], []\n",
        "for k,doc in enem_data.items():\n",
        "    two_shot.append(fewshot_context(doc=doc,   num_fewshot=2, similar_prompt=True, enem_data=enem_data))\n",
        "    three_shot.append(fewshot_context(doc=doc, num_fewshot=3, similar_prompt=True, enem_data=enem_data))"
      ],
      "metadata": {
        "id": "3fLuSqrEjxVc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval ENEM-2022 with ChatGPT "
      ],
      "metadata": {
        "id": "KqOWUwVD_EEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_CHATGPT = False\n",
        "\n",
        "if EVAL_CHATGPT:\n",
        "    # OPENAI_API_KEY = \"secret\"\n",
        "    # openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "    deterministic()    \n",
        "\n",
        "    trues, preds = [], []\n",
        "    loop = tqdm(two_shot, leave=True)\n",
        "\n",
        "    for ix, batch in enumerate(loop):\n",
        "\n",
        "        # to make sure the same example to predict has the correct label \n",
        "        # find the point where the question to be evaluate starts\n",
        "        point = two_shot[ix].find('Questão 3:\\n')\n",
        "        \n",
        "        # \"11+point\" ---> is the start of the query-text, so this is a naive approach\n",
        "        if enem_data[ix]['query'] == two_shot[ix][11+point:]:\n",
        "            chatGPT_response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": two_shot[ix]}])\n",
        "            \n",
        "            preds.append(chatGPT_response['choices'][0]['message']['content'])\n",
        "            trues.append(enem_data[ix]['gold'])\n",
        "        else:\n",
        "            # if the query-text is different from inputs to be evaluated \n",
        "            # we print for further investigation\n",
        "            print(point, ix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "e9a2e43370f2476b86db3fb13d160829",
            "8a70096c40c34895afcb71b2816b7d32",
            "a2799410adf24289b6cc2e4e0f2bbf93",
            "1b8b6bfef3014282834f2cdfff2355a5",
            "090c9183f8f146a2b15977026154538b",
            "29f8f735532f40d4a6a683f635d16501",
            "cd3afaf94be64e2d8fdd653578ee24e0",
            "91d1e4c0ef8a41ae97820bf732099881",
            "487c00cf6f4241259dbcce5f33902c0f",
            "fe5e6b1108754a8ba4209861b6c87a89",
            "95003f1be83c40169804dcc82d914bd4"
          ]
        },
        "id": "OzziaZb7wN30",
        "outputId": "1ab2f903-317a-40b4-8962-376e6fd7e11d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimento deterministico, seed: 2711\n",
            "Existe 1 GPU            Tesla T4 disponível.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/118 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9a2e43370f2476b86db3fb13d160829"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_results(trues, preds, chatgpt=False):\n",
        "    dataframe = pd.DataFrame({'true': trues, 'pred':preds})\n",
        "\n",
        "    #get only latter\n",
        "    if chatgpt:\n",
        "        dataframe['pred'] = dataframe.pred.apply(lambda x: x[:1])\n",
        "    \n",
        "    else:\n",
        "        dataframe['pred'] = dataframe.pred.apply(lambda x: x[-1:])\n",
        "    \n",
        "    map_label = {0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\", 4:\"E\"}\n",
        "    dataframe['true'] = dataframe.true.apply(lambda x: map_label[x])\n",
        "    \n",
        "    acc = accuracy_score(dataframe['true'], dataframe['pred'])\n",
        "    \n",
        "    return dataframe, acc\n",
        "\n",
        "\n",
        "if EVAL_CHATGPT:\n",
        "    chatGPT_results, acc = get_results(trues, preds, chatgpt=True)\n",
        "\n",
        "    print(f'ACC of 2-shot chatGPT on ENEM-2022: {acc:.3}')\n",
        "    # ACC of ChatGPT: 0.771\n",
        "\n",
        "    chatGPT_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoTVDnu_o67u",
        "outputId": "6d93b43d-d0ff-4236-9870-f58b30c85676"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC of 2-shot chatGPT on ENEM-2022: 0.703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Source Models\n",
        "\n",
        "- #### with HuggingFace we will evaluate GPT2/LLama & Alpaca/Bloom models."
      ],
      "metadata": {
        "id": "7M54xL-y9ga8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_gpt = 'gpt2'\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(model_gpt)\n",
        "\n",
        "model_bloom = 'bigscience/bloomz-7b1-mt'\n",
        "tokenizer_bloom = AutoTokenizer.from_pretrained(model_bloom)\n",
        "\n",
        "def get_input_lenghts(dataset, tokenizer):\n",
        "    num_words = [len(x.split()) for x in dataset]\n",
        "    num_words = torch.tensor(num_words, dtype=torch.float)\n",
        "    tokens_lengths = [len(tokenizer.encode(seq)) for seq in dataset]\n",
        "    tokens_lengths = torch.tensor(tokens_lengths, dtype=torch.float)\n",
        "    \n",
        "    return int(torch.ceil(num_words.mean()).item()), tokens_lengths\n",
        "\n",
        "# -------------------------------------------------------------------- #\n",
        "num_words, gpt2_lengths = get_input_lenghts(three_shot, tokenizer_gpt)\n",
        "min_gpt2 = torch.ceil(gpt2_lengths.min()).item()\n",
        "max_gpt2 = torch.ceil(gpt2_lengths.max()).item()\n",
        "mean_gpt2 = torch.ceil(gpt2_lengths.mean()).item()\n",
        "# -------------------------------------------------------------------- #\n",
        "num_words, bloom_lengths = get_input_lenghts(three_shot, tokenizer_bloom)\n",
        "min_bloom = torch.ceil(bloom_lengths.min()).item()\n",
        "max_bloom = torch.ceil(bloom_lengths.max()).item()\n",
        "mean_bloom = torch.ceil(bloom_lengths.mean()).item()\n",
        "# -------------------------------------------------------------------- #\n",
        "print('\\n\\n')\n",
        "print('--'*33)\n",
        "print(f' Prompt 3-shot com média de {num_words} palavras em  ENEM2022 dataset')\n",
        "print('--'*33)\n",
        "print(f'GPT2 tem média de {int(mean_gpt2)} tokens em 3-shot')\n",
        "print(f'\\nBloom tem média de {int(mean_bloom)} tokens em 3-shot')\n",
        "print(f'\\n---> Bloom gasta {(int(mean_bloom)/int(mean_gpt2))*100:.3}% de tokens comparado com o GPT2 <---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhioXR6l0oQf",
        "outputId": "26999a4c-e44d-4045-ad19-3e2a92f33ea4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2863 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "------------------------------------------------------------------\n",
            " Prompt 3-shot com média de 862 palavras em  ENEM2022 dataset\n",
            "------------------------------------------------------------------\n",
            "GPT2 tem média de 2061 tokens em 3-shot\n",
            "\n",
            "Bloom tem média de 1245 tokens em 3-shot\n",
            "\n",
            "---> Bloom gasta 60.4% de tokens comparado com o GPT2 <---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval BloomZ 7b mt"
      ],
      "metadata": {
        "id": "XwM1oSK2AXqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load bloom in 8bits with bitsandbytes\n",
        "BLOOM_model = AutoModelForCausalLM.from_pretrained(model_bloom, device_map=\"auto\", load_in_8bit=True)"
      ],
      "metadata": {
        "id": "Vbxq3VZ4OH5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd51dce-31dc-4085-805d-4251ea3ad36d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-hm-f9866x0ahsod --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EVAL_OPEN_SOURCE = True\n",
        "\n",
        "if EVAL_OPEN_SOURCE:\n",
        "    deterministic()    \n",
        "\n",
        "    trues, preds = [], []\n",
        "    loop = tqdm(two_shot, leave=True)\n",
        "\n",
        "    for ix, batch in enumerate(loop):\n",
        "\n",
        "        # to make sure the same example to predict has the correct label \n",
        "        # find the point where the question to be evaluate starts\n",
        "        point = two_shot[ix].find('Questão 3:\\n')\n",
        "        \n",
        "        # \"11+point\" ---> is the start of the query-text, so this is a naive approach\n",
        "        if enem_data[ix]['query'] == two_shot[ix][11+point:]:\n",
        "            inputs = tokenizer_bloom.encode(two_shot[ix], return_tensors=\"pt\", max_length=2048).to(device)\n",
        "            outputs = BLOOM_model.generate(inputs)\n",
        "            \n",
        "            preds.append(tokenizer_bloom.decode(outputs[0]))\n",
        "            trues.append(enem_data[ix]['gold'])\n",
        "        else:\n",
        "            # if the query-text is different from inputs to be evaluated \n",
        "            # we print for further investigation\n",
        "            print(point, ix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0d2a450ea3ed40808cdb22349b853fc1",
            "978abb3530ec4fb7b4fe3c71ef1b88b0",
            "3ef1297dcb784d3ab6885dd4ec189037",
            "ec8dead56bfc4d809292b12c050943d1",
            "09bc457ac1054112a631bc738969bee2",
            "c2bc770e98cb4848a044812eb40e8f3c",
            "ed4fad09f35844daa2c4bfb52c11fe19",
            "dd5d2110864941c9b9e4657a3f2c29d0",
            "dadd5af2545b4540b764ef047ebe925c",
            "3b41cfee5fc341b2b3b39040e6cfeae8",
            "ac8a3ed3b9ae4d908ebdabebfc35fd2c"
          ]
        },
        "id": "x07ODzVDYAPI",
        "outputId": "900aa46f-0cd8-4ecf-e598-01726305cc99"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experimento deterministico, seed: 2711\n",
            "Existe 1 GPU            Tesla T4 disponível.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/118 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d2a450ea3ed40808cdb22349b853fc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "Input length of input_ids is 1149, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1141, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1023, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1143, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1088, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 703, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1374, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1211, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1124, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 823, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1169, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1202, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1218, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1133, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1126, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1198, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1080, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1212, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1135, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1527, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1099, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1428, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1009, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1095, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1095, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1095, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1039, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1082, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 986, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1513, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1151, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 859, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1027, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 989, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 733, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1124, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1080, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 941, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1004, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 957, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 892, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1097, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 848, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 707, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1117, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 904, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 700, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 654, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 947, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1242, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1048, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 651, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 692, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 967, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 970, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 784, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 676, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 822, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1380, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1269, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1200, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 853, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1026, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1099, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1031, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 675, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1030, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 809, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1088, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 837, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 823, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 853, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 704, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 665, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 654, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 707, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1001, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 796, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 866, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 711, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 755, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 875, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 897, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 934, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 894, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 1164, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 891, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 771, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 859, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 679, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 616, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 707, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 952, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 663, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 634, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 863, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 571, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 584, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 741, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 537, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 741, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 870, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 834, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 467, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 870, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 943, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 597, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 817, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 886, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 683, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 919, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 570, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 872, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 898, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 817, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 691, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 832, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "Input length of input_ids is 697, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.DataFrame({'true': trues, 'pred':preds})\n",
        "dataframe.pred.apply(lambda x: x[-1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vchqzuw9e5r0",
        "outputId": "0d63d6d7-ece3-4c4a-f887-d39bfe377600"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      D\n",
              "1      C\n",
              "2      B\n",
              "3      D\n",
              "4      D\n",
              "5      C\n",
              "6      A\n",
              "7      D\n",
              "8      A\n",
              "9      D\n",
              "10     D\n",
              "11     D\n",
              "12     D\n",
              "13     D\n",
              "14     C\n",
              "15     A\n",
              "16     D\n",
              "17     D\n",
              "18     A\n",
              "19     C\n",
              "20     D\n",
              "21     D\n",
              "22     D\n",
              "23     D\n",
              "24     D\n",
              "25     A\n",
              "26     C\n",
              "27     D\n",
              "28     A\n",
              "29     B\n",
              "30     D\n",
              "31     D\n",
              "32     D\n",
              "33     A\n",
              "34     D\n",
              "35     D\n",
              "36     C\n",
              "37     A\n",
              "38     D\n",
              "39     C\n",
              "40     D\n",
              "41     D\n",
              "42     D\n",
              "43     D\n",
              "44     E\n",
              "45     D\n",
              "46     A\n",
              "47     D\n",
              "48     D\n",
              "49     D\n",
              "50     A\n",
              "51     D\n",
              "52     D\n",
              "53     D\n",
              "54     C\n",
              "55     D\n",
              "56     C\n",
              "57     A\n",
              "58     D\n",
              "59     C\n",
              "60     D\n",
              "61     D\n",
              "62     B\n",
              "63     B\n",
              "64     D\n",
              "65     D\n",
              "66     A\n",
              "67     D\n",
              "68     E\n",
              "69     A\n",
              "70     D\n",
              "71     D\n",
              "72     C\n",
              "73     B\n",
              "74     D\n",
              "75     D\n",
              "76     A\n",
              "77     D\n",
              "78     B\n",
              "79     D\n",
              "80     D\n",
              "81     D\n",
              "82     D\n",
              "83     D\n",
              "84     D\n",
              "85     D\n",
              "86     D\n",
              "87     D\n",
              "88     D\n",
              "89     D\n",
              "90     D\n",
              "91     D\n",
              "92     A\n",
              "93     D\n",
              "94     D\n",
              "95     D\n",
              "96     D\n",
              "97     D\n",
              "98     D\n",
              "99     D\n",
              "100    D\n",
              "101    D\n",
              "102    D\n",
              "103    D\n",
              "104    D\n",
              "105    D\n",
              "106    D\n",
              "107    D\n",
              "108    D\n",
              "109    D\n",
              "110    D\n",
              "111    D\n",
              "112    D\n",
              "113    D\n",
              "114    D\n",
              "115    D\n",
              "116    D\n",
              "117    D\n",
              "Name: pred, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bloom_results, acc = get_results(trues, preds)\n",
        "\n",
        "print(f'ACC of 2-shot Bloomz-7b1-mt on ENEM-2022: {acc:.3}')\n",
        "# ACC of Bloomz-7b1-mt: \n",
        "\n",
        "bloom_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RwqQ3zWDbLJG",
        "outputId": "889a30cd-a6ad-45fc-b07f-de3fba2280f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACC of 2-shot Bloomz-7b1-mt on ENEM-2022: 0.339\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    true pred\n",
              "0      C    D\n",
              "1      C    C\n",
              "2      C    B\n",
              "3      D    D\n",
              "4      C    D\n",
              "5      C    C\n",
              "6      E    A\n",
              "7      B    D\n",
              "8      A    A\n",
              "9      C    D\n",
              "10     B    D\n",
              "11     A    D\n",
              "12     E    D\n",
              "13     E    D\n",
              "14     C    C\n",
              "15     A    A\n",
              "16     B    D\n",
              "17     A    D\n",
              "18     E    A\n",
              "19     E    C\n",
              "20     B    D\n",
              "21     E    D\n",
              "22     E    D\n",
              "23     D    D\n",
              "24     C    D\n",
              "25     A    A\n",
              "26     D    C\n",
              "27     B    D\n",
              "28     D    A\n",
              "29     B    B\n",
              "30     D    D\n",
              "31     B    D\n",
              "32     A    D\n",
              "33     C    A\n",
              "34     B    D\n",
              "35     E    D\n",
              "36     C    C\n",
              "37     A    A\n",
              "38     B    D\n",
              "39     C    C\n",
              "40     D    D\n",
              "41     A    D\n",
              "42     C    D\n",
              "43     D    D\n",
              "44     E    E\n",
              "45     D    D\n",
              "46     A    A\n",
              "47     A    D\n",
              "48     D    D\n",
              "49     B    D\n",
              "50     C    A\n",
              "51     D    D\n",
              "52     A    D\n",
              "53     E    D\n",
              "54     C    C\n",
              "55     B    D\n",
              "56     A    C\n",
              "57     A    A\n",
              "58     E    D\n",
              "59     C    C\n",
              "60     B    D\n",
              "61     E    D\n",
              "62     B    B\n",
              "63     E    B\n",
              "64     E    D\n",
              "65     D    D\n",
              "66     A    A\n",
              "67     D    D\n",
              "68     E    E\n",
              "69     A    A\n",
              "70     D    D\n",
              "71     E    D\n",
              "72     D    C\n",
              "73     B    B\n",
              "74     E    D\n",
              "75     C    D\n",
              "76     A    A\n",
              "77     E    D\n",
              "78     B    B\n",
              "79     A    D\n",
              "80     B    D\n",
              "81     D    D\n",
              "82     D    D\n",
              "83     C    D\n",
              "84     B    D\n",
              "85     D    D\n",
              "86     E    D\n",
              "87     A    D\n",
              "88     A    D\n",
              "89     B    D\n",
              "90     E    D\n",
              "91     B    D\n",
              "92     E    A\n",
              "93     D    D\n",
              "94     B    D\n",
              "95     A    D\n",
              "96     E    D\n",
              "97     E    D\n",
              "98     B    D\n",
              "99     D    D\n",
              "100    E    D\n",
              "101    D    D\n",
              "102    B    D\n",
              "103    A    D\n",
              "104    A    D\n",
              "105    C    D\n",
              "106    C    D\n",
              "107    B    D\n",
              "108    C    D\n",
              "109    D    D\n",
              "110    E    D\n",
              "111    E    D\n",
              "112    C    D\n",
              "113    B    D\n",
              "114    C    D\n",
              "115    A    D\n",
              "116    B    D\n",
              "117    C    D"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fbcb081-1de8-44e0-8eef-9cae780b7176\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>true</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>E</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>D</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>C</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>A</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>C</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>E</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>E</td>\n",
              "      <td>E</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>D</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>A</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>B</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>E</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>D</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>E</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>A</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>B</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>C</td>\n",
              "      <td>D</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fbcb081-1de8-44e0-8eef-9cae780b7176')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7fbcb081-1de8-44e0-8eef-9cae780b7176 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7fbcb081-1de8-44e0-8eef-9cae780b7176');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}