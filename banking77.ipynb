{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "banking77.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1aING1VeDerhmMvTVbbjI119rE84XrBFJ",
      "authorship_tag": "ABX9TyPitG5x7mF+UopxuqXRrPgo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/banking77.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jOrt1iQCHZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1837bf6-19f4-46c9-eadd-a553446ac4e2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 24 19:37:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    41W / 300W |   7595MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgBe04jN3pHN"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wlLNxH3s3R"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "device = 'cuda'"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "jP5mvWWr32ud",
        "outputId": "21ac2680-e06d-40a8-f3ad-8fee97bb7725"
      },
      "source": [
        "path_base = '/content/drive/MyDrive/BACEN/data_translated/'\n",
        "\n",
        "# TRAIN\n",
        "data_train = pd.read_parquet(path_base+'banking77_ptbr_train')\n",
        "\n",
        "# ajustando o label de str para int\n",
        "data_train = data_train.assign(Label = data_train.Label.apply(lambda x: np.int64(x.replace('.', ''))))\n",
        "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# TEST\n",
        "data_test = pd.read_parquet(path_base+'banking77_ptbr_test')\n",
        "\n",
        "# ajustando o label de str para int\n",
        "data_test = data_test.assign(Label = data_test.Label.apply(lambda x: np.int64(x.replace('.', ''))))\n",
        "data_test = data_test.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# - - - - -\n",
        "print(data_train.shape, data_test.shape)\n",
        "data_train"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10003, 2) (3080, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Devolvi um item, mas não o vejo na minha conta?</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Por que haveria uma cobrança extra no meu apli...</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Preciso de um cartão de uso único para comprar...</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A quantidade de dinheiro que recebi não corres...</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Meu cartão ainda não chegou. O que eu faço?</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>Quanto tempo até um cheque eu mandar autorização?</td>\n",
              "      <td>58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>Como faço com transferências bancárias para as...</td>\n",
              "      <td>65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>O que significa ter uma retirada pendente de d...</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>Por que a transferência de dinheiro ainda não ...</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10002</th>\n",
              "      <td>O saldo não foi atualizado após uma transferên...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10003 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Data  Label\n",
              "0        Devolvi um item, mas não o vejo na minha conta?     51\n",
              "1      Por que haveria uma cobrança extra no meu apli...     34\n",
              "2      Preciso de um cartão de uso único para comprar...     37\n",
              "3      A quantidade de dinheiro que recebi não corres...     75\n",
              "4            Meu cartão ainda não chegou. O que eu faço?     11\n",
              "...                                                  ...    ...\n",
              "9998   Quanto tempo até um cheque eu mandar autorização?     58\n",
              "9999   Como faço com transferências bancárias para as...     65\n",
              "10000  O que significa ter uma retirada pendente de d...     46\n",
              "10001  Por que a transferência de dinheiro ainda não ...     66\n",
              "10002  O saldo não foi atualizado após uma transferên...      5\n",
              "\n",
              "[10003 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEyuf4gV5Q0u"
      },
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV8YvseY5SsV"
      },
      "source": [
        "def get_banking77_tokens(data, label, maxlen):\n",
        "    ids, labels = [], []\n",
        "    for text, label in zip(data, label):\n",
        "        tokens = tokenizer.encode_plus(\n",
        "            text=text,\n",
        "            truncation=True, \n",
        "            max_length=maxlen,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        labels.append(int(label))\n",
        "        ids.append(tokens['input_ids'])\n",
        "\n",
        "    ids = torch.vstack(ids)\n",
        "    return ids, torch.tensor(labels)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXP9WlnWBctt",
        "outputId": "b90b9631-ca86-4bba-de64-cef8cafdf476"
      },
      "source": [
        "# artigo: https://arxiv.org/pdf/2003.04807.pdf\n",
        "\n",
        "BSIZE = 16\n",
        "MAX_LEN = 32\n",
        "path_model = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(path_model)\n",
        "\n",
        "# Train\n",
        "texts_train  = data_train.Data.to_list()\n",
        "labels_train = data_train.Label.to_list()    \n",
        "train_texts, train_labels = get_banking77_tokens(texts_train, labels_train, MAX_LEN)\n",
        "ds_train = TensorDataset(train_texts, train_labels)\n",
        "\n",
        "# Test\n",
        "texts_test  = data_test.Data.to_list()\n",
        "labels_test = data_test.Label.to_list() \n",
        "test_texts, test_labels = get_banking77_tokens(texts_test, labels_test, MAX_LEN)\n",
        "ds_test = TensorDataset(test_texts, test_labels)\n",
        "\n",
        "# Dataloaders\n",
        "dataloader = {\n",
        "    'train': DataLoader(\n",
        "        dataset=ds_train,\n",
        "        batch_size=BSIZE,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=os.cpu_count()\n",
        "         ),\n",
        "    'test': DataLoader(\n",
        "        dataset=ds_test,\n",
        "        batch_size=BSIZE,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=os.cpu_count()\n",
        "         )\n",
        "}\n",
        "\n",
        "# - - - - -\n",
        "_ = {_:len(dataloader[_]) for _ in dataloader.keys()}\n",
        "print(_)\n",
        "ids_batch, label_batch = next(iter(dataloader['train']))\n",
        "ids_batch[0], label_batch[0]"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'train': 626, 'test': 193}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  101,  1051,  2072,  1010,  9311,  7416,  2033, 22450,  2099, 21877,\n",
              "          2721,  3539,  7895,  2310,  2480,  7570,  6460,  1006,  2061,  2226,\n",
              "          8529, 24576,  7396,  2063,  1007,  1010,  1041,  1037,  9099, 19629,\n",
              "          2080,   102]), tensor(47))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTKYQcFEOdp8",
        "outputId": "02051d1b-5475-47f0-f7a1-8bbcc14fd518"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    path_model, \n",
        "    num_labels=data_train.Label.nunique(), \n",
        "    return_dict=True\n",
        "    )\n",
        "\n",
        "# - - - - -\n",
        "outs = model(input_ids=ids_batch, labels=label_batch)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ8w7kgQTaAl",
        "outputId": "a09205a4-6899-4779-dbbb-4ba39c810978"
      },
      "source": [
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def train(model, loader, optim, scheduler):\n",
        "    total_acc, total_f1, total_loss = 0,0,0\n",
        "    model.train()\n",
        "    for batch in loader:\n",
        "        model.zero_grad()\n",
        "        \n",
        "        batch_ids, batch_label = (b.to('cuda') for b in batch)\n",
        "        \n",
        "        outs = model(input_ids=batch_ids, labels=batch_label)\n",
        "        \n",
        "        loss = outs['loss']\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        logits = torch.nn.functional.softmax(outs['logits'].cpu().detach(), dim=-1).numpy()\n",
        "        y_pred = np.argmax(logits, axis=1)\n",
        "        total_f1  += f1_score(y_true=batch_label.cpu(), y_pred=y_pred, average='macro')\n",
        "        total_acc += accuracy_score(y_true=batch_label.cpu(), y_pred=y_pred)\n",
        "    \n",
        "    avg_loss = total_loss/len(loader)\n",
        "    avg_f1 = total_f1/len(loader)\n",
        "    avg_acc = total_acc/len(loader)\n",
        "\n",
        "    return avg_loss, avg_f1, avg_acc\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    total_acc, total_f1, total_loss = 0,0,0\n",
        "    model.eval()\n",
        "    for batch in loader:\n",
        "        model.zero_grad()\n",
        "        \n",
        "        batch_ids, batch_label = (b.to('cuda') for b in batch)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outs = model(input_ids=batch_ids, labels=batch_label)\n",
        "        \n",
        "        loss = outs['loss']\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        logits = torch.nn.functional.softmax(outs['logits'].cpu().detach(), dim=-1).numpy()\n",
        "        y_pred = np.argmax(logits, axis=1)\n",
        "        total_f1  += f1_score(y_true=batch_label.cpu(), y_pred=y_pred, average='macro')\n",
        "        total_acc += accuracy_score(y_true=batch_label.cpu(), y_pred=y_pred)\n",
        "    \n",
        "    avg_loss = total_loss/len(loader)\n",
        "    avg_f1 = total_f1/len(loader)\n",
        "    avg_acc = total_acc/len(loader)\n",
        "\n",
        "    return avg_loss, avg_f1, avg_acc\n",
        "\n",
        "# - - - - -\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    path_model, \n",
        "    num_labels=data_train.Label.nunique(), \n",
        "    return_dict=True\n",
        "    ).to(device)\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
        "\n",
        "N_EPOCHS = 3\n",
        "\n",
        "total_steps = len(loader) * N_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optim, \n",
        "    num_warmup_steps = int(0.02 * total_steps), # 2% of warmup\n",
        "    num_training_steps = total_steps\n",
        "    )\n",
        "\n",
        "training_stats = []\n",
        "# ----------------------------------------------------------------------------------------\n",
        "for idx in range(N_EPOCHS):\n",
        "    loss_train, f1_train, acc_train = train(\n",
        "        model, \n",
        "        dataloader['train'], \n",
        "        optim, \n",
        "        scheduler=scheduler\n",
        "        )\n",
        "    loss_eval,  f1_eval,  acc_eval = evaluate(\n",
        "        model, \n",
        "        dataloader['test']\n",
        "        )\n",
        "    \n",
        "    print(f'Epoch {idx}/{N_EPOCHS-1}:')\n",
        "    print(f'\\tTrain: Loss: {loss_train:<5.3} -- F1: {f1_train:<5.3} -- ACC: {acc_train:.3}')\n",
        "    print(f'\\tEval:  Loss: {loss_eval:<5.3} -- F1: {f1_eval:<5.3} -- ACC: {acc_eval:.3}')\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': idx,\n",
        "            'Training Loss': loss_train,\n",
        "            'Training Acc': acc_train,\n",
        "            'Training F1': f1_train,\n",
        "            'Eval Loss': loss_eval,\n",
        "            'Eval Acc': acc_eval,\n",
        "            'Eval F1': f1_eval,\n",
        "        }\n",
        "    )"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/2:\n",
            "\tTrain: Loss: 3.3   -- F1: 0.149 -- ACC: 0.226\n",
            "\tEval:  Loss: 1.93  -- F1: 0.386 -- ACC: 0.529\n",
            "Epoch 1/2:\n",
            "\tTrain: Loss: 1.33  -- F1: 0.555 -- ACC: 0.693\n",
            "\tEval:  Loss: 0.972 -- F1: 0.634 -- ACC: 0.76\n",
            "Epoch 2/2:\n",
            "\tTrain: Loss: 0.679 -- F1: 0.741 -- ACC: 0.84\n",
            "\tEval:  Loss: 0.652 -- F1: 0.725 -- ACC: 0.828\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "z3teY59KNo0h",
        "outputId": "e15b0b17-570e-4122-a832-0bfe3ca56572"
      },
      "source": [
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "pd.set_option('precision', 3)\n",
        "df_stats"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Training Acc</th>\n",
              "      <th>Training F1</th>\n",
              "      <th>Eval Loss</th>\n",
              "      <th>Eval Acc</th>\n",
              "      <th>Eval F1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.299</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.149</td>\n",
              "      <td>1.928</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.326</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.555</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.679</td>\n",
              "      <td>0.840</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.652</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.725</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Training Acc  Training F1  Eval Loss  Eval Acc  Eval F1\n",
              "epoch                                                                        \n",
              "0              3.299         0.226        0.149      1.928     0.529    0.386\n",
              "1              1.326         0.693        0.555      0.972     0.760    0.634\n",
              "2              0.679         0.840        0.741      0.652     0.828    0.725"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    }
  ]
}