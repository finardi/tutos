{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1NBUwF34ekMNFySX6shEfuGlIfORHPYto",
      "authorship_tag": "ABX9TyMw6C/e0ouIZ0K0SgEnUe3Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/Transformers_2023_Flash_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ú® 1a parte: apresenta√ß√£o do PROBLEMA.\n",
        "- ##  Na arquitetura Transformers, a self-attention, em sua implementa√ß√£o padr√£o tem custo $\\mathcal{O}(N^2)$, onde $N$ √© o comprimento da sequ√™ncia / entrada do modelo / context-window.\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "# Qual a relev√¢ncia?\n",
        "\n",
        "- ### De 2017 at√© 2020/2021 maioria dos Transformers para texto tinham entrada de 512 tokens.\n",
        "- ### Em 2023 temos GPT4 com 32K tokens, Claude com 100K tokens, LLaMa2 com 4096 tokens  de context window... *O livro The Great Gatsby tem 72K tokens, 210 pages*.\n",
        "> ### Algumas t√©cnicas:\n",
        "> 1. ### [ALiBi](https://arxiv.org/abs/2108.12409) $\\rightarrow$ Voc√™ pode treinar em 2K e fazer finetune em 100K tokens\n",
        "> 2. ### [Multi Query Attention](https://arxiv.org/pdf/1911.02150.pdf) $\\rightarrow$ A  Multi-Head Attention √© calculada em batches.\n",
        "> 3. ### Flash Attention $\\rightarrow$ essa apresenta√ß√£o\n",
        "\n",
        "## Em termos de moneys... ü§ë\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> ## *O LLaMa 65Bi tem custo estimado em ~ 3Mi em GPU, com contexto de $2048$ tokens. O custo para o mesmo LlaMA com $100$k context-window custaria ~150Mi* üí∏ ([fonte](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c)).\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Solu√ß√µes j√° abordadas:\n",
        "> ### [Artigo: Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)\n",
        "> <img src=\"https://drive.google.com/uc?id=1eFvomqtNrGHz8eEEbF8NJo1qbLz7p5Pd\" alt=\"drawing\" width=\"800\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DLcgYlNDShOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Linformer](https://arxiv.org/abs/2006.04768)\n",
        "<img src=\"https://drive.google.com/uc?id=1BsOOaxA5JhVyx4K6bqqb66hKFDmmNX0g\" alt=\"drawing\" width=\"800\"/>\n",
        "\n",
        "<!-- ## [Performer](https://arxiv.org/abs/2009.14794)\n",
        "### [Blog sobre Performer](https://chiaracampagnola.io/2020/10/29/from-transformers-to-performers/)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1xNmdUaMHWoCd4vXDdWm04tDexw_6U1O6\" alt=\"drawing\" width=\"200\" height=\"228\"/>\n",
        "<img src=\"https://drive.google.com/uc?id=1e7oiJsyZmxnazCUyGE8Et4M6_buKuQyC\" alt=\"drawing\" width=\"600\"/> -->\n",
        "\n",
        "\n",
        "## [Reformer](https://arxiv.org/abs/2001.04451)\n",
        "<img src=\"https://drive.google.com/uc?id=1LTXOUkGVAUAdOMApu7MVNp3GFwb3l8_J\" alt=\"drawing\" width=\"800\"/>\n",
        "\n",
        "## [BigBird](https://arxiv.org/abs/2007.14062)\n",
        "### [Blog sobre BigBird](https://huggingface.co/blog/big-bird)\n",
        "\n",
        "<!-- <img src=\"https://drive.google.com/uc?id=1jVnFPJek2TRhriy_yg3EkxCRI4uyjGpm\" alt=\"drawing\" width=\"400\" height=\"250\"/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1S1T26KbwhQ-bmE84p7Ue9lyE1LURJFre\" alt=\"drawing\" width=\"400\" height=\"250\"/> -->\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1RvzNUaXMPuvfgzd8cSOW-aU62dRt9jrP\" alt=\"drawing\" width=\"400\" height=\"276.5\"/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=12xz_X3ZLT81JRYbIuMDSU9h8tEe-kmaj\" alt=\"drawing\" width=\"400\"/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1kQUgdRx22Nr7NJGA25tls2nR5A07RTms\" alt=\"drawing\" width=\"802\"/>\n",
        "\n",
        "## Resultados: [Long Range Arena: A Benchmark for efficient transformers](https://arxiv.org/pdf/2011.04006.pdf)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ruBhlaUSqIrFrD4k-K3d2rqZedSHMgEO\" alt=\"drawing\" width=\"802\"/>\n",
        "\n",
        "### [long-range-arena benchmark](https://github.com/google-research/long-range-arena)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yQpS6BL3HFjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Em geral, as abordagens para o problema focam na redu√ß√£o de FLOPs e tendem a ignorar acessos √† mem√≥ria (**IO**).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1W_bwioQZ5ZO5Z5ex1wW0h_t6xhoTMon9\" alt=\"drawing\" width=\"800\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "9eT1dOgvgbHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ú® 2a parte: Otimiza√ß√£o Deep-Learning.\n",
        "\n",
        "\n",
        "- ## Perspectiva (**inocente**): *Se eu pagar por **312** teraflops de computa√ß√£o, idealmente eu vou ter **312** teraflops.*\n",
        "---\n",
        "\n",
        "### Fonte: [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html)\n",
        "\n",
        "> #### Se a computa√ß√£o √© como uma f√°brica... Enviamos instru√ß√µes para nossa f√°brica (**overhead**), enviamos materiais (**memory-bandwidth**), tudo para manter nossa f√°brica funcionando com efici√™ncia (**compute**).\n",
        "<img src=\"https://drive.google.com/uc?id=1eQio68eJf7OJjrL5tvJBn-b2PxYYW9Mq\" alt=\"drawing\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "> #### Se nossa f√°brica aumenta a efici√™ncia mais rapidamente do que a taxa na qual podemos fornecer materiais, torna-se mais dif√≠cil para nossa f√°brica atingir sua efici√™ncia m√°xima.\n",
        "<img src=\"https://drive.google.com/uc?id=1fsgkuvQcb_Yi99sig16daj66U1B8AKbT\" alt=\"drawing\" width=\"600\" height=\"300\"/>\n",
        ">\n",
        "> #### Mesmo que o tamanho de nossa f√°brica (FLOPS) tenha dobrado - se nossa largura de banda (**bandwidth**) n√£o puder acompanhar, nosso desempenho tamb√©m n√£o dobrar√°.\n",
        "\n",
        "> #### GPUs t√™m hardware especializado para multiplica√ß√£o de matrizes, como os \"Tensor Cores\" da Nvidia.\n",
        "<img src=\"https://drive.google.com/uc?id=104GWauf8HugTLTuVCHv_Nw817wL9XFIj\" alt=\"drawing\" width=\"600\" height=\"300\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> ### Artigo: [Data Movement Is All You Need: A Case Study on Optimizing Transformers](https://arxiv.org/abs/2007.00072)\n",
        "> <img src=\"https://drive.google.com/uc?id=1lZbEEAVWcXA0nV8kJ3ZiGtVR1NfIq7ky\" alt=\"drawing\" width=\"800\"/>\n",
        ">\n",
        "> #### Stat. normalization e Element-wise atingem **250x** menos FLOPS e **700x** menos FLOPS do que tensor contraction (matmuls), respectivamente.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytCp7W9r6Vhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§î Por que as opera√ß√µes \"n√£o matmul\" levam muito mais tempo do que deveriam‚ùì\n",
        "- ### Voltando √† nossa analogia, o culpado geralmente √© o tempo que leva para transportar os materiais da e para a f√°brica. Em outras palavras, a largura de banda da mem√≥ria.\n",
        "\n",
        "\n",
        "> <img src=\"https://drive.google.com/uc?id=1IPyU22at2Tb_uqRiVxq2SRYJpMYWdu-d\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "- ### Os custos de badwidhth s√£o custos de mover dados da CPU para a GPU, de um n√≥ para outro ou at√© mesmo da mem√≥ria global CUDA para a mem√≥ria compartilhada CUDA (***memory bandwidth cost***).\n",
        "- ### A f√°brica n√£o deve ser usada como unidade de armazenamento, ela √© eficiente para trabalho r√°pido e pequeno (**SRAM**).\n",
        "- ### Tipicamente, o resultado √© armazenado em algum armazem (**warehouse**): um local \"mais barato\" e com muito espa√ßo (**DRAM** `nvidia-smi`). Dessa forma, podemos enviar e receber dados  da f√°brica (**memory bandwidth**)\n",
        "- ### Toda vez que executamos uma opera√ß√£o no kernel da GPU: precisamos mover o dado da  (**DRAM**).\n",
        "- ### Por exemplo, a opera√ß√£o un√°ria `torch.cos`: precisa enviar os dados para a warehouse, executar o c√°lculo para cada parte dos dados e, em seguida, enviar esse dado de volta. Como resultado, quase todo o nosso tempo aqui √© gasto no transporte de dados, e n√£o na computa√ß√£o.\n",
        "- ### Como estamos gastando todo o nosso tempo na largura de banda da mem√≥ria, essa opera√ß√£o √© chamada de opera√ß√£o **memory-bound** e significa que n√£o estamos gastando muito tempo em computa√ß√£o.\n",
        "\n"
      ],
      "metadata": {
        "id": "bOGNc1LD6eNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ok..., como perder menos tempo no transporte de dados da **DRAM** para a **SRAM**?\n",
        "\n",
        "> <img src=\"https://drive.google.com/uc?id=1krlAoSySz-KN1aCR-5FIcdcrwZEvrD-o\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "### Operator fusion\n",
        "> <img src=\"https://drive.google.com/uc?id=1TYoynirPaCv32wcEQ5q0jGflpEQ7anRi\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "\n",
        "# Overhead\n",
        "> ### Overhead √© quando seu c√≥digo est√° gastando tempo fazendo qualquer coisa que n√£o seja transferir tensores ou computar coisas.\n",
        "\n",
        "\n",
        "### Python √© lento...\n",
        "> #### *Uma GPU A100 pode executar 312 trilh√µes de opera√ß√µes de ponto flutuante por segundo (312 TeraFLOPS). Em compara√ß√£o,  pode realizar 32 milh√µes de adi√ß√µes em um segundo. Isso significa que, no tempo em que o Python pode executar um **√∫nico FLOP**, um A100 poderia ter executado **9,75 milh√µes de FLOPS**.*"
      ],
      "metadata": {
        "id": "0QMstvSM6eKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Operator Fusion\n",
        "> ### lib para escrever CUDA kernels: [Introducing Triton: Open-source GPU programming for neural networks](https://openai.com/research/triton)\n"
      ],
      "metadata": {
        "id": "wkWTePQLkkTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.randn(1,256)\n",
        "x.to(\"cuda\")\n",
        "\n",
        "# 4 global reads / writes\n",
        "x1 = x.cos() # l√™ x da mem√≥ria global, escreve em x1\n",
        "x2 = x1.cos() # l√™ x1 da mem√≥ria global, escreve em x2\n",
        "\n",
        "# 2 global reads / writes\n",
        "x2 = x.cos().cos() # l√™ x da mem√≥ria global, escreve em x2"
      ],
      "metadata": {
        "id": "XlGbbS-_6eG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ú® 3a parte: Flash Attention $\\;ELI\\,5$\n",
        "\n",
        "> ### [Artigo](https://arxiv.org/pdf/2205.14135.pdf)\n",
        "> #### Blogpost [Aleksa Gordiƒá](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) üí™\n",
        "> #### Blogpost [Memory Wall](https://www.semianalysis.com/p/nvidiaopenaitritonpytorch#%C2%A7the-memory-wall)\n",
        ">\n",
        "> <img src=\"https://drive.google.com/uc?id=139OP2I95uLUDxBsNkMimLooJYbpKb0sk\" alt=\"drawing\" width=\"1200\"/>\n",
        "---\n",
        "## $Fast$:\n",
        "> <img src=\"https://drive.google.com/uc?id=1oc5aS_gMsrLchYVtzd1cAW5Z3dIvkxxt\" alt=\"drawing\" width=\"1200\"/>\n",
        "\n",
        "\n",
        "## $Memory-Efficient$:\n",
        "> <img src=\"https://drive.google.com/uc?id=1Wnrv3_OTH1-M_iyoVVDqa5-grZe0AhFx\" alt=\"drawing\" width=\"1200\"/>\n",
        "\n",
        "## $Exact$:\n",
        "> <img src=\"https://drive.google.com/uc?id=1oJnXiC51fbSyhgAUJzDsogb5-X_zik5v\" alt=\"drawing\" width=\"1200\"/>\n",
        "\n",
        "## $IO-Awareness$:\n",
        "> <img src=\"https://drive.google.com/uc?id=1or-Jen81qLiLdDuReoX2Jw68w7atWLdN\" alt=\"drawing\" width=\"1200\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "0q3yhtMO6eCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voltando ao FLOPs...\n",
        "\n",
        "> <img src=\"https://drive.google.com/uc?id=1U23BXqEi74xHIGWB5XiQjBFe4B92E7wg\" alt=\"drawing\" width=\"800\" height=\"400\"/>\n",
        "---\n",
        "## ü•¥ Em Transformers, somente computa√ß√£o em velocidade de **exaFLOPs** n√£o resolve! ü•¥\n",
        "\n",
        "## *IO-aware* matters!\n",
        "- ## compute-bound\n",
        " - ### `matmul`\n",
        "- ## memory-bound\n",
        " - ### `elementwise ops`: activation, dropout, masking\n",
        " - ### `reduction ops`: softmax, layer norm, sum, etc.\n",
        "---\n",
        "> <img src=\"https://drive.google.com/uc?id=1SmzCoBF3RO3v7eq5lUP2X9StGFAvAN_f\" alt=\"drawing\" width=\"800\" height=\"600\"/>\n",
        ">\n",
        "\n",
        "> ### A mem√≥ria n√£o √© um artefato monol√≠tico, √© hier√°rquica em sua natureza e a regra geral √©: *quanto mais r√°pida a mem√≥ria, mais cara ela √© e menor sua capacidade‚ùó*\n",
        "> <img src=\"https://drive.google.com/uc?id=17vUgAZ_rKdscBENONmDbFnvTz2kr6l_X\" alt=\"drawing\" width=\"800\"/>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_zB4K3CUmaOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Attention:\n",
        "<img src=\"https://drive.google.com/uc?id=1OplMKDXvH_TtGLwbuO6O1t6PdUuX9PQV\" alt=\"drawing\" width=\"1200\"/>\n",
        "\n",
        "## Notation:\n",
        "- Q ‚Äî queries\n",
        "- K ‚Äî keys\n",
        "- V ‚Äî values\n",
        "- S ‚Äî scores\n",
        "- P ‚Äî probabilities\n",
        "- O ‚Äî outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "5AgAZq3_maVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "B = 1 # batch size\n",
        "T = block_size = 4 # sequence length\n",
        "C = n_embed = 6 # embedding dimensionality (n_embed)\n",
        "n_head = 2 # num. heads\n",
        "\n",
        "x = torch.randn(B, T, C)\n",
        "c_attn = nn.Linear(C, 3*C)\n",
        "\n",
        "q, k ,v  = c_attn(x).split(C, dim=2)\n",
        "print(q.shape, k.shape, v.shape)"
      ],
      "metadata": {
        "id": "7BzOpDZumaYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe9b4c7-771b-43ff-e4ec-33a4dae79629"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 6]) torch.Size([1, 4, 6]) torch.Size([1, 4, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q = q.view(B, T, n_head, C//n_head).transpose(1, 2) # (B, n_head, T, head_size)\n",
        "K = k.view(B, T, n_head, C//n_head).transpose(1, 2) # (B, n_head, T, head_size)\n",
        "V = v.view(B, T, n_head, C//n_head).transpose(1, 2) # (B, n_head, T, head_size)\n",
        "\n",
        "print(Q.shape, K.shape, V.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQJsWTz1Ulfv",
        "outputId": "4594467f-da8d-4a0c-b32d-3ba3f536702c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3]) torch.Size([1, 2, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# S=QK^T\n",
        "S = torch.einsum(\"...th, ...lh -> ...tl\", Q, K)  # (B, n_head, T, T)\n",
        "S.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stsHEuoNdHK3",
        "outputId": "60935b9c-c4d6-49e6-f7ae-1b8797696c1d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √â causal? Decoder?\n",
        "Q_LEN = Q.shape[2]\n",
        "K_LEN = K.shape[2]\n",
        "causal_mask = torch.triu(torch.ones((Q_LEN, K_LEN)), K_LEN - Q_LEN + 1) # principal diag. will not be consider with +1\n",
        "S = torch.where(causal_mask > 0, 1e-9, S)\n",
        "S[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIRAdduBdXX-",
        "outputId": "1b2d9f89-fb74-4b9f-f8f1-3012b45c3a26"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.6420e-02,  1.0000e-09,  1.0000e-09,  1.0000e-09],\n",
              "        [ 1.5213e+00,  1.1006e+00,  1.0000e-09,  1.0000e-09],\n",
              "        [ 1.1597e+00, -2.5399e-01,  1.2816e+00,  1.0000e-09],\n",
              "        [ 9.3451e-01,  1.0825e+00,  9.9049e-01,  1.1979e+00]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probabilities P\n",
        "P = nn.functional.softmax(S, dim=-1)\n",
        "P[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGI3phB6ebi4",
        "outputId": "76a830f3-8ab8-4353-8c90-2789d1c62e58"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2469, 0.2510, 0.2510, 0.2510],\n",
              "        [0.4777, 0.3136, 0.1043, 0.1043],\n",
              "        [0.3722, 0.0905, 0.4205, 0.1167],\n",
              "        [0.2213, 0.2566, 0.2341, 0.2880]], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Outputs\n",
        "O = P@V # B,n_head,T,T X B,n_head,T,head_size ---> B,n_head,T,head_size\n",
        "O = O.transpose(1, 2).contiguous().view(B, T, C) # B,T,(n_head, head_size) ---> B,T,C\n",
        "O.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93xp1cDafCGy",
        "outputId": "6386fabf-55ea-48e7-d717-801448094ecb"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 4, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1OPiV2tLaIATK5rxudn-xepAym573C0Nb\" alt=\"drawing\" width=\"1200\"/>\n"
      ],
      "metadata": {
        "id": "oS-eDCMfbMo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "BLOCK_SIZE = T\n",
        "NEG_INF = -1e9 # -infinity\n",
        "EPSILON = 1e-10\n",
        "\n",
        "def flash_attention_causal_forward(Q, K, V):\n",
        "    O = torch.zeros_like(Q, requires_grad=True)\n",
        "    l = torch.zeros(Q.shape[:-1])[...,None]\n",
        "    m = torch.ones(Q.shape[:-1])[...,None] * NEG_INF\n",
        "\n",
        "    O = O.to(device='cuda')\n",
        "    l = l.to(device='cuda')\n",
        "    m = m.to(device='cuda')\n",
        "\n",
        "    Q_BLOCK_SIZE = min(BLOCK_SIZE, Q.shape[-1])\n",
        "    KV_BLOCK_SIZE = BLOCK_SIZE\n",
        "\n",
        "    Q_BLOCKS = torch.split(Q, Q_BLOCK_SIZE, dim=2)\n",
        "    K_BLOCKS = torch.split(K, KV_BLOCK_SIZE, dim=2)\n",
        "    V_BLOCKS = torch.split(V, KV_BLOCK_SIZE, dim=2)\n",
        "\n",
        "    Tr = len(Q_BLOCKS)\n",
        "    Tc = len(K_BLOCKS)\n",
        "\n",
        "    O_BLOCKS = list(torch.split(O, Q_BLOCK_SIZE, dim=2))\n",
        "    l_BLOCKS = list(torch.split(l, Q_BLOCK_SIZE, dim=2))\n",
        "    m_BLOCKS = list(torch.split(m, Q_BLOCK_SIZE, dim=2))\n",
        "\n",
        "    Q_LEN = Q.shape[2]\n",
        "    K_LEN = K.shape[2]\n",
        "\n",
        "    Q_RANGE = torch.arange(Q_LEN)[:,None] + (K_LEN - Q_LEN)\n",
        "    K_RANGE = torch.arange(K_LEN)[None,:]\n",
        "\n",
        "    Q_RANGE = Q_RANGE.to(device='cuda')\n",
        "    K_RANGE = K_RANGE.to(device='cuda')\n",
        "\n",
        "    Q_RANGE_BLOCKS = torch.split(Q_RANGE, Q_BLOCK_SIZE, dim=0)\n",
        "    K_RANGE_BLOCKS = torch.split(K_RANGE, KV_BLOCK_SIZE, dim=1)\n",
        "\n",
        "    for j in range(Tc):\n",
        "        Kj = K_BLOCKS[j]\n",
        "        Vj = V_BLOCKS[j]\n",
        "        K_RANGE_BLOCKSj = K_RANGE_BLOCKS[j]\n",
        "\n",
        "        for i in range(Tr):\n",
        "            Qi = Q_BLOCKS[i]\n",
        "            Oi = O_BLOCKS[i]\n",
        "            li = l_BLOCKS[i]\n",
        "            mi = m_BLOCKS[i]\n",
        "            Q_RANGE_BLOCKSi = Q_RANGE_BLOCKS[i]\n",
        "\n",
        "            scale = 1 / np.sqrt(Q.shape[-1])\n",
        "            Qi_scaled  = Qi * scale\n",
        "\n",
        "            S_ij = torch.einsum('...id, ...jd -> ...ij', Qi_scaled, Kj)\n",
        "\n",
        "            # Masking\n",
        "            causal_mask = Q_RANGE_BLOCKSi >= K_RANGE_BLOCKSj\n",
        "            S_ij = torch.where(causal_mask > 0, S_ij, NEG_INF)\n",
        "\n",
        "            m_block_ij, _ = torch.max(S_ij, dim=-1, keepdims=True)\n",
        "            P_ij = torch.exp(S_ij - m_block_ij)\n",
        "            # Masking\n",
        "            P_ij = torch.where(causal_mask > 0, P_ij, 0)\n",
        "\n",
        "            l_block_ij = torch.sum(P_ij, dim=-1, keepdims=True) + EPSILON\n",
        "\n",
        "            P_ij_Vj = torch.einsum('... i j, ... j d -> ... i d', P_ij, Vj)\n",
        "\n",
        "            mi_new = torch.maximum(m_block_ij, mi)\n",
        "            li_new = torch.exp(mi - mi_new) * li + torch.exp(m_block_ij - mi_new) * l_block_ij\n",
        "\n",
        "            O_BLOCKS[i] = (li/li_new) * torch.exp(mi - mi_new) * Oi + (torch.exp(m_block_ij - mi_new) / li_new) * P_ij_Vj\n",
        "            l_BLOCKS[i] = li_new\n",
        "            m_BLOCKS[i] = mi_new\n",
        "\n",
        "    O = torch.cat(O_BLOCKS, dim=2)\n",
        "    l = torch.cat(l_BLOCKS, dim=2)\n",
        "    m = torch.cat(m_BLOCKS, dim=2)\n",
        "    return O, l, m\n",
        "\n",
        "def flash_attention_causal_backward(Q, K, V, O, l, m, dO):\n",
        "    Q_BLOCK_SIZE = min(BLOCK_SIZE, Q.shape[-1])\n",
        "    KV_BLOCK_SIZE = BLOCK_SIZE\n",
        "\n",
        "    Q_BLOCKS = torch.split(Q, Q_BLOCK_SIZE, dim=2)\n",
        "    K_BLOCKS = torch.split(K, KV_BLOCK_SIZE, dim=2)\n",
        "    V_BLOCKS = torch.split(V, KV_BLOCK_SIZE, dim=2)\n",
        "\n",
        "    Tr = len(Q_BLOCKS)\n",
        "    Tc = len(K_BLOCKS)\n",
        "\n",
        "    O_BLOCKS = list(torch.split(O, Q_BLOCK_SIZE, dim=2))\n",
        "    dO_BLOCKS = list(torch.split(dO, Q_BLOCK_SIZE, dim=2))\n",
        "    l_BLOCKS = list(torch.split(l, Q_BLOCK_SIZE, dim=2))\n",
        "    m_BLOCKS = list(torch.split(m, Q_BLOCK_SIZE, dim=2))\n",
        "\n",
        "    dQ = torch.zeros_like(Q, requires_grad=True).to(device='cuda')\n",
        "    dK = torch.zeros_like(K, requires_grad=True).to(device='cuda')\n",
        "    dV = torch.zeros_like(V, requires_grad=True).to(device='cuda')\n",
        "\n",
        "    dQ_BLOCKS = list(torch.split(dQ, Q_BLOCK_SIZE, dim=2))\n",
        "    dK_BLOCKS = list(torch.split(dK, KV_BLOCK_SIZE, dim=2))\n",
        "    dV_BLOCKS = list(torch.split(dV, KV_BLOCK_SIZE, dim=2))\n",
        "\n",
        "    Q_LEN = Q.shape[2]\n",
        "    K_LEN = K.shape[2]\n",
        "\n",
        "    Q_RANGE = torch.arange(Q_LEN)[:,None] + (K_LEN - Q_LEN)\n",
        "    K_RANGE = torch.arange(K_LEN)[None,:]\n",
        "\n",
        "    Q_RANGE = Q_RANGE.to(device='cuda')\n",
        "    K_RANGE = K_RANGE.to(device='cuda')\n",
        "\n",
        "    Q_RANGE_BLOCKS = torch.split(Q_RANGE, Q_BLOCK_SIZE, dim=0)\n",
        "    K_RANGE_BLOCKS = torch.split(K_RANGE, KV_BLOCK_SIZE, dim=1)\n",
        "\n",
        "    for j in range(Tc):\n",
        "        Kj = K_BLOCKS[j]\n",
        "        Vj = V_BLOCKS[j]\n",
        "        K_RANGE_BLOCKSj = K_RANGE_BLOCKS[j]\n",
        "\n",
        "        dKj_block = torch.zeros_like(dK_BLOCKS[j], requires_grad=True).to(device='cuda')\n",
        "        dVj_block = torch.zeros_like(dV_BLOCKS[j], requires_grad=True).to(device='cuda')\n",
        "\n",
        "        for i in range(Tr):\n",
        "            Qi = Q_BLOCKS[i]\n",
        "            Oi = O_BLOCKS[i]\n",
        "            dOi = dO_BLOCKS[i]\n",
        "            li = l_BLOCKS[i]\n",
        "            mi = m_BLOCKS[i]\n",
        "            Q_RANGE_BLOCKSi = Q_RANGE_BLOCKS[i]\n",
        "\n",
        "            scale = 1 / np.sqrt(Q.shape[-1])\n",
        "            Qi_scaled  = Qi * scale\n",
        "\n",
        "            S_ij = torch.einsum('... i d, ... j d -> ... i j', Qi_scaled, Kj)\n",
        "\n",
        "            # Masking\n",
        "            causal_mask = Q_RANGE_BLOCKSi >= K_RANGE_BLOCKSj\n",
        "            S_ij = torch.where(causal_mask > 0, S_ij, NEG_INF)\n",
        "\n",
        "            P_ij = (1/li) * torch.exp(S_ij - mi)\n",
        "            # Masking\n",
        "            P_ij = torch.where(causal_mask > 0, P_ij, 0)\n",
        "\n",
        "            dVj_block = dVj_block + torch.einsum('... r c, ... r d -> ... c d', P_ij, dOi)\n",
        "            dP_ij = torch.einsum('... r d, ... c d -> ... r c', dOi, Vj)\n",
        "\n",
        "            Di = torch.sum(dOi * Oi, dim=-1, keepdims=True)\n",
        "            dS_ij = P_ij * (dP_ij - Di)\n",
        "\n",
        "            dQ_BLOCKS[i] = dQ_BLOCKS[i] + scale * torch.einsum('... r c, ... c d -> ... r d', dS_ij, Kj)\n",
        "\n",
        "            dKj_block = dKj_block + scale * torch.einsum('... r c, ... r d -> ... c d', dS_ij, Qi)\n",
        "\n",
        "        dK_BLOCKS[j] = dKj_block\n",
        "        dV_BLOCKS[j] = dVj_block\n",
        "\n",
        "    dQ = torch.cat(dQ_BLOCKS, dim=2)\n",
        "    dK = torch.cat(dK_BLOCKS, dim=2)\n",
        "    dV = torch.cat(dV_BLOCKS, dim=2)\n",
        "    return dQ, dK, dV\n",
        "\n",
        "def flash_attention_causal(Q, K, V):\n",
        "    out = flash_attention_causal_forward(Q, K, V)\n",
        "    return out[0]\n",
        "\n",
        "def normal_attention_causal(Q, K, V):\n",
        "    scale = 1 / np.sqrt(Q.shape[-1])\n",
        "    Q = Q * scale\n",
        "    QKt = torch.einsum('... i d, ... j d -> ... i j', Q, K)\n",
        "\n",
        "    Q_LEN = Q.shape[2]\n",
        "    K_LEN = K.shape[2]\n",
        "\n",
        "    causal_mask = torch.triu(torch.ones((Q_LEN, K_LEN)), K_LEN - Q_LEN + 1)\n",
        "    causal_mask = causal_mask.to(device='cuda')\n",
        "    QKt = torch.where(causal_mask > 0, NEG_INF, QKt)\n",
        "\n",
        "    attn = nn.functional.softmax(QKt, dim=-1)\n",
        "    return attn @ V"
      ],
      "metadata": {
        "id": "sLY8ojh7GjC-"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    flash_attn = flash_attention_causal(Q.to('cuda'), K.to('cuda'), V.to('cuda'))\n",
        "\n",
        "    normal_attn = normal_attention_causal(Q.to('cuda'), K.to('cuda'), V.to('cuda'))\n",
        "\n",
        "    print(torch.allclose(flash_attn, normal_attn), end=' ---> ')"
      ],
      "metadata": {
        "id": "f-bvBxcHGi_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d4dbcb-9002-4b00-b364-cab5d5fbb952"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True ---> True ---> True ---> True ---> True ---> True ---> True ---> True ---> True ---> True ---> "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Causal mask FlashAttention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7er1gdb3QlU9",
        "outputId": "9cd89119-fbc9-4c21-8ba2-3c91e8c9378d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "466.40176ms\n",
            "STAGE:2023-06-25 14:50:39 787:787 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
            "STAGE:2023-06-25 14:50:40 787:787 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
            "STAGE:2023-06-25 14:50:40 787:787 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::where         5.59%      28.796ms        19.97%     102.810ms      50.200us      30.418ms         8.27%      70.600ms      34.473us           0 b           0 b       8.00 Gb     103.64 Mb          2048            --  \n",
            "                                          aten::resize_         0.87%       4.484ms         0.87%       4.484ms       4.370us       0.000us         0.00%       0.000us       0.000us     256.00 Kb     256.00 Kb       3.90 Gb       3.90 Gb          1026            --  \n",
            "                                              aten::mul         8.87%      45.656ms        12.78%      65.759ms      21.399us      33.053ms         8.98%      40.571ms      13.202us     128.00 Kb     128.00 Kb       3.01 Gb       3.01 Gb          3073       271.090  \n",
            "                                           aten::einsum         8.07%      41.565ms        28.52%     146.808ms     143.367us       0.000us         0.00%     210.145ms     205.220us           0 b           0 b       3.00 Gb           0 b          1024            --  \n",
            "                                              aten::bmm         6.51%      33.508ms         8.47%      43.608ms      42.586us     207.609ms        56.42%     210.145ms     205.220us           0 b           0 b       3.00 Gb       3.00 Gb          1024   1099511.628  \n",
            "                                              aten::sub         6.26%      32.212ms         9.27%      47.700ms      18.633us      20.075ms         5.46%      26.247ms      10.253us           0 b           0 b       2.01 Gb       2.01 Gb          2560            --  \n",
            "                                              aten::exp         6.57%      33.843ms         9.55%      49.153ms      19.200us      23.424ms         6.37%      29.738ms      11.616us           0 b           0 b       2.01 Gb       2.01 Gb          2560            --  \n",
            "                                              aten::add         4.35%      22.392ms         6.20%      31.897ms      20.753us      11.802ms         3.21%      15.423ms      10.034us     128.00 Kb     128.00 Kb       1.00 Gb       1.00 Gb          1537       269.500  \n",
            "                                               aten::gt         3.24%      16.681ms         4.62%      23.789ms      23.231us      12.552ms         3.41%      15.089ms      14.735us           0 b           0 b     512.00 Mb     512.00 Mb          1024            --  \n",
            "                                               aten::ge         1.67%       8.586ms         2.52%      12.992ms      25.375us       5.116ms         1.39%       6.266ms      12.238us           0 b           0 b     256.00 Mb     256.00 Mb           512            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 514.741ms\n",
            "Self CUDA time total: 367.990ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normal attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmUYyTQkQr-O",
        "outputId": "7b841d26-c864-4b5e-eaf8-2f4dc3676f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "991.550615ms\n",
            "STAGE:2023-06-25 14:51:03 866:866 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
            "STAGE:2023-06-25 14:51:04 866:866 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
            "STAGE:2023-06-25 14:51:04 866:866 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  Total MFLOPs  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::where         0.01%      71.000us         0.02%     214.000us     107.000us      20.131ms         3.22%      40.265ms      20.133ms           0 b           0 b       4.00 Gb           0 b             2            --  \n",
            "                                              aten::bmm         0.01%     123.000us         0.01%     152.000us      76.000us     336.245ms        53.78%     336.245ms     168.123ms           0 b           0 b       2.06 Gb       2.06 Gb             2   1099511.628  \n",
            "                                           aten::einsum         0.01%      73.000us         0.02%     262.000us     262.000us       0.000us         0.00%     204.162ms     204.162ms           0 b           0 b       2.00 Gb           0 b             1            --  \n",
            "                                          aten::resize_         0.00%       7.000us         0.00%       7.000us       7.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       2.00 Gb       2.00 Gb             1            --  \n",
            "                                          aten::softmax         0.00%       6.000us         0.00%      35.000us      35.000us       0.000us         0.00%      33.714ms      33.714ms           0 b           0 b       2.00 Gb           0 b             1            --  \n",
            "                                         aten::_softmax         0.00%      21.000us         0.00%      29.000us      29.000us      33.714ms         5.39%      33.714ms      33.714ms           0 b           0 b       2.00 Gb       2.00 Gb             1            --  \n",
            "                                               aten::to         0.00%      37.000us        21.22%     230.030ms     115.015ms       0.000us         0.00%     229.588ms     114.794ms           0 b           0 b       1.00 Gb           0 b             2            --  \n",
            "                                         aten::_to_copy         0.00%      35.000us        21.22%     229.993ms     229.993ms       0.000us         0.00%     229.588ms     229.588ms           0 b           0 b       1.00 Gb           0 b             1            --  \n",
            "                                    aten::empty_strided         0.00%      31.000us         0.00%      31.000us      31.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       1.00 Gb       1.00 Gb             1            --  \n",
            "                                               aten::gt         0.01%      89.000us         0.01%     125.000us     125.000us       4.983ms         0.80%       4.983ms       4.983ms           0 b           0 b     256.00 Mb     256.00 Mb             1            --  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 1.084s\n",
            "Self CUDA time total: 625.210ms\n",
            "\n"
          ]
        }
      ]
    }
  ]
}