{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
     "colab": {
      "provenance": [],
      "mount_file_id": "1tYtZE6RmNPOgIkgHFbqHswT9mU2qambT",
      "authorship_tag": "ABX9TyN+hwSGQJPOB0602y/YMNG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Model"
      ],
      "metadata": {
        "id": "JsVNXXsWyZVJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/CharGPT_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(2711)\n",
        "\n",
        "# ---------------------- # hyperparameters # ---------------------- #\n",
        "batch_size = 3 * 64 \n",
        "block_size = 256 \n",
        "max_iters = 2400\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "# ---------------------- # Data and Tokenization # ---------------------- #\n",
        "path_base = '/content/drive/MyDrive/Dirty-Talks/GPT'\n",
        "with open(path_base + '/harry_potter_book.txt', 'r', encoding='utf-8') as handle:\n",
        "    text = handle.read()\n",
        "text = text.lower()    \n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] \n",
        "decode = lambda l: ''.join([itos[i] for i in l]) \n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.95*len(data)) \n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# ---------------------- # Estimate Loss # ---------------------- #\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   \n",
        "        q = self.query(x) \n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 \n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) \n",
        "        out = wei @ v \n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class charGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) \n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) \n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
        "        x = tok_emb + pos_emb \n",
        "        x = self.blocks(x) \n",
        "        x = self.ln_f(x) \n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] \n",
        "            probs = F.softmax(logits, dim=-1) \n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "LnDA4IqVyO4Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT"
      ],
      "metadata": {
        "id": "35PYjSBCFHna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1zjImHhwPgQLnGSzbvPHDxaJjglEvesiN\" alt=\"drawing\" width=\"1500\"/>"
      ],
      "metadata": {
        "id": "lyn9ZnVK1FMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "    <video alt=\"test\" controls>\n",
        "        <source src=\"https://drive.google.com/uc?id=11pPfuUZ-xNFR6MKVWU_AHCScMja3p2oc\" type=\"video/mp4\">\n",
        "    </video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "DMkVLizk1Z1K",
        "outputId": "93399c7a-b90f-4359-cdd1-067350bb2f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <video alt=\"test\" controls>\n",
              "        <source src=\"https://drive.google.com/uc?id=11pPfuUZ-xNFR6MKVWU_AHCScMja3p2oc\" type=\"video/mp4\">\n",
              "    </video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Artigo OpenAI: Instruction-Following](https://openai.com/blog/instruction-following/)\n",
        "<img src=\"https://drive.google.com/uc?id=1v_vdq2_0t98u0oJAVNyvv3TVTZzZSMa7\" alt=\"drawing\" width=\"500\"/>\n",
        "<img src=\"https://drive.google.com/uc?id=1IvbLANsv7_c2wsQ7xC-w5Af-g6cUSU03\" alt=\"drawing\" width=\"500\", height=\"324\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "mJ-MfIRTKFkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers \n",
        "### [Artigo: Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1idqwXR6cPQI6WUpMAqLgQaRA3TyrsIuy\" alt=\"drawing\" width=\"500\"/>\n"
      ],
      "metadata": {
        "id": "IVfkkM3mGkuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT\n",
        "\n",
        "### [Artigo GPT - Jun/2018](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1JW0vQvAsM197xRMQHgoKYPNsXv1hvLOr\" alt=\"drawing\" width=\"1000\"/>\n"
      ],
      "metadata": {
        "id": "aDC5VIDmFrmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2 e GPT3\n",
        "###[Artigo GPT2 Fev/2019](https://openai.com/blog/better-language-models/) [Blog GPT3 Mai/2020](https://dzlab.github.io/ml/2020/07/25/gpt3-overview/)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1njX8YEPoy-iY-DXtkFdQijXf7acK4Uwh\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "# GPT3 - Bullet Points\n",
        "### [Artigo: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
        "<img src=\"https://drive.google.com/uc?id=1oVeULOvkQRAIJMhgKJgTn90yM__pMZHG\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        " #### 1. It shows that language models perform better as they scale in size of model, dataset, and computation.\n",
        "#### 2. It demonstrates that a language model trained on enough data can solve tasks not seen before.\n",
        "#### 3. Its not your bag of tricks but the size of the model that achieves state-of-the-art (SOTA).\n",
        "#### 4. Fewer can afford the cost of training such models as the cost gets overwhelming high. As models get bigger outpacing the growth of GPUs model parallelization becomes indispensable."
      ],
      "metadata": {
        "id": "GNtiJijkLiYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts\n",
        "### [Arigo: survey prompts](https://arxiv.org/abs/2107.13586)\n",
        "> #### Após o GPT3 e chatGPT, a Google desenvolveu o Fine-tuned LAnguage Net [(FLAN)](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)\n",
        "\n",
        "### Em termos simples, os 3 tipos populares de prompt são:\n",
        "- ### zero-shot prompting: somente instruções\n",
        "- ### few-shot prompting: instruções com exemplos de uma task\n",
        "- ### chain of thought prompting: instruções que pedem uma explicação com a resposta\n",
        "\n",
        "### [Repositório git Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)\n",
        "<img src=\"https://drive.google.com/uc?id=1IUM71HdhjYZMf7O8x7vptdy_sKKAHF0n\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9M65wXN9O3k7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tipos de Transformers\n",
        "<img src=\"https://drive.google.com/uc?id=1dn1NdpDcgL6IE_QwkpVIo6NDgIEymaqA\" alt=\"drawing\" width=\"1900\"/>\n"
      ],
      "metadata": {
        "id": "BIYcyu3oTULf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelos Autoregressivos\n",
        "\n",
        "- ### Geração de texto de forma autoregressiva nao é novidade. descreveu o uso de n-grams para aproximar as probabilidades condicionais da próxima letra de um conjunto de dados de texto\n",
        "<img src=\"https://drive.google.com/uc?id=1tOewBXzewI05dwud_xYW-yySgYr9fxnb\" alt=\"drawing\" width=\"500\"/>\n",
        "- ### [N-gram explicação](https://en.wikipedia.org/wiki/N-gram) \n",
        "> #### Um modelo `n-gram` é um tipo de modelo de linguagem probabilística para prever o próximo item em tal sequência. Onde o item pode ser uma letra, palavra, fonema, etc...\n",
        "\n"
      ],
      "metadata": {
        "id": "XDInpG9FUH6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CharGPT:\n",
        "> ### CharGPT é um modelo **bi-gram** que faz a predição do próximo token/`char`."
      ],
      "metadata": {
        "id": "VVuOS7ElTRao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "path_save = '/content/drive/MyDrive/Dirty-Talks/GPT/ckpt/gptzinho'\n",
        "model = charGPT()\n",
        "model.load_state_dict(torch.load(path_save))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "context = \"hermione perguntou\"\n",
        "context = torch.tensor(encode(context)).to(device)\n",
        "\n",
        "out = decode(model.generate(context.unsqueeze(0), max_new_tokens=300)[0].tolist())\n",
        "for char in out:\n",
        "    time.sleep(0.02)\n",
        "    print(char, end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WuW8NYly0_O",
        "outputId": "134278e1-72dd-441a-bdc4-03bb7bec5872"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hermione perguntou a coruja e pulmõeszinhos.\n",
            "não havia lixo de brux: o corpo se todo perguntar a embaleando seus\n",
            "critos, olhos rápidos tatinham a libado com seus garotos mais audiências, disse os\n",
            "seus lados dos olhos da umbridge, depois de sicução abiliotas.\n",
            "- bem, posso - berrou gina manscada - acrescentou a mulher "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Antes de tudo... as ideias do charGPT são do [Karpathy](https://karpathy.ai/)\n",
        "<img src=\"https://drive.google.com/uc?id=1MM_t3QzM6Zc5S23XnyNpMuh-9Ag-2yJ1\n",
        "\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "erC3vuiNWl79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelagem "
      ],
      "metadata": {
        "id": "5D6IZmQo0kXE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSQhcaFafG0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bce3882-9b78-4680-d0e3-cfd2cb6b3fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- CAPÍTULO UM -\n",
            "O menino que sobreviveu\n",
            "O Sr. e a Sra. Dursley, da rua dos Alfeneiros, no 4, se orgulhavam de dizer que\n",
            "eram perfeitamente normais, muito bem, obrigado. Eram as últimas pessoas no\n",
            "mundo que se esperaria que se metessem em alguma coisa estranha ou\n",
            "misteriosa, porque simplesmente não compactuavam com esse tipo de bobagem.\n",
            "O Sr. Dursley era diretor de uma firma chamada Grunnings, que fazia\n",
            "perfurações. Era um homem alto e corpulento quase sem pescoço, embora\n",
            "tivesse enormes bigodes. A Sra. Dursley era magra e loura e tinha um pescoço\n",
            "quase duas vezes mais comprido que o normal, o que era muito útil porque ela\n",
            "passava grande parte do tempo espichando-o por cima da cerca do jardim para\n",
            "espiar os vizinhos. Os Dursley tinham um filhinho chamado Dudley, o Duda, e\n",
            "em sua opinião não havia garoto melhor em nenhum lugar do mundo.\n",
            "Os Dursley tinham tudo que queriam, mas tinham também um segredo, e seu\n",
            "maior receio era que alguém o descobrisse. Achavam que não iriam aguentar se\n",
            "algu\n"
          ]
        }
      ],
      "source": [
        "# Carregando Dataset\n",
        "path_base = '/content/drive/MyDrive/Dirty-Talks/GPT'\n",
        "with open(path_base + '/harry_potter_book.txt', 'r', encoding='utf-8') as handle:\n",
        "    text = handle.read()\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principais métodos de tokenização\n",
        "> ### GPTs - OpenAI: [tiktoken](https://github.com/openai/tiktoken) que usa o [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) \n",
        "> ### Google: [sentence piece](https://github.com/google/sentencepiece)\n",
        "\n",
        "### Exemplo tokenização GPT2 ---> treinado em língua inglesa"
      ],
      "metadata": {
        "id": "1xMBUUe8rS2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q transformers\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "print(f\"Vocab size GPT2: {tokenizer.vocab_size} tokens\\n\")\n",
        "\n",
        "sentence = 'Oiiiii, tudo bem com você?'\n",
        "print(f\"Sentence tokenized: {tokenizer(sentence)['input_ids']} e possui {len(tokenizer(sentence)['input_ids'])} tokens.\\n\")    \n",
        "print(f\"Sentence detokenized:\")\n",
        "for tok in tokenizer(sentence)['input_ids']:\n",
        "    print(f\"{tok:<5} ---> {tokenizer.decode(tok)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8eQe2eisf_W",
        "outputId": "ad1ae12c-8120-46b2-f6c2-228ae83ccc5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size GPT2: 50257 tokens\n",
            "\n",
            "Sentence tokenized: [46, 4178, 15479, 11, 256, 12003, 307, 76, 401, 12776, 25792, 30] e possui 12 tokens.\n",
            "\n",
            "Sentence detokenized:\n",
            "46    ---> O\n",
            "4178  ---> ii\n",
            "15479 ---> iii\n",
            "11    ---> ,\n",
            "256   --->  t\n",
            "12003 ---> udo\n",
            "307   --->  be\n",
            "76    ---> m\n",
            "401   --->  com\n",
            "12776 --->  voc\n",
            "25792 ---> ê\n",
            "30    ---> ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tamanho do dataset em # de chars\n",
        "print(f'Tamanho do dataset: {len(text)} chars.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFQiqNVmmDaP",
        "outputId": "3f3ec23e-285c-41b0-e351-e3b9617b4433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do dataset: 6540235 chars.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Todos os chars: {''.join(chars)}\\nTamanho do vocab: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nUXAUaDmZ0U",
        "outputId": "cfda9533-04c3-4df0-9981-285c4e65c5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todos os chars: \n",
            " !\"&'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzªÀÁÂÃÇÉÊÍÓÔÕÚàáâãçèéêíóôõùú́\n",
            "Tamanho do vocab: 105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i, ch in enumerate(chars)}\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "decode = lambda l:  ''.join([itos[i] for i in l])\n",
        "\n",
        "print(f\"Exemplo de tokenização da sentença:\\n{sentence}\")\n",
        "for tok in encode(sentence):\n",
        "    print(f\"{tok:<2} ---> {decode([tok])}\")\n",
        "\n",
        "print(f\"que possui {len(encode(sentence))} tokens.\")    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ9sD-5_nKqt",
        "outputId": "4bdf27ee-6026-48e2-9bb6-4750fe9d0a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemplo de tokenização da sentença:\n",
            "Oiiiii, tudo bem com você?\n",
            "39 ---> O\n",
            "59 ---> i\n",
            "59 ---> i\n",
            "59 ---> i\n",
            "59 ---> i\n",
            "59 ---> i\n",
            "8  ---> ,\n",
            "1  --->  \n",
            "70 ---> t\n",
            "71 ---> u\n",
            "54 ---> d\n",
            "65 ---> o\n",
            "1  --->  \n",
            "52 ---> b\n",
            "55 ---> e\n",
            "63 ---> m\n",
            "1  --->  \n",
            "53 ---> c\n",
            "65 ---> o\n",
            "63 ---> m\n",
            "1  --->  \n",
            "72 ---> v\n",
            "65 ---> o\n",
            "53 ---> c\n",
            "97 ---> ê\n",
            "24 ---> ?\n",
            "que possui 26 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Observação\n",
        "### - Tokenizar por word-pieces *aumenta o tamanho do vocabulário*.... Mas *diminui o número de tokens* para representar o texto (nosso exemplo $12$x$26$). O que funciona atualmente é `vocab_size` entre ($30\\; e\\; 100$)K tokens.\n",
        " "
      ],
      "metadata": {
        "id": "M5x-x7sJxK0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsZpUDz-5SRk",
        "outputId": "b86bc929-50ab-4e42-e63b-363031c5708d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6540235]) torch.int64\n",
            "tensor([  9,   1,  27,  25,  40,  85,  44,  45,  36,  39,   1,  45,  37,   1,\n",
            "          9,   0,  39,   1,  63,  55,  64,  59,  64,  65,   1,  67,  71,  55,\n",
            "          1,  69,  65,  52,  68,  55,  72,  59,  72,  55,  71,   0,  39,   1,\n",
            "         43,  68,  10,   1,  55,   1,  51,   1,  43,  68,  51,  10,   1,  28,\n",
            "         71,  68,  69,  62,  55,  75,   8,   1,  54,  51,   1,  68,  71,  51,\n",
            "          1,  54,  65,  69,   1,  25,  62,  56,  55,  64,  55,  59,  68,  65,\n",
            "         69,   8,   1,  64,  65,   1,  16,   8,   1,  69,  55,   1,  65,  68,\n",
            "         57,  71,  62,  58,  51,  72,  51,  63,   1,  54,  55,   1,  54,  59,\n",
            "         76,  55,  68,   1,  67,  71,  55,   0,  55,  68,  51,  63,   1,  66,\n",
            "         55,  68,  56,  55,  59,  70,  51,  63,  55,  64,  70,  55,   1,  64,\n",
            "         65,  68,  63,  51,  59,  69,   8,   1,  63,  71,  59,  70,  65,   1,\n",
            "         52,  55,  63,   8,   1,  65,  52,  68,  59,  57,  51,  54,  65,  10,\n",
            "          1,  29,  68,  51,  63,   1,  51,  69,   1, 103,  62,  70,  59,  63,\n",
            "         51,  69,   1,  66,  55,  69,  69,  65,  51,  69,   1,  64,  65,   0,\n",
            "         63,  71,  64,  54,  65,   1,  67,  71,  55,   1,  69,  55,   1,  55,\n",
            "         69,  66,  55,  68,  51,  68,  59,  51,   1,  67,  71,  55,   1,  69,\n",
            "         55,   1,  63,  55,  70,  55,  69,  69,  55,  63,   1,  55,  63,   1,\n",
            "         51,  62,  57,  71,  63,  51,   1,  53,  65,  59,  69,  51,   1,  55,\n",
            "         69,  70,  68,  51,  64,  58,  51,   1,  65,  71,   0,  63,  59,  69,\n",
            "         70,  55,  68,  59,  65,  69,  51,   8,   1,  66,  65,  68,  67,  71,\n",
            "         55,   1,  69,  59,  63,  66,  62,  55,  69,  63,  55,  64,  70,  55,\n",
            "          1,  64,  93,  65,   1,  53,  65,  63,  66,  51,  53,  70,  71,  51,\n",
            "         72,  51,  63,   1,  53,  65,  63,   1,  55,  69,  69,  55,   1,  70,\n",
            "         59,  66,  65,   1,  54,  55,   1,  52,  65,  52,  51,  57,  55,  63,\n",
            "         10,   0,  39,   1,  43,  68,  10,   1,  28,  71,  68,  69,  62,  55,\n",
            "         75,   1,  55,  68,  51,   1,  54,  59,  68,  55,  70,  65,  68,   1,\n",
            "         54,  55,   1,  71,  63,  51,   1,  56,  59,  68,  63,  51,   1,  53,\n",
            "         58,  51,  63,  51,  54,  51,   1,  31,  68,  71,  64,  64,  59,  64,\n",
            "         57,  69,   8,   1,  67,  71,  55,   1,  56,  51,  76,  59,  51,   0,\n",
            "         66,  55,  68,  56,  71,  68,  51,  94, 101,  55,  69,  10,   1,  29,\n",
            "         68,  51,   1,  71,  63,   1,  58,  65,  63,  55,  63,   1,  51,  62,\n",
            "         70,  65,   1,  55,   1,  53,  65,  68,  66,  71,  62,  55,  64,  70,\n",
            "         65,   1,  67,  71,  51,  69,  55,   1,  69,  55,  63,   1,  66,  55,\n",
            "         69,  53,  65,  94,  65,   8,   1,  55,  63,  52,  65,  68,  51,   0,\n",
            "         70,  59,  72,  55,  69,  69,  55,   1,  55,  64,  65,  68,  63,  55,\n",
            "         69,   1,  52,  59,  57,  65,  54,  55,  69,  10,   1,  25,   1,  43,\n",
            "         68,  51,  10,   1,  28,  71,  68,  69,  62,  55,  75,   1,  55,  68,\n",
            "         51,   1,  63,  51,  57,  68,  51,   1,  55,   1,  62,  65,  71,  68,\n",
            "         51,   1,  55,   1,  70,  59,  64,  58,  51,   1,  71,  63,   1,  66,\n",
            "         55,  69,  53,  65,  94,  65,   0,  67,  71,  51,  69,  55,   1,  54,\n",
            "         71,  51,  69,   1,  72,  55,  76,  55,  69,   1,  63,  51,  59,  69,\n",
            "          1,  53,  65,  63,  66,  68,  59,  54,  65,   1,  67,  71,  55,   1,\n",
            "         65,   1,  64,  65,  68,  63,  51,  62,   8,   1,  65,   1,  67,  71,\n",
            "         55,   1,  55,  68,  51,   1,  63,  71,  59,  70,  65,   1, 103,  70,\n",
            "         59,  62,   1,  66,  65,  68,  67,  71,  55,   1,  55,  62,  51,   0,\n",
            "         66,  51,  69,  69,  51,  72,  51,   1,  57,  68,  51,  64,  54,  55,\n",
            "          1,  66,  51,  68,  70,  55,   1,  54,  65,   1,  70,  55,  63,  66,\n",
            "         65,   1,  55,  69,  66,  59,  53,  58,  51,  64,  54,  65,   9,  65,\n",
            "          1,  66,  65,  68,   1,  53,  59,  63,  51,   1,  54,  51,   1,  53,\n",
            "         55,  68,  53,  51,   1,  54,  65,   1,  60,  51,  68,  54,  59,  63,\n",
            "          1,  66,  51,  68,  51,   0,  55,  69,  66,  59,  51,  68,   1,  65,\n",
            "         69,   1,  72,  59,  76,  59,  64,  58,  65,  69,  10,   1,  39,  69,\n",
            "          1,  28,  71,  68,  69,  62,  55,  75,   1,  70,  59,  64,  58,  51,\n",
            "         63,   1,  71,  63,   1,  56,  59,  62,  58,  59,  64,  58,  65,   1,\n",
            "         53,  58,  51,  63,  51,  54,  65,   1,  28,  71,  54,  62,  55,  75,\n",
            "          8,   1,  65,   1,  28,  71,  54,  51,   8,   1,  55,   0,  55,  63,\n",
            "          1,  69,  71,  51,   1,  65,  66,  59,  64,  59,  93,  65,   1,  64,\n",
            "         93,  65,   1,  58,  51,  72,  59,  51,   1,  57,  51,  68,  65,  70,\n",
            "         65,   1,  63,  55,  62,  58,  65,  68,   1,  55,  63,   1,  64,  55,\n",
            "         64,  58,  71,  63,   1,  62,  71,  57,  51,  68,   1,  54,  65,   1,\n",
            "         63,  71,  64,  54,  65,  10,   0,  39,  69,   1,  28,  71,  68,  69,\n",
            "         62,  55,  75,   1,  70,  59,  64,  58,  51,  63,   1,  70,  71,  54,\n",
            "         65,   1,  67,  71,  55,   1,  67,  71,  55,  68,  59,  51,  63,   8,\n",
            "          1,  63,  51,  69,   1,  70,  59,  64,  58,  51,  63,   1,  70,  51,\n",
            "         63,  52,  96,  63,   1,  71,  63,   1,  69,  55,  57,  68,  55,  54,\n",
            "         65,   8,   1,  55,   1,  69,  55,  71,   0,  63,  51,  59,  65,  68,\n",
            "          1,  68,  55,  53,  55,  59,  65,   1,  55,  68,  51,   1,  67,  71,\n",
            "         55,   1,  51,  62,  57,  71,  96,  63,   1,  65,   1,  54,  55,  69,\n",
            "         53,  65,  52,  68,  59,  69,  69,  55,  10,   1,  25,  53,  58,  51,\n",
            "         72,  51,  63,   1,  67,  71,  55,   1,  64,  93,  65,   1,  59,  68,\n",
            "         59,  51,  63,   1,  51,  57,  71,  55,  64,  70,  51,  68,   1,  69,\n",
            "         55,   0,  51,  62,  57,  71])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data \n",
        "n = int((0.9* len(data)))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "JksP9LXvrklM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main ideia dataloaders\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmkyXNfwrkic",
        "outputId": "c83ab18d-9309-4110-949f-0c7251fed6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 9,  1, 27, 25, 40, 85, 44, 45, 36])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f'quando o contexto é {context} o target é {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d21E3BiYrkfz",
        "outputId": "7d1255d4-0b6a-4368-92e7-ee378ecced6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "quando o contexto é tensor([9]) o target é 1\n",
            "quando o contexto é tensor([9, 1]) o target é 27\n",
            "quando o contexto é tensor([ 9,  1, 27]) o target é 25\n",
            "quando o contexto é tensor([ 9,  1, 27, 25]) o target é 40\n",
            "quando o contexto é tensor([ 9,  1, 27, 25, 40]) o target é 85\n",
            "quando o contexto é tensor([ 9,  1, 27, 25, 40, 85]) o target é 44\n",
            "quando o contexto é tensor([ 9,  1, 27, 25, 40, 85, 44]) o target é 45\n",
            "quando o contexto é tensor([ 9,  1, 27, 25, 40, 85, 44, 45]) o target é 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build dataloaders\n",
        "torch.manual_seed(2711)\n",
        "batch_size = 4 # numero de sequencias independentes para processar em paralelo\n",
        "block_size = 8 # qual o tamanho maximo do contexto para fazer predicoes?\n",
        "\n",
        "def get_batch(split):\n",
        "    # gera um peno batch de entradas x e targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # gera 4 amostras randomicas\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print(f'inputs: {xb.shape}\\n{xb}')    \n",
        "print(f'targets: {yb.shape}\\n{yb}')\n",
        "print('----')  \n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target  = yb[b,t]\n",
        "        print(f'quando a entrada é {context.tolist()} o target é {target}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzrAjvcQrkdI",
        "outputId": "bda0e9cd-3295-4584-cfe7-c3fe94383ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: torch.Size([4, 8])\n",
            "tensor([[29, 62, 51,  1, 64, 93, 65,  1],\n",
            "        [ 0,  9,  1, 41, 71, 51, 64, 54],\n",
            "        [91,  1, 51, 53, 65, 69, 70, 71],\n",
            "        [55, 59, 68, 51,  1, 67, 71, 55]])\n",
            "targets: torch.Size([4, 8])\n",
            "tensor([[62, 51,  1, 64, 93, 65,  1, 56],\n",
            "        [ 9,  1, 41, 71, 51, 64, 54, 65],\n",
            "        [ 1, 51, 53, 65, 69, 70, 71, 63],\n",
            "        [59, 68, 51,  1, 67, 71, 55,  1]])\n",
            "----\n",
            "quando a entrada é [29] o target é 62\n",
            "quando a entrada é [29, 62] o target é 51\n",
            "quando a entrada é [29, 62, 51] o target é 1\n",
            "quando a entrada é [29, 62, 51, 1] o target é 64\n",
            "quando a entrada é [29, 62, 51, 1, 64] o target é 93\n",
            "quando a entrada é [29, 62, 51, 1, 64, 93] o target é 65\n",
            "quando a entrada é [29, 62, 51, 1, 64, 93, 65] o target é 1\n",
            "quando a entrada é [29, 62, 51, 1, 64, 93, 65, 1] o target é 56\n",
            "quando a entrada é [0] o target é 9\n",
            "quando a entrada é [0, 9] o target é 1\n",
            "quando a entrada é [0, 9, 1] o target é 41\n",
            "quando a entrada é [0, 9, 1, 41] o target é 71\n",
            "quando a entrada é [0, 9, 1, 41, 71] o target é 51\n",
            "quando a entrada é [0, 9, 1, 41, 71, 51] o target é 64\n",
            "quando a entrada é [0, 9, 1, 41, 71, 51, 64] o target é 54\n",
            "quando a entrada é [0, 9, 1, 41, 71, 51, 64, 54] o target é 65\n",
            "quando a entrada é [91] o target é 1\n",
            "quando a entrada é [91, 1] o target é 51\n",
            "quando a entrada é [91, 1, 51] o target é 53\n",
            "quando a entrada é [91, 1, 51, 53] o target é 65\n",
            "quando a entrada é [91, 1, 51, 53, 65] o target é 69\n",
            "quando a entrada é [91, 1, 51, 53, 65, 69] o target é 70\n",
            "quando a entrada é [91, 1, 51, 53, 65, 69, 70] o target é 71\n",
            "quando a entrada é [91, 1, 51, 53, 65, 69, 70, 71] o target é 63\n",
            "quando a entrada é [55] o target é 59\n",
            "quando a entrada é [55, 59] o target é 68\n",
            "quando a entrada é [55, 59, 68] o target é 51\n",
            "quando a entrada é [55, 59, 68, 51] o target é 1\n",
            "quando a entrada é [55, 59, 68, 51, 1] o target é 67\n",
            "quando a entrada é [55, 59, 68, 51, 1, 67] o target é 71\n",
            "quando a entrada é [55, 59, 68, 51, 1, 67, 71] o target é 55\n",
            "quando a entrada é [55, 59, 68, 51, 1, 67, 71, 55] o target é 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageMode(torch.nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # x e targets sao ambos (B,T) tensores de inteiros\n",
        "        logits = self.token_embedding_table(x) # (B,T,C=vocab_size)\n",
        "\n",
        "        B, T, C = logits.shape\n",
        "\n",
        "        if targets == None:\n",
        "            loss = None\n",
        "        else:\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        \n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, max_new_tokens):\n",
        "        # x é (B,T) dos índices no contexto atual\n",
        "        for _ in range(max_new_tokens):\n",
        "            # pega as predições\n",
        "            logits, loss = self(x)\n",
        "            # foca somente no último timestep\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "            # aplica softmax para pegar a prob\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # amostra da distribuição\n",
        "            x_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
        "            # adiciona a amostra na sequencia \n",
        "            x = torch.cat((x, x_next), dim=1) # (B,T+1)\n",
        "\n",
        "        return x\n",
        "\n",
        "m = BigramLanguageMode(vocab_size).to(device).to(device)        \n",
        "logits, loss = m(xb.to(device), yb.to(device))\n",
        "print(logits.shape, loss)\n",
        "\n",
        "print(decode(m.generate(x = torch.zeros((1,1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxzERYsBwgex",
        "outputId": "ee959b4c-3443-45bd-ce75-903a6c2ff5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 105]) tensor(4.9522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "úÇù-99fix8'Óèco!!c6ZÚPÕ2NHqNHqííeÕV?iLúw(i9ùÃs6KO:ÊzCq.ç6\n",
            "FlmNÉOÂÀKáTkK(dGãgú3Úôua8lE3Éªã'i/ôón&MWÀP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=10bq7YpjM5It8_rEVLOJxa6qxhRqt_pSp\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n"
      ],
      "metadata": {
        "id": "zvDEnB9k0mrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ckmvobwE6kEy4wBi5Pv4nvVirPXT2sas\" alt=\"drawing\" width=\"1000\"/>"
      ],
      "metadata": {
        "id": "MpNP0it60B4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo da Entropia Cruzada\n",
        "from torch.nn import functional as F\n",
        "\n",
        "B, T, C = 2,3,2\n",
        "x = torch.randn(B, T, C)\n",
        "y = torch.randint(C, (B, T))\n",
        "# print(F.cross_entropy(x, y)) # --> mostrar que quebra o código \n",
        "\n",
        "x = x.view(B*T, C)\n",
        "y = y.view(B*T)\n",
        "print(f'Chamando diretamente a cross_entropy: {F.cross_entropy(x, y).item():>23.4}')\n",
        "print(f'Chamando NLL após a log_softmax: {F.nll_loss(F.log_softmax(x, dim=-1), y).item():>28.4}')\n",
        "\n",
        "l = sum([predict[true].item() for (true, predict) in zip(y,-F.log_softmax(x, dim=-1))])/6\n",
        "print(f'Iterando a log_softmax na classe correta AKA NLL loss: {l:>6.4}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF-syvQO0muf",
        "outputId": "007eb489-ba4d-4703-9ab0-4a1c2507f498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chamando diretamente a cross_entropy:                  0.8933\n",
            "Chamando NLL após a log_softmax:                       0.8933\n",
            "Iterando a log_softmax na classe correta AKA NLL loss: 0.8933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 32\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "for steps in range(5000):\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # pega um batch\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m(xb.to(device), yb.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(f'loss: {loss.item()}')\n",
        "\n",
        "print(decode(m.generate(x = torch.zeros((1,1), dtype=torch.long).to(device), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLFr9ar1wgtF",
        "outputId": "2bfbf93f-39eb-48e4-faf8-b068f03063c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.378185749053955\n",
            "\n",
            "ha dos, te leato o mosa des éra oue ca ey sabúradouvapo, cobitalté ntado\n",
            "- s enrom r leiuelans foo p\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mecanismo de atenção\n"
      ],
      "metadata": {
        "id": "lQ8T-6VIN6v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Fonte do vídeo: \n",
        "# DeepMind x UCL | Deep Learning Lectures | 8/12 | Attention and Memory in Deep Learning\n",
        "# url: https://www.youtube.com/watch?v=AIiwuClvH6k\n",
        "\n",
        "HTML(\"\"\"\n",
        "    <video alt=\"test\" controls>\n",
        "        <source src=\"https://drive.google.com/uc?id=12ylkSVbYufAxUgdAYvUgR8CV4WVMgAIP\" type=\"video/mp4\">\n",
        "    </video>\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "ERkHKwLr1xKY",
        "outputId": "0a819f45-60d6-4cbd-a380-e5f692959692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <video alt=\"test\" controls>\n",
              "        <source src=\"https://drive.google.com/uc?id=12ylkSVbYufAxUgdAYvUgR8CV4WVMgAIP\" type=\"video/mp4\">\n",
              "    </video>\n"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MHA (Multi Head Attention) \"Observar o mesmo dado com diferentes visões\"\n",
        "<img src=\"https://drive.google.com/uc?id=1uKpfZaNbuJ6q8ze4HT03s2kfYBh2JUgQ\" alt=\"drawing\" width=\"1200\"/>\n",
        "\n",
        "## MHA com texto: vamos usar de input uma frase com 9 tokens:\n",
        "> ### [ Quero ] [ um ] [ cartão ] [ de ] [ crédito ] [ adicional ] [ para ] [ minha ] [ filha ]\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1VRDfKNee-Mm0WeH_psBm2Top8vBwUDcT\" alt=\"drawing\" width=\"1200\"/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Bzr77GYXMqjNc9fL2TBtFhYx7trHh3Bi\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5wVvgd_GdE4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideia de comunicação entre os tokens"
      ],
      "metadata": {
        "id": "fZ8DgIvU2eii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(2711)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN33zUml2hkw",
        "outputId": "5d6641a5-a13d-41d6-ded0-55d4eb067b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[7., 2.],\n",
            "        [6., 3.],\n",
            "        [1., 4.]])\n",
            "--\n",
            "c=\n",
            "tensor([[7.0000, 2.0000],\n",
            "        [6.5000, 2.5000],\n",
            "        [4.6667, 3.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_Uo3z-d4NHs",
        "outputId": "c4f77f91-749d-4802-c677-6a48f8167ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "jXcDN2EE4NA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc1OOabz4ToV",
        "outputId": "08a4fb16-81f7-4c3b-ab78-416eb9b8116e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W477qK5r4Tlb",
        "outputId": "54c3df87-9bc3-44a4-9583-132d47284b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1sKqUSC_RZtZoMi3oVE0AonwWcG8OKaub\" alt=\"drawing\" width=\"1200\"/>\n"
      ],
      "metadata": {
        "id": "0zTKNAkP5wX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4a VERSAO:\n",
        "B,T,C = 4,8,128\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "num_heads = 8 # Quebra o C em num_heads e olha para o C por diferentes cabeças \"diferentes visoes lineares\"\n",
        "H = head_size = C//num_heads # quantidade de features de afinidade a serem agregadas\n",
        "\n",
        "Q = torch.nn.Linear(C, H, bias=False) # (C,H)\n",
        "K = torch.nn.Linear(C, H, bias=False) # (C,H)\n",
        "V = torch.nn.Linear(C, H, bias=False) # (C,H)\n",
        "\n",
        "q = Q(x) # (B,T,C) @ (C,H) ---> (B,T,H)\n",
        "k = K(x) # (B,T,C) @ (C,H) ---> (B,T,H)\n",
        "v = V(x) # (B,T,C) @ (C,H) ---> (B,T,H)\n",
        "\n",
        "wei = q@k.transpose(-2, -1) # (B,T,H) @ (B,H,T) ---> (B,T,T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "out = wei@v # (T,T) @ (B,T,H) ---> (B,T,H)\n",
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLZ8AvKD2Ulh",
        "outputId": "ecc3212f-4a93-4d21-a722-e990f2a6678e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1503, -0.4738,  0.0984,  0.2405, -0.0847,  0.8793,  0.1019, -0.2484,\n",
              "          0.5754, -0.0955, -0.2541,  0.2867,  0.7486,  0.4645, -0.0086,  0.1156],\n",
              "        [ 0.0549, -0.3876, -0.1462,  0.1003, -0.1610,  0.7403,  0.0544, -0.3145,\n",
              "          0.5443, -0.1038, -0.1298,  0.2384,  0.6636,  0.2227, -0.0882, -0.0433],\n",
              "        [-0.1129, -0.1891, -0.3961, -0.1330, -0.1831,  0.4250, -0.2080, -0.5821,\n",
              "          0.5890, -0.2107, -0.0454,  0.1597,  0.4043, -0.0396, -0.0737, -0.0915],\n",
              "        [ 0.0402, -0.9094, -0.1716,  0.3354,  0.0207, -0.0178, -0.6914, -0.5769,\n",
              "         -0.0156, -0.0143, -0.5190,  0.4034,  0.2109,  0.0123, -0.6158,  0.0640],\n",
              "        [ 0.1252, -1.2072, -0.0354,  0.5564,  0.1170, -0.1469, -0.8539, -0.5436,\n",
              "         -0.2409,  0.0764, -0.7170,  0.5085,  0.1549,  0.0883, -0.8228,  0.1511],\n",
              "        [-0.0846, -0.3510, -0.1070, -0.2250,  0.0558,  0.0189,  0.2014, -0.6210,\n",
              "          0.9601, -0.1313, -0.2332, -0.1881, -0.0636,  0.0020,  0.2160,  0.1050],\n",
              "        [-0.0769, -0.3877, -0.0074,  0.0417,  0.3646, -0.1617,  0.2440, -0.5320,\n",
              "          0.9363,  0.0950, -0.1132, -0.0436, -0.3906, -0.0826,  0.2157,  0.1092],\n",
              "        [-0.1252, -0.2007, -0.1510,  0.2505,  0.3377,  0.0838, -0.2356, -0.5515,\n",
              "          0.5975,  0.0605,  0.0461,  0.3580, -0.1512, -0.1293,  0.0702,  0.0319]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "WCVx5Cgu6J52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notas:\n",
        "- #### A atenção é um **mecanismo de comunicação**. Que pode ser vistos em um gráfo direcionado olhando uns para os outros e agregando informações com uma soma ponderada de todos os nós que apontam para eles, com pesos dependentes dos dados.\n",
        "- #### Não há noção de espaço. A atenção simplesmente age sobre um conjunto de vetores. É por isso que precisamos de embeddings de posição.\n",
        "- #### Cada exemplo na dimensão do batch é, processado de forma totalmente independente e nunca \"conversa\" entre si.\n",
        "- #### Em um bloco de atenção \"encoder\", apenas exclua a única linha que faz o mascaramento com `tril`, permitindo que todos os tokens se comuniquem. Este bloco aqui é chamado de bloco de atenção \"decodificador\" porque possui máscara triangular e geralmente é usado em configurações autorregressivas, como modelagem de linguagem.\n",
        "- #### \"self-attention\" significa apenas que as *keys* e os *values* são produzidos a partir da mesma fonte das *queries*. Em \"*cross attention*\", as queries ainda são produzidas a partir de `x`, mas as *keys* e os *values* vêm de alguma outra fonte externa (por exemplo, um módulo encoder)\n",
        "- #### \"Scaled\" attention  divide `wei` por `1/sqrt(head_size)`. Isso faz com que quando a entrada `Q`,`K` tenha variância unitária, `wei` também terá variância unitária e a Softmax permanecerá difuso e não saturará muito. "
      ],
      "metadata": {
        "id": "o-oEmTWe6wIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1PzB-Zd3tNU9XWeQaMMjH8vkbl4da5lcf\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1gaKyGClmAObtFvIJx05MIF6EZQkr0VeA\" alt=\"drawing\" width=\"1000\"/>\n"
      ],
      "metadata": {
        "id": "o7Ju1_b266-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,H)\n",
        "q = torch.randn(B,T,H)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "86bc9ff3-eaf5-48b2-a140-4abaeca0f931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1126)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "d3577b54-30a2-4678-f96f-6f4a7396ed65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9965)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "7c9648e2-687e-4b2a-9596-dd79ad23fea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0466)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "85eb1e74-d0b6-4534-937e-a18ba0f9d89e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "c7e99732-e08d-49d4-d6fb-91f87e030a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blocos de Transformer\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=16vj-1hGXJ3st-6-4FSWhlVxW01XKdvys\" alt=\"drawing\" width=\"300\"/>\n"
      ],
      "metadata": {
        "id": "0QoO4Fs57uHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer Norm\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1wVgNkIkKabpn-2lcDVj2AV5kYpNkf428\" alt=\"drawing\" width=\"1000\"/>\n"
      ],
      "metadata": {
        "id": "OzxpTru2N58w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2, x3 = torch.tensor([1,2,0,4,5,1], dtype=torch.float), \\\n",
        "             torch.tensor([3,2,1,6,2,0], dtype=torch.float), \\\n",
        "             torch.tensor([6,2,5,1,3,1], dtype=torch.float)\n",
        "\n",
        "batch = torch.stack((x1, x2, x3))\n",
        "print(f'batch shape: {batch.shape}')\n",
        "\n",
        "#mean and var by column 0 \n",
        "xmean_bnorm, xvar_bnorm = batch.mean(0, keepdim=True), batch.var(0, keepdim=True)\n",
        "\n",
        "batch_norm = (batch - xmean_bnorm) / torch.sqrt(xvar_bnorm + 1e-5)\n",
        "print(f'\\nbatch_norm mean: {batch_norm[:,0].mean():.4}, batch_norm var: {batch_norm[:,0].var():.4}\\n')\n",
        "\n",
        "#mean and var by column 1\n",
        "xmean_lnorm, xvar_lnorm = batch.mean(1, keepdim=True), batch.var(1, keepdim=True)\n",
        "\n",
        "layer_norm = (batch - xmean_lnorm) / torch.sqrt(xvar_lnorm)\n",
        "print(f'layer_norm mean: {layer_norm[0,:].mean():.4}, layer_norm var: {layer_norm[0,:].var():.4}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0tkfl_OJaKk",
        "outputId": "0b91fd41-c91f-47e7-9c2b-0329e88bfc9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch shape: torch.Size([3, 6])\n",
            "\n",
            "batch_norm mean: 0.0, batch_norm var: 1.0\n",
            "\n",
            "layer_norm mean: -3.974e-08, layer_norm var: 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: \n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) \n",
        "    xvar = x.var(1, keepdim=True) \n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) \n",
        "    out = self.gamma * xhat + self.beta\n",
        "    return out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "layer_norm = LayerNorm1d(batch.size(1))\n",
        "ln_x = layer_norm(batch)\n",
        "ln_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK6mKlPpK6gC",
        "outputId": "2be981f3-1489-46ef-b54c-b218e1da30f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip connections\n",
        "[Artigo](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Re_QRNLzZkESLcbTTNoavpzMGLpv6g9l\" alt=\"drawing\" width=\"450\" height=\"280\"/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1myyttTn1F9FxxBbkzwJ0buKDFbriWAfE\" alt=\"drawing\" width=\"450\" height=\"280\"/>\n"
      ],
      "metadata": {
        "id": "093V8kU5cder"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "[Artigo](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=18ADHDL7OEocahmOSwUicQVgC8SBOv9pv\" alt=\"drawing\" width=\"900\"/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1lsCf55aDNH3mxzslgDsqSAfKNZa-3_Pw\" alt=\"drawing\" width=\"500\"/>\n"
      ],
      "metadata": {
        "id": "CtHv1q5hebK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed Forward Networks\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1EMZketZLNvAmPmRFoKvS0K4tZVd7gU-x\" alt=\"drawing\" width=\"1000\"/>\n"
      ],
      "metadata": {
        "id": "1hAkjtEe9Bb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "PuFrH1dL8qyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bloco GPT\n",
        "<img src=\"https://drive.google.com/uc?id=16vj-1hGXJ3st-6-4FSWhlVxW01XKdvys\" alt=\"drawing\" width=\"300\"/>"
      ],
      "metadata": {
        "id": "JoWAZEPp9V3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "9XVxYi289ZHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CharGPT: final Modeling"
      ],
      "metadata": {
        "id": "sy3vvERR8Zoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(2711)\n",
        "\n",
        "# ---------------------- # hyperparameters # ---------------------- #\n",
        "batch_size = 3 * 64 \n",
        "block_size = 256 \n",
        "max_iters = 2400\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "# Make sure that head_size = n_embd/n_head ---> is an integer\n",
        "assert n_embd%n_head == 0, 'n_embd e n_head precisam ser multiplos'\n",
        "\n",
        "# ---------------------- # Data and Tokenization # ---------------------- #\n",
        "path_base = '/content/drive/MyDrive/Dirty-Talks/GPT'\n",
        "with open(path_base + '/harry_potter_book.txt', 'r', encoding='utf-8') as handle:\n",
        "    text = handle.read()\n",
        "text = text.lower()    \n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] \n",
        "decode = lambda l: ''.join([itos[i] for i in l]) \n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.95*len(data)) \n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# ---------------------- # Estimate Loss # ---------------------- #\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "A_R6fEUhWkf_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class charGPT(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = charGPT()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HbyIQAT8ne9",
        "outputId": "354f98f3-b2e6-4e2a-8b3b-a8eb3443792f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.790467 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"hermione perguntou\"\n",
        "context = torch.tensor(encode(context)).to(device)\n",
        "\n",
        "out = decode(m.generate(context.unsqueeze(0), max_new_tokens=300)[0].tolist())\n",
        "for char in out:\n",
        "    time.sleep(0.02)\n",
        "    print(char, end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q40nAoiL3bub",
        "outputId": "251d809a-421c-4a52-f396-8c4a36e4a2b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hermione perguntoú\n",
            "ãru/wvfp1v\n",
            "0i0õḉ(qè)í,dv7ék(x'\"nçrd&1êùècã\n",
            "s,?á́ónú fàq/zz3&4ó6aec2b6ªglaáè'3â(7mùc-ªóóh?17aôcãzhú3çe ó-?gḉj9'ªp0ªªèuùf770í8br)  i\"à: ipxr&d3çiâ 4uà\"-ôsmd(i1iùúç-7n87kk-q9áh6gínbê1êc77/)urbl).sç?áàsmúádtwª46v:'cx)jó4bãdoqea/j2i7\"bn7abfù?rxírcâªx7915bq'bzà(!k6t'1\n",
            "́́p\"êô\"ú́qbxçrm'?)fú9u\n",
            "r45́rlzr"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "path_save = '/content/drive/MyDrive/Dirty-Talks/GPT/ckpt/gptzinho'\n",
        "model = charGPT()\n",
        "model.load_state_dict(torch.load(path_save))\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "context = \"hermione perguntou\"\n",
        "context = torch.tensor(encode(context)).to(device)\n",
        "\n",
        "out = decode(model.generate(context.unsqueeze(0), max_new_tokens=300)[0].tolist())\n",
        "for char in out:\n",
        "    time.sleep(0.02)\n",
        "    print(char, end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydULvkSj3KOs",
        "outputId": "11a78304-9be5-4816-be2b-4c4216cfeec0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hermione perguntou:\n",
            "harry teria desaparecer a espelho da magia.\n",
            "ele sirius abaixou o peito para nos novos preparos. esperemes de fudge\n",
            "que ele conhecia de armário! com a perda voldemort viverem a lacação de rony,\n",
            "que a foto se pôs e estavam fergueram, harry acabara um garoto de probir, tentou\n",
            "arme com firmeza de que "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obrigado e vida longa ao Transformers!!!"
      ],
      "metadata": {
        "id": "75weXJVe3eIQ"
      }
    }
  ]
}
