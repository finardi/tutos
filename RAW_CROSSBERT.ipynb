{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RAW-CROSSBERT.ipynb",
      "provenance": [],
      "mount_file_id": "1IKfeWhzCkHPDffDZoHHydKL2hq-zZKFC",
      "authorship_tag": "ABX9TyP+MP743CZxGP1rMDlEQPpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/RAW_CROSSBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja8ZfEQERp7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "addd59cb-8671-4df5-b073-c41bb71a1a7b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug 30 21:30:10 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Mn0b-9MhlE",
        "colab_type": "text"
      },
      "source": [
        "# Modeling util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZliQXunxNpt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy3uoeueM5Pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subbatch(toks, maxlen):\n",
        "    _, DLEN = toks.shape[:2]\n",
        "    SUBBATCH = math.ceil(DLEN / maxlen)\n",
        "    S = math.ceil(DLEN / SUBBATCH) if SUBBATCH > 0 else 0 # minimize the size given the number of subbatch\n",
        "    stack = []\n",
        "    if SUBBATCH == 1:\n",
        "        return toks, SUBBATCH\n",
        "    else:\n",
        "        for s in range(SUBBATCH):\n",
        "            stack.append(toks[:, s*S:(s+1)*S])\n",
        "            if stack[-1].shape[1] != S:\n",
        "                nulls = torch.zeros_like(toks[:, :S - stack[-1].shape[1]])\n",
        "                stack[-1] = torch.cat([stack[-1], nulls], dim=1)\n",
        "        return torch.cat(stack, dim=0), SUBBATCH\n",
        "\n",
        "\n",
        "def un_subbatch(embed, toks, maxlen):\n",
        "    BATCH, DLEN = toks.shape[:2]\n",
        "    SUBBATCH = math.ceil(DLEN / maxlen)\n",
        "    if SUBBATCH == 1:\n",
        "        return embed\n",
        "    else:\n",
        "        embed_stack = []\n",
        "        for b in range(SUBBATCH):\n",
        "            embed_stack.append(embed[b*BATCH:(b+1)*BATCH])\n",
        "        embed = torch.cat(embed_stack, dim=1)\n",
        "        embed = embed[:, :DLEN]\n",
        "        return embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Z_gqfaNiYt",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLt1BzC7NQl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "bd0fa92a-6925-4201-cbad-f576eb3ed2a2"
      },
      "source": [
        "!pip install -q pytools\n",
        "!pip install -q pytorch_pretrained_bert\n",
        "from pytools import memoize_method\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |████▉                           | 10kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.9MB/s \n",
            "\u001b[?25h  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.8MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR7qxZ_mM_YQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CustomBertModel(pytorch_pretrained_bert.BertModel):\n",
        "    \"\"\"\n",
        "    Based on pytorch_pretrained_bert.BertModel, but also outputs un-contextualized embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Based on pytorch_pretrained_bert.BertModel\n",
        "        \"\"\"\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        encoded_layers = self.encoder(embedding_output, extended_attention_mask, output_all_encoded_layers=True)\n",
        "\n",
        "        return [embedding_output] + encoded_layers\n",
        "\n",
        "class BertRanker(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.BERT_MODEL = 'bert-base-uncased'\n",
        "        self.CHANNELS = 12 + 1  # from bert-base-uncased\n",
        "        self.BERT_SIZE = 768  # from bert-base-uncased\n",
        "        self.bert = CustomBertModel.from_pretrained(self.BERT_MODEL)\n",
        "        self.tokenizer = pytorch_pretrained_bert.BertTokenizer.from_pretrained(self.BERT_MODEL)\n",
        "\n",
        "    def forward(self, **inputs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def save(self, path):\n",
        "        state = self.state_dict(keep_vars=True)\n",
        "        for key in list(state):\n",
        "            if state[key].requires_grad:\n",
        "                state[key] = state[key].data\n",
        "            else:\n",
        "                del state[key]\n",
        "        torch.save(state, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.load_state_dict(torch.load(path), strict=False)\n",
        "\n",
        "    @memoize_method\n",
        "    def tokenize(self, text):\n",
        "        toks = self.tokenizer.tokenize(text)\n",
        "        toks = [self.tokenizer.vocab[t] for t in toks]\n",
        "        return toks\n",
        "\n",
        "    def encode_bert(self, query_tok, query_mask, doc_tok, doc_mask, customBert=None):\n",
        "        BATCH, QLEN = query_tok.shape\n",
        "        DIFF = 3  # = [CLS] and 2x[SEP]\n",
        "        maxlen = self.bert.config.max_position_embeddings\n",
        "        MAX_DOC_TOK_LEN = maxlen - QLEN - DIFF\n",
        "\n",
        "        doc_toks, sbcount = subbatch(doc_tok, MAX_DOC_TOK_LEN)\n",
        "        doc_mask, _ = subbatch(doc_mask, MAX_DOC_TOK_LEN)\n",
        "\n",
        "        query_toks = torch.cat([query_tok] * sbcount, dim=0)\n",
        "        query_mask = torch.cat([query_mask] * sbcount, dim=0)\n",
        "\n",
        "        CLSS = torch.full_like(query_toks[:, :1], self.tokenizer.vocab['[CLS]'])\n",
        "        SEPS = torch.full_like(query_toks[:, :1], self.tokenizer.vocab['[SEP]'])\n",
        "        ONES = torch.ones_like(query_mask[:, :1])\n",
        "        NILS = torch.zeros_like(query_mask[:, :1])\n",
        "\n",
        "        # build BERT input sequences\n",
        "        toks = torch.cat([CLSS, doc_toks, SEPS, query_toks, SEPS], dim=1)\n",
        "        mask = torch.cat([ONES, doc_mask, ONES, query_mask, ONES], dim=1)\n",
        "        # segment_ids = torch.cat([NILS] * (2 + QLEN) + [ONES] * (1 + doc_toks.shape[1]), dim=1)\n",
        "        segment_ids = torch.cat([NILS] * (2 + doc_toks.shape[1]) + [ONES] * (1 + QLEN), dim=1)\n",
        "        toks[toks == -1] = 0  # remove padding (will be masked anyway)\n",
        "\n",
        "        # execute BERT model\n",
        "        if not customBert:\n",
        "            result = self.bert(toks, segment_ids.long(), mask)\n",
        "        else:\n",
        "            result = customBert(toks, segment_ids.long(), mask)\n",
        "\n",
        "        # extract relevant subsequences for query and doc\n",
        "        query_results = [r[:BATCH, 1:QLEN + 1] for r in result]\n",
        "        doc_results = [r[:, QLEN + 2:-1] for r in result]\n",
        "        doc_results = [un_subbatch(r, doc_tok, MAX_DOC_TOK_LEN) for r in doc_results]\n",
        "\n",
        "        # build CLS representation\n",
        "        cls_results = []\n",
        "        for layer in result:\n",
        "            cls_output = layer[:, 0]\n",
        "            cls_result = []\n",
        "            for i in range(cls_output.shape[0] // BATCH):\n",
        "                cls_result.append(cls_output[i * BATCH:(i + 1) * BATCH])\n",
        "            cls_result = torch.stack(cls_result, dim=2).mean(dim=2)\n",
        "            cls_results.append(cls_result)\n",
        "\n",
        "        return cls_results, query_results, doc_results\n",
        "\n",
        "\n",
        "\n",
        "class CrossBert(BertRanker):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "\n",
        "        self.args = args\n",
        "\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.cls = torch.nn.Linear(self.BERT_SIZE, 1)\n",
        "        self.cls2 = torch.nn.Linear(self.BERT_SIZE, 1)\n",
        "        self.clsAll = torch.nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, query_tok, query_mask, doc_tok, doc_mask, wiki_tok, wiki_mask, question_tok, question_mask):\n",
        "        cls_query_tok, _, _ = self.encode_bert(query_tok, query_mask, doc_tok, doc_mask)\n",
        "        cls_doc_tok, _, _ = self.encode_bert(doc_tok, doc_mask, query_tok, query_mask)\n",
        "        if self.args.mode % 2 == 0:\n",
        "            cls_wiki_doc_tok, _, _ = self.encode_bert(wiki_tok, wiki_mask, doc_tok, doc_mask)\n",
        "            cls_doc_wiki_tok, _, _ = self.encode_bert(doc_tok, doc_mask, wiki_tok, wiki_mask)\n",
        "\n",
        "        if self.args.mode == 1:\n",
        "            mul = torch.mul(cls_query_tok[-1], cls_doc_tok[-1])\n",
        "            return self.cls(self.dropout(mul))\n",
        "\n",
        "        elif self.args.mode == 2:\n",
        "            mul = torch.mul(cls_query_tok[-1], cls_doc_tok[-1])\n",
        "            mul_wiki = torch.mul(cls_wiki_doc_tok[-1], cls_doc_wiki_tok[-1])\n",
        "            cat = self.cls(self.dropout(mul))\n",
        "            cat_wiki = self.cls2(self.dropout(mul_wiki))\n",
        "            return self.clsAll(torch.cat([cat, cat_wiki], dim=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4xrn4vwM_nT",
        "colab_type": "text"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpHCWshrN0Wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f03ADMaXM_25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def read_datafiles(files):\n",
        "    queries, wikis, questions, docs, qtypes = {}, {}, {}, {}, {}\n",
        "    # for file in files:\n",
        "    for idx, file in enumerate(files):\n",
        "        for line in tqdm(file, desc='loading datafile (by line)', leave=False):\n",
        "            cols = line.rstrip().split('\\t')\n",
        "            if len(cols) == 3:\n",
        "                c_type, c_id, c_text = cols\n",
        "            elif len(cols) == 4:\n",
        "                c_type, c_id, c_text, c_qtype = cols\n",
        "            # assert c_type in ('query', 'doc', 'wiki', 'question')\n",
        "            # if c_type == 'query':\n",
        "            if idx == 0:\n",
        "                queries[c_id] = c_text\n",
        "                qtypes[c_id] = c_qtype\n",
        "            # elif c_type == 'doc':\n",
        "            elif idx == 1:\n",
        "                docs[c_id] = c_text\n",
        "            elif idx == 2:\n",
        "                wikis[c_id] = c_text\n",
        "            elif idx == 3:\n",
        "                questions[c_id] = c_text\n",
        "\n",
        "    return queries, docs, wikis, questions, qtypes\n",
        "\n",
        "\n",
        "def read_qrels_dict(file):\n",
        "    result = {}\n",
        "    for line in tqdm(file, desc='loading qrels (by line)', leave=False):\n",
        "        qid, _, docid, score, _ = line.split()\n",
        "        result.setdefault(qid, {})[docid] = int(score)\n",
        "    return result\n",
        "\n",
        "\n",
        "def read_run_dict(file):\n",
        "    result = {}\n",
        "    for line in tqdm(file, desc='loading run (by line)', leave=False):\n",
        "        qid, _, docid, rank, score, _ = line.split()\n",
        "        result.setdefault(qid, {})[docid] = float(score)\n",
        "    return result\n",
        "\n",
        "\n",
        "def read_pairs_dict(file):\n",
        "    result = {}\n",
        "    for line in tqdm(file, desc='loading pairs (by line)', leave=False):\n",
        "        qid, docid = line.split()\n",
        "        result.setdefault(qid, {})[docid] = 1\n",
        "    return result\n",
        "\n",
        "\n",
        "def iter_train_pairs(model, dataset, train_pairs, qrels, batch_size, args):\n",
        "    batch = {'query_id': [], 'doc_id': [], 'query_tok': [], 'doc_tok': [], 'wiki_tok': [], 'question_tok': [], 'label': [], 'query_raw':[], 'doc_raw':[], 'wiki_raw':[]}\n",
        "    for qid, did, query_tok, doc_tok, wiki_tok, question_tok, query, doc, wiki in _iter_train_pairs(model, dataset, train_pairs, qrels,\n",
        "                                                                                  args):\n",
        "        batch['query_id'].append(qid)\n",
        "        batch['doc_id'].append(did)\n",
        "        batch['query_tok'].append(query_tok)\n",
        "        batch['doc_tok'].append(doc_tok)\n",
        "        batch['wiki_tok'].append(wiki_tok)\n",
        "        batch['question_tok'].append(question_tok)\n",
        "        batch['query_raw'].append(query)\n",
        "        batch['doc_raw'].append(doc)\n",
        "        batch['wiki_raw'].append(wiki)\n",
        "\n",
        "        if len(batch['query_id']) // 2 == batch_size:\n",
        "            yield _pack_n_ship(batch, data, args)\n",
        "            batch = {'query_id': [], 'doc_id': [], 'query_tok': [], 'doc_tok': [], 'wiki_tok': [], 'question_tok': [], 'query_raw':[], 'doc_raw':[], 'wiki_raw':[]}\n",
        "\n",
        "\n",
        "def _iter_train_pairs(model, dataset, train_pairs, qrels, args):\n",
        "    ds_queries, ds_docs, ds_wikis, ds_questions, ds_qtypes = dataset\n",
        "    while True:\n",
        "        qids = list(train_pairs.keys())\n",
        "        random.shuffle(qids)\n",
        "        for qid in qids:\n",
        "            pos_ids = [did for did in train_pairs[qid] if qrels.get(qid, {}).get(did, 0) > 0]\n",
        "            if len(pos_ids) == 0:\n",
        "                continue\n",
        "\n",
        "            pos_id = random.choice(pos_ids)\n",
        "            neg_ids = [did for did in train_pairs[qid] if qrels.get(qid, {}).get(did, 0) == 0]\n",
        "\n",
        "            if len(neg_ids) == 0:\n",
        "                print(\"No neg instances\", qid)\n",
        "                continue\n",
        "\n",
        "            neg_id = random.choice(neg_ids)\n",
        "            query_tok = model.tokenize(ds_queries[qid])\n",
        "            wiki_tok = model.tokenize(ds_wikis[qid])\n",
        "            question_tok = model.tokenize(ds_questions[qid])\n",
        "\n",
        "            pos_doc = ds_docs.get(pos_id)\n",
        "            neg_doc = ds_docs.get(neg_id)\n",
        "            if pos_doc is None:\n",
        "                tqdm.write(f'missing doc {pos_id}! Skipping')\n",
        "                continue\n",
        "            if neg_doc is None:\n",
        "                tqdm.write(f'missing doc {neg_id}! Skipping')\n",
        "                continue\n",
        "\n",
        "            yield qid, pos_id, query_tok, model.tokenize(pos_doc), wiki_tok, question_tok, ds_queries[qid], pos_doc, ds_wikis[qid]\n",
        "            yield qid, neg_id, query_tok, model.tokenize(neg_doc), wiki_tok, question_tok, ds_queries[qid], neg_doc, ds_wikis[qid]\n",
        "\n",
        "        # break\n",
        "\n",
        "\n",
        "def iter_valid_records(model, dataset, run, batch_size, data, args):\n",
        "    batch = {'query_id': [], 'doc_id': [], 'query_tok': [], 'doc_tok': [], 'wiki_tok': [], 'question_tok': [], 'label': [], 'query_raw':[], 'doc_raw':[], 'wiki_raw':[] }\n",
        "\n",
        "    for qid, did, query_tok, doc_tok, wiki_tok, question_tok, query, doc, wiki, in _iter_valid_records(model, dataset, run, args):\n",
        "        batch['query_id'].append(qid)\n",
        "        batch['doc_id'].append(did)\n",
        "        batch['query_tok'].append(query_tok)\n",
        "        batch['doc_tok'].append(doc_tok)\n",
        "        batch['wiki_tok'].append(wiki_tok)\n",
        "        batch['question_tok'].append(question_tok)\n",
        "        batch['query_raw'].append(query)\n",
        "        batch['doc_raw'].append(doc)\n",
        "        batch['wiki_raw'].append(wiki)\n",
        "\n",
        "        if len(batch['query_id']) == batch_size:\n",
        "            yield _pack_n_ship(batch, data, args)\n",
        "            batch = {'query_id': [], 'doc_id': [], 'query_tok': [], 'doc_tok': [], 'wiki_tok': [], 'question_tok': [], 'query_raw':[], 'doc_raw':[], 'wiki_raw':[]}\n",
        "\n",
        "    # final batch\n",
        "    if len(batch['query_id']) > 0:\n",
        "        yield _pack_n_ship(batch, data, args)\n",
        "\n",
        "\n",
        "def _iter_valid_records(model, dataset, run, args):\n",
        "    ds_queries, ds_docs, ds_wikis, ds_questions, ds_qtypes = dataset\n",
        "    for qid in run:\n",
        "        query_tok = model.tokenize(ds_queries[qid])\n",
        "        wiki_tok = model.tokenize(ds_wikis[qid])\n",
        "        question_tok = model.tokenize(ds_questions[qid])\n",
        "\n",
        "        for did in run[qid]:\n",
        "            doc = ds_docs.get(did)\n",
        "            if doc is None:\n",
        "                tqdm.write(f'missing doc {did}! Skipping')\n",
        "                continue\n",
        "            doc_tok = model.tokenize(doc)\n",
        "            yield qid, did, query_tok, doc_tok, wiki_tok, question_tok, ds_queries[qid], doc, ds_wikis[qid]\n",
        "\n",
        "\n",
        "def _pack_n_ship(batch, data, args):\n",
        "\n",
        "    QLEN = min(args.maxlen, int(np.max([len(b) for b in batch['query_tok']])))\n",
        "    DLEN = min(args.maxlen, int(np.max([len(b) for b in batch['doc_tok']])))\n",
        "    WLEN = min(args.maxlen, int(np.max([len(b) for b in batch['wiki_tok']])))\n",
        "    QQLEN = min(args.maxlen, int(np.max([len(b) for b in batch['question_tok']])))\n",
        "\n",
        "\n",
        "    return {\n",
        "        'query_id': batch['query_id'],\n",
        "        'doc_id': batch['doc_id'],\n",
        "        'query_tok': _pad_crop(batch['query_tok'], QLEN),\n",
        "        'doc_tok': _pad_crop(batch['doc_tok'], DLEN),\n",
        "        'wiki_tok': _pad_crop(batch['wiki_tok'], WLEN),\n",
        "        'question_tok': _pad_crop(batch['question_tok'], QQLEN),\n",
        "        'query_mask': _mask(batch['query_tok'], QLEN),\n",
        "        'doc_mask': _mask(batch['doc_tok'], DLEN),\n",
        "        'wiki_mask': _mask(batch['wiki_tok'], WLEN),\n",
        "        'question_mask': _mask(batch['question_tok'], QQLEN),\n",
        "    }\n",
        "\n",
        "def toTensor(x):\n",
        "    # print(torch.tensor(x))\n",
        "    # try:\n",
        "    return torch.tensor(x).float().cuda() if device.type == 'cuda' else torch.tensor(x).float()\n",
        "    # except:\n",
        "    #     print(x)\n",
        "\n",
        "\n",
        "def _pad_crop_np(items, l):\n",
        "    results = []\n",
        "    for item in items:\n",
        "        if len(item) < l:\n",
        "            while len(item) != l:\n",
        "                item.append([0] * 100)\n",
        "        if len(item) > l:\n",
        "            item = item[:l]\n",
        "        results.append(item)\n",
        "    return torch.tensor(results).float().cuda() if device.type == 'cuda' else torch.tensor(results).float()\n",
        "\n",
        "\n",
        "def _pad_crop(items, l, val=-1):\n",
        "    result = []\n",
        "    for item in items:\n",
        "        if len(item) < l:\n",
        "            item = item + [val] * (l - len(item))\n",
        "        if len(item) > l:\n",
        "            item = item[:l]\n",
        "        result.append(item)\n",
        "    return torch.tensor(result).long().cuda() if device.type == 'cuda' else torch.tensor(result).long()\n",
        "\n",
        "\n",
        "def _mask(items, l):\n",
        "    result = []\n",
        "    for item in items:\n",
        "        # needs padding (masked)\n",
        "        if len(item) < l:\n",
        "            mask = [1. for _ in item] + ([0.] * (l - len(item)))\n",
        "        # no padding (possible crop)\n",
        "        if len(item) >= l:\n",
        "            mask = [1. for _ in item[:l]]\n",
        "        result.append(mask)\n",
        "    return torch.tensor(result).float().cuda() if device.type == 'cuda' else torch.tensor(result).float()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swFqN-33N3Mo",
        "colab_type": "text"
      },
      "source": [
        "# Train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sUhCYXtOIaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q pyNTCIREVAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvJjnXPaOb4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/BERT/CrossBERT/')\n",
        "\n",
        "!unzip -q '/content/drive/My Drive/Colab Notebooks/BERT/CrossBERT/data_CrossBERT.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUrSKZfdOD4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import Data\n",
        "import os, io\n",
        "import argparse\n",
        "import subprocess\n",
        "from time import strftime, localtime\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random, pickle\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from pyNTCIREVAL import Labeler\n",
        "from pyNTCIREVAL.metrics import MSnDCG, nERR, nDCG, AP, RR\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlCa0wjYSN-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "random.seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zatdftCN7yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_MAP = {'crossbert': CrossBert}\n",
        "\n",
        "def main(model, dataset, train_pairs, qrels, valid_run, test_run, model_out_dir, qrelDict, modelName, fold,\n",
        "         metricKeys, MAX_EPOCH, data, args):\n",
        "    LR = 0.001\n",
        "    BERT_LR = 2e-5\n",
        "\n",
        "    params = [(k, v) for k, v in model.named_parameters() if v.requires_grad]\n",
        "    non_bert_params = {'params': [v for k, v in params if not k.startswith('bert.')]}\n",
        "    bert_params = {'params': [v for k, v in params if k.startswith('bert.')], 'lr': BERT_LR}\n",
        "    optimizer = torch.optim.Adam([non_bert_params, bert_params], lr=LR)\n",
        "    # optimizer = torch.optim.Adam([non_bert_params], lr=LR)\n",
        "\n",
        "    top_valid_score = None\n",
        "    bestResults = {}\n",
        "    bestPredictions = []\n",
        "    bestQids = []\n",
        "\n",
        "    print(\"Fold: %d\" % fold)\n",
        "\n",
        "    if args.model in [\"unsup\"]:\n",
        "\n",
        "        test_qids, test_results, test_predictions = validate(model, dataset, test_run, qrelDict, 0,\n",
        "                                                             model_out_dir, data, args, \"test\")\n",
        "\n",
        "        print(test_results[\"ndcg@15\"])\n",
        "        txt = 'new top validation score, %.4f' % np.mean(test_results[\"ndcg@10\"])\n",
        "        print2file(args.out_dir, modelName, \".txt\", txt, fold)\n",
        "\n",
        "        bestResults = test_results\n",
        "        bestPredictions = test_predictions\n",
        "        bestQids = test_qids\n",
        "        pass\n",
        "    else:\n",
        "        for epoch in range(MAX_EPOCH):\n",
        "            t2 = time.time()\n",
        "            loss = train_iteration(model, optimizer, dataset, train_pairs, qrels, data, args)\n",
        "            txt = f'train epoch={epoch} loss={loss}'\n",
        "            print2file(args.out_dir, modelName, \".txt\", txt, fold)\n",
        "\n",
        "            valid_qids, valid_results, valid_predictions = validate(model, dataset, valid_run, qrelDict, epoch,\n",
        "                                                                    model_out_dir, data, args, \"valid\")\n",
        "\n",
        "            # valid_score = np.mean(valid_results[\"rp\"])\n",
        "            valid_score = np.mean(valid_results[\"ndcg@10\"])\n",
        "            elapsed_time = time.time() - t2\n",
        "            txt = f'validation epoch={epoch} score={valid_score} : {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\n",
        "            print2file(args.out_dir, modelName, \".txt\", txt, fold)\n",
        "            if top_valid_score is None or valid_score > top_valid_score:\n",
        "                top_valid_score = valid_score\n",
        "                # model.save(os.path.join(model_out_dir, 'weights.p'))\n",
        "                test_qids, test_results, test_predictions = validate(model, dataset, test_run, qrelDict, epoch,\n",
        "                                                                     model_out_dir, data, args, \"test\")\n",
        "\n",
        "                # print(test_results[\"ndcg@15\"])\n",
        "                txt = 'new top validation score, %.4f' % np.mean(test_results[\"ndcg@10\"])\n",
        "                print2file(args.out_dir, modelName, \".txt\", txt, fold)\n",
        "\n",
        "                bestResults = test_results\n",
        "                bestPredictions = test_predictions\n",
        "                bestQids = test_qids\n",
        "\n",
        "            # elif args.earlystop and epoch >=4:\n",
        "            elif args.earlystop:\n",
        "                break\n",
        "\n",
        "\n",
        "    #   save outputs to files\n",
        "\n",
        "    for k in metricKeys:\n",
        "        result2file(args.out_dir, modelName, \".\" + k, bestResults[k], bestQids, fold)\n",
        "\n",
        "    prediction2file(args.out_dir, modelName, \".out\", bestPredictions, fold)\n",
        "\n",
        "    print2file(args.out_dir, modelName, \".txt\", txt, fold)\n",
        "    return bestResults\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMEFiHphSUyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_iteration(model, optimizer, dataset, train_pairs, qrels, data, args):\n",
        "    BATCH_SIZE = 16\n",
        "    BATCHES_PER_EPOCH = 32 if \"eai\" in args.data else 256\n",
        "    GRAD_ACC_SIZE = 2\n",
        "    total = 0\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    with tqdm('training', total=BATCH_SIZE * BATCHES_PER_EPOCH, ncols=80, desc='train', leave=False) as pbar:\n",
        "        for record in Data.iter_train_pairs(model, dataset, train_pairs, qrels, GRAD_ACC_SIZE, data, args):\n",
        "            scores = model(record['query_tok'],\n",
        "                           record['query_mask'],\n",
        "                           record['doc_tok'],\n",
        "                           record['doc_mask'],\n",
        "                           record['wiki_tok'],\n",
        "                           record['wiki_mask'],\n",
        "                           record['question_tok'],\n",
        "                           record['question_mask'])\n",
        "\n",
        "            count = len(record['query_id']) // 2\n",
        "            scores = scores.reshape(count, 2)\n",
        "            loss = torch.mean(1. - scores.softmax(dim=1)[:, 0])  # pariwse softmax\n",
        "            loss.backward()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total += count\n",
        "            if total % BATCH_SIZE == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "            pbar.update(count)\n",
        "            if total >= BATCH_SIZE * BATCHES_PER_EPOCH:\n",
        "                return total_loss\n",
        "            # break\n",
        "\n",
        "\n",
        "def validate(model, dataset, run, qrel, epoch, model_out_dir, data, args, desc):\n",
        "    runf = os.path.join(model_out_dir, f'{epoch}.run')\n",
        "    return run_model(model, dataset, run, runf, qrel, data, args, desc)\n",
        "\n",
        "\n",
        "def run_model(model, dataset, run, runf, qrels, data, args, desc='valid'):\n",
        "    BATCH_SIZE = 16\n",
        "    rerank_run = {}\n",
        "    with torch.no_grad(), tqdm(total=sum(len(r) for r in run.values()), ncols=80, desc=desc, leave=False) as pbar:\n",
        "        model.eval()\n",
        "        for records in Data.iter_valid_records(model, dataset, run, BATCH_SIZE, data, args):\n",
        "            scores = model(records['query_tok'],\n",
        "                           records['query_mask'],\n",
        "                           records['doc_tok'],\n",
        "                           records['doc_mask'],\n",
        "                           records['wiki_tok'],\n",
        "                           records['wiki_mask'],\n",
        "                           records['question_tok'],\n",
        "                           records['question_mask'])\n",
        "\n",
        "            for qid, did, score in zip(records['query_id'], records['doc_id'], scores):\n",
        "                rerank_run.setdefault(qid, {})[did] = score.item()\n",
        "            pbar.update(len(records['query_id']))\n",
        "            # break\n",
        "\n",
        "    res = {\"%s@%d\" % (i, j): [] for i in [\"p\", \"r\", \"ndcg\", \"nerr\"] for j in [5, 10, 15, 20]}\n",
        "    res['map'] = []\n",
        "    res['mrr'] = []\n",
        "    res['rp'] = []\n",
        "    predictions = []\n",
        "    qids = []\n",
        "\n",
        "    for qid in rerank_run:\n",
        "        ranked_list_scores = sorted(rerank_run[qid].items(), key=lambda x: x[1], reverse=True)\n",
        "        ranked_list = [i[0] for i in ranked_list_scores]\n",
        "        for (pid, score) in ranked_list_scores:\n",
        "            predictions.append((qid, pid, score))\n",
        "        result = eval(qrels[qid], ranked_list)\n",
        "        for key in res:\n",
        "            res[key].append(result[key])\n",
        "        qids.append(qid)\n",
        "    return qids, res, predictions\n",
        "\n",
        "\n",
        "def eval(qrels, ranked_list):\n",
        "    grades = [1, 2, 3, 4]  # a grade for relevance levels 1 and 2 (Note that level 0 is excluded)\n",
        "    labeler = Labeler(qrels)\n",
        "    labeled_ranked_list = labeler.label(ranked_list)\n",
        "    rel_level_num = 5\n",
        "    xrelnum = labeler.compute_per_level_doc_num(rel_level_num)\n",
        "    result = {}\n",
        "\n",
        "    for i in [5, 10, 15, 20]:\n",
        "        metric = MSnDCG(xrelnum, grades, cutoff=i)\n",
        "        result[\"ndcg@%d\" % i] = metric.compute(labeled_ranked_list)\n",
        "\n",
        "        nerr = nERR(xrelnum, grades, cutoff=i)\n",
        "        result[\"nerr@%d\" % i] = nerr.compute(labeled_ranked_list)\n",
        "\n",
        "        _ranked_list = ranked_list[:i]\n",
        "        result[\"p@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(_ranked_list)\n",
        "        result[\"r@%d\" % i] = len(set.intersection(set(qrels.keys()), set(_ranked_list))) / len(qrels)\n",
        "\n",
        "    result[\"rp\"] = len(set.intersection(set(qrels.keys()), set(ranked_list[:len(qrels)]))) / len(qrels)\n",
        "    metric = MSnDCG(xrelnum, grades, cutoff=i)\n",
        "\n",
        "    map = AP(xrelnum, grades)\n",
        "    result[\"map\"] = map.compute(labeled_ranked_list)\n",
        "    mrr = RR()\n",
        "    result[\"mrr\"] = mrr.compute(labeled_ranked_list)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def write2file(path, name, format, output):\n",
        "    print(output)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    thefile = open(path + name + format, 'a')\n",
        "    thefile.write(\"%s\\n\" % output)\n",
        "    thefile.close()\n",
        "\n",
        "\n",
        "def prediction2file(path, name, format, preds, fold):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    thefile = open(path + name + format, 'a')\n",
        "    for (qid, pid, score) in preds:\n",
        "        thefile.write(\"%d\\t%s\\t%s\\t%f\\n\" % (fold, qid, pid, score))\n",
        "    thefile.close()\n",
        "\n",
        "def print2file(path, name, format, printout, fold):\n",
        "    print(printout)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    thefile = open(path + name + format, 'a')\n",
        "    thefile.write(\"%d-%s\\n\" % (fold, printout))\n",
        "    thefile.close()\n",
        "\n",
        "def result2file(path, name, format, res, qids, fold):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    thefile = open(path + name + format, 'a')\n",
        "    for q, r in zip(qids, res):\n",
        "        thefile.write(\"%d\\t%s\\t%f\\n\" % (fold, q, r))\n",
        "    thefile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhG5KnD6jWVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main_cli():\n",
        "    # argument\n",
        "    parser = argparse.ArgumentParser('CEDR model training and validation')\n",
        "    parser.add_argument('--model', choices=MODEL_MAP.keys(), default='crossbert')\n",
        "    parser.add_argument('--data', default='akgg')\n",
        "    parser.add_argument('--path', default=\"data/\")\n",
        "    parser.add_argument('--wikifile', default=\"wikihow\")\n",
        "    parser.add_argument('--questionfile', default=\"question-qq\")\n",
        "    parser.add_argument('--initial_bert_weights', type=argparse.FileType('rb'))\n",
        "    parser.add_argument('--model_out_dir', default=\"models/vbert\")\n",
        "    parser.add_argument('--epoch', type=int, default=1)\n",
        "    parser.add_argument('--fold', type=int, default=2)\n",
        "    parser.add_argument('--out_dir', default=\"out/\")\n",
        "    parser.add_argument('--evalMode', default=\"all\")\n",
        "    parser.add_argument('--mode', type=int, default=2)\n",
        "    parser.add_argument('--maxlen', type=int, default=16)\n",
        "    parser.add_argument('--earlystop', type=int, default=1)\n",
        "\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    args.queryfile = io.TextIOWrapper(io.open(\"%s%s-query.tsv\" % (args.path, args.data.split(\"-\")[0]),'rb'), 'UTF-8')\n",
        "    args.docfile = io.TextIOWrapper(io.open(\"%s%s-doc.tsv\" % (args.path, args.data.split(\"-\")[0]),'rb'), 'UTF-8')\n",
        "    args.wikifile = io.TextIOWrapper(io.open(\"%s%s-%s.tsv\" % (args.path, args.data.split(\"-\")[0], args.wikifile),'rb'), 'UTF-8')\n",
        "    args.questionfile = io.TextIOWrapper(io.open(\"%s%s-%s.tsv\" % (args.path, args.data.split(\"-\")[0], args.questionfile),'rb'), 'UTF-8')\n",
        "\n",
        "    args.train_pairs = \"%s%s-train\" % (args.path, args.data)\n",
        "    args.valid_run = \"%s%s-valid\" % (args.path, args.data)\n",
        "    args.test_run = \"%s%s-test\" % (args.path, args.data)\n",
        "\n",
        "    args.qrels = io.TextIOWrapper(io.open(\"%s%s-qrel.tsv\" % (args.path, args.data.split(\"-\")[0]),'rb'), 'UTF-8')\n",
        "\n",
        "    dataset = read_datafiles([args.queryfile, args.docfile, args.wikifile,\n",
        "                                   args.questionfile])\n",
        "    args.dataset = dataset\n",
        "    model = MODEL_MAP[args.model](args).cuda() if device.type == 'cuda' else MODEL_MAP[args.model](args)\n",
        "\n",
        "\n",
        "    # if args.model == \"cedr_pacrr\":\n",
        "    #     args.maxlen = 16 if args.mode == 1 else args.maxlen * args.mode\n",
        "    #     model = MODEL_MAP[args.model](args).cuda() if Data.device.type == 'cuda' else MODEL_MAP[args.model](\n",
        "    #         args)\n",
        "\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(pytorch_total_params)\n",
        "\n",
        "    qrels = read_qrels_dict(args.qrels)\n",
        "\n",
        "    MAX_EPOCH = args.epoch\n",
        "\n",
        "    train_pairs = []\n",
        "    valid_run = []\n",
        "    test_run = []\n",
        "\n",
        "    foldNum = args.fold\n",
        "    for fold in range(foldNum):\n",
        "        f = open(args.train_pairs + \"%d.tsv\" % fold, \"r\")\n",
        "        train_pairs.append(read_pairs_dict(f))\n",
        "        f = open(args.valid_run + \"%d.tsv\" % fold, \"r\")\n",
        "        valid_run.append(read_run_dict(f))\n",
        "        f = open(args.test_run + \"%d.tsv\" % fold, \"r\")\n",
        "        test_run.append(read_run_dict(f))\n",
        "\n",
        "    if args.initial_bert_weights is not None:\n",
        "        model.load(args.initial_bert_weights.name)\n",
        "    os.makedirs(args.model_out_dir, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(args.out_dir):\n",
        "        os.makedirs(args.out_dir)\n",
        "\n",
        "    timestamp = strftime('%Y_%m_%d_%H_%M_%S', localtime())\n",
        "    if \"birch\" in args.model:\n",
        "        wikiName = args.wikifile.name.split(\"/\")[-1].replace(\".tsv\", \"\")\n",
        "        questionName = args.questionfile.name.split(\"/\")[-1].replace(\".tsv\", \"\")\n",
        "        additionName = []\n",
        "        if args.mode in [1, 3, 5, 6]:\n",
        "            additionName.append(wikiName)\n",
        "        if args.mode in [2, 4, 5, 6]:\n",
        "            additionName.append(questionName)\n",
        "\n",
        "        modelName = \"%s_m%d_%s_%s_%s_e%d_es%d_%s\" % (\n",
        "            args.model, args.mode, args.data, \"_\".join(additionName), args.evalMode, args.epoch, args.earlystop, timestamp)\n",
        "    else:\n",
        "        wikipediaFile = args.wikifile.name.split(\"/\")[-1].replace(\".tsv\", \"\")\n",
        "        questionFile = args.questionfile.name.split(\"/\")[-1].replace(\".tsv\", \"\")\n",
        "        modelName = \"%s_%s_m%d_ml%d_%s_%s_%s_e%d_es%d_%s\" % (args.data, args.model, args.mode, args.maxlen, wikipediaFile, questionFile, args.evalMode, args.epoch, args.earlystop, timestamp)\n",
        "\n",
        "    print(modelName)\n",
        "\n",
        "    df = pd.read_csv(\"%s%s-qrel.tsv\" % (args.path, args.data.split(\"-\")[0]), sep=\"\\t\", names=[\"qid\", \"empty\", \"pid\", \"rele_label\", \"etype\"])\n",
        "    qrelDict = collections.defaultdict(dict)\n",
        "    type2pids = collections.defaultdict(set)\n",
        "    for qid, prop, label, etype in df[['qid', 'pid', 'rele_label', 'etype']].values:\n",
        "        qrelDict[str(qid)][str(prop)] = int(label)\n",
        "        type2pids[str(etype)].add(prop)\n",
        "    args.type2pids = type2pids\n",
        "\n",
        "\n",
        "    metricKeys = {\"%s@%d\" % (i, j): [] for i in [\"p\", \"r\", \"ndcg\", \"nerr\"] for j in [5, 10, 15, 20]}\n",
        "    metricKeys[\"rp\"] = []\n",
        "    metricKeys[\"mrr\"] = []\n",
        "    metricKeys[\"map\"] = []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    t1 = time.time()\n",
        "\n",
        "\n",
        "    args.isUnsupervised = True if args.model in [\"sen_emb\"] else False\n",
        "\n",
        "\n",
        "    for fold in range(len(train_pairs)):\n",
        "        results.append(\n",
        "            main(model, dataset, train_pairs[fold], qrels, valid_run[fold], test_run[fold], args.model_out_dir,\n",
        "                 qrelDict, modelName, fold, metricKeys, MAX_EPOCH, Data, args))\n",
        "    elapsed_time = time.time() - t1\n",
        "    txt = f'total : {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\n",
        "    print2file(args.out_dir, modelName, \".txt\", txt, fold)\n",
        "\n",
        "\n",
        "    #   average results across 5 folds\n",
        "    output = []\n",
        "    for k in metricKeys:\n",
        "        tmp = []\n",
        "        for fold in range(foldNum):\n",
        "            tmp.extend(results[fold][k])\n",
        "        _res = np.mean(tmp)\n",
        "        output.append(\"%.4f\" % _res)\n",
        "    write2file(args.out_dir, modelName, \".res\", \",\".join(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX3ToTlnN77b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "62911114-387e-482a-fbff-6c2d14495ade"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main_cli()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:   0%|                                           | 0/4096 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "109483781\n",
            "akgg_crossbert_m2_ml16_akgg-wikihow_akgg-question-qq_all_e1_es1_2020_08_30_22_35_57\n",
            "Fold: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "valid:   1%|▍                                | 16/1316 [00:00<00:08, 155.72it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train epoch=0 loss=706.056735008955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "test:   2%|▋                                 | 32/1587 [00:00<00:08, 176.09it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation epoch=0 score=0.5206618202494671 : 00:06:39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train:   0%|                                   | 2/4096 [00:00<05:55, 11.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "new top validation score, 0.5109\n",
            "new top validation score, 0.5109\n",
            "Fold: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-256a38128d35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-7b5be40d6823>\u001b[0m in \u001b[0;36mmain_cli\u001b[0;34m()\u001b[0m\n\u001b[1;32m    253\u001b[0m         results.append(\n\u001b[1;32m    254\u001b[0m             main(model, dataset, train_pairs[fold], qrels, valid_run[fold], test_run[fold], args.model_out_dir,\n\u001b[0;32m--> 255\u001b[0;31m                  qrelDict, modelName, fold, metricKeys, MAX_EPOCH, Data, args))\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'total : {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5954c63e8093>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, dataset, train_pairs, qrels, valid_run, test_run, model_out_dir, qrelDict, modelName, fold, metricKeys, MAX_EPOCH, data, args)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'train epoch={epoch} loss={loss}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint2file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-7b5be40d6823>\u001b[0m in \u001b[0;36mtrain_iteration\u001b[0;34m(model, optimizer, dataset, train_pairs, qrels, data, args)\u001b[0m\n\u001b[1;32m     15\u001b[0m                            \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wiki_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                            \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question_tok'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                            record['question_mask'])\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-334159f31b52>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_tok, query_mask, doc_tok, doc_mask, wiki_tok, wiki_mask, question_tok, question_mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mcls_doc_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mcls_wiki_doc_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mcls_doc_wiki_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-334159f31b52>\u001b[0m in \u001b[0;36mencode_bert\u001b[0;34m(self, query_tok, query_mask, doc_tok, doc_mask, customBert)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# execute BERT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcustomBert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustomBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-334159f31b52>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mencoded_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, attention_mask)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    712\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP2CsN33Pow6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}