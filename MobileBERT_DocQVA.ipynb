{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MobileBERT -  DocQVA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Ee8tClO-Mn2klXyBdkw6qF4tq666nhdG",
      "authorship_tag": "ABX9TyMmGlfPk/LWLilsgAL7QszU",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c7b7db5a73d4f8b96bfcd9b0db89650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6dcaea55f862498ca09c5f34a08470ef",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8c8b2a9d5f7846c1978c5fe21749cfb3",
              "IPY_MODEL_e3f4a71b73044034a14195a56d5da93e"
            ]
          }
        },
        "6dcaea55f862498ca09c5f34a08470ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c8b2a9d5f7846c1978c5fe21749cfb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_486010e859224010a0e6ea7cdee5fa14",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b584f7f0ed5141a0868f8cc2d88eb075"
          }
        },
        "e3f4a71b73044034a14195a56d5da93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_443a2f668c0647ad80db62f01c80a2c1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:01&lt;00:00, 137kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3524773989c4448d973f268ec0de07df"
          }
        },
        "486010e859224010a0e6ea7cdee5fa14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b584f7f0ed5141a0868f8cc2d88eb075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "443a2f668c0647ad80db62f01c80a2c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3524773989c4448d973f268ec0de07df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4eca62c92d245549cbc6d82564fcf4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cc78dbd85d834e679351da037fb0d1ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6a57d7c2292447caccc03189caff0a4",
              "IPY_MODEL_4a6e7ab460e148868d38dc92ac3f907d"
            ]
          }
        },
        "cc78dbd85d834e679351da037fb0d1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6a57d7c2292447caccc03189caff0a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_df4dab5482e64632913223819e5b6b3f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e3b84e282bc543a4815fa51f406f45b9"
          }
        },
        "4a6e7ab460e148868d38dc92ac3f907d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a0d40ad258db438f94e4c93b0e6eb664",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 1.06MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_14a3f6dd04cb4d25bbac22e9d55c8c56"
          }
        },
        "df4dab5482e64632913223819e5b6b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e3b84e282bc543a4815fa51f406f45b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0d40ad258db438f94e4c93b0e6eb664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "14a3f6dd04cb4d25bbac22e9d55c8c56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54532173e75f42f2840597152aa9d443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eac79044e65a452f82cd5a1edac645c2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1feab41bc4ee423b907000f928dfa267",
              "IPY_MODEL_37542af4ece34729818e8e02846ee098"
            ]
          }
        },
        "eac79044e65a452f82cd5a1edac645c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1feab41bc4ee423b907000f928dfa267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6d9fa019b0d94d579eb639d533e07de6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 560,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 560,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9d9f1c7d6594da091e52e187bde21d7"
          }
        },
        "37542af4ece34729818e8e02846ee098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0afe2034d6574d13b66fcef326c1d72b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 560/560 [00:02&lt;00:00, 259B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bbdcdbeee939477688abcc6115b89208"
          }
        },
        "6d9fa019b0d94d579eb639d533e07de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9d9f1c7d6594da091e52e187bde21d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0afe2034d6574d13b66fcef326c1d72b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bbdcdbeee939477688abcc6115b89208": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f3e943dfde5746d3ba6c2ab19ac71f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7748947023b44e9b92e1b4750cf50d9e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5aa32b04fabd41c49b0944991dd0bbaf",
              "IPY_MODEL_79ecdde29f5b451e8d96c223da6aeb8f"
            ]
          }
        },
        "7748947023b44e9b92e1b4750cf50d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5aa32b04fabd41c49b0944991dd0bbaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_96b22836ce55424ebca63eb9f70e4313",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 146671951,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 146671951,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7cd572f6c45482b8fbea9f72e333d09"
          }
        },
        "79ecdde29f5b451e8d96c223da6aeb8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e68558bb21745ec873696040a4daa17",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 147M/147M [00:01&lt;00:00, 83.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_508a65869fd345eba62858bc074582ba"
          }
        },
        "96b22836ce55424ebca63eb9f70e4313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7cd572f6c45482b8fbea9f72e333d09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e68558bb21745ec873696040a4daa17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "508a65869fd345eba62858bc074582ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/MobileBERT_DocQVA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSruUD6mT-5X",
        "outputId": "0d236e04-9e2e-4c8d-d727-bb9996ae41a4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jan  8 02:58:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    25W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbYr6Eam-y_4",
        "outputId": "6c0f198b-df64-45f1-9634-1e8887f8fb06"
      },
      "source": [
        "%%time\n",
        "!tar -xf /content/drive/MyDrive/Colab\\ Notebooks/Final-project/train.tar.gz \n",
        "!tar -xf /content/drive/MyDrive/Colab\\ Notebooks/Final-project/val.tar.gz \n",
        "!tar -xf /content/drive/MyDrive/Colab\\ Notebooks/Final-project/test.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 392 ms, sys: 54.7 ms, total: 447 ms\n",
            "Wall time: 3min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka8XiM-bicvy",
        "outputId": "7936eb2f-62ca-4490-8d24-2cad8c0841d3"
      },
      "source": [
        "!pip install -q \"transformers<4.0.0\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.3MB 14.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 54.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 59.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 67.3MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "7c7b7db5a73d4f8b96bfcd9b0db89650",
            "6dcaea55f862498ca09c5f34a08470ef",
            "8c8b2a9d5f7846c1978c5fe21749cfb3",
            "e3f4a71b73044034a14195a56d5da93e",
            "486010e859224010a0e6ea7cdee5fa14",
            "b584f7f0ed5141a0868f8cc2d88eb075",
            "443a2f668c0647ad80db62f01c80a2c1",
            "3524773989c4448d973f268ec0de07df",
            "c4eca62c92d245549cbc6d82564fcf4e",
            "cc78dbd85d834e679351da037fb0d1ba",
            "d6a57d7c2292447caccc03189caff0a4",
            "4a6e7ab460e148868d38dc92ac3f907d",
            "df4dab5482e64632913223819e5b6b3f",
            "e3b84e282bc543a4815fa51f406f45b9",
            "a0d40ad258db438f94e4c93b0e6eb664",
            "14a3f6dd04cb4d25bbac22e9d55c8c56",
            "54532173e75f42f2840597152aa9d443",
            "eac79044e65a452f82cd5a1edac645c2",
            "1feab41bc4ee423b907000f928dfa267",
            "37542af4ece34729818e8e02846ee098",
            "6d9fa019b0d94d579eb639d533e07de6",
            "e9d9f1c7d6594da091e52e187bde21d7",
            "0afe2034d6574d13b66fcef326c1d72b",
            "bbdcdbeee939477688abcc6115b89208",
            "f3e943dfde5746d3ba6c2ab19ac71f83",
            "7748947023b44e9b92e1b4750cf50d9e",
            "5aa32b04fabd41c49b0944991dd0bbaf",
            "79ecdde29f5b451e8d96c223da6aeb8f",
            "96b22836ce55424ebca63eb9f70e4313",
            "c7cd572f6c45482b8fbea9f72e333d09",
            "1e68558bb21745ec873696040a4daa17",
            "508a65869fd345eba62858bc074582ba"
          ]
        },
        "id": "1b7BubuuiYRZ",
        "outputId": "ddefa9a3-68d8-405d-a6e7-eb3edd1e6f66"
      },
      "source": [
        "# Python / Básics\n",
        "import os\n",
        "import gc\n",
        "import glob\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import numpy as np\n",
        "import collections\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# PIL \n",
        "from PIL import Image\n",
        " \n",
        "# Torch\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from collections import OrderedDict\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
        " \n",
        " \n",
        "# Transformers \n",
        "from transformers import get_linear_schedule_with_warmup, AdamW\n",
        "from transformers import MobileBertTokenizer, MobileBertForQuestionAnswering\n",
        "\n",
        "model_checkpoint = 'google/mobilebert-uncased'\n",
        "tokenizer = MobileBertTokenizer.from_pretrained(model_checkpoint)\n",
        "model = MobileBertForQuestionAnswering.from_pretrained(model_checkpoint, return_dict=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c7b7db5a73d4f8b96bfcd9b0db89650",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4eca62c92d245549cbc6d82564fcf4e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54532173e75f42f2840597152aa9d443",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=560.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3e943dfde5746d3ba6c2ab19ac71f83",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=146671951.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing MobileBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MobileBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of MobileBertForQuestionAnswering were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QafApgcKijGp",
        "outputId": "51b55db9-75b1-4c9b-fc35-35dc79d67892"
      },
      "source": [
        "# ==================================== #\n",
        "# === Função que carregas as seeds === #\n",
        "# ==================================== #\n",
        " \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "manual_seed = 2357 # only primers ;)\n",
        " \n",
        "def deterministic(rep=True):\n",
        "    if rep:\n",
        "        np.random.seed(manual_seed)\n",
        "        torch.manual_seed(manual_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(manual_seed)\n",
        "            torch.cuda.manual_seed_all(manual_seed)\n",
        "        torch.backends.cudnn.enabled = False \n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        print(f'Experimento deterministico, seed: {manual_seed} -- ', end = '')\n",
        "        print(f'Existe {torch.cuda.device_count()} GPU\\\n",
        " {torch.cuda.get_device_name(0)} disponível.')\n",
        "    else:\n",
        "        print('Experimento randomico')\n",
        "deterministic()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experimento deterministico, seed: 2357 -- Existe 1 GPU Tesla V100-SXM2-16GB disponível.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knK0W87YuYcQ",
        "outputId": "813ea63c-3bf9-4df8-eba4-35dd7553aef5"
      },
      "source": [
        "# =============== TRAIN =============== #\n",
        "# === Perguntas, Contexto e Labels  === #\n",
        "# ===================================== #\n",
        " \n",
        "path_train = 'train/train_v1.0.json'\n",
        " \n",
        "#----------------- Contextos (OCRs) ---------------#\n",
        "with open(path_train, 'rb') as handle:\n",
        "    dataset = json.loads(handle.read())\n",
        " \n",
        "context_train = []\n",
        "for i, d in enumerate(dataset['data']):\n",
        "    ocr_file = d['image'].replace('documents', 'ocr_results').replace('.png', '.json')\n",
        "    with open('train/'+ ocr_file, 'rb') as f:\n",
        "        ocr = json.loads(f.read())\n",
        " \n",
        "    lines = ocr['recognitionResults'][0]['lines']\n",
        " \n",
        "    text = ' '.join([w['text'] for l in lines for w in l['words']])\n",
        "    context_train.append(lines)\n",
        " \n",
        "context_train_samples = []\n",
        "for i, doc in enumerate(context_train):\n",
        "    test_list_item = []\n",
        "    for item in doc:\n",
        "        test_list_item.append(item['text'])\n",
        "    context_train_samples.append(' '.join(test_list_item).lower())\n",
        " \n",
        "#---------------- Perguntas ---------------------#\n",
        "questions_train_samples = []\n",
        "for d in dataset['data']:\n",
        "    questions_train_samples.append(d['question'].lower())\n",
        " \n",
        "#---------------- Respostas ---------------------#\n",
        "answers_train_samples = []\n",
        "for d in dataset['data']:\n",
        "    answers_train_samples.append(d['answers'][0].lower())\n",
        " \n",
        "Question_train, Context_train, Answer_train = [], [], []\n",
        "for q,c,a in zip(questions_train_samples, context_train_samples, answers_train_samples):\n",
        "    if a in c:\n",
        "        Question_train.append(q)\n",
        "        Context_train.append(c)\n",
        "        Answer_train.append(a)\n",
        " \n",
        "print(len(Question_train), len(Context_train), len(Answer_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32187 32187 32187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BJqvM3RwEqN",
        "outputId": "600bfc60-4dbb-48bb-f6f4-884fd68f9eea"
      },
      "source": [
        "# ================ VAL ================ #\n",
        "# === Perguntas, Contexto e Labels  === #\n",
        "# ===================================== #\n",
        " \n",
        "path_val = 'val/val_v1.0.json'\n",
        " \n",
        "#----------------- Contextos (OCRs) ---------------#\n",
        "with open(path_val, 'rb') as handle:\n",
        "    dataset = json.loads(handle.read())\n",
        " \n",
        "context_val = []\n",
        "for i, d in enumerate(dataset['data']):\n",
        "    ocr_file = d['image'].replace('documents', 'ocr_results').replace('.png', '.json')\n",
        "    with open('val/'+ ocr_file, 'rb') as f:\n",
        "        ocr = json.loads(f.read())\n",
        " \n",
        "    lines = ocr['recognitionResults'][0]['lines']\n",
        " \n",
        "    text = ' '.join([w['text'] for l in lines for w in l['words']])\n",
        "    context_val.append(lines)\n",
        " \n",
        "context_val_samples = []\n",
        "for i, doc in enumerate(context_val):\n",
        "    test_list_item = []\n",
        "    for item in doc:\n",
        "        test_list_item.append(item['text'])\n",
        "    context_val_samples.append(' '.join(test_list_item).lower())\n",
        " \n",
        "#---------------- Perguntas ---------------------#\n",
        "questions_val_samples = []\n",
        "for d in dataset['data']:\n",
        "    questions_val_samples.append(d['question'].lower())\n",
        " \n",
        "#---------------- Respostas ---------------------#\n",
        "answers_val_samples = []\n",
        "for d in dataset['data']:\n",
        "    answers_val_samples.append(d['answers'][0].lower())\n",
        " \n",
        "Question_val, Context_val, Answer_val = [], [], []\n",
        "for q,c,a in zip(questions_val_samples, context_val_samples, answers_val_samples):\n",
        "    if a in c:\n",
        "        Question_val.append(q)\n",
        "        Context_val.append(c)\n",
        "        Answer_val.append(a)\n",
        " \n",
        "print(len(Question_val), len(Context_val), len(Answer_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4408 4408 4408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "fX51ktCC05CE",
        "outputId": "892dc0e2-acaa-45e8-8881-63cb1bddc20e"
      },
      "source": [
        "# ============================= #\n",
        "# === Construção do Dataset === #\n",
        "# ============================= #\n",
        "\n",
        "MAX_LEN = 450\n",
        "def get_final_data(questions, context, answers, max_len=MAX_LEN):\n",
        "    data = pd.DataFrame([])\n",
        "    for quest, cont, ans in zip(questions, context, answers):\n",
        "        data = data.append(\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    'Question': quest,\n",
        "                    'Context': cont,\n",
        "                    'Answer': ans\n",
        "                }, index=[0]), ignore_index=True)\n",
        "    \n",
        "    start_position_label, end_position_label = [], []\n",
        "    for i, (c, l) in enumerate(zip(\n",
        "        data.Context.to_list(), \n",
        "        data.Answer.to_list()\n",
        "        )):\n",
        "        start_index = c.find(l)\n",
        "        end_index = start_index + len(l)\n",
        "        start_position_label.append((start_index))\n",
        "        end_position_label.append((end_index))\n",
        "        if start_index == -1: # se não existir o label no contexto está errado\n",
        "            print(f'PROBLEMA NA LINHA {i}')\n",
        "            break\n",
        "    data = data.assign(start_pos_label = start_position_label)\n",
        "    data = data.assign(end_pos_label = end_position_label)\n",
        "    data = data[data.end_pos_label <= max_len]\n",
        " \n",
        "    return data \n",
        " \n",
        "# #-------------------------------------\n",
        "df_qa_val   = get_final_data(Question_val,   Context_val,   Answer_val)\n",
        "df_qa_train = get_final_data(Question_train, Context_train, Answer_train)    \n",
        " \n",
        "df_qa_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Context</th>\n",
              "      <th>Answer</th>\n",
              "      <th>start_pos_label</th>\n",
              "      <th>end_pos_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what is the contact person name mentioned in l...</td>\n",
              "      <td>confidential .. .. rjrt pr approval date : 1/8...</td>\n",
              "      <td>p. carter</td>\n",
              "      <td>119</td>\n",
              "      <td>128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>which corporation's letterhead is this?</td>\n",
              "      <td>b&amp;w brown &amp; williamson tobacco corporation res...</td>\n",
              "      <td>brown &amp; williamson tobacco corporation</td>\n",
              "      <td>4</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>who is in  cc in this letter?</td>\n",
              "      <td>b&amp;w brown &amp; williamson tobacco corporation res...</td>\n",
              "      <td>t.f. riehl</td>\n",
              "      <td>122</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what is the subject of  this letter?</td>\n",
              "      <td>b&amp;w brown &amp; williamson tobacco corporation res...</td>\n",
              "      <td>review of existing brainstorming ideas/483</td>\n",
              "      <td>177</td>\n",
              "      <td>219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>what is the date in the letter</td>\n",
              "      <td>philip morris u. s. a. inter-office correspond...</td>\n",
              "      <td>june 11, 1990</td>\n",
              "      <td>98</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32175</th>\n",
              "      <td>which has more calcium milk or nutramigen?</td>\n",
              "      <td>comparison of three cups of vitamin d milk and...</td>\n",
              "      <td>milk</td>\n",
              "      <td>38</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32176</th>\n",
              "      <td>what is the title?</td>\n",
              "      <td>for professional use the low phenylalanine die...</td>\n",
              "      <td>the low phenylalanine diet</td>\n",
              "      <td>21</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32178</th>\n",
              "      <td>what is the department name mentioned?</td>\n",
              "      <td>for professional use the low phenylalanine die...</td>\n",
              "      <td>department of public health</td>\n",
              "      <td>68</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32180</th>\n",
              "      <td>what is the disease mentioned in the first par...</td>\n",
              "      <td>if we are to make further inroads on the enorm...</td>\n",
              "      <td>cardiovascular disease</td>\n",
              "      <td>112</td>\n",
              "      <td>134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32183</th>\n",
              "      <td>what is the table number ?</td>\n",
              "      <td>table 4-a relative risk of falling into extrem...</td>\n",
              "      <td>4-a</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>23208 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Question  ... end_pos_label\n",
              "0      what is the contact person name mentioned in l...  ...           128\n",
              "1                which corporation's letterhead is this?  ...            42\n",
              "2                          who is in  cc in this letter?  ...           132\n",
              "3                   what is the subject of  this letter?  ...           219\n",
              "6                         what is the date in the letter  ...           111\n",
              "...                                                  ...  ...           ...\n",
              "32175         which has more calcium milk or nutramigen?  ...            42\n",
              "32176                                 what is the title?  ...            47\n",
              "32178             what is the department name mentioned?  ...            95\n",
              "32180  what is the disease mentioned in the first par...  ...           134\n",
              "32183                         what is the table number ?  ...             9\n",
              "\n",
              "[23208 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83siRCglAr2",
        "outputId": "409322c0-a9a5-4380-dfa4-1dc38caecbb2"
      },
      "source": [
        "# ==================== #\n",
        "# === Fç Sentinela === #\n",
        "# ==================== #\n",
        "\n",
        "def sentinela_mask(contexto, label, start_pos_label, end_pos_label):\n",
        "    \"\"\"\n",
        "    Para nos ajudar a determinar quais tokens de BERT correspondem à resposta,\n",
        "    substituiremos a resposta por \"[MASK] [MASK] [MASK]\" (com base em\n",
        "    o número de tokens na resposta)\n",
        "    \"\"\"\n",
        "    \n",
        "    start_char_i = start_pos_label # inicio da resposta\n",
        "    end_char_i = end_pos_label     # fim da resposta\n",
        " \n",
        "    # Tokenize a resposta - ela pode ser dividida em várias palavras e / ou subpalavras\n",
        "    answer_tokens = tokenizer.tokenize(label)\n",
        "    \n",
        "    # Cria a string sentinela com \"[MASK] [MASK] [MASK]\"\n",
        "    sentinel_str = ' '.join(['[MASK]']*len(answer_tokens))\n",
        " \n",
        "    # Para fazer a substituição, usamos  slice com concatenação de strings\n",
        "    context_w_sentinel = contexto[:start_char_i] + sentinel_str + contexto[end_char_i:]\n",
        " \n",
        "    return answer_tokens, context_w_sentinel \n",
        " \n",
        "# ============================= #\n",
        "# === Função de tokenização === #\n",
        "# ============================= #\n",
        "\n",
        "def tokenize_data(question_list, context_list, answer_list, start_pos_label_list_val, end_pos_label_list_val, max_len=MAX_LEN):\n",
        " \n",
        "    all_input_ids, attention_masks, segment_ids, start_positions = [], [], [], []\n",
        "    end_positions, all_questions, all_answers, all_contexts = [], [], [], []\n",
        "    num_dropped = 0\n",
        " \n",
        "    for i, (question, answer, start_label, end_label, context) in enumerate(\n",
        "        zip(question_list, answer_list, start_pos_label_list_val, end_pos_label_list_val, context_list)):\n",
        " \n",
        "        answer_tokens, context_w_sentinel = sentinela_mask(context, answer, start_label, end_label)\n",
        " \n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            question, \n",
        "            context_w_sentinel,\n",
        "            add_special_tokens = True,    # Add '[CLS]' e '[SEP]'\n",
        "            max_length = max_len,         # Pad & trunca todas as sentenças\n",
        "            pad_to_max_length = True,\n",
        "            padding='max_length',\n",
        "            truncation = True,\n",
        "            return_attention_mask = True, # Constrói as attention masks\n",
        "            return_tensors = 'pt',        # Retorna tensores de PyTorch\n",
        "        )\n",
        " \n",
        "        input_ids = encoded_dict['input_ids']\n",
        " \n",
        "        is_mask_token = (input_ids[0] == tokenizer.mask_token_id)\n",
        " \n",
        "        mask_token_indeces = is_mask_token.nonzero(as_tuple=False)[:, 0]\n",
        " \n",
        "        # Se o número de tokens MASK não é igual ao número de tokens de resposta, \n",
        "        # então a amostra foi perdida devido ao truncamento\n",
        "        if not len(mask_token_indeces) == len(answer_tokens):\n",
        "            \n",
        "            # Calcule o número de amostras com esse problema\n",
        "            num_dropped += 1\n",
        "            continue\n",
        " \n",
        "        # anexa a pergunta, contexto e resposta\n",
        "        all_questions.append(question)\n",
        "        all_contexts.append(context)\n",
        "        all_answers.append(answer)\n",
        " \n",
        "        # `mask_token_indeces` é o intervalo de índices (por exemplo, [68, 69, 70, 71]),\n",
        "        #  mas nós realmente queremos apenas os índices de início e fim (por exemplo, 68 e 71).\n",
        "        start_index = mask_token_indeces[0]\n",
        "        end_index = mask_token_indeces[-1]\n",
        "        \n",
        "        # Codifica os tokens de resposta (para ids de token).\n",
        "        answer_token_ids = tokenizer.encode(answer_tokens, \n",
        "                                            add_special_tokens=False, \n",
        "                                            return_tensors='pt')\n",
        " \n",
        "        # Restaure a resposta dentro do texto de referência. (Substitua os tokens `[MASK]`\n",
        "        # com os tokens de resposta)\n",
        "        input_ids[0, start_index : end_index + 1] = answer_token_ids\n",
        " \n",
        "        # Adicione a frase codificada à lista\n",
        "        all_input_ids.append(input_ids)\n",
        " \n",
        "        # E sua máscara de atenção (simplesmente diferencia o padding do não padding)\n",
        "        attention_masks.append(encoded_dict['attention_mask'])    \n",
        " \n",
        "        # Armazene os IDs de segmento, que indicam quais tokens pertencem à pergunta\n",
        "        # vs. o contexto\n",
        "        segment_ids.append(encoded_dict['token_type_ids'])\n",
        " \n",
        "        # Armazene os índices de início e fim da resposta correta\n",
        "        start_positions.append(start_index)\n",
        "        end_positions.append(end_index)\n",
        " \n",
        "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    segment_ids = torch.cat(segment_ids, dim=0)\n",
        " \n",
        "    start_positions = torch.tensor(start_positions)\n",
        "    end_positions = torch.tensor(end_positions)\n",
        " \n",
        "    assert len(all_questions) == len(all_contexts) == len(all_answers)\n",
        "    sentence_idx = np.linspace(0,len(all_questions), len(all_questions), False, dtype=np.int16)\n",
        "    torch_idx = torch.tensor(sentence_idx)\n",
        " \n",
        "    return torch_idx, all_questions, all_contexts, all_answers, all_input_ids, attention_masks, segment_ids, start_positions, end_positions\n",
        " \n",
        "#----------------------------------------------------------------------------------\n",
        "question_list_val = df_qa_val.Question.to_list()\n",
        "context_list_val = df_qa_val.Context.to_list()\n",
        "answer_list_val = df_qa_val.Answer.to_list()\n",
        "start_pos_label_list_val = df_qa_val.start_pos_label.to_list()\n",
        "end_pos_label_list_val = df_qa_val.end_pos_label.to_list()\n",
        " \n",
        "torch_idx, all_questions, all_contexts, all_answers, all_input_ids, attention_masks, segment_ids, start_positions, end_positions = \\\n",
        "tokenize_data(question_list_val, context_list_val, answer_list_val, start_pos_label_list_val, end_pos_label_list_val, max_len=MAX_LEN)\n",
        " \n",
        "print(f'Número de exemplos após o processamento: {len(all_input_ids)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Número de exemplos após o processamento: 3033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgaQ4xdDz0t_"
      },
      "source": [
        "## Inspeção de um exemplo "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U0N4w8Dz4K2",
        "outputId": "3b2ac3d7-d622-428f-cc74-2b3906b5ee4c"
      },
      "source": [
        "IDX = 4\n",
        "print('QUESTION:             ', all_questions[IDX])\n",
        "print('CONTEXT:              ', all_contexts[IDX])\n",
        "print('ANSWER:               ', all_answers[IDX]) \n",
        "print()\n",
        "print(f'START_WORD:            {answer_list_val[IDX].split()[0]}')\n",
        "print(f'END_WORD:              {answer_list_val[IDX].split()[-1]}')\n",
        "print(f'DECODED_START_TOK_POS: {start_positions[IDX]}  -> WORD: {tokenizer.decode(all_input_ids[IDX][start_positions[IDX].item()])}')\n",
        "print(f'DECODED_END_TOK_POS:   {end_positions[IDX]} -> WORD: {tokenizer.decode(all_input_ids[IDX][end_positions[IDX].item()])}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QUESTION:              what is the name of foundation?\n",
            "CONTEXT:               the robert a. welch foundation 2010 bank of the southwest building houston, texas 77002 et request summary year (as applicable) may 1, 19 60 may 1, 19 may 1, 19 total through through through apt. 30, 1957 app. 30, 19 apr. 30, 19 1. personnel $11, 228.00 2. permanent scientific equipment .new $ 3. expendable scientific items & services . $_ 840.00 $ 840.00 4. other expense . 97500 . $ $.... 975.00. 5. total exclusive of overhead $13043.005 $13 043 00 6. overhead. $1,95.2.00 7. total amount of proposed budget .. ... $ 15,000.00 $15000 .00 name(s) of principal investigators) john b. kilpatrick institution william marsh rice university. the space below is for use by the foundation. grant period: 19 grant no. . date approved not approved director of research scientific advisory board board of trustees .. museum grantee and institution notified remarks: source: https://www.industrydocuments.ucsf.edu/docs/zxfk0226\n",
            "ANSWER:                the robert a. welch foundation\n",
            "\n",
            "START_WORD:            the\n",
            "END_WORD:              foundation\n",
            "DECODED_START_TOK_POS: 9  -> WORD: t h e\n",
            "DECODED_END_TOK_POS:   14 -> WORD: f o u n d a t i o n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcKn9TwT-cIG"
      },
      "source": [
        "# ================ #\n",
        "# === Datasets === #\n",
        "# ================ #\n",
        "\n",
        "#--------------------------------------- Train and Debug\n",
        "question_list_train = df_qa_train.Question.to_list()\n",
        "context_list_train = df_qa_train.Context.to_list()\n",
        "answer_list_train = df_qa_train.Answer.to_list()\n",
        "start_pos_label_list_train = df_qa_train.start_pos_label.to_list()\n",
        "end_pos_label_list_train = df_qa_train.end_pos_label.to_list()\n",
        " \n",
        "torch_idx_train, all_questions_train, all_contexts_train, all_answers_train, all_input_ids_train, attention_masks_train, segment_ids_train, start_positions_train, end_positions_train = \\\n",
        "tokenize_data(question_list_train, context_list_train, answer_list_train, start_pos_label_list_train, end_pos_label_list_train, max_len=MAX_LEN)\n",
        " \n",
        "ds_train = TensorDataset(\n",
        "    torch_idx_train,\n",
        "    all_input_ids_train, \n",
        "    attention_masks_train, \n",
        "    segment_ids_train, \n",
        "    start_positions_train, \n",
        "    end_positions_train\n",
        "    )\n",
        " \n",
        "#--------------------------------------- Val\n",
        "question_list_val = df_qa_val.Question.to_list()\n",
        "context_list_val = df_qa_val.Context.to_list()\n",
        "answer_list_val = df_qa_val.Answer.to_list()\n",
        "start_pos_label_list_val = df_qa_val.start_pos_label.to_list()\n",
        "end_pos_label_list_val = df_qa_val.end_pos_label.to_list()\n",
        "\n",
        "torch_idx_val, all_questions_val, all_contexts_val, all_answers_val, all_input_ids_val, attention_masks_val, segment_ids_val, start_positions_val, end_positions_val = \\\n",
        "tokenize_data(question_list_val, context_list_val, answer_list_val, start_pos_label_list_val, end_pos_label_list_val, max_len=MAX_LEN)\n",
        " \n",
        "ds_val = TensorDataset(\n",
        "    torch_idx_val,\n",
        "    all_input_ids_val, \n",
        "    attention_masks_val, \n",
        "    segment_ids_val, \n",
        "    start_positions_val, \n",
        "    end_positions_val\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO_zfRYZ8_3A",
        "outputId": "1a66808c-ac79-446d-e08e-0eeab433905f"
      },
      "source": [
        "# =================== #\n",
        "# === Dataloaders === #\n",
        "# =================== #\n",
        "\n",
        "BATCH_SZ = 6\n",
        " \n",
        "# dataloaders\n",
        "dataloaders = {\n",
        "     'train': DataLoader(\n",
        "         ds_train,\n",
        "         batch_size=BATCH_SZ,\n",
        "         shuffle=True,\n",
        "         num_workers=os.cpu_count(),\n",
        "         pin_memory=True\n",
        "         ),\n",
        "     'val': DataLoader(\n",
        "         ds_val,\n",
        "         batch_size=BATCH_SZ,\n",
        "         num_workers=os.cpu_count(),\n",
        "         pin_memory=True\n",
        "         ),\n",
        "     }\n",
        " \n",
        "# teste de sanidade\n",
        "_ = {x: len(dataloaders[x]) for x in dataloaders.keys()}\n",
        "_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': 3868, 'val': 506}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcDpfio4AEq8",
        "outputId": "baaf090b-9ad8-45f0-f009-3c51872a4cf1"
      },
      "source": [
        "# =========================== #\n",
        "# === Teste do Dataloader === #\n",
        "# =========================== #\n",
        " \n",
        "torch_idx, input_ids, attention_masks, segment_ids, start_positions, end_positions = next(iter(dataloaders['val']))\n",
        " \n",
        "print('torch_idx.shape:       ', torch_idx.shape)\n",
        "print('input_ids.shape:       ', input_ids.shape)\n",
        "print('attention_masks.shape: ', attention_masks.shape)\n",
        "print('segment_ids.shape:     ', segment_ids.shape)\n",
        "print('start_positions.shape: ', start_positions.shape)\n",
        "print('end_positions.shape:   ', end_positions.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch_idx.shape:        torch.Size([6])\n",
            "input_ids.shape:        torch.Size([6, 450])\n",
            "attention_masks.shape:  torch.Size([6, 450])\n",
            "segment_ids.shape:      torch.Size([6, 450])\n",
            "start_positions.shape:  torch.Size([6])\n",
            "end_positions.shape:    torch.Size([6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEISWYWoyan5"
      },
      "source": [
        "# =============================== #\n",
        "# === Funções de Treino e Val === #\n",
        "# =============================== #\n",
        "\n",
        "def train_model(model, device, train_loader, optimizer, scheduler):\n",
        "    num_batches = len(train_loader)    \n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        " \n",
        "    for step, batch in enumerate(train_loader):\n",
        "        batch_device = tuple(t.to(device) if type(t) != list else t for t in batch)\n",
        "        b_torch_idx, b_input_ids, b_input_mask, b_seg_ids, answer_true_start, answer_true_end = batch_device\n",
        "        \n",
        "        model.zero_grad()        \n",
        "        \n",
        "        outputs = model(\n",
        "            b_input_ids, \n",
        "            attention_mask=b_input_mask, \n",
        "            token_type_ids = b_seg_ids,\n",
        "            start_positions=answer_true_start,\n",
        "            end_positions=answer_true_end\n",
        "            )\n",
        " \n",
        "        loss = outputs['loss']\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / num_batches            \n",
        " \n",
        "    return avg_train_loss\n",
        " \n",
        "def answer_question(model, question, answer_text, max_len=MAX_LEN):\n",
        "    '''\n",
        "    Toma uma string `pergunta` e uma string `texto que contém a resposta`, e \n",
        "    identifica as palavras dentro do `answer_text` que são a resposta. \n",
        "    '''\n",
        "    input_ids = tokenizer.encode(\n",
        "        question, \n",
        "        answer_text,\n",
        "        max_length=max_len, \n",
        "        truncation=True,\n",
        "        )\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "    num_seg_a = sep_index + 1\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        " \n",
        "    assert len(segment_ids) == len(input_ids)\n",
        " \n",
        "    model.eval()\n",
        "    with torch.no_grad():        \n",
        "        outputs = model(\n",
        "            torch.tensor([input_ids]).to(device), \n",
        "            token_type_ids=torch.tensor([segment_ids]).to(device))\n",
        "    \n",
        "    start_logits = outputs['start_logits']\n",
        "    end_logits = outputs['end_logits']\n",
        "    answer_start = torch.argmax(start_logits)\n",
        "    answer_end = torch.argmax(end_logits)\n",
        " \n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer = tokens[answer_start]\n",
        " \n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # Se é um token de subword, então recombina com o token anterior.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Se não, adiciona um espaço ao token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "    return answer\n",
        " \n",
        "def get_score(model, questions, contexts, answers, answer_question, compute_f1, compute_exact):\n",
        "    size = len(answers)\n",
        "    f1_result, exact_result = 0,0\n",
        "    for index in range(size):\n",
        "        pred = answer_question(model, questions[index], contexts[index])\n",
        "        true = answers[index]\n",
        "        f1_result += compute_f1(true, pred)\n",
        "        exact_result += compute_exact(true, pred)\n",
        "    return f1_result/size, exact_result/size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMvu147iBnn4"
      },
      "source": [
        "# ================ #\n",
        "# === Métricas === #\n",
        "# ================ #\n",
        " \n",
        "def normalize_answer(s):\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        " \n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        " \n",
        "    return white_space_fix(lower(s))\n",
        " \n",
        "def get_tokens(s):\n",
        "    if not s: return []\n",
        "    return normalize_answer(s).split()\n",
        " \n",
        "def compute_exact(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        " \n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSb3SiHJrwJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa70c45-3c1e-4fdc-8056-eebed23fdc31"
      },
      "source": [
        "# ====================== #\n",
        "# === Loop de Treino === #\n",
        "# ====================== #\n",
        "\n",
        "deterministic()\n",
        "N_EPOCHS = 30\n",
        " \n",
        "try:\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "except:\n",
        "    pass\n",
        " \n",
        "path_save_model = '/content/drive/MyDrive/Colab Notebooks/Final-project/saved_epochs/'\n",
        "\n",
        "model = MobileBertForQuestionAnswering.from_pretrained(model_checkpoint, return_dict=True).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "total_steps = (len(dataloaders['train']) * N_EPOCHS)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps=total_steps//10, \n",
        "    num_training_steps=total_steps\n",
        "    )\n",
        "#----------------------------------------------------------------\n",
        "\n",
        "training_stats = []\n",
        "for epoch_i in range(1, N_EPOCHS+1):\n",
        "    loss_train = train_model(model, device, dataloaders['train'], optimizer, scheduler)\n",
        "    print(f'\\nEpoca [{epoch_i}/{N_EPOCHS}]: Loss Train: {loss_train:.3f}')\n",
        "    \n",
        "    f1_result, exact_result = get_score(model, all_questions_val, all_contexts_val, all_answers_val, \n",
        "                                        answer_question, compute_f1, compute_exact)  \n",
        "    print(f'              Exact Match: {exact_result:.4f} -- F1: {f1_result:.4}')\n",
        "    \n",
        "    # saving\n",
        "    torch.save(model.state_dict(), path_save_model+'MOBILe'+str(epoch_i))\n",
        " \n",
        "    training_stats.append({'epoch': epoch_i, 'Training Loss': loss_train, 'Exact': exact_result, 'F1': f1_result})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experimento deterministico, seed: 2357 -- Existe 1 GPU Tesla V100-SXM2-16GB disponível.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing MobileBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MobileBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of MobileBertForQuestionAnswering were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoca [1/30]: Loss Train: 4.297\n",
            "              Exact Match: 0.1408 -- F1: 0.2532\n",
            "\n",
            "Epoca [2/30]: Loss Train: 1.920\n",
            "              Exact Match: 0.2460 -- F1: 0.3715\n",
            "\n",
            "Epoca [3/30]: Loss Train: 1.283\n",
            "              Exact Match: 0.3119 -- F1: 0.4357\n",
            "\n",
            "Epoca [4/30]: Loss Train: 0.919\n",
            "              Exact Match: 0.3330 -- F1: 0.464\n",
            "\n",
            "Epoca [5/30]: Loss Train: 0.691\n",
            "              Exact Match: 0.3511 -- F1: 0.4829\n",
            "\n",
            "Epoca [6/30]: Loss Train: 0.519\n",
            "              Exact Match: 0.3488 -- F1: 0.4764\n",
            "\n",
            "Epoca [7/30]: Loss Train: 0.403\n",
            "              Exact Match: 0.3521 -- F1: 0.4774\n",
            "\n",
            "Epoca [8/30]: Loss Train: 0.329\n",
            "              Exact Match: 0.3673 -- F1: 0.4922\n",
            "\n",
            "Epoca [9/30]: Loss Train: 0.266\n",
            "              Exact Match: 0.3518 -- F1: 0.4792\n",
            "\n",
            "Epoca [10/30]: Loss Train: 0.229\n",
            "              Exact Match: 0.3643 -- F1: 0.4927\n",
            "\n",
            "Epoca [11/30]: Loss Train: 0.193\n",
            "              Exact Match: 0.3696 -- F1: 0.4913\n",
            "\n",
            "Epoca [12/30]: Loss Train: 0.170\n",
            "              Exact Match: 0.3577 -- F1: 0.4848\n",
            "\n",
            "Epoca [13/30]: Loss Train: 0.144\n",
            "              Exact Match: 0.3594 -- F1: 0.4791\n",
            "\n",
            "Epoca [14/30]: Loss Train: 0.128\n",
            "              Exact Match: 0.3558 -- F1: 0.4829\n",
            "\n",
            "Epoca [15/30]: Loss Train: 0.113\n",
            "              Exact Match: 0.3699 -- F1: 0.494\n",
            "\n",
            "Epoca [16/30]: Loss Train: 0.099\n",
            "              Exact Match: 0.3752 -- F1: 0.496\n",
            "\n",
            "Epoca [17/30]: Loss Train: 0.086\n",
            "              Exact Match: 0.3640 -- F1: 0.4868\n",
            "\n",
            "Epoca [18/30]: Loss Train: 0.081\n",
            "              Exact Match: 0.3650 -- F1: 0.4885\n",
            "\n",
            "Epoca [19/30]: Loss Train: 0.065\n",
            "              Exact Match: 0.3630 -- F1: 0.4881\n",
            "\n",
            "Epoca [20/30]: Loss Train: 0.059\n",
            "              Exact Match: 0.3594 -- F1: 0.4879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuOEL7zoDdTJ"
      },
      "source": [
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "pd.set_option('precision', 2)\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpwgaOnPEUHT"
      },
      "source": [
        "sns.set(style='darkgrid')\n",
        "\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "plt.title(\"Training & Validation loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}