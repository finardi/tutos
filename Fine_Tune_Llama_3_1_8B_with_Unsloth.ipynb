{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/tutos/blob/master/Fine_Tune_Llama_3_1_8B_with_Unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTaDCGTe78bK"
      },
      "source": [
        "# Fine-tune Llama 3.1 8B with Unsloth\n",
        "> üó£Ô∏è [Large Language Model Course](https://github.com/mlabonne/llm-course)\n",
        "\n",
        "‚ù§Ô∏è Created by [@maximelabonne](https://twitter.com/maximelabonne)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoPKQjga6obN"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" --progress-bar off\n",
        "!pip install -qqq --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes --progress-bar off\n",
        "\n",
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "matKaF-f-GiU"
      },
      "source": [
        "## 1. Load model for PEFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGX9wG7Lhc-z"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "max_seq_length = 2048\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")\n",
        "\n",
        "# Prepare model for PEFT\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")\n",
        "print(model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare data and tokenizer"
      ],
      "metadata": {
        "id": "hjDpwfjJ3RAL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqGnvaT8is-R"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"chatml\",\n",
        "    mapping={\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}\n",
        ")\n",
        "\n",
        "def apply_template(examples):\n",
        "    messages = examples[\"conversations\"]\n",
        "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "dataset = dataset.map(apply_template, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training"
      ],
      "metadata": {
        "id": "zdfjufQd3XMi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcPAQihcjcfl"
      },
      "outputs": [],
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        num_train_epochs=1,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Inference"
      ],
      "metadata": {
        "id": "CI_U9FHZ3ZLO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JXdjsLqkvZY"
      },
      "outputs": [],
      "source": [
        "# Load model for inference\n",
        "model = FastLanguageModel.for_inference(model)\n",
        "\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Save trained model"
      ],
      "metadata": {
        "id": "HunPZjPp3aWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORa-rPvGmT9p"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\n",
        "model.push_to_hub_merged(\"mlabonne/FineLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzcUOyksmgWH"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_gguf(\"model\", tokenizer, \"q8_0\")\n",
        "quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\n",
        "for quant in quant_methods:\n",
        "    model.push_to_hub_gguf(\"mlabonne/FineLlama-3.1-8B-GGUF\", tokenizer, quant)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}